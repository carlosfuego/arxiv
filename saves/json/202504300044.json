[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2405.12747v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v3",
                "updated": "2025-04-28T17:17:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    17,
                    53,
                    0,
                    118,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay\n  using Combinatorial t-Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay\n  using Combinatorial t-Designs"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "IEEE Internet of Things Journal (Accepted for publication). The\n  Hierarchical coded caching scheme in this updated version unifies the scheme\n  in the previous version and the schemes in arxiv:2402.07188. This version\n  includes a more comprehensive performance analysis. To reflect these the\n  title has been updated",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19984v1",
                "updated": "2025-04-28T16:59:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    59,
                    13,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T16:59:13Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    59,
                    13,
                    0,
                    118,
                    0
                ],
                "title": "3D MPSoC with On-Chip Cache Support -- Design and Exploitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D MPSoC with On-Chip Cache Support -- Design and Exploitation"
                },
                "summary": "The increasing density of transistors in Integrated Circuits (ICs) has\nenabled the development of highly integrated Systems-on-Chip (SoCs) and, more\nrecently, Multiprocessor Systems-on-Chip (MPSoCs). To address scalability\nchallenges in communication and memory performance, three-dimensional (3D)\nNetwork-on-Chip (NoC) architectures have emerged, offering improvements in\ncommunication latency and throughput. However, memory system efficiency remains\na critical bottleneck in NoC-based designs. This work proposes the design and\nexperimental exploration of 3D MPSoCs with on-chip cache support by employing\ndistinct communication infrastructures for inter-processor and memory\ninteractions. Specifically, packet-based NoCs are adopted for inter-processor\ncommunication, while a crossbar-based infrastructure supports a cache coherence\nhierarchy for memory access. A two-layer system architecture is introduced,\ncombining a Uniform Memory Access (UMA) model within clusters and a No Remote\nMemory Access (NORMA) model between clusters, aiming to balance scalability and\ncoherence requirements. Emerging memory technologies such as PCRAM and MRAM are\nexplored to optimize performance, energy consumption, and area usage.\nExperimental evaluations are conducted using the Gem5 simulator, targeting a\nmodel based on the ARM Versatile Express platform. The outcomes of this study\naim to enhance MPSoC scalability while meeting the stringent demands of\nmemory-centric applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing density of transistors in Integrated Circuits (ICs) has\nenabled the development of highly integrated Systems-on-Chip (SoCs) and, more\nrecently, Multiprocessor Systems-on-Chip (MPSoCs). To address scalability\nchallenges in communication and memory performance, three-dimensional (3D)\nNetwork-on-Chip (NoC) architectures have emerged, offering improvements in\ncommunication latency and throughput. However, memory system efficiency remains\na critical bottleneck in NoC-based designs. This work proposes the design and\nexperimental exploration of 3D MPSoCs with on-chip cache support by employing\ndistinct communication infrastructures for inter-processor and memory\ninteractions. Specifically, packet-based NoCs are adopted for inter-processor\ncommunication, while a crossbar-based infrastructure supports a cache coherence\nhierarchy for memory access. A two-layer system architecture is introduced,\ncombining a Uniform Memory Access (UMA) model within clusters and a No Remote\nMemory Access (NORMA) model between clusters, aiming to balance scalability and\ncoherence requirements. Emerging memory technologies such as PCRAM and MRAM are\nexplored to optimize performance, energy consumption, and area usage.\nExperimental evaluations are conducted using the Gem5 simulator, targeting a\nmodel based on the ARM Versatile Express platform. The outcomes of this study\naim to enhance MPSoC scalability while meeting the stringent demands of\nmemory-centric applications."
                },
                "authors": [
                    {
                        "name": "Rodrigo Cataldo"
                    },
                    {
                        "name": "Cesar Marcon"
                    },
                    {
                        "name": "Debora Matos"
                    }
                ],
                "author_detail": {
                    "name": "Debora Matos"
                },
                "author": "Debora Matos",
                "arxiv_comment": "Progress Seminar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19874v1",
                "updated": "2025-04-28T15:05:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    5,
                    35,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:05:35Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    5,
                    35,
                    0,
                    118,
                    0
                ],
                "title": "TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate"
                },
                "summary": "Vector quantization, a problem rooted in Shannon's source coding theory, aims\nto quantize high-dimensional Euclidean vectors while minimizing distortion in\ntheir geometric structure. We propose TurboQuant to address both mean-squared\nerror (MSE) and inner product distortion, overcoming limitations of existing\nmethods that fail to achieve optimal distortion rates. Our data-oblivious\nalgorithms, suitable for online applications, achieve near-optimal distortion\nrates (within a small constant factor) across all bit-widths and dimensions.\nTurboQuant achieves this by randomly rotating input vectors, inducing a\nconcentrated Beta distribution on coordinates, and leveraging the\nnear-independence property of distinct coordinates in high dimensions to simply\napply optimal scalar quantizers per each coordinate. Recognizing that\nMSE-optimal quantizers introduce bias in inner product estimation, we propose a\ntwo-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL\n(QJL) transform on the residual, resulting in an unbiased inner product\nquantizer. We also provide a formal proof of the information-theoretic lower\nbounds on best achievable distortion rate by any vector quantizer,\ndemonstrating that TurboQuant closely matches these bounds, differing only by a\nsmall constant ($\\approx 2.7$) factor. Experimental results validate our\ntheoretical findings, showing that for KV cache quantization, we achieve\nabsolute quality neutrality with 3.5 bits per channel and marginal quality\ndegradation with 2.5 bits per channel. Furthermore, in nearest neighbor search\ntasks, our method outperforms existing product quantization techniques in\nrecall while reducing indexing time to virtually zero.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector quantization, a problem rooted in Shannon's source coding theory, aims\nto quantize high-dimensional Euclidean vectors while minimizing distortion in\ntheir geometric structure. We propose TurboQuant to address both mean-squared\nerror (MSE) and inner product distortion, overcoming limitations of existing\nmethods that fail to achieve optimal distortion rates. Our data-oblivious\nalgorithms, suitable for online applications, achieve near-optimal distortion\nrates (within a small constant factor) across all bit-widths and dimensions.\nTurboQuant achieves this by randomly rotating input vectors, inducing a\nconcentrated Beta distribution on coordinates, and leveraging the\nnear-independence property of distinct coordinates in high dimensions to simply\napply optimal scalar quantizers per each coordinate. Recognizing that\nMSE-optimal quantizers introduce bias in inner product estimation, we propose a\ntwo-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL\n(QJL) transform on the residual, resulting in an unbiased inner product\nquantizer. We also provide a formal proof of the information-theoretic lower\nbounds on best achievable distortion rate by any vector quantizer,\ndemonstrating that TurboQuant closely matches these bounds, differing only by a\nsmall constant ($\\approx 2.7$) factor. Experimental results validate our\ntheoretical findings, showing that for KV cache quantization, we achieve\nabsolute quality neutrality with 3.5 bits per channel and marginal quality\ndegradation with 2.5 bits per channel. Furthermore, in nearest neighbor search\ntasks, our method outperforms existing product quantization techniques in\nrecall while reducing indexing time to virtually zero."
                },
                "authors": [
                    {
                        "name": "Amir Zandieh"
                    },
                    {
                        "name": "Majid Daliri"
                    },
                    {
                        "name": "Majid Hadian"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    }
                ],
                "author_detail": {
                    "name": "Vahab Mirrokni"
                },
                "author": "Vahab Mirrokni",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19867v1",
                "updated": "2025-04-28T15:00:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    0,
                    3,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:00:03Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    0,
                    3,
                    0,
                    118,
                    0
                ],
                "title": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\n  Computation and Unified Storage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\n  Computation and Unified Storage"
                },
                "summary": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models."
                },
                "authors": [
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Lufang Chen"
                    },
                    {
                        "name": "Zhong Wang"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Qiuli Mao"
                    },
                    {
                        "name": "Jianping Ma"
                    },
                    {
                        "name": "Chao Xiong"
                    },
                    {
                        "name": "Guanyu Wu"
                    },
                    {
                        "name": "Buhe Han"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yun Liang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "18 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19602v1",
                "updated": "2025-04-28T09:04:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    4,
                    30,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T09:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    4,
                    30,
                    0,
                    118,
                    0
                ],
                "title": "Soft-Label Caching and Sharpening for Communication-Efficient Federated\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft-Label Caching and Sharpening for Communication-Efficient Federated\n  Distillation"
                },
                "summary": "Federated Learning (FL) enables collaborative model training across\ndecentralized clients, enhancing privacy by keeping data local. Yet\nconventional FL, relying on frequent parameter-sharing, suffers from high\ncommunication overhead and limited model heterogeneity. Distillation-based FL\napproaches address these issues by sharing predictions (soft-labels) instead,\nbut they often involve redundant transmissions across communication rounds,\nreducing efficiency. We propose SCARLET, a novel framework integrating\nsynchronized soft-label caching and an enhanced Entropy Reduction Aggregation\n(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing\ncached soft-labels, achieving up to 50% reduction in communication costs\ncompared to existing methods while maintaining accuracy. Enhanced ERA can be\ntuned to adapt to non-IID data variations, ensuring robust aggregation and\nperformance in diverse client scenarios. Experimental evaluations demonstrate\nthat SCARLET consistently outperforms state-of-the-art distillation-based FL\nmethods in terms of accuracy and communication efficiency. The implementation\nof SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables collaborative model training across\ndecentralized clients, enhancing privacy by keeping data local. Yet\nconventional FL, relying on frequent parameter-sharing, suffers from high\ncommunication overhead and limited model heterogeneity. Distillation-based FL\napproaches address these issues by sharing predictions (soft-labels) instead,\nbut they often involve redundant transmissions across communication rounds,\nreducing efficiency. We propose SCARLET, a novel framework integrating\nsynchronized soft-label caching and an enhanced Entropy Reduction Aggregation\n(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing\ncached soft-labels, achieving up to 50% reduction in communication costs\ncompared to existing methods while maintaining accuracy. Enhanced ERA can be\ntuned to adapt to non-IID data variations, ensuring robust aggregation and\nperformance in diverse client scenarios. Experimental evaluations demonstrate\nthat SCARLET consistently outperforms state-of-the-art distillation-based FL\nmethods in terms of accuracy and communication efficiency. The implementation\nof SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET."
                },
                "authors": [
                    {
                        "name": "Kitsuya Azuma"
                    },
                    {
                        "name": "Takayuki Nishio"
                    },
                    {
                        "name": "Yuichi Kitagawa"
                    },
                    {
                        "name": "Wakako Nakano"
                    },
                    {
                        "name": "Takahito Tanimura"
                    }
                ],
                "author_detail": {
                    "name": "Takahito Tanimura"
                },
                "author": "Takahito Tanimura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19601v1",
                "updated": "2025-04-28T09:03:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    3,
                    45,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T09:03:45Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    3,
                    45,
                    0,
                    118,
                    0
                ],
                "title": "Characterizing the Optimal Memory-Rate Tradeoff in Secure Coded Caching\n  for Small Buffer or Small Rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing the Optimal Memory-Rate Tradeoff in Secure Coded Caching\n  for Small Buffer or Small Rate"
                },
                "summary": "We consider the secure coded caching problem proposed by Ravindrakumar et. al\nwhere no user can obtain information about files other than the one requested.\nWe first propose three new schemes for the three cases of cache size $M=1$,\n$N=2$ files and arbitrary $K$ users, delivery rate $ R=1$, arbitrary $N$ files\nand $K$ users, and the general case for arbitrary $N$ files and $K$ users,\nrespectively. Then we derive converse results by characterizing new properties\nof secure coded caching schemes. As a result, we characterize the two\nend-points of the optimal memory-rate tradeoff curve for arbitrary number of\nusers and files. Furthermore, for the case of $N=2$ files and arbitrary number\nof users, we also characterize a segment of the optimal memory-rate tradeoff\ncurve, where the cache size is relatively small.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the secure coded caching problem proposed by Ravindrakumar et. al\nwhere no user can obtain information about files other than the one requested.\nWe first propose three new schemes for the three cases of cache size $M=1$,\n$N=2$ files and arbitrary $K$ users, delivery rate $ R=1$, arbitrary $N$ files\nand $K$ users, and the general case for arbitrary $N$ files and $K$ users,\nrespectively. Then we derive converse results by characterizing new properties\nof secure coded caching schemes. As a result, we characterize the two\nend-points of the optimal memory-rate tradeoff curve for arbitrary number of\nusers and files. Furthermore, for the case of $N=2$ files and arbitrary number\nof users, we also characterize a segment of the optimal memory-rate tradeoff\ncurve, where the cache size is relatively small."
                },
                "authors": [
                    {
                        "name": "Han Fang"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Wei Kang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Kang"
                },
                "author": "Wei Kang",
                "arxiv_comment": "Submitted to IEEE Transactions on Information Theory",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19561v1",
                "updated": "2025-04-28T08:12:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    12,
                    30,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T08:12:30Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    12,
                    30,
                    0,
                    118,
                    0
                ],
                "title": "Quantifying Memory Utilization with Effective State-Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Memory Utilization with Effective State-Size"
                },
                "summary": "The need to develop a general framework for architecture analysis is becoming\nincreasingly important, given the expanding design space of sequence models. To\nthis end, we draw insights from classical signal processing and control theory,\nto develop a quantitative measure of \\textit{memory utilization}: the internal\nmechanisms through which a model stores past information to produce future\noutputs. This metric, which we call \\textbf{\\textit{effective state-size}}\n(ESS), is tailored to the fundamental class of systems with\n\\textit{input-invariant} and \\textit{input-varying linear operators},\nencompassing a variety of computational units such as variants of attention,\nconvolutions, and recurrences. Unlike prior work on memory utilization, which\neither relies on raw operator visualizations (e.g. attention maps), or simply\nthe total \\textit{memory capacity} (i.e. cache size) of a model, our metrics\nprovide highly interpretable and actionable measurements. In particular, we\nshow how ESS can be leveraged to improve initialization strategies, inform\nnovel regularizers and advance the performance-efficiency frontier through\nmodel distillation. Furthermore, we demonstrate that the effect of context\ndelimiters (such as end-of-speech tokens) on ESS highlights cross-architectural\ndifferences in how large language models utilize their available memory to\nrecall information. Overall, we find that ESS provides valuable insights into\nthe dynamics that dictate memory utilization, enabling the design of more\nefficient and effective sequence models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need to develop a general framework for architecture analysis is becoming\nincreasingly important, given the expanding design space of sequence models. To\nthis end, we draw insights from classical signal processing and control theory,\nto develop a quantitative measure of \\textit{memory utilization}: the internal\nmechanisms through which a model stores past information to produce future\noutputs. This metric, which we call \\textbf{\\textit{effective state-size}}\n(ESS), is tailored to the fundamental class of systems with\n\\textit{input-invariant} and \\textit{input-varying linear operators},\nencompassing a variety of computational units such as variants of attention,\nconvolutions, and recurrences. Unlike prior work on memory utilization, which\neither relies on raw operator visualizations (e.g. attention maps), or simply\nthe total \\textit{memory capacity} (i.e. cache size) of a model, our metrics\nprovide highly interpretable and actionable measurements. In particular, we\nshow how ESS can be leveraged to improve initialization strategies, inform\nnovel regularizers and advance the performance-efficiency frontier through\nmodel distillation. Furthermore, we demonstrate that the effect of context\ndelimiters (such as end-of-speech tokens) on ESS highlights cross-architectural\ndifferences in how large language models utilize their available memory to\nrecall information. Overall, we find that ESS provides valuable insights into\nthe dynamics that dictate memory utilization, enabling the design of more\nefficient and effective sequence models."
                },
                "authors": [
                    {
                        "name": "Rom N. Parnichkun"
                    },
                    {
                        "name": "Neehal Tumma"
                    },
                    {
                        "name": "Armin W. Thomas"
                    },
                    {
                        "name": "Alessandro Moro"
                    },
                    {
                        "name": "Qi An"
                    },
                    {
                        "name": "Taiji Suzuki"
                    },
                    {
                        "name": "Atsushi Yamashita"
                    },
                    {
                        "name": "Michael Poli"
                    },
                    {
                        "name": "Stefano Massaroli"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Massaroli"
                },
                "author": "Stefano Massaroli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19475v1",
                "updated": "2025-04-28T04:31:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T04:31:24Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "title": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video"
                },
                "summary": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field."
                },
                "authors": [
                    {
                        "name": "Sonia Joseph"
                    },
                    {
                        "name": "Praneet Suresh"
                    },
                    {
                        "name": "Lorenz Hufe"
                    },
                    {
                        "name": "Edward Stevinson"
                    },
                    {
                        "name": "Robert Graham"
                    },
                    {
                        "name": "Yash Vadi"
                    },
                    {
                        "name": "Danilo Bzdok"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    },
                    {
                        "name": "Lee Sharkey"
                    },
                    {
                        "name": "Blake Aaron Richards"
                    }
                ],
                "author_detail": {
                    "name": "Blake Aaron Richards"
                },
                "author": "Blake Aaron Richards",
                "arxiv_comment": "4 pages, 3 figures, 9 tables. Oral and Tutorial at the CVPR\n  Mechanistic Interpretability for Vision (MIV) Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18001v2",
                "updated": "2025-04-28T04:02:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    2,
                    30,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-25T01:10:49Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    1,
                    10,
                    49,
                    4,
                    115,
                    0
                ],
                "title": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data"
                },
                "summary": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc."
                },
                "authors": [
                    {
                        "name": "Daniel Zavorotny"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "arxiv_comment": "11 pages, 11 figures, EGPGV25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v3",
                "updated": "2025-04-28T02:58:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    2,
                    58,
                    27,
                    0,
                    118,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19365v1",
                "updated": "2025-04-27T22:05:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T22:05:14Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "title": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration"
                },
                "summary": "Graphics Processing Units (GPUs) have become essential for computationally\nintensive applications. However, emerging workloads such as recommender\nsystems, graph analytics, and data analytics often involve processing data\nexceeding GPU on-chip memory capacity. To mitigate this issue, existing\nsolutions enable GPUs to use CPU DRAM or SSDs as external memory. Among them,\nthe GPU-centric approach lets GPU threads directly initiate NVMe requests,\neliminating CPU intervention overhead over traditional methods. However, the\nSOTA GPU-centric approach adopts a synchronous IO model, and threads must\ntolerate the long latency in communication before starting any tasks.\n  In this work, we propose AGILE, a lightweight and efficient asynchronous\nlibrary allowing GPU threads to access SSDs asynchronously while eliminating\ndeadlock risks. AGILE also integrates a flexible software cache using GPU\nHigh-Bandwidth Mamory (HBM). We demonstrate that the asynchronous GPU-centric\nIO achieves up to 1.88$\\times$ improvement in workloads with different\ncomputation-to-communication (CTC) ratios. We also compare AGILE with the SOTA\nwork BaM on Deep Learning Recommendation Models (DLRM) with various settings,\nand the results show that AGILE achieves 1.75$\\times$ performance improvement\ndue to its efficient design and the overlapping strategy enabled by an\nasynchronous IO model. We further evaluate AGILE's API overhead on graph\napplications, and the results demonstrate AGILE reduces software cache overhead\nby up to 3.12$\\times$ and overhead in NVMe IO requests by up to 2.85$\\times$.\nCompared with BaM, AGILE consumes fewer registers and exhibits up to\n1.32$\\times$ reduction in the usage of registers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphics Processing Units (GPUs) have become essential for computationally\nintensive applications. However, emerging workloads such as recommender\nsystems, graph analytics, and data analytics often involve processing data\nexceeding GPU on-chip memory capacity. To mitigate this issue, existing\nsolutions enable GPUs to use CPU DRAM or SSDs as external memory. Among them,\nthe GPU-centric approach lets GPU threads directly initiate NVMe requests,\neliminating CPU intervention overhead over traditional methods. However, the\nSOTA GPU-centric approach adopts a synchronous IO model, and threads must\ntolerate the long latency in communication before starting any tasks.\n  In this work, we propose AGILE, a lightweight and efficient asynchronous\nlibrary allowing GPU threads to access SSDs asynchronously while eliminating\ndeadlock risks. AGILE also integrates a flexible software cache using GPU\nHigh-Bandwidth Mamory (HBM). We demonstrate that the asynchronous GPU-centric\nIO achieves up to 1.88$\\times$ improvement in workloads with different\ncomputation-to-communication (CTC) ratios. We also compare AGILE with the SOTA\nwork BaM on Deep Learning Recommendation Models (DLRM) with various settings,\nand the results show that AGILE achieves 1.75$\\times$ performance improvement\ndue to its efficient design and the overlapping strategy enabled by an\nasynchronous IO model. We further evaluate AGILE's API overhead on graph\napplications, and the results demonstrate AGILE reduces software cache overhead\nby up to 3.12$\\times$ and overhead in NVMe IO requests by up to 2.85$\\times$.\nCompared with BaM, AGILE consumes fewer registers and exhibits up to\n1.32$\\times$ reduction in the usage of registers."
                },
                "authors": [
                    {
                        "name": "Zhuoping Yang"
                    },
                    {
                        "name": "Jinming Zhuang"
                    },
                    {
                        "name": "Xingzhen Chen"
                    },
                    {
                        "name": "Alex K. Jones"
                    },
                    {
                        "name": "Peipei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Peipei Zhou"
                },
                "author": "Peipei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19266v1",
                "updated": "2025-04-27T14:46:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    46,
                    43,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T14:46:43Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    46,
                    43,
                    6,
                    117,
                    0
                ],
                "title": "OpenFusion++: An Open-vocabulary Real-time Scene Understanding System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenFusion++: An Open-vocabulary Real-time Scene Understanding System"
                },
                "summary": "Real-time open-vocabulary scene understanding is essential for efficient 3D\nperception in applications such as vision-language navigation, embodied\nintelligence, and augmented reality. However, existing methods suffer from\nimprecise instance segmentation, static semantic updates, and limited handling\nof complex queries. To address these issues, we present OpenFusion++, a\nTSDF-based real-time 3D semantic-geometric reconstruction system. Our approach\nrefines 3D point clouds by fusing confidence maps from foundational models,\ndynamically updates global semantic labels via an adaptive cache based on\ninstance area, and employs a dual-path encoding framework that integrates\nobject attributes with environmental context for precise query responses.\nExperiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate\nthat OpenFusion++ significantly outperforms the baseline in both semantic\naccuracy and query responsiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time open-vocabulary scene understanding is essential for efficient 3D\nperception in applications such as vision-language navigation, embodied\nintelligence, and augmented reality. However, existing methods suffer from\nimprecise instance segmentation, static semantic updates, and limited handling\nof complex queries. To address these issues, we present OpenFusion++, a\nTSDF-based real-time 3D semantic-geometric reconstruction system. Our approach\nrefines 3D point clouds by fusing confidence maps from foundational models,\ndynamically updates global semantic labels via an adaptive cache based on\ninstance area, and employs a dual-path encoding framework that integrates\nobject attributes with environmental context for precise query responses.\nExperiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate\nthat OpenFusion++ significantly outperforms the baseline in both semantic\naccuracy and query responsiveness."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Jin"
                    },
                    {
                        "name": "Matteo Frosi"
                    },
                    {
                        "name": "Matteo Matteucci"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Matteucci"
                },
                "author": "Matteo Matteucci",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45, 68U05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.4.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19191v1",
                "updated": "2025-04-27T10:48:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    10,
                    48,
                    56,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T10:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    10,
                    48,
                    56,
                    6,
                    117,
                    0
                ],
                "title": "WuNeng: Hybrid State with Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WuNeng: Hybrid State with Attention"
                },
                "summary": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures."
                },
                "authors": [
                    {
                        "name": "Liu Xiao"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Lin Yueyu"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yueyu"
                },
                "author": "Lin Yueyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10883v2",
                "updated": "2025-04-26T12:07:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    26,
                    12,
                    7,
                    35,
                    5,
                    116,
                    0
                ],
                "published": "2024-11-16T20:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "title": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs"
                },
                "summary": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack."
                },
                "authors": [
                    {
                        "name": "Cheng Gu"
                    },
                    {
                        "name": "Yicheng Zhang"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "arxiv_comment": "Accepted to IEEE S&P 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v3",
                "updated": "2025-04-25T19:40:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    19,
                    40,
                    54,
                    4,
                    115,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18434v1",
                "updated": "2025-04-25T15:45:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    45,
                    36,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:45:36Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    45,
                    36,
                    4,
                    115,
                    0
                ],
                "title": "Constructing Hamiltonian Decompositions of Complete $k$-Uniform\n  Hypergraphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing Hamiltonian Decompositions of Complete $k$-Uniform\n  Hypergraphs"
                },
                "summary": "Motivated by the wide-ranging applications of Hamiltonian decompositions in\ndistributed computing, coded caching, routing, resource allocation, load\nbalancing, and fault tolerance, our work presents a comprehensive design for\nHamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$.\nBuilding upon the resolution of the long-standing conjecture of the existence\nof Hamiltonian decompositions of complete hypergraphs, a problem that was\nresolved using existence-based methods, our contribution goes beyond the\nprevious explicit designs, which were confined to the specific cases of $k=2$\nand $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing\nfor a broad applicability of Hamiltonian decompositions in various settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the wide-ranging applications of Hamiltonian decompositions in\ndistributed computing, coded caching, routing, resource allocation, load\nbalancing, and fault tolerance, our work presents a comprehensive design for\nHamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$.\nBuilding upon the resolution of the long-standing conjecture of the existence\nof Hamiltonian decompositions of complete hypergraphs, a problem that was\nresolved using existence-based methods, our contribution goes beyond the\nprevious explicit designs, which were confined to the specific cases of $k=2$\nand $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing\nfor a broad applicability of Hamiltonian decompositions in various settings."
                },
                "authors": [
                    {
                        "name": "Javad Maheri"
                    },
                    {
                        "name": "Petros Elia"
                    }
                ],
                "author_detail": {
                    "name": "Petros Elia"
                },
                "author": "Petros Elia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18432v1",
                "updated": "2025-04-25T15:44:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    44,
                    38,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:44:38Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    44,
                    38,
                    4,
                    115,
                    0
                ],
                "title": "FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack"
                },
                "summary": "As the gap between network and CPU speeds rapidly increases, the CPU-centric\nnetwork stack proves inadequate due to excessive CPU and memory overhead. While\nhardware-offloaded network stacks alleviate these issues, they suffer from\nlimited flexibility in both control and data planes. Offloading network stack\nto off-path SmartNIC seems promising to provide high flexibility; however,\nthroughput remains constrained by inherent SmartNIC architectural limitations.\n  To this end, we design FlexiNS, a SmartNIC-centric network stack with\nsoftware transport programmability and line-rate packet processing\ncapabilities. To grapple with the limitation of SmartNIC-induced challenges,\nFlexiNS introduces: (a) a header-only offloading TX path; (b) an\nunlimited-working-set in-cache processing RX path; (c) a high-performance\nDMA-only notification pipe; and (d) a programmable offloading engine. We\nprototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box\nRDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\\times$ higher\nthroughput than the microkernel-based baseline in block storage disaggregation\nand 1.3$\\times$ higher throughput than the hardware-offloaded baseline in\nKVCache transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the gap between network and CPU speeds rapidly increases, the CPU-centric\nnetwork stack proves inadequate due to excessive CPU and memory overhead. While\nhardware-offloaded network stacks alleviate these issues, they suffer from\nlimited flexibility in both control and data planes. Offloading network stack\nto off-path SmartNIC seems promising to provide high flexibility; however,\nthroughput remains constrained by inherent SmartNIC architectural limitations.\n  To this end, we design FlexiNS, a SmartNIC-centric network stack with\nsoftware transport programmability and line-rate packet processing\ncapabilities. To grapple with the limitation of SmartNIC-induced challenges,\nFlexiNS introduces: (a) a header-only offloading TX path; (b) an\nunlimited-working-set in-cache processing RX path; (c) a high-performance\nDMA-only notification pipe; and (d) a programmable offloading engine. We\nprototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box\nRDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\\times$ higher\nthroughput than the microkernel-based baseline in block storage disaggregation\nand 1.3$\\times$ higher throughput than the hardware-offloaded baseline in\nKVCache transfer."
                },
                "authors": [
                    {
                        "name": "Xuzheng Chen"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Baolin Zhu"
                    },
                    {
                        "name": "Xueying Zhu"
                    },
                    {
                        "name": "Zhongqing Chen"
                    },
                    {
                        "name": "Shu Ma"
                    },
                    {
                        "name": "Lingjun Zhu"
                    },
                    {
                        "name": "Chao Shi"
                    },
                    {
                        "name": "Yin Zhang"
                    },
                    {
                        "name": "Zeke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zeke Wang"
                },
                "author": "Zeke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18242v1",
                "updated": "2025-04-25T10:43:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    43,
                    23,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T10:43:23Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    43,
                    23,
                    4,
                    115,
                    0
                ],
                "title": "Demand Private Coded Caching: Small Cache Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demand Private Coded Caching: Small Cache Size"
                },
                "summary": "We investigate the demand private coded caching problem, which is an $(N,K)$\ncoded caching problem with $N$ files, $K$ users, each equipped with a cache of\nsize $M$, and an additional privacy constraint on user demands, i.e., each user\ncan not gain any information about the demands of other users. We focus on\nscenarios where the size of users' caches is small, aiming to further\ncharacterize the fundamental limits of this problem. We first present a new\nvirtual-user-based achievable scheme for arbitrary number of users and files,\nand two MDS-code-based achievable schemes for the case $N \\le K$. With a newly\nderived converse bound for the case $N \\le K$, these proposed schemes lead to\nthe optimal memory-rate tradeoff of the demand private coded caching problem\nfor $M \\in \\big[0, \\frac{N}{(K+1)(N-1)} \\big] $ where $N \\le K \\le 2N-2$, and\nthe optimal memory-rate tradeoff for $M \\in \\big[0, \\frac{1}{K+1} \\big] $ where\n$ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users,\nby deriving another new converse bound, the optimal memory-rate tradeoff is\ncharacterized for $M\\in \\big[0,\\frac{2}{K}\\big] \\cup\n\\big[\\frac{2(K-1)}{K+1},2\\big]$. Finally, we provide the optimal memory-rate\ntradeoff of the demand private coded caching problem for 2 files and 3 users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the demand private coded caching problem, which is an $(N,K)$\ncoded caching problem with $N$ files, $K$ users, each equipped with a cache of\nsize $M$, and an additional privacy constraint on user demands, i.e., each user\ncan not gain any information about the demands of other users. We focus on\nscenarios where the size of users' caches is small, aiming to further\ncharacterize the fundamental limits of this problem. We first present a new\nvirtual-user-based achievable scheme for arbitrary number of users and files,\nand two MDS-code-based achievable schemes for the case $N \\le K$. With a newly\nderived converse bound for the case $N \\le K$, these proposed schemes lead to\nthe optimal memory-rate tradeoff of the demand private coded caching problem\nfor $M \\in \\big[0, \\frac{N}{(K+1)(N-1)} \\big] $ where $N \\le K \\le 2N-2$, and\nthe optimal memory-rate tradeoff for $M \\in \\big[0, \\frac{1}{K+1} \\big] $ where\n$ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users,\nby deriving another new converse bound, the optimal memory-rate tradeoff is\ncharacterized for $M\\in \\big[0,\\frac{2}{K}\\big] \\cup\n\\big[\\frac{2(K-1)}{K+1},2\\big]$. Finally, we provide the optimal memory-rate\ntradeoff of the demand private coded caching problem for 2 files and 3 users."
                },
                "authors": [
                    {
                        "name": "Qinyi Lu"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Wei Kang"
                    },
                    {
                        "name": "Chunguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Chunguo Li"
                },
                "author": "Chunguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18082v1",
                "updated": "2025-04-25T05:16:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    16,
                    53,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T05:16:53Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    16,
                    53,
                    4,
                    115,
                    0
                ],
                "title": "Efficient GNN Training Through Structure-Aware Randomized Mini-Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient GNN Training Through Structure-Aware Randomized Mini-Batching"
                },
                "summary": "Graph Neural Networks (GNNs) enable learning on realworld graphs and\nmini-batch training has emerged as the de facto standard for training GNNs\nbecause it can scale to very large graphs and improve convergence. Current\nmini-batch construction policies largely ignore efficiency considerations of\nGNN training. Specifically, existing mini-batching techniques employ\nrandomization schemes to improve accuracy and convergence. However, these\nrandomization schemes are often agnostic to the structural properties of the\ngraph (for eg. community structure), resulting in highly irregular memory\naccess patterns during GNN training that make suboptimal use of on-chip GPU\ncaches. On the other hand, while deterministic mini-batching based solely on\ngraph structure delivers fast runtime performance, the lack of randomness\ncompromises both the final model accuracy and training convergence speed. In\nthis paper, we present Community-structure-aware Randomized Mini-batching\n(COMM-RAND), a novel methodology that bridges the gap between the above\nextremes. COMM-RAND allows practitioners to explore the space between pure\nrandomness and pure graph structural awareness during mini-batch construction,\nleading to significantly more efficient GNN training with similar accuracy. We\nevaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND\ncuts down GNN training time by up to 2.76x (1.8x on average) while achieving an\naccuracy that is within 1.79% points (0.42% on average) compared to popular\nrandom mini-batching approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) enable learning on realworld graphs and\nmini-batch training has emerged as the de facto standard for training GNNs\nbecause it can scale to very large graphs and improve convergence. Current\nmini-batch construction policies largely ignore efficiency considerations of\nGNN training. Specifically, existing mini-batching techniques employ\nrandomization schemes to improve accuracy and convergence. However, these\nrandomization schemes are often agnostic to the structural properties of the\ngraph (for eg. community structure), resulting in highly irregular memory\naccess patterns during GNN training that make suboptimal use of on-chip GPU\ncaches. On the other hand, while deterministic mini-batching based solely on\ngraph structure delivers fast runtime performance, the lack of randomness\ncompromises both the final model accuracy and training convergence speed. In\nthis paper, we present Community-structure-aware Randomized Mini-batching\n(COMM-RAND), a novel methodology that bridges the gap between the above\nextremes. COMM-RAND allows practitioners to explore the space between pure\nrandomness and pure graph structural awareness during mini-batch construction,\nleading to significantly more efficient GNN training with similar accuracy. We\nevaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND\ncuts down GNN training time by up to 2.76x (1.8x on average) while achieving an\naccuracy that is within 1.79% points (0.42% on average) compared to popular\nrandom mini-batching approaches."
                },
                "authors": [
                    {
                        "name": "Vignesh Balaji"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    },
                    {
                        "name": "Gal Chechik"
                    },
                    {
                        "name": "Haggai Maron"
                    }
                ],
                "author_detail": {
                    "name": "Haggai Maron"
                },
                "author": "Haggai Maron",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v2",
                "updated": "2025-04-25T05:08:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    8,
                    45,
                    4,
                    115,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). That is, C3 on average achieves only 21% of ideal\nspeedup. This is so, due to known challenges of compute and memory interference\nbetween concurrent GPU kernels (that is, sharing of GPU's compute units, caches\nand HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build concurrent communication collectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). That is, C3 on average achieves only 21% of ideal\nspeedup. This is so, due to known challenges of compute and memory interference\nbetween concurrent GPU kernels (that is, sharing of GPU's compute units, caches\nand HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build concurrent communication collectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16620v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16620v2",
                "updated": "2025-04-25T05:05:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    5,
                    49,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-23T11:18:34Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    18,
                    34,
                    2,
                    113,
                    0
                ],
                "title": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV$_3$Sb$_5$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV$_3$Sb$_5$"
                },
                "summary": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, F$_c$, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of T$_c$ = 78 K. Notably, the pump-induced band shifts\nat F$_c$ were comparable to those caused by thermal effects at T$_c$. These\nfindings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane\nCDW emerges above 150 K, with out-of-plane electronic correlations leading to\nthe $2\\times2 \\times 2$ CDW near T$_c$, offering key insights into the\ninterplay between the electronic and structural dynamics in AV$_3$Sb$_5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, F$_c$, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of T$_c$ = 78 K. Notably, the pump-induced band shifts\nat F$_c$ were comparable to those caused by thermal effects at T$_c$. These\nfindings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane\nCDW emerges above 150 K, with out-of-plane electronic correlations leading to\nthe $2\\times2 \\times 2$ CDW near T$_c$, offering key insights into the\ninterplay between the electronic and structural dynamics in AV$_3$Sb$_5$."
                },
                "authors": [
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Shaofeng Duan"
                    },
                    {
                        "name": "Xiangqi Liu"
                    },
                    {
                        "name": "Zhihua Liu"
                    },
                    {
                        "name": "Shichong Wang"
                    },
                    {
                        "name": "Lingxiao Gu"
                    },
                    {
                        "name": "Jiongyu Huang"
                    },
                    {
                        "name": "Wenxuan Yang"
                    },
                    {
                        "name": "Jianzhe Liu"
                    },
                    {
                        "name": "Dong Qian"
                    },
                    {
                        "name": "Yanfeng Guo"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_doi": "10.1016/j.scib.2025.02.018",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.scib.2025.02.018",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.16620v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16620v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 4 figures",
                "arxiv_journal_ref": "Science Bulletin 70, 1211-1214 (2025)",
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17995v1",
                "updated": "2025-04-25T00:41:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T00:41:43Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "title": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study"
                },
                "summary": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal electronic correlations. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal electronic correlations. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials."
                },
                "authors": [
                    {
                        "name": "Indukuru Ramesh Reddy"
                    },
                    {
                        "name": "Sayandeep Ghosh"
                    },
                    {
                        "name": "Bongjae Kim"
                    },
                    {
                        "name": "Chang-Jong Kang"
                    }
                ],
                "author_detail": {
                    "name": "Chang-Jong Kang"
                },
                "author": "Chang-Jong Kang",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17866v1",
                "updated": "2025-04-24T18:09:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    18,
                    9,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T18:09:25Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    18,
                    9,
                    25,
                    3,
                    114,
                    0
                ],
                "title": "Updated parameters of the LArQL model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Updated parameters of the LArQL model"
                },
                "summary": "The need for a microscopic description of scintillation light generation in\nliquid argon becomes increasingly desirable with the upcoming operation of\nlarge scale LArTPCs in the next decade. While a detailed mathematical account\nof the process is still to be achieved, a phenomenological model for\nsimultaneously treating ionization and scintillation, LArQL, has been\nsuccessfully employed to describe the range of electric fields from 0 to 0.75\nkV/cm and dE/dx from 2 to 40 MeV/cm providing the anti-correlation between the\nfree ionization charge and scintillation light. A reanalysis of the original\nmodel parameter values has been performed within a global fit procedure and is\npresented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need for a microscopic description of scintillation light generation in\nliquid argon becomes increasingly desirable with the upcoming operation of\nlarge scale LArTPCs in the next decade. While a detailed mathematical account\nof the process is still to be achieved, a phenomenological model for\nsimultaneously treating ionization and scintillation, LArQL, has been\nsuccessfully employed to describe the range of electric fields from 0 to 0.75\nkV/cm and dE/dx from 2 to 40 MeV/cm providing the anti-correlation between the\nfree ionization charge and scintillation light. A reanalysis of the original\nmodel parameter values has been performed within a global fit procedure and is\npresented."
                },
                "authors": [
                    {
                        "name": "L. Paulucci"
                    },
                    {
                        "name": "F. Cavanna"
                    },
                    {
                        "name": "V. Vale"
                    },
                    {
                        "name": "F. Marinho"
                    }
                ],
                "author_detail": {
                    "name": "F. Marinho"
                },
                "author": "F. Marinho",
                "arxiv_comment": "Part of the proceedings of LIDINE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17584v1",
                "updated": "2025-04-24T14:14:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T14:14:07Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "title": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes."
                },
                "authors": [
                    {
                        "name": "Qingyuan Liu"
                    },
                    {
                        "name": "Liyan Chen"
                    },
                    {
                        "name": "Yanning Yang"
                    },
                    {
                        "name": "Haocheng Wang"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Zhigang Mao"
                    },
                    {
                        "name": "Naifeng Jing"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17554v1",
                "updated": "2025-04-24T13:47:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    47,
                    35,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:47:35Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    47,
                    35,
                    3,
                    114,
                    0
                ],
                "title": "Rethinking PM Crash Consistency in the CXL Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking PM Crash Consistency in the CXL Era"
                },
                "summary": "Persistent Memory (PM) introduces new opportunities for designing\ncrash-consistent applications without the traditional storage overheads.\nHowever, ensuring crash consistency in PM demands intricate knowledge of CPU,\ncache, and memory interactions. Hardware and software mechanisms have been\nproposed to ease this burden, but neither proved sufficient, prompting a\nvariety of bug detection tools.\n  With the sunset of Intel Optane comes the rise of Compute Express Link (CXL)\nfor PM. In this position paper, we discuss the impact of CXL's disaggregated\nand heterogeneous nature in the development of crash-consistent PM\napplications, and outline three research directions: hardware primitives,\npersistency frameworks, and bug detection tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Memory (PM) introduces new opportunities for designing\ncrash-consistent applications without the traditional storage overheads.\nHowever, ensuring crash consistency in PM demands intricate knowledge of CPU,\ncache, and memory interactions. Hardware and software mechanisms have been\nproposed to ease this burden, but neither proved sufficient, prompting a\nvariety of bug detection tools.\n  With the sunset of Intel Optane comes the rise of Compute Express Link (CXL)\nfor PM. In this position paper, we discuss the impact of CXL's disaggregated\nand heterogeneous nature in the development of crash-consistent PM\napplications, and outline three research directions: hardware primitives,\npersistency frameworks, and bug detection tools."
                },
                "authors": [
                    {
                        "name": "Joo Oliveira"
                    },
                    {
                        "name": "Joo Gonalves"
                    },
                    {
                        "name": "Miguel Matos"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Matos"
                },
                "author": "Miguel Matos",
                "arxiv_comment": "5 pages (2 extra pages for references), 1 figure, 2 algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v3",
                "updated": "2025-04-24T08:39:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    39,
                    13,
                    3,
                    114,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA)\nto offload data transfer, descriptor rings for buffering and queuing, and\ninterrupts for asynchrony between cores and device.\n  In this paper we question this wisdom in the light of two trends: modern and\nemerging cache-coherent interconnects like CXL3.0, and workloads, particularly\nmicroservices and serverless computing. Like some others before us, we argue\nthat the assumptions of the DMA-based model are obsolete, and in many use-cases\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, delivers a more efficient system.\n  However, we push this idea much further. We show, in a real hardware\nimplementation, the gains in latency for fine-grained communication achievable\nusing an open cache-coherence protocol which exposes cache transitions to a\nsmart device, and that throughput is competitive with DMA over modern\ninterconnects. We also demonstrate three use-cases: fine-grained RPC-style\ninvocation of functions on an accelerator, offloading of operators in a\nstreaming dataflow engine, and a network interface targeting serverless\nfunctions, comparing our use of coherence with both traditional DMA-style\ninteraction and a highly-optimized implementation using memory-mapped\nprogrammed I/O over PCIe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA)\nto offload data transfer, descriptor rings for buffering and queuing, and\ninterrupts for asynchrony between cores and device.\n  In this paper we question this wisdom in the light of two trends: modern and\nemerging cache-coherent interconnects like CXL3.0, and workloads, particularly\nmicroservices and serverless computing. Like some others before us, we argue\nthat the assumptions of the DMA-based model are obsolete, and in many use-cases\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, delivers a more efficient system.\n  However, we push this idea much further. We show, in a real hardware\nimplementation, the gains in latency for fine-grained communication achievable\nusing an open cache-coherence protocol which exposes cache transitions to a\nsmart device, and that throughput is competitive with DMA over modern\ninterconnects. We also demonstrate three use-cases: fine-grained RPC-style\ninvocation of functions on an accelerator, offloading of operators in a\nstreaming dataflow engine, and a network interface targeting serverless\nfunctions, comparing our use of coherence with both traditional DMA-style\ninteraction and a highly-optimized implementation using memory-mapped\nprogrammed I/O over PCIe."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15192v2",
                "updated": "2025-04-24T04:36:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    36,
                    20,
                    3,
                    114,
                    0
                ],
                "published": "2025-02-21T04:07:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "SPAARC: Spatial Proximity and Association based prefetching for\n  Augmented Reality in edge Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPAARC: Spatial Proximity and Association based prefetching for\n  Augmented Reality in edge Cache"
                },
                "summary": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects."
                },
                "authors": [
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Abhishek Chandra"
                    },
                    {
                        "name": "Jon Weissman"
                    }
                ],
                "author_detail": {
                    "name": "Jon Weissman"
                },
                "author": "Jon Weissman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14992v2",
                "updated": "2025-04-24T04:13:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    13,
                    49,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-21T09:41:26Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    41,
                    26,
                    0,
                    111,
                    0
                ],
                "title": "Efficient Pretraining Length Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Pretraining Length Scaling"
                },
                "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Xun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xun Zhou"
                },
                "author": "Xun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02441v2",
                "updated": "2025-04-24T01:47:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    1,
                    47,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-03T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Cognitive Memory in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Memory in Large Language Models"
                },
                "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."
                },
                "authors": [
                    {
                        "name": "Lianlei Shan"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Zezhou Zhu"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "arxiv_comment": "37 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15364v2",
                "updated": "2025-04-23T18:02:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    18,
                    2,
                    55,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T18:12:46Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    18,
                    12,
                    46,
                    0,
                    111,
                    0
                ],
                "title": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments"
                },
                "summary": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B."
                },
                "authors": [
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matt J Morse"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "8 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15437v2",
                "updated": "2025-04-23T15:02:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    2,
                    16,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T21:01:57Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    21,
                    1,
                    57,
                    0,
                    111,
                    0
                ],
                "title": "Iris: A Next Generation Digital Pathology Rendering Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iris: A Next Generation Digital Pathology Rendering Engine"
                },
                "summary": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology. Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 us per\nslide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology. Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 us per\nslide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms."
                },
                "authors": [
                    {
                        "name": "Ryan Erik Landvater"
                    },
                    {
                        "name": "Ulysses Balis"
                    }
                ],
                "author_detail": {
                    "name": "Ulysses Balis"
                },
                "author": "Ulysses Balis",
                "arxiv_doi": "10.1016/j.jpi.2024.100414",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jpi.2024.100414",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.15437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 8 figures",
                "arxiv_journal_ref": "Journal of Pathology Informatics, 16, 100414 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10138v2",
                "updated": "2025-04-23T10:48:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    48,
                    52,
                    2,
                    113,
                    0
                ],
                "published": "2025-01-17T12:01:28Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "title": "The NIC should be part of the OS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NIC should be part of the OS"
                },
                "summary": "The network interface adapter (NIC) is a critical component of a cloud server\noccupying a unique position. Not only is network performance vital to efficient\noperation of the machine, but unlike compute accelerators like GPUs, the\nnetwork subsystem must react to unpredictable events like the arrival of a\nnetwork packet and communicate with the appropriate application end point with\nminimal latency.\n  Current approaches to server stacks navigate a trade-off between flexibility,\nefficiency, and performance: the fastest kernel-bypass approaches dedicate\ncores to applications, busy-wait on receive queues, etc. while more flexible\napproaches appropriate to more dynamic workload mixes incur much greater\nsoftware overhead on the data path.\n  However, we reject this trade-off, which we ascribe to an arbitrary (and\nsub-optimal) split in system state between the OS and the NIC. Instead, by\nexploiting the properties of cache-coherent interconnects and integrating the\nNIC closely with the OS kernel, we can achieve something surprising:\nperformance for RPC workloads better than the fastest kernelbypass approaches\nwithout sacrificing the robustness and dynamic adaptation of kernel-based\nnetwork subsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The network interface adapter (NIC) is a critical component of a cloud server\noccupying a unique position. Not only is network performance vital to efficient\noperation of the machine, but unlike compute accelerators like GPUs, the\nnetwork subsystem must react to unpredictable events like the arrival of a\nnetwork packet and communicate with the appropriate application end point with\nminimal latency.\n  Current approaches to server stacks navigate a trade-off between flexibility,\nefficiency, and performance: the fastest kernel-bypass approaches dedicate\ncores to applications, busy-wait on receive queues, etc. while more flexible\napproaches appropriate to more dynamic workload mixes incur much greater\nsoftware overhead on the data path.\n  However, we reject this trade-off, which we ascribe to an arbitrary (and\nsub-optimal) split in system state between the OS and the NIC. Instead, by\nexploiting the properties of cache-coherent interconnects and integrating the\nNIC closely with the OS kernel, we can achieve something surprising:\nperformance for RPC workloads better than the fastest kernelbypass approaches\nwithout sacrificing the robustness and dynamic adaptation of kernel-based\nnetwork subsystems."
                },
                "authors": [
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "arxiv_doi": "10.1145/3713082.3730388",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713082.3730388",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.10138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Camera ready for HotOS'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v2",
                "updated": "2025-04-23T05:04:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    5,
                    4,
                    58,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Caching through Attention Output Error based Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Caching through Attention Output Error based Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "14 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06015v2",
                "updated": "2025-04-23T04:21:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    4,
                    21,
                    49,
                    2,
                    113,
                    0
                ],
                "published": "2025-03-08T02:35:16Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "title": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems"
                },
                "summary": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e they do not adapt to changing cache access\npatterns. Newer developments such as the High-Luminosity - Large Hadron\nCollider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move\ntoward streaming readout based Data Acquisition systems (DAQs) will increase\nthe data production exponentially and hence burden the storage, compute &\nnetwork infrastructures. Moreover, existing caching frameworks are optimized to\nreduce latency, but not optimized for storage. This, in combination with\nlimited cache capacities relative to total data, makes it difficult to achieve\ndata locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, we first present a Long Short-Term Memory-based (LSTM) hourly and\nmulti-step cache usage prediction. Second, we present an hourly file-level\naccess prediction model based on CatboostRegressor. To date, most ML-based\ncache prediction strategies in HEP have focused on daily cache usage and\nlimited works tackled hourly cache usage and even fewer strategies addressed\nhourly file-level access prediction. File-level access prediction allows for\nthe design of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending the\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e they do not adapt to changing cache access\npatterns. Newer developments such as the High-Luminosity - Large Hadron\nCollider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move\ntoward streaming readout based Data Acquisition systems (DAQs) will increase\nthe data production exponentially and hence burden the storage, compute &\nnetwork infrastructures. Moreover, existing caching frameworks are optimized to\nreduce latency, but not optimized for storage. This, in combination with\nlimited cache capacities relative to total data, makes it difficult to achieve\ndata locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, we first present a Long Short-Term Memory-based (LSTM) hourly and\nmulti-step cache usage prediction. Second, we present an hourly file-level\naccess prediction model based on CatboostRegressor. To date, most ML-based\ncache prediction strategies in HEP have focused on daily cache usage and\nlimited works tackled hourly cache usage and even fewer strategies addressed\nhourly file-level access prediction. File-level access prediction allows for\nthe design of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending the\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations."
                },
                "authors": [
                    {
                        "name": "Venkat Sai Suman Lamba Karanam"
                    },
                    {
                        "name": "Sarat Sasank Barla"
                    },
                    {
                        "name": "Byrav Ramamurthy"
                    },
                    {
                        "name": "Derek Weitzel"
                    }
                ],
                "author_detail": {
                    "name": "Derek Weitzel"
                },
                "author": "Derek Weitzel",
                "arxiv_comment": "Submitted as a contribution to the CHEP 2024 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16324v1",
                "updated": "2025-04-22T23:52:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    23,
                    52,
                    13,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T23:52:13Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    23,
                    52,
                    13,
                    1,
                    112,
                    0
                ],
                "title": "The Dawn of Disaggregation and the Coherence Conundrum: A Call for\n  Federated Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dawn of Disaggregation and the Coherence Conundrum: A Call for\n  Federated Coherence"
                },
                "summary": "Disaggregated memory is an upcoming data center technology that will allow\nnodes (servers) to share data efficiently. Sharing data creates a debate on the\nlevel of cache coherence the system should provide. While current proposals aim\nto provide coherence for all or parts of the disaggregated memory, we argue\nthat this approach is problematic, because of scalability limitations and\nhardware complexity. Instead, we propose and formally define federated\ncoherence, a model that provides coherence only within nodes, not across nodes.\nFederated coherence can use current intra-node coherence provided by processors\nwithout requiring expensive mechanisms for inter-node coherence. Developers can\nuse federated coherence with a few simple programming paradigms and a\nsynchronization library. We sketch some potential applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory is an upcoming data center technology that will allow\nnodes (servers) to share data efficiently. Sharing data creates a debate on the\nlevel of cache coherence the system should provide. While current proposals aim\nto provide coherence for all or parts of the disaggregated memory, we argue\nthat this approach is problematic, because of scalability limitations and\nhardware complexity. Instead, we propose and formally define federated\ncoherence, a model that provides coherence only within nodes, not across nodes.\nFederated coherence can use current intra-node coherence provided by processors\nwithout requiring expensive mechanisms for inter-node coherence. Developers can\nuse federated coherence with a few simple programming paradigms and a\nsynchronization library. We sketch some potential applications."
                },
                "authors": [
                    {
                        "name": "Jaewan Hong"
                    },
                    {
                        "name": "Marcos K. Aguilera"
                    },
                    {
                        "name": "Emmanuel Amaro"
                    },
                    {
                        "name": "Vincent Liu"
                    },
                    {
                        "name": "Aurojit Panda"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v3",
                "updated": "2025-04-22T17:34:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    34,
                    34,
                    1,
                    112,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Presented at IEEE Custom Integrated Circuits Conference (CICC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v2",
                "updated": "2025-04-22T17:23:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    23,
                    28,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators"
                },
                "summary": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hofeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "15 pages, 10 figures. Updated references and author name presentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14489v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14489v2",
                "updated": "2025-04-22T15:19:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    19,
                    48,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-20T04:46:34Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    4,
                    46,
                    34,
                    6,
                    110,
                    0
                ],
                "title": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing"
                },
                "summary": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads."
                },
                "authors": [
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14489v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14489v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15720v1",
                "updated": "2025-04-22T09:08:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T09:08:46Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "title": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference"
                },
                "summary": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions."
                },
                "authors": [
                    {
                        "name": "Yihao Zhao"
                    },
                    {
                        "name": "Jiadun Chen"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v3",
                "updated": "2025-04-21T22:13:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    22,
                    13,
                    7,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v2",
                "updated": "2025-04-21T20:10:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    20,
                    10,
                    11,
                    0,
                    111,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "Accepted by MLSys 2025, code available at\n  http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15260v1",
                "updated": "2025-04-21T17:39:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    39,
                    59,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:39:59Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    39,
                    59,
                    0,
                    111,
                    0
                ],
                "title": "Joint Knowledge and Power Management for Secure Semantic Communication\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Knowledge and Power Management for Secure Semantic Communication\n  Networks"
                },
                "summary": "Recently, semantic communication (SemCom) has shown its great superiorities\nin resource savings and information exchanges. However, while its unique\nbackground knowledge guarantees accurate semantic reasoning and recovery,\nsemantic information security-related concerns are introduced at the same time.\nSince the potential eavesdroppers may have the same background knowledge to\naccurately decrypt the private semantic information transmitted between legal\nSemCom users, this makes the knowledge management in SemCom networks rather\nchallenging in joint consideration with the power control. To this end, this\npaper focuses on jointly addressing three core issues of power allocation,\nknowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in\nsecure SemCom networks. We first develop a novel performance metric, namely\nsemantic secrecy throughput (SST), to quantify the information security level\nthat can be achieved at each pair of D2D SemCom users. Next, an SST\nmaximization problem is formulated subject to secure SemCom-related delay and\nreliability constraints. Afterward, we propose a security-aware resource\nmanagement solution using the Lagrange primal-dual method and a two-stage\nmethod. Simulation results demonstrate our proposed solution nearly doubles the\nSST performance and realizes less than half of the queuing delay performance\ncompared to different benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, semantic communication (SemCom) has shown its great superiorities\nin resource savings and information exchanges. However, while its unique\nbackground knowledge guarantees accurate semantic reasoning and recovery,\nsemantic information security-related concerns are introduced at the same time.\nSince the potential eavesdroppers may have the same background knowledge to\naccurately decrypt the private semantic information transmitted between legal\nSemCom users, this makes the knowledge management in SemCom networks rather\nchallenging in joint consideration with the power control. To this end, this\npaper focuses on jointly addressing three core issues of power allocation,\nknowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in\nsecure SemCom networks. We first develop a novel performance metric, namely\nsemantic secrecy throughput (SST), to quantify the information security level\nthat can be achieved at each pair of D2D SemCom users. Next, an SST\nmaximization problem is formulated subject to secure SemCom-related delay and\nreliability constraints. Afterward, we propose a security-aware resource\nmanagement solution using the Lagrange primal-dual method and a two-stage\nmethod. Simulation results demonstrate our proposed solution nearly doubles the\nSST performance and realizes less than half of the queuing delay performance\ncompared to different benchmarks."
                },
                "authors": [
                    {
                        "name": "Xuesong Liu"
                    },
                    {
                        "name": "Yansong Liu"
                    },
                    {
                        "name": "Haoyu Tang"
                    },
                    {
                        "name": "Fangzhou Zhao"
                    },
                    {
                        "name": "Le Xia"
                    },
                    {
                        "name": "Yao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yao Sun"
                },
                "author": "Yao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15247v1",
                "updated": "2025-04-21T17:22:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    22,
                    18,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:22:18Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    22,
                    18,
                    0,
                    111,
                    0
                ],
                "title": "Lance: Efficient Random Access in Columnar Storage through Adaptive\n  Structural Encodings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lance: Efficient Random Access in Columnar Storage through Adaptive\n  Structural Encodings"
                },
                "summary": "The growing interest in artificial intelligence has created workloads that\nrequire both sequential and random access. At the same time, NVMe-backed\nstorage solutions have emerged, providing caching capability for large columnar\ndatasets in cloud storage. Current columnar storage libraries fall short of\neffectively utilizing an NVMe device's capabilities, especially when it comes\nto random access. Historically, this has been assumed an implicit weakness in\ncolumnar storage formats, but this has not been sufficiently explored. In this\npaper, we examine the effectiveness of popular columnar formats such as Apache\nArrow, Apache Parquet, and Lance in both random access and full scan tasks\nagainst NVMe storage.\n  We argue that effective encoding of a column's structure, such as the\nrepetition and validity information, is the key to unlocking the disk's\nperformance. We show that Parquet, when configured correctly, can achieve over\n60x better random access performance than default settings. We also show that\nthis high random access performance requires making minor trade-offs in scan\nperformance and RAM utilization. We then describe the Lance structural encoding\nscheme, which alternates between two different structural encodings based on\ndata width, and achieves better random access performance without making\ntrade-offs in scan performance or RAM utilization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing interest in artificial intelligence has created workloads that\nrequire both sequential and random access. At the same time, NVMe-backed\nstorage solutions have emerged, providing caching capability for large columnar\ndatasets in cloud storage. Current columnar storage libraries fall short of\neffectively utilizing an NVMe device's capabilities, especially when it comes\nto random access. Historically, this has been assumed an implicit weakness in\ncolumnar storage formats, but this has not been sufficiently explored. In this\npaper, we examine the effectiveness of popular columnar formats such as Apache\nArrow, Apache Parquet, and Lance in both random access and full scan tasks\nagainst NVMe storage.\n  We argue that effective encoding of a column's structure, such as the\nrepetition and validity information, is the key to unlocking the disk's\nperformance. We show that Parquet, when configured correctly, can achieve over\n60x better random access performance than default settings. We also show that\nthis high random access performance requires making minor trade-offs in scan\nperformance and RAM utilization. We then describe the Lance structural encoding\nscheme, which alternates between two different structural encodings based on\ndata width, and achieves better random access performance without making\ntrade-offs in scan performance or RAM utilization."
                },
                "authors": [
                    {
                        "name": "Weston Pace"
                    },
                    {
                        "name": "Chang She"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Will Jones"
                    },
                    {
                        "name": "Albert Lockett"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Raunak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Raunak Shah"
                },
                "author": "Raunak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v3",
                "updated": "2025-04-21T15:36:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    36,
                    53,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14866v2",
                "updated": "2025-04-21T15:13:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    13,
                    44,
                    0,
                    111,
                    0
                ],
                "published": "2025-02-20T18:59:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15021v1",
                "updated": "2025-04-21T11:09:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    43,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T11:09:43Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    43,
                    0,
                    111,
                    0
                ],
                "title": "Is Intelligence the Right Direction in New OS Scheduling for Multiple\n  Resources in Cloud Environments?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Intelligence the Right Direction in New OS Scheduling for Multiple\n  Resources in Cloud Environments?"
                },
                "summary": "Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies."
                },
                "authors": [
                    {
                        "name": "Xinglei Dou"
                    },
                    {
                        "name": "Lei Liu"
                    },
                    {
                        "name": "Limin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Limin Xiao"
                },
                "author": "Limin Xiao",
                "arxiv_comment": "25 pages, 14 figures, to be published in ACM Transactions on Storage",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v3",
                "updated": "2025-04-21T03:40:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    3,
                    40,
                    10,
                    0,
                    111,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14775v1",
                "updated": "2025-04-21T00:07:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T00:07:49Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "title": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling"
                },
                "summary": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Xianwei Zhang"
                    },
                    {
                        "name": "Jiangsu Du"
                    },
                    {
                        "name": "Zhiguang Chen"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Yutong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Lu"
                },
                "author": "Yutong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v2",
                "updated": "2025-04-20T21:50:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    21,
                    50,
                    3,
                    6,
                    110,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Code: https://github.com/NVIDIA/Star-Attention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09775v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09775v3",
                "updated": "2025-04-20T19:57:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    19,
                    57,
                    16,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-14T00:29:49Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    0,
                    29,
                    49,
                    0,
                    104,
                    0
                ],
                "title": "Understanding and Optimizing Multi-Stage AI Inference Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Optimizing Multi-Stage AI Inference Pipelines"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads."
                },
                "authors": [
                    {
                        "name": "Abhimanyu Rajeshkumar Bambhaniya"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "name": "Sudarshan Srinivasan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Amir Yazdanbakhsh"
                    },
                    {
                        "name": "Midhilesh Elavazhagan"
                    },
                    {
                        "name": "Madhu Kumar"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "arxiv_comment": "Inference System Design for Multi-Stage AI Inference Pipelines. 13\n  Pages, 15 Figues, 3 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09775v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09775v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11208v2",
                "updated": "2025-04-20T07:53:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    7,
                    53,
                    9,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-15T14:11:38Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    11,
                    38,
                    1,
                    105,
                    0
                ],
                "title": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye"
                },
                "summary": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively."
                },
                "authors": [
                    {
                        "name": "Bradley Morgan"
                    },
                    {
                        "name": "Gal Horowitz"
                    },
                    {
                        "name": "Sioli O'Connell"
                    },
                    {
                        "name": "Stephan van Schaik"
                    },
                    {
                        "name": "Chitchanok Chuengsatiansup"
                    },
                    {
                        "name": "Daniel Genkin"
                    },
                    {
                        "name": "Olaf Maennel"
                    },
                    {
                        "name": "Paul Montague"
                    },
                    {
                        "name": "Eyal Ronen"
                    },
                    {
                        "name": "Yuval Yarom"
                    }
                ],
                "author_detail": {
                    "name": "Yuval Yarom"
                },
                "author": "Yuval Yarom",
                "arxiv_comment": "Added reference to the ID3 decision tree induction algorithm by J. R.\n  Quinlan in Section 5.4",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14435v1",
                "updated": "2025-04-20T00:49:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    0,
                    49,
                    27,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-20T00:49:27Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    0,
                    49,
                    27,
                    6,
                    110,
                    0
                ],
                "title": "Deuteronomy 2.0: Record Caching and Latch Freedom",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deuteronomy 2.0: Record Caching and Latch Freedom"
                },
                "summary": "The Deuteronomy transactional key-value store is unique architecturally in\nproviding separation between transaction functionality -- its Transactional\nComponent (TC) and data management -- its Data Component (DC). It is unique in\ntechnology by (1) supporting record caching, a smaller unit than the\ntraditional page; and (2) protecting resources during concurrent execution\nusing a latch-free approach. Both technologies are enabled by delta updating.\nThis paper explains how record caching improves cache cost/performance. It also\nshows how a new latch-free approach makes implementation easier and improves\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Deuteronomy transactional key-value store is unique architecturally in\nproviding separation between transaction functionality -- its Transactional\nComponent (TC) and data management -- its Data Component (DC). It is unique in\ntechnology by (1) supporting record caching, a smaller unit than the\ntraditional page; and (2) protecting resources during concurrent execution\nusing a latch-free approach. Both technologies are enabled by delta updating.\nThis paper explains how record caching improves cache cost/performance. It also\nshows how a new latch-free approach makes implementation easier and improves\nperformance."
                },
                "authors": [
                    {
                        "name": "David Lomet"
                    }
                ],
                "author_detail": {
                    "name": "David Lomet"
                },
                "author": "David Lomet",
                "arxiv_comment": "6 pages, 5 figures, potential CIDR submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v1",
                "updated": "2025-04-19T18:25:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max Lbke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) submitted\n  to \"25th International Conference on Computational Science\" (ICCS25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14196v1",
                "updated": "2025-04-19T06:18:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    18,
                    56,
                    5,
                    109,
                    0
                ],
                "published": "2025-04-19T06:18:56Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    18,
                    56,
                    5,
                    109,
                    0
                ],
                "title": "Room-temperature high-average-power strong-field terahertz source based\n  on industrial high-repetition-rate femtosecond laser",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Room-temperature high-average-power strong-field terahertz source based\n  on industrial high-repetition-rate femtosecond laser"
                },
                "summary": "Free-space strong-field terahertz (THz) pulses, generated via optical\nrectification of femtosecond lasers in nonlinear crystals, are pivotal in\nvarious applications. However, conventional Ti:sapphire lasers struggle to\nproduce high-average-power THz due to their limited output power. While\nkilowatt ytterbium lasers are increasingly adopted, their application in THz\ngeneration faces challenges: low optical-to-THz conversion efficiency\n(attributed to long pulse durations and low energy) and crystal damage under\nhigh pumping power. Here, we report a high-average-power strong-field THz\nsource using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ,\n50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By\nsystematically optimizing TPFP implementations and comparing grating- and\nechelon-type configurations, we achieve a THz source with 64.5 mW average power\nat 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at\n0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in\ncobalt-iron ferromagnetic nanofilms. This high-repetition-rate,\nhigh-average-power THz system, combined with its potential capabilities in high\nsignal-to-noise spectroscopy and imaging, promises transformative impacts in\nquantum matter manipulation, non-destructive testing, and biomedicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free-space strong-field terahertz (THz) pulses, generated via optical\nrectification of femtosecond lasers in nonlinear crystals, are pivotal in\nvarious applications. However, conventional Ti:sapphire lasers struggle to\nproduce high-average-power THz due to their limited output power. While\nkilowatt ytterbium lasers are increasingly adopted, their application in THz\ngeneration faces challenges: low optical-to-THz conversion efficiency\n(attributed to long pulse durations and low energy) and crystal damage under\nhigh pumping power. Here, we report a high-average-power strong-field THz\nsource using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ,\n50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By\nsystematically optimizing TPFP implementations and comparing grating- and\nechelon-type configurations, we achieve a THz source with 64.5 mW average power\nat 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at\n0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in\ncobalt-iron ferromagnetic nanofilms. This high-repetition-rate,\nhigh-average-power THz system, combined with its potential capabilities in high\nsignal-to-noise spectroscopy and imaging, promises transformative impacts in\nquantum matter manipulation, non-destructive testing, and biomedicine."
                },
                "authors": [
                    {
                        "name": "Deyin Kong"
                    },
                    {
                        "name": "Yichen Su"
                    },
                    {
                        "name": "Cheng Song"
                    },
                    {
                        "name": "Xiaojun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wu"
                },
                "author": "Xiaojun Wu",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v5",
                "updated": "2025-04-19T05:57:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    5,
                    57,
                    44,
                    5,
                    109,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14089v1",
                "updated": "2025-04-18T22:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T22:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average."
                },
                "authors": [
                    {
                        "name": "Kang He"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13989v1",
                "updated": "2025-04-18T13:46:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T13:46:58Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "title": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs"
                },
                "summary": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40\\% increase\nin accuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40\\% increase\nin accuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization."
                },
                "authors": [
                    {
                        "name": "Lucas Maisonnave"
                    },
                    {
                        "name": "Cyril Moineau"
                    },
                    {
                        "name": "Olivier Bichler"
                    },
                    {
                        "name": "Fabrice Rastello"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice Rastello"
                },
                "author": "Fabrice Rastello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13981v1",
                "updated": "2025-04-18T06:34:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    34,
                    57,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T06:34:57Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    34,
                    57,
                    4,
                    108,
                    0
                ],
                "title": "CacheFormer: High Attention-Based Segment Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFormer: High Attention-Based Segment Caching"
                },
                "summary": "Efficiently handling long contexts in transformer-based language models with\nlow perplexity is an active area of research. Numerous recent approaches like\nLinformer, Longformer, Performer, and Structured state space models (SSMs).,\nhave not fully resolved this problem. All these models strive to reduce the\nquadratic time complexity of the attention mechanism while minimizing the loss\nin quality due to the effective compression of the long context. Inspired by\nthe cache and virtual memory principle in computers, where in case of a cache\nmiss, not only the needed data is retrieved from the memory, but the adjacent\ndata is also obtained, we apply this concept to handling long contexts by\ndividing it into small segments. In our design, we retrieve the nearby segments\nin an uncompressed form when high segment-level attention occurs at the\ncompressed level. Our en-hancements for handling long context include\naggregating four attention mechanisms consisting of short sliding window\nattention, long compressed segmented attention, dynamically retrieving top k\nhigh attention uncompressed segments, and overlapping segments in long segment\nattention to avoid segment fragmentation. These enhancements result in an\narchitecture that outperforms ex-isting SOTA architectures with an average\nperplexity improvement of 8.5% over similar model sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently handling long contexts in transformer-based language models with\nlow perplexity is an active area of research. Numerous recent approaches like\nLinformer, Longformer, Performer, and Structured state space models (SSMs).,\nhave not fully resolved this problem. All these models strive to reduce the\nquadratic time complexity of the attention mechanism while minimizing the loss\nin quality due to the effective compression of the long context. Inspired by\nthe cache and virtual memory principle in computers, where in case of a cache\nmiss, not only the needed data is retrieved from the memory, but the adjacent\ndata is also obtained, we apply this concept to handling long contexts by\ndividing it into small segments. In our design, we retrieve the nearby segments\nin an uncompressed form when high segment-level attention occurs at the\ncompressed level. Our en-hancements for handling long context include\naggregating four attention mechanisms consisting of short sliding window\nattention, long compressed segmented attention, dynamically retrieving top k\nhigh attention uncompressed segments, and overlapping segments in long segment\nattention to avoid segment fragmentation. These enhancements result in an\narchitecture that outperforms ex-isting SOTA architectures with an average\nperplexity improvement of 8.5% over similar model sizes."
                },
                "authors": [
                    {
                        "name": "Sushant Singh"
                    },
                    {
                        "name": "Ausif Mahmood"
                    }
                ],
                "author_detail": {
                    "name": "Ausif Mahmood"
                },
                "author": "Ausif Mahmood",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09146v2",
                "updated": "2025-04-18T05:13:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    13,
                    52,
                    4,
                    108,
                    0
                ],
                "published": "2025-01-15T20:55:13Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "title": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs"
                },
                "summary": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "32 pages, 11 figures, 1 table, 4 algorithms, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16112v1",
                "updated": "2025-04-18T03:31:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    3,
                    31,
                    8,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T03:31:08Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    3,
                    31,
                    8,
                    4,
                    108,
                    0
                ],
                "title": "HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM\n  Inference via GPU Co-processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM\n  Inference via GPU Co-processing"
                },
                "summary": "The attention layer, a core component of Transformer-based LLMs, brings out\ninefficiencies in current GPU systems due to its low operational intensity and\nthe substantial memory requirements of KV caches. We propose a High-bandwidth\nProcessing Unit (HPU), a memoryintensive co-processor that enhances GPU\nresource utilization during large-batched LLM inference. By offloading\nmemory-bound operations, the HPU allows the GPU to focus on compute-intensive\ntasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales\nout to accommodate surging memory demands driven by large batch sizes and\nextended sequence lengths. In this paper, we show the HPU prototype implemented\nwith PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU\nheterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy\nefficiency improvements over a GPUonly system, providing scalability without\nincreasing the number of GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The attention layer, a core component of Transformer-based LLMs, brings out\ninefficiencies in current GPU systems due to its low operational intensity and\nthe substantial memory requirements of KV caches. We propose a High-bandwidth\nProcessing Unit (HPU), a memoryintensive co-processor that enhances GPU\nresource utilization during large-batched LLM inference. By offloading\nmemory-bound operations, the HPU allows the GPU to focus on compute-intensive\ntasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales\nout to accommodate surging memory demands driven by large batch sizes and\nextended sequence lengths. In this paper, we show the HPU prototype implemented\nwith PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU\nheterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy\nefficiency improvements over a GPUonly system, providing scalability without\nincreasing the number of GPUs."
                },
                "authors": [
                    {
                        "name": "Myunghyun Rhee"
                    },
                    {
                        "name": "Joonseop Sim"
                    },
                    {
                        "name": "Taeyoung Ahn"
                    },
                    {
                        "name": "Seungyong Lee"
                    },
                    {
                        "name": "Daegun Yoon"
                    },
                    {
                        "name": "Euiseok Kim"
                    },
                    {
                        "name": "Kyoung Park"
                    },
                    {
                        "name": "Youngpyo Joo"
                    },
                    {
                        "name": "Hosik Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hosik Kim"
                },
                "author": "Hosik Kim",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13385v1",
                "updated": "2025-04-18T00:21:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    0,
                    21,
                    0,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T00:21:00Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    0,
                    21,
                    0,
                    4,
                    108,
                    0
                ],
                "title": "EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for\n  Enhanced Cache Occupancy Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for\n  Enhanced Cache Occupancy Attacks"
                },
                "summary": "Cache occupancy attacks exploit the shared nature of cache hierarchies to\ninfer a victim's activities by monitoring overall cache usage, unlike\naccess-driven cache attacks that focus on specific cache lines or sets. There\nexists some prior work that target the last-level cache (LLC) of Intel\nprocessors, which is inclusive of higher-level caches, and L2 caches of ARM\nsystems. In this paper, we target the System-Level Cache (SLC) of Apple\nM-series SoCs, which is exclusive to higher-level CPU caches. We address the\nchallenges of the exclusiveness and propose a suite of SLC-cache occupancy\nattacks, the first of its kind, where an adversary can monitor GPU and other\nCPU cluster activities from their own CPU cluster. We first discover the\nstructure of SLC in Apple M1 SOC and various policies pertaining to access and\nsharing through reverse engineering. We propose two attacks against websites.\nOne is a coarse-grained fingerprinting attack, recognizing which website is\naccessed based on their different GPU memory access patterns monitored through\nthe SLC occupancy channel. The other attack is a fine-grained pixel stealing\nattack, which precisely monitors the GPU memory usage for rendering different\npixels, through the SLC occupancy channel. Third, we introduce a novel screen\ncapturing attack which works beyond webpages, with the monitoring granularity\nof 57 rows of pixels (there are 1600 rows for the screen). This significantly\nexpands the attack surface, allowing the adversary to retrieve any screen\ndisplay, posing a substantial new threat to system security. Our findings\nreveal critical vulnerabilities in Apple's M-series SoCs and emphasize the\nurgent need for effective countermeasures against cache occupancy attacks in\nheterogeneous computing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache occupancy attacks exploit the shared nature of cache hierarchies to\ninfer a victim's activities by monitoring overall cache usage, unlike\naccess-driven cache attacks that focus on specific cache lines or sets. There\nexists some prior work that target the last-level cache (LLC) of Intel\nprocessors, which is inclusive of higher-level caches, and L2 caches of ARM\nsystems. In this paper, we target the System-Level Cache (SLC) of Apple\nM-series SoCs, which is exclusive to higher-level CPU caches. We address the\nchallenges of the exclusiveness and propose a suite of SLC-cache occupancy\nattacks, the first of its kind, where an adversary can monitor GPU and other\nCPU cluster activities from their own CPU cluster. We first discover the\nstructure of SLC in Apple M1 SOC and various policies pertaining to access and\nsharing through reverse engineering. We propose two attacks against websites.\nOne is a coarse-grained fingerprinting attack, recognizing which website is\naccessed based on their different GPU memory access patterns monitored through\nthe SLC occupancy channel. The other attack is a fine-grained pixel stealing\nattack, which precisely monitors the GPU memory usage for rendering different\npixels, through the SLC occupancy channel. Third, we introduce a novel screen\ncapturing attack which works beyond webpages, with the monitoring granularity\nof 57 rows of pixels (there are 1600 rows for the screen). This significantly\nexpands the attack surface, allowing the adversary to retrieve any screen\ndisplay, posing a substantial new threat to system security. Our findings\nreveal critical vulnerabilities in Apple's M-series SoCs and emphasize the\nurgent need for effective countermeasures against cache occupancy attacks in\nheterogeneous computing environments."
                },
                "authors": [
                    {
                        "name": "Tianhong Xu"
                    },
                    {
                        "name": "Aidong Adam Ding"
                    },
                    {
                        "name": "Yunsi Fei"
                    }
                ],
                "author_detail": {
                    "name": "Yunsi Fei"
                },
                "author": "Yunsi Fei",
                "arxiv_comment": "Accepted to ACM ASIA CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01291v2",
                "updated": "2025-04-17T23:45:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    23,
                    45,
                    51,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-02T01:49:58Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "title": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures"
                },
                "summary": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs."
                },
                "authors": [
                    {
                        "name": "Seungheon Shin"
                    },
                    {
                        "name": "Kyle Liddy"
                    },
                    {
                        "name": "Yinxuan Zhu"
                    },
                    {
                        "name": "Chandan Joishi"
                    },
                    {
                        "name": "Brianna A. Klein"
                    },
                    {
                        "name": "Andrew Armstrong"
                    },
                    {
                        "name": "Andrew A. Allerman"
                    },
                    {
                        "name": "Siddharth Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Rajan"
                },
                "author": "Siddharth Rajan",
                "arxiv_comment": "12 pages, 7 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19325v2",
                "updated": "2025-04-17T15:26:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    26,
                    4,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-25T03:38:06Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    3,
                    38,
                    6,
                    1,
                    84,
                    0
                ],
                "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction"
                },
                "summary": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR."
                },
                "authors": [
                    {
                        "name": "Yuchao Gu"
                    },
                    {
                        "name": "Weijia Mao"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "arxiv_comment": "Project page at https://farlongctx.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12876v3",
                "updated": "2025-04-17T03:51:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    3,
                    51,
                    6,
                    3,
                    107,
                    0
                ],
                "published": "2024-10-15T05:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    1,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context KV-Cache Eviction for LLMs via Attention-Gate"
                },
                "summary": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance."
                },
                "authors": [
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10074v2",
                "updated": "2025-04-17T00:38:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    0,
                    38,
                    24,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-13T05:43:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "title": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension"
                },
                "summary": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions."
                },
                "authors": [
                    {
                        "name": "Taehun Kim"
                    },
                    {
                        "name": "Hyerean Jang"
                    },
                    {
                        "name": "Youngjoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoo Shin"
                },
                "author": "Youngjoo Shin",
                "arxiv_comment": "The modified version of this preprint has been submitted to ESORICS\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12526v1",
                "updated": "2025-04-16T23:15:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    23,
                    15,
                    9,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T23:15:09Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    23,
                    15,
                    9,
                    2,
                    106,
                    0
                ],
                "title": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context\n  Language Models"
                },
                "summary": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Tianyi Zhu"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "arxiv_comment": "Submitted to COLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v1",
                "updated": "2025-04-16T18:03:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2504.11704",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12240v1",
                "updated": "2025-04-16T16:45:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T16:45:19Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cobra: Efficient Line Art COlorization with BRoAder References"
                },
                "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/."
                },
                "authors": [
                    {
                        "name": "Junhao Zhuang"
                    },
                    {
                        "name": "Lingen Li"
                    },
                    {
                        "name": "Xuan Ju"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project page with code: https://zhuang2002.github.io/Cobra/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11816v1",
                "updated": "2025-04-16T07:02:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T07:02:38Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading"
                },
                "summary": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads."
                },
                "authors": [
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Hyunsun Chung"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "author": "Youngjae Kim",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08334v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08334v3",
                "updated": "2025-04-16T05:57:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    57,
                    8,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-11T07:59:06Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    59,
                    6,
                    4,
                    101,
                    0
                ],
                "title": "Efficient Architecture for RISC-V Vector Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Architecture for RISC-V Vector Memory Access"
                },
                "summary": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors."
                },
                "authors": [
                    {
                        "name": "Hongyi Guan"
                    },
                    {
                        "name": "Yichuan Gao"
                    },
                    {
                        "name": "Chenlu Miao"
                    },
                    {
                        "name": "Haoyang Wu"
                    },
                    {
                        "name": "Hang Zhu"
                    },
                    {
                        "name": "Mingfeng Lin"
                    },
                    {
                        "name": "Huayue Liang"
                    }
                ],
                "author_detail": {
                    "name": "Huayue Liang"
                },
                "author": "Huayue Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08334v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11765v1",
                "updated": "2025-04-16T04:59:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T04:59:18Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "title": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs"
                },
                "summary": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration."
                },
                "authors": [
                    {
                        "name": "Hyungwoo Lee"
                    },
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Jungmin So"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "James J. Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "arxiv_affiliation": "Dept. of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea",
                "author": "Youngjae Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11729v1",
                "updated": "2025-04-16T03:07:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    7,
                    7,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T03:07:07Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    7,
                    7,
                    2,
                    106,
                    0
                ],
                "title": "EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G\n  Networks"
                },
                "summary": "As sixth-generation (6G) networks advance, large language models (LLMs) are\nincreasingly integrated into 6G infrastructure to enhance network management\nand intelligence. However, traditional LLMs architecture struggle to meet the\nstringent latency and security requirements of 6G, especially as the increasing\nin sequence length leads to greater task complexity. This paper proposes\nEdge-Prompt, a cloud-edge collaborative framework based on a hierarchical\nattention splicing mechanism. EdgePrompt employs distributed key-value (KV)\npair optimization techniques to accelerate inference and adapt to network\nconditions. Additionally, to reduce the risk of data leakage, EdgePrompt\nincorporates a privacy preserving strategy by isolating sensitive information\nduring processing. Experiments on public dataset show that EdgePrompt\neffectively improves the inference throughput and reduces the latency, which\nprovides a reliable solution for LLMs deployment in 6G environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As sixth-generation (6G) networks advance, large language models (LLMs) are\nincreasingly integrated into 6G infrastructure to enhance network management\nand intelligence. However, traditional LLMs architecture struggle to meet the\nstringent latency and security requirements of 6G, especially as the increasing\nin sequence length leads to greater task complexity. This paper proposes\nEdge-Prompt, a cloud-edge collaborative framework based on a hierarchical\nattention splicing mechanism. EdgePrompt employs distributed key-value (KV)\npair optimization techniques to accelerate inference and adapt to network\nconditions. Additionally, to reduce the risk of data leakage, EdgePrompt\nincorporates a privacy preserving strategy by isolating sensitive information\nduring processing. Experiments on public dataset show that EdgePrompt\neffectively improves the inference throughput and reduces the latency, which\nprovides a reliable solution for LLMs deployment in 6G environments."
                },
                "authors": [
                    {
                        "name": "Jiahong Ning"
                    },
                    {
                        "name": "Pengyan Zhu"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Gary Lee"
                    },
                    {
                        "name": "Sumei Sun"
                    },
                    {
                        "name": "Tingting Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yang"
                },
                "author": "Tingting Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11652v1",
                "updated": "2025-04-15T22:38:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    22,
                    38,
                    54,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T22:38:54Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    22,
                    38,
                    54,
                    1,
                    105,
                    0
                ],
                "title": "Engineering MultiQueues: Fast Relaxed Concurrent Priority Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering MultiQueues: Fast Relaxed Concurrent Priority Queues"
                },
                "summary": "Priority queues are used in a wide range of applications, including\nprioritized online scheduling, discrete event simulation, and greedy\nalgorithms. In parallel settings, classical priority queues often become a\nsevere bottleneck, resulting in low throughput. Consequently, there has been\nsignificant interest in concurrent priority queues with relaxed semantics. In\nthis article, we present the MultiQueue, a flexible approach to relaxed\npriority queues that uses multiple internal sequential priority queues. The\nscalability of the MultiQueue is enhanced by buffering elements, batching\noperations on the internal queues, and optimizing access patterns for high\ncache locality. We investigate the complementary quality criteria of rank\nerror, which measures how close deleted elements are to the global minimum, and\ndelay, which quantifies how many smaller elements were deleted before a given\nelement. Extensive experimental evaluation shows that the MultiQueue\noutperforms competing approaches across several benchmarks. This includes\nshortest-path and branch-and-bound benchmarks that resemble real applications.\nMoreover, the MultiQueue can be configured easily to balance throughput and\nquality according to the application's requirements. We employ a seemingly\nparadoxical technique of wait-free locking that might be of broader interest\nfor converting sequential data structures into relaxed concurrent data\nstructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Priority queues are used in a wide range of applications, including\nprioritized online scheduling, discrete event simulation, and greedy\nalgorithms. In parallel settings, classical priority queues often become a\nsevere bottleneck, resulting in low throughput. Consequently, there has been\nsignificant interest in concurrent priority queues with relaxed semantics. In\nthis article, we present the MultiQueue, a flexible approach to relaxed\npriority queues that uses multiple internal sequential priority queues. The\nscalability of the MultiQueue is enhanced by buffering elements, batching\noperations on the internal queues, and optimizing access patterns for high\ncache locality. We investigate the complementary quality criteria of rank\nerror, which measures how close deleted elements are to the global minimum, and\ndelay, which quantifies how many smaller elements were deleted before a given\nelement. Extensive experimental evaluation shows that the MultiQueue\noutperforms competing approaches across several benchmarks. This includes\nshortest-path and branch-and-bound benchmarks that resemble real applications.\nMoreover, the MultiQueue can be configured easily to balance throughput and\nquality according to the application's requirements. We employ a seemingly\nparadoxical technique of wait-free locking that might be of broader interest\nfor converting sequential data structures into relaxed concurrent data\nstructures."
                },
                "authors": [
                    {
                        "name": "Marvin Williams"
                    },
                    {
                        "name": "Peter Sanders"
                    }
                ],
                "author_detail": {
                    "name": "Peter Sanders"
                },
                "author": "Peter Sanders",
                "arxiv_comment": "40 pages, extended journal version of arXiv:2107.01350",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11320v1",
                "updated": "2025-04-15T16:00:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:00:21Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "title": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints"
                },
                "summary": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints."
                },
                "authors": [
                    {
                        "name": "Ruicheng Ao"
                    },
                    {
                        "name": "Gan Luo"
                    },
                    {
                        "name": "David Simchi-Levi"
                    },
                    {
                        "name": "Xinshang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinshang Wang"
                },
                "author": "Xinshang Wang",
                "arxiv_comment": "42 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v3",
                "updated": "2025-04-15T15:40:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    40,
                    25,
                    1,
                    105,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13195v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13195v5",
                "updated": "2025-04-15T15:37:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    37,
                    58,
                    1,
                    105,
                    0
                ],
                "published": "2024-04-19T22:06:14Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    22,
                    6,
                    14,
                    4,
                    110,
                    0
                ],
                "title": "Automatic BLAS Offloading on Unified Memory Architecture: A Study on\n  NVIDIA Grace-Hopper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic BLAS Offloading on Unified Memory Architecture: A Study on\n  NVIDIA Grace-Hopper"
                },
                "summary": "Porting codes to GPU often requires major efforts. While several tools exist\nfor automatically offload numerical libraries such as BLAS and LAPACK, they\noften prove impractical due to the high cost of mandatory data transfer. The\nnew unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth\ncache-coherent memory access of all memory from both CPU and GPU, potentially\neliminating bottleneck faced in conventional architecture. This breakthrough\nopens up new avenues for application development and porting strategies. In\nthis study, we introduce a new tool for automatic BLAS offload, the tool\nleverages the high speed cache coherent NVLink C2C interconnect in\nGrace-Hopper, and enables performant GPU offload for BLAS heavy applications\nwith no code changes or recompilation. The tool was tested on two quantum\nchemistry or physics codes, great performance benefits were observed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Porting codes to GPU often requires major efforts. While several tools exist\nfor automatically offload numerical libraries such as BLAS and LAPACK, they\noften prove impractical due to the high cost of mandatory data transfer. The\nnew unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth\ncache-coherent memory access of all memory from both CPU and GPU, potentially\neliminating bottleneck faced in conventional architecture. This breakthrough\nopens up new avenues for application development and porting strategies. In\nthis study, we introduce a new tool for automatic BLAS offload, the tool\nleverages the high speed cache coherent NVLink C2C interconnect in\nGrace-Hopper, and enables performant GPU offload for BLAS heavy applications\nwith no code changes or recompilation. The tool was tested on two quantum\nchemistry or physics codes, great performance benefits were observed."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Yinzhi Wang"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Hang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Liu"
                },
                "author": "Hang Liu",
                "arxiv_doi": "10.1145/3626203.3670561",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3626203.3670561",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.13195v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13195v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11067v1",
                "updated": "2025-04-15T11:02:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    2,
                    34,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T11:02:34Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    2,
                    34,
                    1,
                    105,
                    0
                ],
                "title": "Morphing-based Compression for Data-centric ML Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Morphing-based Compression for Data-centric ML Pipelines"
                },
                "summary": "Data-centric ML pipelines extend traditional machine learning (ML) pipelines\n-- of feature transformations and ML model training -- by outer loops for data\ncleaning, augmentation, and feature engineering to create high-quality input\ndata. Existing lossless matrix compression applies lightweight compression\nschemes to numeric matrices and performs linear algebra operations such as\nmatrix-vector multiplications directly on the compressed representation but\nstruggles to efficiently rediscover structural data redundancy. Compressed\noperations are effective at fitting data in available memory, reducing I/O\nacross the storage-memory-cache hierarchy, and improving instruction\nparallelism. The applied data cleaning, augmentation, and feature\ntransformations provide a rich source of information about data characteristics\nsuch as distinct items, column sparsity, and column correlations. In this\npaper, we introduce BWARE -- an extension of AWARE for workload-aware lossless\nmatrix compression -- that pushes compression through feature transformations\nand engineering to leverage information about structural transformations.\nBesides compressed feature transformations, we introduce a novel technique for\nlightweight morphing of a compressed representation into workload-optimized\ncompressed representations without decompression. BWARE shows substantial\nend-to-end runtime improvements, reducing the execution time for training\ndata-centric ML pipelines from days to hours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-centric ML pipelines extend traditional machine learning (ML) pipelines\n-- of feature transformations and ML model training -- by outer loops for data\ncleaning, augmentation, and feature engineering to create high-quality input\ndata. Existing lossless matrix compression applies lightweight compression\nschemes to numeric matrices and performs linear algebra operations such as\nmatrix-vector multiplications directly on the compressed representation but\nstruggles to efficiently rediscover structural data redundancy. Compressed\noperations are effective at fitting data in available memory, reducing I/O\nacross the storage-memory-cache hierarchy, and improving instruction\nparallelism. The applied data cleaning, augmentation, and feature\ntransformations provide a rich source of information about data characteristics\nsuch as distinct items, column sparsity, and column correlations. In this\npaper, we introduce BWARE -- an extension of AWARE for workload-aware lossless\nmatrix compression -- that pushes compression through feature transformations\nand engineering to leverage information about structural transformations.\nBesides compressed feature transformations, we introduce a novel technique for\nlightweight morphing of a compressed representation into workload-optimized\ncompressed representations without decompression. BWARE shows substantial\nend-to-end runtime improvements, reducing the execution time for training\ndata-centric ML pipelines from days to hours."
                },
                "authors": [
                    {
                        "name": "Sebastian Baunsgaard"
                    },
                    {
                        "name": "Matthias Boehm"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Boehm"
                },
                "author": "Matthias Boehm",
                "arxiv_comment": "20 pages, 28 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10326v1",
                "updated": "2025-04-14T15:34:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:34:26Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference"
                },
                "summary": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks."
                },
                "authors": [
                    {
                        "name": "Yangshen Deng"
                    },
                    {
                        "name": "Zhengxin You"
                    },
                    {
                        "name": "Long Xiang"
                    },
                    {
                        "name": "Qilong Li"
                    },
                    {
                        "name": "Peiqi Yuan"
                    },
                    {
                        "name": "Zhaoyang Hong"
                    },
                    {
                        "name": "Yitao Zheng"
                    },
                    {
                        "name": "Wanting Li"
                    },
                    {
                        "name": "Runzhong Li"
                    },
                    {
                        "name": "Haotian Liu"
                    },
                    {
                        "name": "Kyriakos Mouratidis"
                    },
                    {
                        "name": "Man Lung Yiu"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Qiaomu Shen"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Bo Tang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Tang"
                },
                "author": "Bo Tang",
                "arxiv_comment": "14 pages, 12 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.1; H.3.2; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10318v1",
                "updated": "2025-04-14T15:27:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    27,
                    32,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:27:32Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    27,
                    32,
                    0,
                    104,
                    0
                ],
                "title": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing\n  Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing\n  Obfuscation"
                },
                "summary": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead."
                },
                "authors": [
                    {
                        "name": "Kartik Ramkrishnan"
                    },
                    {
                        "name": "Antonia Zhai"
                    },
                    {
                        "name": "Stephen McCamant"
                    },
                    {
                        "name": "Pen Chung Yew"
                    }
                ],
                "author_detail": {
                    "name": "Pen Chung Yew"
                },
                "author": "Pen Chung Yew",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10181v1",
                "updated": "2025-04-14T12:34:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    34,
                    20,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:34:20Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    34,
                    20,
                    0,
                    104,
                    0
                ],
                "title": "A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis"
                },
                "summary": "The fault characteristics of inverter-based resources (IBRs) are different\nfrom conventional synchronous generators. The fault response of IBRs is\nnon-linear due to saturation states and mainly determined by fault ride through\n(FRT) strategies of the associated voltage source converter (VSC). This results\nin prohibitively large solution times for power flows considering these short\ncircuit characteristics, especially when the power system states change fast\ndue to uncertainty in IBR generations. To overcome this, a phasor-domain steady\nstate (SS) short circuit (SC) solver for IBR dominated power systems is\nproposed in this paper, and subsequently the developed IBR models are\nincorporated with a novel Jacobian-based Power Flow (PF) solver. In this\nmultiphase PF solver, any power system components can be modeled by considering\ntheir original non-linear or linear mathematical representations. Moreover, two\nnovel FRT strategies are proposed to fully utilize the converter capacity and\nto comply with IEEE-2800 2022 std and German grid code. The results are\ncompared with the Electromagnetic Transient (EMT) simulation on the IEEE 34\ntest network and the 120 kV EPRI benchmark system. The developed IBR sequence\ndomain PF model demonstrates more accurate behavior compared to the classical\nIBR generator model. The error in calculating the short circuit current with\nthe proposed SC solver is less than 3%, while achieving significant speed\nimprovements of three order of magnitudes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fault characteristics of inverter-based resources (IBRs) are different\nfrom conventional synchronous generators. The fault response of IBRs is\nnon-linear due to saturation states and mainly determined by fault ride through\n(FRT) strategies of the associated voltage source converter (VSC). This results\nin prohibitively large solution times for power flows considering these short\ncircuit characteristics, especially when the power system states change fast\ndue to uncertainty in IBR generations. To overcome this, a phasor-domain steady\nstate (SS) short circuit (SC) solver for IBR dominated power systems is\nproposed in this paper, and subsequently the developed IBR models are\nincorporated with a novel Jacobian-based Power Flow (PF) solver. In this\nmultiphase PF solver, any power system components can be modeled by considering\ntheir original non-linear or linear mathematical representations. Moreover, two\nnovel FRT strategies are proposed to fully utilize the converter capacity and\nto comply with IEEE-2800 2022 std and German grid code. The results are\ncompared with the Electromagnetic Transient (EMT) simulation on the IEEE 34\ntest network and the 120 kV EPRI benchmark system. The developed IBR sequence\ndomain PF model demonstrates more accurate behavior compared to the classical\nIBR generator model. The error in calculating the short circuit current with\nthe proposed SC solver is less than 3%, while achieving significant speed\nimprovements of three order of magnitudes."
                },
                "authors": [
                    {
                        "name": "Zahid Javid"
                    },
                    {
                        "name": "Firdous Ul Nazir"
                    },
                    {
                        "name": "Wentao Zhu"
                    },
                    {
                        "name": "Diptargha Chakravorty"
                    },
                    {
                        "name": "Ahmed Aboushady"
                    },
                    {
                        "name": "Mohamed Galeela"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Galeela"
                },
                "author": "Mohamed Galeela",
                "arxiv_comment": "12 Pages, First Revision Submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00601v3",
                "updated": "2025-04-14T11:20:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    20,
                    56,
                    0,
                    104,
                    0
                ],
                "published": "2024-11-01T14:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    3,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Diversity in Network-Friendly Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in Network-Friendly Recommendations"
                },
                "summary": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms."
                },
                "authors": [
                    {
                        "name": "Evangelia Tzimpimpaki"
                    },
                    {
                        "name": "Thrasyvoulos Spyropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thrasyvoulos Spyropoulos"
                },
                "author": "Thrasyvoulos Spyropoulos",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09984v1",
                "updated": "2025-04-14T08:51:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    8,
                    51,
                    35,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T08:51:35Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    8,
                    51,
                    35,
                    0,
                    104,
                    0
                ],
                "title": "On Precomputation and Caching in Information Retrieval Experiments with\n  Pipeline Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Precomputation and Caching in Information Retrieval Experiments with\n  Pipeline Architectures"
                },
                "summary": "Modern information retrieval systems often rely on multiple components\nexecuted in a pipeline. In a research setting, this can lead to substantial\nredundant computations (e.g., retrieving the same query multiple times for\nevaluating different downstream rerankers). To overcome this, researchers take\ncached \"result\" files as inputs, which represent the output of another\npipeline. However, these result files can be brittle and can cause a disconnect\nbetween the conceptual design of the pipeline and its logical implementation.\nTo overcome both the redundancy problem (when executing complete pipelines) and\nthe disconnect problem (when relying on intermediate result files), we describe\nour recent efforts to improve the caching capabilities in the open-source\nPyTerrier IR platform. We focus on two main directions: (1) automatic implicit\ncaching of common pipeline prefixes when comparing systems and (2) explicit\ncaching of operations through a new extension package, pyterrier-caching. These\napproaches allow for the best of both worlds: pipelines can be fully expressed\nend-to-end, while also avoiding redundant computations between pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern information retrieval systems often rely on multiple components\nexecuted in a pipeline. In a research setting, this can lead to substantial\nredundant computations (e.g., retrieving the same query multiple times for\nevaluating different downstream rerankers). To overcome this, researchers take\ncached \"result\" files as inputs, which represent the output of another\npipeline. However, these result files can be brittle and can cause a disconnect\nbetween the conceptual design of the pipeline and its logical implementation.\nTo overcome both the redundancy problem (when executing complete pipelines) and\nthe disconnect problem (when relying on intermediate result files), we describe\nour recent efforts to improve the caching capabilities in the open-source\nPyTerrier IR platform. We focus on two main directions: (1) automatic implicit\ncaching of common pipeline prefixes when comparing systems and (2) explicit\ncaching of operations through a new extension package, pyterrier-caching. These\napproaches allow for the best of both worlds: pipelines can be fully expressed\nend-to-end, while also avoiding redundant computations between pipelines."
                },
                "authors": [
                    {
                        "name": "Sean MacAvaney"
                    },
                    {
                        "name": "Craig Macdonald"
                    }
                ],
                "author_detail": {
                    "name": "Craig Macdonald"
                },
                "author": "Craig Macdonald",
                "arxiv_comment": "WOWS @ ECIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09952v1",
                "updated": "2025-04-14T07:30:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    7,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T07:30:03Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    7,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "title": "Secrecy and Privacy in Multi-Access Combinatorial Topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secrecy and Privacy in Multi-Access Combinatorial Topology"
                },
                "summary": "In this work, we consider the multi-access combinatorial topology with $C$\ncaches where each user accesses a unique set of $r$ caches. For this setup, we\nconsider secrecy, where each user should not know anything about the files it\ndid not request, and demand privacy, where each user's demand must be kept\nprivate from other non-colluding users. We propose a scheme satisfying both\nconditions and derive a lower bound based on cut-set arguments. Also, we prove\nthat our scheme is optimal when $r\\geq C-1$, and it is order-optimal when the\ncache memory size $M$ is greater than or equal to a certain threshold for\n$r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same\nrate as the one given by the secretive scheme for the dedicated cache setup by\nRavindrakumar et al. ( 'Private Coded Caching,' in \\textit{IEEE Transactions on\nInformation Forensics and Security}, 2018), while satisfying both secrecy and\ndemand privacy conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we consider the multi-access combinatorial topology with $C$\ncaches where each user accesses a unique set of $r$ caches. For this setup, we\nconsider secrecy, where each user should not know anything about the files it\ndid not request, and demand privacy, where each user's demand must be kept\nprivate from other non-colluding users. We propose a scheme satisfying both\nconditions and derive a lower bound based on cut-set arguments. Also, we prove\nthat our scheme is optimal when $r\\geq C-1$, and it is order-optimal when the\ncache memory size $M$ is greater than or equal to a certain threshold for\n$r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same\nrate as the one given by the secretive scheme for the dedicated cache setup by\nRavindrakumar et al. ( 'Private Coded Caching,' in \\textit{IEEE Transactions on\nInformation Forensics and Security}, 2018), while satisfying both secrecy and\ndemand privacy conditions."
                },
                "authors": [
                    {
                        "name": "Mallikharjuna Chinnapadamala"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "11 pages and 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09936v1",
                "updated": "2025-04-14T06:58:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    58,
                    0,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T06:58:00Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    58,
                    0,
                    0,
                    104,
                    0
                ],
                "title": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for\n  Efficient LLMs Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for\n  Efficient LLMs Inference"
                },
                "summary": "Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets."
                },
                "authors": [
                    {
                        "name": "Yuxuan Tian"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Yebo Peng"
                    },
                    {
                        "name": "Aomufei Yuan"
                    },
                    {
                        "name": "Zhiming Wang"
                    },
                    {
                        "name": "Bairen Yi"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12280v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12280v2",
                "updated": "2025-04-13T14:17:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    17,
                    57,
                    6,
                    103,
                    0
                ],
                "published": "2024-02-19T16:47:04Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    16,
                    47,
                    4,
                    0,
                    50,
                    0
                ],
                "title": "Plato: Plan to Efficiently Decode for Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plato: Plan to Efficiently Decode for Large Language Model Inference"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage tasks, but their inference incurs substantial computational and memory\noverhead. To improve efficiency, parallel decoding methods like\nSkeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent\nprocessing. However, these methods significantly compromise answer quality by\ntreating semantically linked sub-problems as independent. We propose Plato, a\nnovel approach that co-designs algorithms and systems for semantic-aware\nparallel decoding. Plato leverages LLMs to organize sub-problems into a\ndependency graph based on logical and causal relationships, enabling concurrent\ndecoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding\nstages, implements a global context cache, and carefully structures node\ninference prompts to maximize key-value cache reuse and minimize overhead. Our\nevaluations show that Plato improves throughput by 68% over autoregressive\ndecoding while achieving a 40% net win rate in answer quality. Compared to SoT,\nPlato demonstrates a remarkable 90% quality net-win rate. Ablation studies\nreveal that our pipeline design improves speedup by 29%, while our KV cache\nreuse optimization reduces overhead by 75%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage tasks, but their inference incurs substantial computational and memory\noverhead. To improve efficiency, parallel decoding methods like\nSkeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent\nprocessing. However, these methods significantly compromise answer quality by\ntreating semantically linked sub-problems as independent. We propose Plato, a\nnovel approach that co-designs algorithms and systems for semantic-aware\nparallel decoding. Plato leverages LLMs to organize sub-problems into a\ndependency graph based on logical and causal relationships, enabling concurrent\ndecoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding\nstages, implements a global context cache, and carefully structures node\ninference prompts to maximize key-value cache reuse and minimize overhead. Our\nevaluations show that Plato improves throughput by 68% over autoregressive\ndecoding while achieving a 40% net win rate in answer quality. Compared to SoT,\nPlato demonstrates a remarkable 90% quality net-win rate. Ablation studies\nreveal that our pipeline design improves speedup by 29%, while our KV cache\nreuse optimization reduces overhead by 75%."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Haizhong Zheng"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Atul Prakash"
                    },
                    {
                        "name": "Matthew Lentz"
                    },
                    {
                        "name": "Danyang Zhuo"
                    },
                    {
                        "name": "Feng Qian"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12280v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09590v1",
                "updated": "2025-04-13T14:16:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    16,
                    57,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T14:16:57Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    16,
                    57,
                    6,
                    103,
                    0
                ],
                "title": "Efficient LLM Serving on Hybrid Real-time and Best-effort Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Serving on Hybrid Real-time and Best-effort Requests"
                },
                "summary": "Recent breakthroughs in large Language Models (LLMs) have enabled various\ngenerative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT\n[27]) powered by an LLM often concurrently support latency-critical requests\nfor interactive applications (e.g., question-answering systems, referred to as\nreal-time or RT requests) and throughput-oriented requests for back-of-house\nprocessing (e.g., documents batch processing [28], referred to best-effort or\nBE requests), with complex hybrid inference workloads to the underlying model.\nState-of-the-art (SOTA) LLM serving systems dedicate machines to each type of\nrequest, towards either low inference latency or high serving throughput,\nrespectively. This practice simplifies request scheduling and management but\nsuffers from poor resource utilization. We propose BROS, a hybrid LLM serving\nsystem that aims to collocate RT/BE requests, meeting RT requests' latency\nrequirements while maintaining BE requests' throughput. BROS formulates the\nproblem of hybrid RT/BE request scheduling and solves it with a dynamic\npriority-based algorithm. BROS designs a bidirectional KV cache management\nmechanism, allowing RT requests to share KV memory with BE requests to remove\nthe scheduling restrictions caused by insufficient KV memory and improve\nutilization. Extensive experiments validate that BROS achieves a good trade-off\nwhen serving hybrid RT and BE requests. It significantly reduces the latency of\nRT requests (up to 74.20%), improving their fine-grained service level\nobjectives (SLOs) attainments (up to 36.38x), with negligible throughput\nreduction for BE requests, showing significant advantages over SOTA systems\nlike vLLM and TGI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in large Language Models (LLMs) have enabled various\ngenerative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT\n[27]) powered by an LLM often concurrently support latency-critical requests\nfor interactive applications (e.g., question-answering systems, referred to as\nreal-time or RT requests) and throughput-oriented requests for back-of-house\nprocessing (e.g., documents batch processing [28], referred to best-effort or\nBE requests), with complex hybrid inference workloads to the underlying model.\nState-of-the-art (SOTA) LLM serving systems dedicate machines to each type of\nrequest, towards either low inference latency or high serving throughput,\nrespectively. This practice simplifies request scheduling and management but\nsuffers from poor resource utilization. We propose BROS, a hybrid LLM serving\nsystem that aims to collocate RT/BE requests, meeting RT requests' latency\nrequirements while maintaining BE requests' throughput. BROS formulates the\nproblem of hybrid RT/BE request scheduling and solves it with a dynamic\npriority-based algorithm. BROS designs a bidirectional KV cache management\nmechanism, allowing RT requests to share KV memory with BE requests to remove\nthe scheduling restrictions caused by insufficient KV memory and improve\nutilization. Extensive experiments validate that BROS achieves a good trade-off\nwhen serving hybrid RT and BE requests. It significantly reduces the latency of\nRT requests (up to 74.20%), improving their fine-grained service level\nobjectives (SLOs) attainments (up to 36.38x), with negligible throughput\nreduction for BE requests, showing significant advantages over SOTA systems\nlike vLLM and TGI."
                },
                "authors": [
                    {
                        "name": "Wan Borui"
                    },
                    {
                        "name": "Zhao Juntao"
                    },
                    {
                        "name": "Jiang Chenyu"
                    },
                    {
                        "name": "Guo Chuanxiong"
                    },
                    {
                        "name": "Wu Chuan"
                    }
                ],
                "author_detail": {
                    "name": "Wu Chuan"
                },
                "author": "Wu Chuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v5",
                "updated": "2025-04-13T14:02:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    2,
                    47,
                    6,
                    103,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient Prefilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient Prefilling"
                },
                "summary": "We introduce Block-attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context in an\nauto-regressive manner. Instead, Block-attention divides retrieved documents\ninto discrete blocks, with each block independently calculating key-value (KV)\nstates except for the final block. In RAG scenarios, by defining each passage\nas a block, Block-attention enables us to reuse the KV states of passages that\nhave been seen before, thereby significantly reducing the latency and the\ncomputation overhead during inference. The implementation of Block-attention\ninvolves block segmentation, position re-encoding, and fine-tuning the LLM to\nadapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,\nincluding RAG, ICL, and general domains, demonstrate that after block\nfine-tuning, the Block-attention model not only achieves performance comparable\nto that of full-attention models, but can also seamlessly switch between the\nblock and full attention modes without any performance loss. Notably,\nBlock-attention significantly reduces the time to first token (TTFT) and\nfloating point operations (FLOPs) to a very low level. It only takes 45 ms to\noutput the first token for an input sequence with a total length of 32K.\nCompared to the full-attention models, the TTFT and corresponding FLOPs are\nreduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we\nelaborate on how Block-attention is applied in Game AI scenario and the\nsubstantial potential benefits it entails. We strongly suggest researchers in\nthe gaming field not to overlook this section.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context in an\nauto-regressive manner. Instead, Block-attention divides retrieved documents\ninto discrete blocks, with each block independently calculating key-value (KV)\nstates except for the final block. In RAG scenarios, by defining each passage\nas a block, Block-attention enables us to reuse the KV states of passages that\nhave been seen before, thereby significantly reducing the latency and the\ncomputation overhead during inference. The implementation of Block-attention\ninvolves block segmentation, position re-encoding, and fine-tuning the LLM to\nadapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,\nincluding RAG, ICL, and general domains, demonstrate that after block\nfine-tuning, the Block-attention model not only achieves performance comparable\nto that of full-attention models, but can also seamlessly switch between the\nblock and full attention modes without any performance loss. Notably,\nBlock-attention significantly reduces the time to first token (TTFT) and\nfloating point operations (FLOPs) to a very low level. It only takes 45 ms to\noutput the first token for an input sequence with a total length of 32K.\nCompared to the full-attention models, the TTFT and corresponding FLOPs are\nreduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we\nelaborate on how Block-attention is applied in Game AI scenario and the\nsubstantial potential benefits it entails. We strongly suggest researchers in\nthe gaming field not to overlook this section."
                },
                "authors": [
                    {
                        "name": "Dongyang Ma"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10540v1",
                "updated": "2025-04-13T08:29:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    8,
                    29,
                    58,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T08:29:58Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    8,
                    29,
                    58,
                    6,
                    103,
                    0
                ],
                "title": "AB-Cache: Training-Free Acceleration of Diffusion Models via\n  Adams-Bashforth Cached Feature Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AB-Cache: Training-Free Acceleration of Diffusion Models via\n  Adams-Bashforth Cached Feature Reuse"
                },
                "summary": "Diffusion models have demonstrated remarkable success in generative tasks,\nyet their iterative denoising process results in slow inference, limiting their\npracticality. While existing acceleration methods exploit the well-known\nU-shaped similarity pattern between adjacent steps through caching mechanisms,\nthey lack theoretical foundation and rely on simplistic computation reuse,\noften leading to performance degradation. In this work, we provide a\ntheoretical understanding by analyzing the denoising process through the\nsecond-order Adams-Bashforth method, revealing a linear relationship between\nthe outputs of consecutive steps. This analysis explains why the outputs of\nadjacent steps exhibit a U-shaped pattern. Furthermore, extending\nAdams-Bashforth method to higher order, we propose a novel caching-based\nacceleration approach for diffusion models, instead of directly reusing cached\nresults, with a truncation error bound of only \\(O(h^k)\\) where $h$ is the step\nsize. Extensive validation across diverse image and video diffusion models\n(including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates\nour method's effectiveness in achieving nearly $3\\times$ speedup while\nmaintaining original performance levels, offering a practical real-time\nsolution without compromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated remarkable success in generative tasks,\nyet their iterative denoising process results in slow inference, limiting their\npracticality. While existing acceleration methods exploit the well-known\nU-shaped similarity pattern between adjacent steps through caching mechanisms,\nthey lack theoretical foundation and rely on simplistic computation reuse,\noften leading to performance degradation. In this work, we provide a\ntheoretical understanding by analyzing the denoising process through the\nsecond-order Adams-Bashforth method, revealing a linear relationship between\nthe outputs of consecutive steps. This analysis explains why the outputs of\nadjacent steps exhibit a U-shaped pattern. Furthermore, extending\nAdams-Bashforth method to higher order, we propose a novel caching-based\nacceleration approach for diffusion models, instead of directly reusing cached\nresults, with a truncation error bound of only \\(O(h^k)\\) where $h$ is the step\nsize. Extensive validation across diverse image and video diffusion models\n(including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates\nour method's effectiveness in achieving nearly $3\\times$ speedup while\nmaintaining original performance levels, offering a practical real-time\nsolution without compromising generation quality."
                },
                "authors": [
                    {
                        "name": "Zichao Yu"
                    },
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Guojiang Shao"
                    },
                    {
                        "name": "Chengwei Zhang"
                    },
                    {
                        "name": "Shengze Xu"
                    },
                    {
                        "name": "Jie Huang"
                    },
                    {
                        "name": "Feng Zhao"
                    },
                    {
                        "name": "Xiaodong Cun"
                    },
                    {
                        "name": "Wenyi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenyi Zhang"
                },
                "author": "Wenyi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09431v1",
                "updated": "2025-04-13T04:46:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    4,
                    46,
                    2,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T04:46:02Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    4,
                    46,
                    2,
                    6,
                    103,
                    0
                ],
                "title": "Sub-nanosecond in-plane magnetization switching induced by field-like\n  spin-orbit torques from ferromagnets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-nanosecond in-plane magnetization switching induced by field-like\n  spin-orbit torques from ferromagnets"
                },
                "summary": "Spin-orbit torques (SOTs) generated in SOT-material/ferromagnet structures\nare classified as damping-like (DL) and field-like (FL) torques for\ncurrent-driven magnetization switching. It is well known that both DL- and\nFL-SOTs originate from the SOT-material and DL-SOT dominates the current-driven\nswitching process while FL-SOT contributes limitedly, resulting in an\nincubation time (several nanoseconds) during collinear magnetization switching\nwith the spin polarization because of the DL attributes. Here we report a\nFL-SOT originated from the ferromagnet, different from the origin of DL-SOT,\nand demonstrate that it dominates the collinear magnetization switching. We\nshow that the FL-SOT and resultant collinear switching can be modulated, one\norder of magnitude and sign reversal, by controlling the ferromagnet. Because\nof no incubation time and higher charge-to-spin efficiencies in the FL\nswitching, we further show that the switching time can be down to 200 ps with\none order lower critical switching current density compared to DL switching.\nThese results indicate that the FL switching may provide a practical solution\nfor magnetic memory in speed-priority cache applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spin-orbit torques (SOTs) generated in SOT-material/ferromagnet structures\nare classified as damping-like (DL) and field-like (FL) torques for\ncurrent-driven magnetization switching. It is well known that both DL- and\nFL-SOTs originate from the SOT-material and DL-SOT dominates the current-driven\nswitching process while FL-SOT contributes limitedly, resulting in an\nincubation time (several nanoseconds) during collinear magnetization switching\nwith the spin polarization because of the DL attributes. Here we report a\nFL-SOT originated from the ferromagnet, different from the origin of DL-SOT,\nand demonstrate that it dominates the collinear magnetization switching. We\nshow that the FL-SOT and resultant collinear switching can be modulated, one\norder of magnitude and sign reversal, by controlling the ferromagnet. Because\nof no incubation time and higher charge-to-spin efficiencies in the FL\nswitching, we further show that the switching time can be down to 200 ps with\none order lower critical switching current density compared to DL switching.\nThese results indicate that the FL switching may provide a practical solution\nfor magnetic memory in speed-priority cache applications."
                },
                "authors": [
                    {
                        "name": "Hanying Zhang"
                    },
                    {
                        "name": "Ziqian Cui"
                    },
                    {
                        "name": "Baiqing Jiang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "C. Bi"
                    }
                ],
                "author_detail": {
                    "name": "C. Bi"
                },
                "author": "C. Bi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09261v1",
                "updated": "2025-04-12T15:42:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    42,
                    17,
                    5,
                    102,
                    0
                ],
                "published": "2025-04-12T15:42:17Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    42,
                    17,
                    5,
                    102,
                    0
                ],
                "title": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive\n  Modeling"
                },
                "summary": "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively."
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Youru Lv"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Zeren Zhang"
                    },
                    {
                        "name": "Danping Zou"
                    },
                    {
                        "name": "Weiyao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weiyao Lin"
                },
                "author": "Weiyao Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17459v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v3",
                "updated": "2025-04-11T12:31:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    12,
                    31,
                    7,
                    4,
                    101,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08378v1",
                "updated": "2025-04-11T09:26:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T09:26:47Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "title": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods."
                },
                "authors": [
                    {
                        "name": "Fucheng Jia"
                    },
                    {
                        "name": "Zewen Wu"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Ju Ren"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Ting Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ting Cao"
                },
                "author": "Ting Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08204v1",
                "updated": "2025-04-11T02:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    10,
                    2,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T02:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    10,
                    2,
                    4,
                    101,
                    0
                ],
                "title": "II-NVM: Enhancing Map Accuracy and Consistency with Normal\n  Vector-Assisted Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "II-NVM: Enhancing Map Accuracy and Consistency with Normal\n  Vector-Assisted Mapping"
                },
                "summary": "SLAM technology plays a crucial role in indoor mapping and localization. A\ncommon challenge in indoor environments is the \"double-sided mapping issue\",\nwhere closely positioned walls, doors, and other surfaces are mistakenly\nidentified as a single plane, significantly hindering map accuracy and\nconsistency. To address this issue this paper introduces a SLAM approach that\nensures accurate mapping using normal vector consistency. We enhance the voxel\nmap structure to store both point cloud data and normal vector information,\nenabling the system to evaluate consistency during nearest neighbor searches\nand map updates. This process distinguishes between the front and back sides of\nsurfaces, preventing incorrect point-to-plane constraints. Moreover, we\nimplement an adaptive radius KD-tree search method that dynamically adjusts the\nsearch radius based on the local density of the point cloud, thereby enhancing\nthe accuracy of normal vector calculations. To further improve realtime\nperformance and storage efficiency, we incorporate a Least Recently Used (LRU)\ncache strategy, which facilitates efficient incremental updates of the voxel\nmap. The code is released as open-source and validated in both simulated\nenvironments and real indoor scenarios. Experimental results demonstrate that\nthis approach effectively resolves the \"double-sided mapping issue\" and\nsignificantly improves mapping precision. Additionally, we have developed and\nopen-sourced the first simulation and real world dataset specifically tailored\nfor the \"double-sided mapping issue\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLAM technology plays a crucial role in indoor mapping and localization. A\ncommon challenge in indoor environments is the \"double-sided mapping issue\",\nwhere closely positioned walls, doors, and other surfaces are mistakenly\nidentified as a single plane, significantly hindering map accuracy and\nconsistency. To address this issue this paper introduces a SLAM approach that\nensures accurate mapping using normal vector consistency. We enhance the voxel\nmap structure to store both point cloud data and normal vector information,\nenabling the system to evaluate consistency during nearest neighbor searches\nand map updates. This process distinguishes between the front and back sides of\nsurfaces, preventing incorrect point-to-plane constraints. Moreover, we\nimplement an adaptive radius KD-tree search method that dynamically adjusts the\nsearch radius based on the local density of the point cloud, thereby enhancing\nthe accuracy of normal vector calculations. To further improve realtime\nperformance and storage efficiency, we incorporate a Least Recently Used (LRU)\ncache strategy, which facilitates efficient incremental updates of the voxel\nmap. The code is released as open-source and validated in both simulated\nenvironments and real indoor scenarios. Experimental results demonstrate that\nthis approach effectively resolves the \"double-sided mapping issue\" and\nsignificantly improves mapping precision. Additionally, we have developed and\nopen-sourced the first simulation and real world dataset specifically tailored\nfor the \"double-sided mapping issue\"."
                },
                "authors": [
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Yina Jian"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Linji Wang"
                    },
                    {
                        "name": "Yongxin Ma"
                    },
                    {
                        "name": "Xinglai Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xinglai Jin"
                },
                "author": "Xinglai Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07596v2",
                "updated": "2025-04-11T02:05:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    5,
                    1,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-10T09:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    48,
                    56,
                    3,
                    100,
                    0
                ],
                "title": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution"
                },
                "summary": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward."
                },
                "authors": [
                    {
                        "name": "Zen Kit Heng"
                    },
                    {
                        "name": "Zimeng Zhao"
                    },
                    {
                        "name": "Tianhao Wu"
                    },
                    {
                        "name": "Yuanfei Wang"
                    },
                    {
                        "name": "Mingdong Wu"
                    },
                    {
                        "name": "Yangang Wang"
                    },
                    {
                        "name": "Hao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Hao Dong"
                },
                "author": "Hao Dong",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13915v1",
                "updated": "2025-04-10T17:13:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    13,
                    8,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T17:13:08Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    13,
                    8,
                    3,
                    100,
                    0
                ],
                "title": "Memory-efficient Streaming VideoLLMs for Real-time Procedural Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-efficient Streaming VideoLLMs for Real-time Procedural Video\n  Understanding"
                },
                "summary": "We introduce ProVideLLM, an end-to-end framework for real-time procedural\nvideo understanding. ProVideLLM integrates a multimodal cache configured to\nstore two types of tokens - verbalized text tokens, which provide compressed\ntextual summaries of long-term observations, and visual tokens, encoded with\nDETR-QFormer to capture fine-grained details from short-term observations. This\ndesign reduces token count by 22x over existing methods in representing one\nhour of long-term observations while effectively encoding fine-granularity of\nthe present. By interleaving these tokens in our multimodal cache, ProVideLLM\nensures sub-linear scaling of memory and compute with video length, enabling\nper-frame streaming inference at 10 FPS and streaming dialogue at 25 FPS, with\na minimal 2GB GPU memory footprint. ProVideLLM also sets new state-of-the-art\nresults on six procedural tasks across four datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ProVideLLM, an end-to-end framework for real-time procedural\nvideo understanding. ProVideLLM integrates a multimodal cache configured to\nstore two types of tokens - verbalized text tokens, which provide compressed\ntextual summaries of long-term observations, and visual tokens, encoded with\nDETR-QFormer to capture fine-grained details from short-term observations. This\ndesign reduces token count by 22x over existing methods in representing one\nhour of long-term observations while effectively encoding fine-granularity of\nthe present. By interleaving these tokens in our multimodal cache, ProVideLLM\nensures sub-linear scaling of memory and compute with video length, enabling\nper-frame streaming inference at 10 FPS and streaming dialogue at 25 FPS, with\na minimal 2GB GPU memory footprint. ProVideLLM also sets new state-of-the-art\nresults on six procedural tasks across four datasets."
                },
                "authors": [
                    {
                        "name": "Dibyadip Chatterjee"
                    },
                    {
                        "name": "Edoardo Remelli"
                    },
                    {
                        "name": "Yale Song"
                    },
                    {
                        "name": "Bugra Tekin"
                    },
                    {
                        "name": "Abhay Mittal"
                    },
                    {
                        "name": "Bharat Bhatnagar"
                    },
                    {
                        "name": "Necati Cihan Camgz"
                    },
                    {
                        "name": "Shreyas Hampali"
                    },
                    {
                        "name": "Eric Sauser"
                    },
                    {
                        "name": "Shugao Ma"
                    },
                    {
                        "name": "Angela Yao"
                    },
                    {
                        "name": "Fadime Sener"
                    }
                ],
                "author_detail": {
                    "name": "Fadime Sener"
                },
                "author": "Fadime Sener",
                "arxiv_comment": "13 pages, 5 figures; https://dibschat.github.io/ProVideLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07815v1",
                "updated": "2025-04-10T14:52:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    52,
                    3,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T14:52:03Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    52,
                    3,
                    3,
                    100,
                    0
                ],
                "title": "Siren Federate: Bridging document, relational, and graph models for\n  exploratory graph analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Siren Federate: Bridging document, relational, and graph models for\n  exploratory graph analysis"
                },
                "summary": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes."
                },
                "authors": [
                    {
                        "name": "Georgeta Bordea"
                    },
                    {
                        "name": "Stephane Campinas"
                    },
                    {
                        "name": "Matteo Catena"
                    },
                    {
                        "name": "Renaud Delbru"
                    }
                ],
                "author_detail": {
                    "name": "Renaud Delbru"
                },
                "author": "Renaud Delbru",
                "arxiv_comment": "36 pages, 16 figures, submitted to the ComSIS journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.11; E.1; H.2.4; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07642v1",
                "updated": "2025-04-10T10:43:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    43,
                    42,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T10:43:42Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    43,
                    42,
                    3,
                    100,
                    0
                ],
                "title": "Cache-a-lot: Pushing the Limits of Unsatisfiable Core Reuse in SMT-Based\n  Program Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-a-lot: Pushing the Limits of Unsatisfiable Core Reuse in SMT-Based\n  Program Analysis"
                },
                "summary": "Satisfiability Modulo Theories (SMT) solvers are integral to program analysis\ntechniques like concolic and symbolic execution, where they help assess the\nsatisfiability of logical formulae to explore execution paths of the program\nunder test. However, frequent solver invocations are still the main performance\nbottleneck of these techniques. One way to mitigate this challenge is through\noptimizations such as caching and reusing solver results. While current methods\ntypically focus on reusing results from fully equivalent or closely related\nformulas, they often miss broader opportunities for reuse. In this paper, we\npropose a novel approach, Cache-a-lot, that extends the reuse of unsatisfiable\n(unsat) results by systematically considering all possible variable\nsubstitutions. This enables more extensive reuse of results, thereby reducing\nthe number of SMT solver invocations and improving the overall efficiency of\nconcolic and symbolic execution. Our evaluation, conducted against the\nstate-of-the-art Utopia solution using two benchmark sets, shows significant\nimprovements, particularly with more complex formulas. Our method achieves up\nto 74% unsat core reuse, compared to Utopia's 41%, and significant increase in\nthe time savings. These results demonstrate that, despite the additional\ncomputational complexity, the broader reuse of unsat results significantly\nenhances performance, offering valuable advancements for formal verification\nand program analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satisfiability Modulo Theories (SMT) solvers are integral to program analysis\ntechniques like concolic and symbolic execution, where they help assess the\nsatisfiability of logical formulae to explore execution paths of the program\nunder test. However, frequent solver invocations are still the main performance\nbottleneck of these techniques. One way to mitigate this challenge is through\noptimizations such as caching and reusing solver results. While current methods\ntypically focus on reusing results from fully equivalent or closely related\nformulas, they often miss broader opportunities for reuse. In this paper, we\npropose a novel approach, Cache-a-lot, that extends the reuse of unsatisfiable\n(unsat) results by systematically considering all possible variable\nsubstitutions. This enables more extensive reuse of results, thereby reducing\nthe number of SMT solver invocations and improving the overall efficiency of\nconcolic and symbolic execution. Our evaluation, conducted against the\nstate-of-the-art Utopia solution using two benchmark sets, shows significant\nimprovements, particularly with more complex formulas. Our method achieves up\nto 74% unsat core reuse, compared to Utopia's 41%, and significant increase in\nthe time savings. These results demonstrate that, despite the additional\ncomputational complexity, the broader reuse of unsat results significantly\nenhances performance, offering valuable advancements for formal verification\nand program analysis."
                },
                "authors": [
                    {
                        "name": "Rustam Sadykov"
                    },
                    {
                        "name": "Azat Abdullin"
                    },
                    {
                        "name": "Marat Akhin"
                    }
                ],
                "author_detail": {
                    "name": "Marat Akhin"
                },
                "author": "Marat Akhin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07494v1",
                "updated": "2025-04-10T06:51:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:51:23Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "title": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving"
                },
                "summary": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems."
                },
                "authors": [
                    {
                        "name": "Shihong Gao"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Yanyan Shen"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_doi": "10.1145/3725394",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725394",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.07494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07479v1",
                "updated": "2025-04-10T06:13:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:13:30Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "title": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference"
                },
                "summary": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference."
                },
                "authors": [
                    {
                        "name": "Weikai Xu"
                    },
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Qianqian Huang"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Ru Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ru Huang"
                },
                "author": "Ru Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v3",
                "updated": "2025-04-10T05:06:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    6,
                    29,
                    3,
                    100,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "arxiv_comment": "MLSys 2025 camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.20040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20040v1",
                "updated": "2025-04-28T17:59:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    59,
                    52,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T17:59:52Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    59,
                    52,
                    0,
                    118,
                    0
                ],
                "title": "MP-SfM: Monocular Surface Priors for Robust Structure-from-Motion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MP-SfM: Monocular Surface Priors for Robust Structure-from-Motion"
                },
                "summary": "While Structure-from-Motion (SfM) has seen much progress over the years,\nstate-of-the-art systems are prone to failure when facing extreme viewpoint\nchanges in low-overlap, low-parallax or high-symmetry scenarios. Because\ncapturing images that avoid these pitfalls is challenging, this severely limits\nthe wider use of SfM, especially by non-expert users. We overcome these\nlimitations by augmenting the classical SfM paradigm with monocular depth and\nnormal priors inferred by deep neural networks. Thanks to a tight integration\nof monocular and multi-view constraints, our approach significantly outperforms\nexisting ones under extreme viewpoint changes, while maintaining strong\nperformance in standard conditions. We also show that monocular priors can help\nreject faulty associations due to symmetries, which is a long-standing problem\nfor SfM. This makes our approach the first capable of reliably reconstructing\nchallenging indoor environments from few images. Through principled uncertainty\npropagation, it is robust to errors in the priors, can handle priors inferred\nby different models with little tuning, and will thus easily benefit from\nfuture progress in monocular depth and normal estimation. Our code is publicly\navailable at https://github.com/cvg/mpsfm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Structure-from-Motion (SfM) has seen much progress over the years,\nstate-of-the-art systems are prone to failure when facing extreme viewpoint\nchanges in low-overlap, low-parallax or high-symmetry scenarios. Because\ncapturing images that avoid these pitfalls is challenging, this severely limits\nthe wider use of SfM, especially by non-expert users. We overcome these\nlimitations by augmenting the classical SfM paradigm with monocular depth and\nnormal priors inferred by deep neural networks. Thanks to a tight integration\nof monocular and multi-view constraints, our approach significantly outperforms\nexisting ones under extreme viewpoint changes, while maintaining strong\nperformance in standard conditions. We also show that monocular priors can help\nreject faulty associations due to symmetries, which is a long-standing problem\nfor SfM. This makes our approach the first capable of reliably reconstructing\nchallenging indoor environments from few images. Through principled uncertainty\npropagation, it is robust to errors in the priors, can handle priors inferred\nby different models with little tuning, and will thus easily benefit from\nfuture progress in monocular depth and normal estimation. Our code is publicly\navailable at https://github.com/cvg/mpsfm."
                },
                "authors": [
                    {
                        "name": "Zador Pataki"
                    },
                    {
                        "name": "Paul-Edouard Sarlin"
                    },
                    {
                        "name": "Johannes L. Schnberger"
                    },
                    {
                        "name": "Marc Pollefeys"
                    }
                ],
                "author_detail": {
                    "name": "Marc Pollefeys"
                },
                "author": "Marc Pollefeys",
                "arxiv_comment": "CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20039v1",
                "updated": "2025-04-28T17:59:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    59,
                    28,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T17:59:28Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    59,
                    28,
                    0,
                    118,
                    0
                ],
                "title": "AutoJudge: Judge Decoding Without Manual Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoJudge: Judge Decoding Without Manual Annotation"
                },
                "summary": "We introduce AutoJudge, a framework that accelerates large language model\n(LLM) inference with task-specific lossy speculative decoding. Instead of\nmatching the original model output distribution token-by-token, we identify\nwhich of the generated tokens affect the downstream quality of the generated\nresponse, relaxing the guarantee so that the \"unimportant\" tokens can be\ngenerated faster. Our approach relies on a semi-greedy search algorithm to test\nwhich of the mismatches between target and draft model should be corrected to\npreserve quality, and which ones may be skipped. We then train a lightweight\nclassifier based on existing LLM embeddings to predict, at inference time,\nwhich mismatching tokens can be safely accepted without compromising the final\nanswer quality. We test our approach with Llama 3.2 1B (draft) and Llama 3.1 8B\n(target) models on zero-shot GSM8K reasoning, where it achieves up to 1.5x more\naccepted tokens per verification cycle with under 1% degradation in answer\naccuracy compared to standard speculative decoding and over 2x with small loss\nin accuracy. When applied to the LiveCodeBench benchmark, our approach\nautomatically detects other, programming-specific important tokens and shows\nsimilar speedups, demonstrating its ability to generalize across tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce AutoJudge, a framework that accelerates large language model\n(LLM) inference with task-specific lossy speculative decoding. Instead of\nmatching the original model output distribution token-by-token, we identify\nwhich of the generated tokens affect the downstream quality of the generated\nresponse, relaxing the guarantee so that the \"unimportant\" tokens can be\ngenerated faster. Our approach relies on a semi-greedy search algorithm to test\nwhich of the mismatches between target and draft model should be corrected to\npreserve quality, and which ones may be skipped. We then train a lightweight\nclassifier based on existing LLM embeddings to predict, at inference time,\nwhich mismatching tokens can be safely accepted without compromising the final\nanswer quality. We test our approach with Llama 3.2 1B (draft) and Llama 3.1 8B\n(target) models on zero-shot GSM8K reasoning, where it achieves up to 1.5x more\naccepted tokens per verification cycle with under 1% degradation in answer\naccuracy compared to standard speculative decoding and over 2x with small loss\nin accuracy. When applied to the LiveCodeBench benchmark, our approach\nautomatically detects other, programming-specific important tokens and shows\nsimilar speedups, demonstrating its ability to generalize across tasks."
                },
                "authors": [
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Fedor Velikonivtsev"
                    },
                    {
                        "name": "Ruslan Svirschevski"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Max Ryabinin"
                    }
                ],
                "author_detail": {
                    "name": "Max Ryabinin"
                },
                "author": "Max Ryabinin",
                "arxiv_comment": "Preprint, Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20026v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20026v1",
                "updated": "2025-04-28T17:48:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    48,
                    58,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T17:48:58Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    48,
                    58,
                    0,
                    118,
                    0
                ],
                "title": "LIRM: Large Inverse Rendering Model for Progressive Reconstruction of\n  Shape, Materials and View-dependent Radiance Fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LIRM: Large Inverse Rendering Model for Progressive Reconstruction of\n  Shape, Materials and View-dependent Radiance Fields"
                },
                "summary": "We present Large Inverse Rendering Model (LIRM), a transformer architecture\nthat jointly reconstructs high-quality shape, materials, and radiance fields\nwith view-dependent effects in less than a second. Our model builds upon the\nrecent Large Reconstruction Models (LRMs) that achieve state-of-the-art\nsparse-view reconstruction quality. However, existing LRMs struggle to\nreconstruct unseen parts accurately and cannot recover glossy appearance or\ngenerate relightable 3D contents that can be consumed by standard Graphics\nengines. To address these limitations, we make three key technical\ncontributions to build a more practical multi-view 3D reconstruction framework.\nFirst, we introduce an update model that allows us to progressively add more\ninput views to improve our reconstruction. Second, we propose a hexa-plane\nneural SDF representation to better recover detailed textures, geometry and\nmaterial parameters. Third, we develop a novel neural directional-embedding\nmechanism to handle view-dependent effects. Trained on a large-scale shape and\nmaterial dataset with a tailored coarse-to-fine training scheme, our model\nachieves compelling results. It compares favorably to optimization-based\ndense-view inverse rendering methods in terms of geometry and relighting\naccuracy, while requiring only a fraction of the inference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Large Inverse Rendering Model (LIRM), a transformer architecture\nthat jointly reconstructs high-quality shape, materials, and radiance fields\nwith view-dependent effects in less than a second. Our model builds upon the\nrecent Large Reconstruction Models (LRMs) that achieve state-of-the-art\nsparse-view reconstruction quality. However, existing LRMs struggle to\nreconstruct unseen parts accurately and cannot recover glossy appearance or\ngenerate relightable 3D contents that can be consumed by standard Graphics\nengines. To address these limitations, we make three key technical\ncontributions to build a more practical multi-view 3D reconstruction framework.\nFirst, we introduce an update model that allows us to progressively add more\ninput views to improve our reconstruction. Second, we propose a hexa-plane\nneural SDF representation to better recover detailed textures, geometry and\nmaterial parameters. Third, we develop a novel neural directional-embedding\nmechanism to handle view-dependent effects. Trained on a large-scale shape and\nmaterial dataset with a tailored coarse-to-fine training scheme, our model\nachieves compelling results. It compares favorably to optimization-based\ndense-view inverse rendering methods in terms of geometry and relighting\naccuracy, while requiring only a fraction of the inference time."
                },
                "authors": [
                    {
                        "name": "Zhengqin Li"
                    },
                    {
                        "name": "Dilin Wang"
                    },
                    {
                        "name": "Ka Chen"
                    },
                    {
                        "name": "Zhaoyang Lv"
                    },
                    {
                        "name": "Thu Nguyen-Phuoc"
                    },
                    {
                        "name": "Milim Lee"
                    },
                    {
                        "name": "Jia-Bin Huang"
                    },
                    {
                        "name": "Lei Xiao"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Yufeng Zhu"
                    },
                    {
                        "name": "Carl S. Marshall"
                    },
                    {
                        "name": "Yufeng Ren"
                    },
                    {
                        "name": "Richard Newcombe"
                    },
                    {
                        "name": "Zhao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Dong"
                },
                "author": "Zhao Dong",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20026v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20026v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20022v1",
                "updated": "2025-04-28T17:48:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    48,
                    13,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T17:48:13Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    48,
                    13,
                    0,
                    118,
                    0
                ],
                "title": "Better To Ask in English? Evaluating Factual Accuracy of Multilingual\n  LLMs in English and Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better To Ask in English? Evaluating Factual Accuracy of Multilingual\n  LLMs in English and Low-Resource Languages"
                },
                "summary": "Multilingual Large Language Models (LLMs) have demonstrated significant\neffectiveness across various languages, particularly in high-resource languages\nsuch as English. However, their performance in terms of factual accuracy across\nother low-resource languages, especially Indic languages, remains an area of\ninvestigation. In this study, we assess the factual accuracy of LLMs - GPT-4o,\nGemma-2-9B, Gemma-2-2B, and Llama-3.1-8B - by comparing their performance in\nEnglish and Indic languages using the IndicQuest dataset, which contains\nquestion-answer pairs in English and 19 Indic languages. By asking the same\nquestions in English and their respective Indic translations, we analyze\nwhether the models are more reliable for regional context questions in Indic\nlanguages or when operating in English. Our findings reveal that LLMs often\nperform better in English, even for questions rooted in Indic contexts.\nNotably, we observe a higher tendency for hallucination in responses generated\nin low-resource Indic languages, highlighting challenges in the multilingual\nunderstanding capabilities of current LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Large Language Models (LLMs) have demonstrated significant\neffectiveness across various languages, particularly in high-resource languages\nsuch as English. However, their performance in terms of factual accuracy across\nother low-resource languages, especially Indic languages, remains an area of\ninvestigation. In this study, we assess the factual accuracy of LLMs - GPT-4o,\nGemma-2-9B, Gemma-2-2B, and Llama-3.1-8B - by comparing their performance in\nEnglish and Indic languages using the IndicQuest dataset, which contains\nquestion-answer pairs in English and 19 Indic languages. By asking the same\nquestions in English and their respective Indic translations, we analyze\nwhether the models are more reliable for regional context questions in Indic\nlanguages or when operating in English. Our findings reveal that LLMs often\nperform better in English, even for questions rooted in Indic contexts.\nNotably, we observe a higher tendency for hallucination in responses generated\nin low-resource Indic languages, highlighting challenges in the multilingual\nunderstanding capabilities of current LLMs."
                },
                "authors": [
                    {
                        "name": "Pritika Rohera"
                    },
                    {
                        "name": "Chaitrali Ginimav"
                    },
                    {
                        "name": "Gayatri Sawant"
                    },
                    {
                        "name": "Raviraj Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Raviraj Joshi"
                },
                "author": "Raviraj Joshi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20020v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20020v1",
                "updated": "2025-04-28T17:42:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    42,
                    2,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T17:42:02Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    42,
                    2,
                    0,
                    118,
                    0
                ],
                "title": "Modular Machine Learning: An Indispensable Path towards New-Generation\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modular Machine Learning: An Indispensable Path towards New-Generation\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) have dramatically advanced machine learning\nresearch including natural language processing, computer vision, data mining,\netc., yet they still exhibit critical limitations in reasoning, factual\nconsistency, and interpretability. In this paper, we introduce a novel learning\nparadigm -- Modular Machine Learning (MML) -- as an essential approach toward\nnew-generation LLMs. MML decomposes the complex structure of LLMs into three\ninterdependent components: modular representation, modular model, and modular\nreasoning, aiming to enhance LLMs' capability of counterfactual reasoning,\nmitigating hallucinations, as well as promoting fairness, safety, and\ntransparency. Specifically, the proposed MML paradigm can: i) clarify the\ninternal working mechanism of LLMs through the disentanglement of semantic\ncomponents; ii) allow for flexible and task-adaptive model design; iii) enable\ninterpretable and logic-driven decision-making process. We present a feasible\nimplementation of MML-based LLMs via leveraging advanced techniques such as\ndisentangled representation learning, neural architecture search and\nneuro-symbolic learning. We critically identify key challenges, such as the\nintegration of continuous neural and discrete symbolic processes, joint\noptimization, and computational scalability, present promising future research\ndirections that deserve further exploration. Ultimately, the integration of the\nMML paradigm with LLMs has the potential to bridge the gap between statistical\n(deep) learning and formal (logical) reasoning, thereby paving the way for\nrobust, adaptable, and trustworthy AI systems across a wide range of real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have dramatically advanced machine learning\nresearch including natural language processing, computer vision, data mining,\netc., yet they still exhibit critical limitations in reasoning, factual\nconsistency, and interpretability. In this paper, we introduce a novel learning\nparadigm -- Modular Machine Learning (MML) -- as an essential approach toward\nnew-generation LLMs. MML decomposes the complex structure of LLMs into three\ninterdependent components: modular representation, modular model, and modular\nreasoning, aiming to enhance LLMs' capability of counterfactual reasoning,\nmitigating hallucinations, as well as promoting fairness, safety, and\ntransparency. Specifically, the proposed MML paradigm can: i) clarify the\ninternal working mechanism of LLMs through the disentanglement of semantic\ncomponents; ii) allow for flexible and task-adaptive model design; iii) enable\ninterpretable and logic-driven decision-making process. We present a feasible\nimplementation of MML-based LLMs via leveraging advanced techniques such as\ndisentangled representation learning, neural architecture search and\nneuro-symbolic learning. We critically identify key challenges, such as the\nintegration of continuous neural and discrete symbolic processes, joint\noptimization, and computational scalability, present promising future research\ndirections that deserve further exploration. Ultimately, the integration of the\nMML paradigm with LLMs has the potential to bridge the gap between statistical\n(deep) learning and formal (logical) reasoning, thereby paving the way for\nrobust, adaptable, and trustworthy AI systems across a wide range of real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Zeyang Zhang"
                    },
                    {
                        "name": "Haibo Chen"
                    },
                    {
                        "name": "Wenwu Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Wenwu Zhu"
                },
                "author": "Wenwu Zhu",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20020v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05779v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05779v3",
                "updated": "2025-04-28T17:36:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    36,
                    27,
                    0,
                    118,
                    0
                ],
                "published": "2024-10-08T08:00:12Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    8,
                    0,
                    12,
                    1,
                    282,
                    0
                ],
                "title": "LightRAG: Simple and Fast Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightRAG: Simple and Fast Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge sources, enabling more accurate and\ncontextually relevant responses tailored to user needs. However, existing RAG\nsystems have significant limitations, including reliance on flat data\nrepresentations and inadequate contextual awareness, which can lead to\nfragmented answers that fail to capture complex inter-dependencies. To address\nthese challenges, we propose LightRAG, which incorporates graph structures into\ntext indexing and retrieval processes. This innovative framework employs a\ndual-level retrieval system that enhances comprehensive information retrieval\nfrom both low-level and high-level knowledge discovery. Additionally, the\nintegration of graph structures with vector representations facilitates\nefficient retrieval of related entities and their relationships, significantly\nimproving response times while maintaining contextual relevance. This\ncapability is further enhanced by an incremental update algorithm that ensures\nthe timely integration of new data, allowing the system to remain effective and\nresponsive in rapidly changing data environments. Extensive experimental\nvalidation demonstrates considerable improvements in retrieval accuracy and\nefficiency compared to existing approaches. We have made our LightRAG\nopen-source and available at the link: https://github.com/HKUDS/LightRAG",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge sources, enabling more accurate and\ncontextually relevant responses tailored to user needs. However, existing RAG\nsystems have significant limitations, including reliance on flat data\nrepresentations and inadequate contextual awareness, which can lead to\nfragmented answers that fail to capture complex inter-dependencies. To address\nthese challenges, we propose LightRAG, which incorporates graph structures into\ntext indexing and retrieval processes. This innovative framework employs a\ndual-level retrieval system that enhances comprehensive information retrieval\nfrom both low-level and high-level knowledge discovery. Additionally, the\nintegration of graph structures with vector representations facilitates\nefficient retrieval of related entities and their relationships, significantly\nimproving response times while maintaining contextual relevance. This\ncapability is further enhanced by an incremental update algorithm that ensures\nthe timely integration of new data, allowing the system to remain effective and\nresponsive in rapidly changing data environments. Extensive experimental\nvalidation demonstrates considerable improvements in retrieval accuracy and\nefficiency compared to existing approaches. We have made our LightRAG\nopen-source and available at the link: https://github.com/HKUDS/LightRAG"
                },
                "authors": [
                    {
                        "name": "Zirui Guo"
                    },
                    {
                        "name": "Lianghao Xia"
                    },
                    {
                        "name": "Yanhua Yu"
                    },
                    {
                        "name": "Tu Ao"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05779v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05779v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20016v1",
                "updated": "2025-04-28T17:35:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    35,
                    46,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T17:35:46Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    35,
                    46,
                    0,
                    118,
                    0
                ],
                "title": "Applying LLM-Powered Virtual Humans to Child Interviews in\n  Child-Centered Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applying LLM-Powered Virtual Humans to Child Interviews in\n  Child-Centered Design"
                },
                "summary": "In child-centered design, directly engaging children is crucial for deeply\nunderstanding their experiences. However, current research often prioritizes\nadult perspectives, as interviewing children involves unique challenges such as\nenvironmental sensitivities and the need for trust-building. AI-powered virtual\nhumans (VHs) offer a promising approach to facilitate engaging and multimodal\ninteractions with children. This study establishes key design guidelines for\nLLM-powered virtual humans tailored to child interviews, standardizing\nmultimodal elements including color schemes, voice characteristics, facial\nfeatures, expressions, head movements, and gestures. Using ChatGPT-based prompt\nengineering, we developed three distinct Human-AI workflows (LLM-Auto,\nLLM-Interview, and LLM-Analyze) and conducted a user study involving 15\nchildren aged 6 to 12. The results indicated that the LLM-Analyze workflow\noutperformed the others by eliciting longer responses, achieving higher user\nexperience ratings, and promoting more effective child engagement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In child-centered design, directly engaging children is crucial for deeply\nunderstanding their experiences. However, current research often prioritizes\nadult perspectives, as interviewing children involves unique challenges such as\nenvironmental sensitivities and the need for trust-building. AI-powered virtual\nhumans (VHs) offer a promising approach to facilitate engaging and multimodal\ninteractions with children. This study establishes key design guidelines for\nLLM-powered virtual humans tailored to child interviews, standardizing\nmultimodal elements including color schemes, voice characteristics, facial\nfeatures, expressions, head movements, and gestures. Using ChatGPT-based prompt\nengineering, we developed three distinct Human-AI workflows (LLM-Auto,\nLLM-Interview, and LLM-Analyze) and conducted a user study involving 15\nchildren aged 6 to 12. The results indicated that the LLM-Analyze workflow\noutperformed the others by eliciting longer responses, achieving higher user\nexperience ratings, and promoting more effective child engagement."
                },
                "authors": [
                    {
                        "name": "Linshi Li"
                    },
                    {
                        "name": "Hanlin Cai"
                    }
                ],
                "author_detail": {
                    "name": "Hanlin Cai"
                },
                "author": "Hanlin Cai",
                "arxiv_doi": "10.1145/3713043.3731551",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713043.3731551",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.20016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper has been accepted as a Work-in-Progress (WiP) paper in the\n  24th annual ACM Interaction Design and Children (IDC) Conference",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20013v1",
                "updated": "2025-04-28T17:32:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    32,
                    38,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T17:32:38Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    32,
                    38,
                    0,
                    118,
                    0
                ],
                "title": "LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case\n  Study on Neural News Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case\n  Study on Neural News Recommendation"
                },
                "summary": "Online fake news moderation now faces a new challenge brought by the\nmalicious use of large language models (LLMs) in fake news production. Though\nexisting works have shown LLM-generated fake news is hard to detect from an\nindividual aspect, it remains underexplored how its large-scale release will\nimpact the news ecosystem. In this study, we develop a simulation pipeline and\na dataset with ~56k generated news of diverse types to investigate the effects\nof LLM-generated fake news within neural news recommendation systems. Our\nfindings expose a truth decay phenomenon, where real news is gradually losing\nits advantageous position in news ranking against fake news as LLM-generated\nnews is involved in news recommendation. We further provide an explanation\nabout why truth decay occurs from a familiarity perspective and show the\npositive correlation between perplexity and news ranking. Finally, we discuss\nthe threats of LLM-generated fake news and provide possible countermeasures. We\nurge stakeholders to address this emerging challenge to preserve the integrity\nof news ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online fake news moderation now faces a new challenge brought by the\nmalicious use of large language models (LLMs) in fake news production. Though\nexisting works have shown LLM-generated fake news is hard to detect from an\nindividual aspect, it remains underexplored how its large-scale release will\nimpact the news ecosystem. In this study, we develop a simulation pipeline and\na dataset with ~56k generated news of diverse types to investigate the effects\nof LLM-generated fake news within neural news recommendation systems. Our\nfindings expose a truth decay phenomenon, where real news is gradually losing\nits advantageous position in news ranking against fake news as LLM-generated\nnews is involved in news recommendation. We further provide an explanation\nabout why truth decay occurs from a familiarity perspective and show the\npositive correlation between perplexity and news ranking. Finally, we discuss\nthe threats of LLM-generated fake news and provide possible countermeasures. We\nurge stakeholders to address this emerging challenge to preserve the integrity\nof news ecosystems."
                },
                "authors": [
                    {
                        "name": "Beizhe Hu"
                    },
                    {
                        "name": "Qiang Sheng"
                    },
                    {
                        "name": "Juan Cao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Danding Wang"
                    }
                ],
                "author_detail": {
                    "name": "Danding Wang"
                },
                "author": "Danding Wang",
                "arxiv_doi": "10.1145/3726302.3730027",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730027",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.20013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACM SIGIR 2025 Full Paper",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20010v1",
                "updated": "2025-04-28T17:29:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    29,
                    51,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T17:29:51Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    29,
                    51,
                    0,
                    118,
                    0
                ],
                "title": "Towards Automated Scoping of AI for Social Good Projects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Automated Scoping of AI for Social Good Projects"
                },
                "summary": "Artificial Intelligence for Social Good (AI4SG) is an emerging effort that\naims to address complex societal challenges with the powerful capabilities of\nAI systems. These challenges range from local issues with transit networks to\nglobal wildlife preservation. However, regardless of scale, a critical\nbottleneck for many AI4SG initiatives is the laborious process of problem\nscoping -- a complex and resource-intensive task -- due to a scarcity of\nprofessionals with both technical and domain expertise. Given the remarkable\napplications of large language models (LLM), we propose a Problem Scoping Agent\n(PSA) that uses an LLM to generate comprehensive project proposals grounded in\nscientific literature and real-world knowledge. We demonstrate that our PSA\nframework generates proposals comparable to those written by experts through a\nblind review and AI evaluations. Finally, we document the challenges of\nreal-world problem scoping and note several areas for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence for Social Good (AI4SG) is an emerging effort that\naims to address complex societal challenges with the powerful capabilities of\nAI systems. These challenges range from local issues with transit networks to\nglobal wildlife preservation. However, regardless of scale, a critical\nbottleneck for many AI4SG initiatives is the laborious process of problem\nscoping -- a complex and resource-intensive task -- due to a scarcity of\nprofessionals with both technical and domain expertise. Given the remarkable\napplications of large language models (LLM), we propose a Problem Scoping Agent\n(PSA) that uses an LLM to generate comprehensive project proposals grounded in\nscientific literature and real-world knowledge. We demonstrate that our PSA\nframework generates proposals comparable to those written by experts through a\nblind review and AI evaluations. Finally, we document the challenges of\nreal-world problem scoping and note several areas for future work."
                },
                "authors": [
                    {
                        "name": "Jacob Emmerson"
                    },
                    {
                        "name": "Rayid Ghani"
                    },
                    {
                        "name": "Zheyuan Ryan Shi"
                    }
                ],
                "author_detail": {
                    "name": "Zheyuan Ryan Shi"
                },
                "author": "Zheyuan Ryan Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20006v1",
                "updated": "2025-04-28T17:24:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    24,
                    36,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T17:24:36Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    24,
                    36,
                    0,
                    118,
                    0
                ],
                "title": "Chatbot Arena Meets Nuggets: Towards Explanations and Diagnostics in the\n  Evaluation of LLM Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chatbot Arena Meets Nuggets: Towards Explanations and Diagnostics in the\n  Evaluation of LLM Responses"
                },
                "summary": "Battles, or side-by-side comparisons in so called arenas that elicit human\npreferences, have emerged as a popular approach to assessing the output quality\nof LLMs. Recently, this idea has been extended to retrieval-augmented\ngeneration (RAG) systems. While undoubtedly representing an advance in\nevaluation, battles have at least two drawbacks, particularly in the context of\ncomplex information-seeking queries: they are neither explanatory nor\ndiagnostic. Recently, the nugget evaluation methodology has emerged as a\npromising approach to evaluate the quality of RAG answers. Nuggets decompose\nlong-form LLM-generated answers into atomic facts, highlighting important\npieces of information necessary in a \"good\" response. In this work, we apply\nour AutoNuggetizer framework to analyze data from roughly 7K Search Arena\nbattles provided by LMArena in a fully automatic manner. Our results show a\nsignificant correlation between nugget scores and human preferences, showcasing\npromise in our approach to explainable and diagnostic system evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Battles, or side-by-side comparisons in so called arenas that elicit human\npreferences, have emerged as a popular approach to assessing the output quality\nof LLMs. Recently, this idea has been extended to retrieval-augmented\ngeneration (RAG) systems. While undoubtedly representing an advance in\nevaluation, battles have at least two drawbacks, particularly in the context of\ncomplex information-seeking queries: they are neither explanatory nor\ndiagnostic. Recently, the nugget evaluation methodology has emerged as a\npromising approach to evaluate the quality of RAG answers. Nuggets decompose\nlong-form LLM-generated answers into atomic facts, highlighting important\npieces of information necessary in a \"good\" response. In this work, we apply\nour AutoNuggetizer framework to analyze data from roughly 7K Search Arena\nbattles provided by LMArena in a fully automatic manner. Our results show a\nsignificant correlation between nugget scores and human preferences, showcasing\npromise in our approach to explainable and diagnostic system evaluations."
                },
                "authors": [
                    {
                        "name": "Sahel Sharifymoghaddam"
                    },
                    {
                        "name": "Shivani Upadhyay"
                    },
                    {
                        "name": "Nandan Thakur"
                    },
                    {
                        "name": "Ronak Pradeep"
                    },
                    {
                        "name": "Jimmy Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Lin"
                },
                "author": "Jimmy Lin",
                "arxiv_comment": "10 pages, 8 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20004v1",
                "updated": "2025-04-28T17:24:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    24,
                    4,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T17:24:04Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    24,
                    4,
                    0,
                    118,
                    0
                ],
                "title": "Socially-Aware Autonomous Driving: Inferring Yielding Intentions for\n  Safer Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Socially-Aware Autonomous Driving: Inferring Yielding Intentions for\n  Safer Interactions"
                },
                "summary": "Since the emergence of autonomous driving technology, it has advanced rapidly\nover the past decade. It is becoming increasingly likely that autonomous\nvehicles (AVs) would soon coexist with human-driven vehicles (HVs) on the\nroads. Currently, safety and reliable decision-making remain significant\nchallenges, particularly when AVs are navigating lane changes and interacting\nwith surrounding HVs. Therefore, precise estimation of the intentions of\nsurrounding HVs can assist AVs in making more reliable and safe lane change\ndecision-making. This involves not only understanding their current behaviors\nbut also predicting their future motions without any direct communication.\nHowever, distinguishing between the passing and yielding intentions of\nsurrounding HVs still remains ambiguous. To address the challenge, we propose a\nsocial intention estimation algorithm rooted in Directed Acyclic Graph (DAG),\ncoupled with a decision-making framework employing Deep Reinforcement Learning\n(DRL) algorithms. To evaluate the method's performance, the proposed framework\ncan be tested and applied in a lane-changing scenario within a simulated\nenvironment. Furthermore, the experiment results demonstrate how our approach\nenhances the ability of AVs to navigate lane changes safely and efficiently on\nroads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the emergence of autonomous driving technology, it has advanced rapidly\nover the past decade. It is becoming increasingly likely that autonomous\nvehicles (AVs) would soon coexist with human-driven vehicles (HVs) on the\nroads. Currently, safety and reliable decision-making remain significant\nchallenges, particularly when AVs are navigating lane changes and interacting\nwith surrounding HVs. Therefore, precise estimation of the intentions of\nsurrounding HVs can assist AVs in making more reliable and safe lane change\ndecision-making. This involves not only understanding their current behaviors\nbut also predicting their future motions without any direct communication.\nHowever, distinguishing between the passing and yielding intentions of\nsurrounding HVs still remains ambiguous. To address the challenge, we propose a\nsocial intention estimation algorithm rooted in Directed Acyclic Graph (DAG),\ncoupled with a decision-making framework employing Deep Reinforcement Learning\n(DRL) algorithms. To evaluate the method's performance, the proposed framework\ncan be tested and applied in a lane-changing scenario within a simulated\nenvironment. Furthermore, the experiment results demonstrate how our approach\nenhances the ability of AVs to navigate lane changes safely and efficiently on\nroads."
                },
                "authors": [
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Yan Jin"
                    },
                    {
                        "name": "Hamid Taghavifar"
                    },
                    {
                        "name": "Fei Ding"
                    },
                    {
                        "name": "Chongfeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Chongfeng Wei"
                },
                "author": "Chongfeng Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02239v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02239v2",
                "updated": "2025-04-28T17:19:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    19,
                    27,
                    0,
                    118,
                    0
                ],
                "published": "2024-08-05T05:15:17Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    5,
                    15,
                    17,
                    0,
                    218,
                    0
                ],
                "title": "Pula: Training Large Language Models for Setswana",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pula: Training Large Language Models for Setswana"
                },
                "summary": "In this work we present Pula, a suite of bilingual language models proficient\nin both Setswana and English. Leveraging recent advancements in data\navailability and efficient fine-tuning, Pula 8B and Pula 14B outperform GPT-4o\nand Gemini 1.5 Pro on English-Setswana translation tasks and achieve\nstate-of-the-art performance on Setswana reasoning tasks for their size. We\nrelease the weights for Pula 1B, 3B, 8B, and 14B as well as training logs and\ntraining and evaluation code. Alongside Pula, we release the largest-ever\nSetswana text corpus, Marothodi, and the first comprehensive Setswana\ninstruction-tuning dataset, Medupi, consisting of reformatted datasets,\ntranslated corpora, and synthetic LLM-generated text. To accompany this data,\nwe release the code used for dataset construction, formatting, filtering, and\nscraping. Last, we release two Setswana LLM-translated benchmarks, MMLU-tsn and\nGSM8K-tsn, to measure Setswana knowledge and reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we present Pula, a suite of bilingual language models proficient\nin both Setswana and English. Leveraging recent advancements in data\navailability and efficient fine-tuning, Pula 8B and Pula 14B outperform GPT-4o\nand Gemini 1.5 Pro on English-Setswana translation tasks and achieve\nstate-of-the-art performance on Setswana reasoning tasks for their size. We\nrelease the weights for Pula 1B, 3B, 8B, and 14B as well as training logs and\ntraining and evaluation code. Alongside Pula, we release the largest-ever\nSetswana text corpus, Marothodi, and the first comprehensive Setswana\ninstruction-tuning dataset, Medupi, consisting of reformatted datasets,\ntranslated corpora, and synthetic LLM-generated text. To accompany this data,\nwe release the code used for dataset construction, formatting, filtering, and\nscraping. Last, we release two Setswana LLM-translated benchmarks, MMLU-tsn and\nGSM8K-tsn, to measure Setswana knowledge and reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Nathan Brown"
                    },
                    {
                        "name": "Vukosi Marivate"
                    }
                ],
                "author_detail": {
                    "name": "Vukosi Marivate"
                },
                "author": "Vukosi Marivate",
                "arxiv_comment": "NAACL 2025. 10 pages, 5 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02239v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02239v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20000v1",
                "updated": "2025-04-28T17:19:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    19,
                    25,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T17:19:25Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    19,
                    25,
                    0,
                    118,
                    0
                ],
                "title": "Knowledge Distillation of Domain-adapted LLMs for Question-Answering in\n  Telecom",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Distillation of Domain-adapted LLMs for Question-Answering in\n  Telecom"
                },
                "summary": "Knowledge Distillation (KD) is one of the approaches to reduce the size of\nLarge Language Models (LLMs). A LLM with smaller number of model parameters\n(student) is trained to mimic the performance of a LLM of a larger size\n(teacher model) on a specific task. For domain-specific tasks, it is not clear\nif teacher or student model, or both, must be considered for domain adaptation.\nIn this work, we study this problem from perspective of telecom domain\nQuestion-Answering (QA) task. We systematically experiment with Supervised\nFine-tuning (SFT) of teacher only, SFT of student only and SFT of both prior to\nKD. We design experiments to study the impact of vocabulary (same and\ndifferent) and KD algorithms (vanilla KD and Dual Space KD, DSKD) on the\ndistilled model. Multi-faceted evaluation of the distillation using 14\ndifferent metrics (N-gram, embedding and LLM-based metrics) is considered.\nExperimental results show that SFT of teacher improves performance of distilled\nmodel when both models have same vocabulary, irrespective of algorithm and\nmetrics. Overall, SFT of both teacher and student results in better performance\nacross all metrics, although the statistical significance of the same depends\non the vocabulary of the teacher models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Distillation (KD) is one of the approaches to reduce the size of\nLarge Language Models (LLMs). A LLM with smaller number of model parameters\n(student) is trained to mimic the performance of a LLM of a larger size\n(teacher model) on a specific task. For domain-specific tasks, it is not clear\nif teacher or student model, or both, must be considered for domain adaptation.\nIn this work, we study this problem from perspective of telecom domain\nQuestion-Answering (QA) task. We systematically experiment with Supervised\nFine-tuning (SFT) of teacher only, SFT of student only and SFT of both prior to\nKD. We design experiments to study the impact of vocabulary (same and\ndifferent) and KD algorithms (vanilla KD and Dual Space KD, DSKD) on the\ndistilled model. Multi-faceted evaluation of the distillation using 14\ndifferent metrics (N-gram, embedding and LLM-based metrics) is considered.\nExperimental results show that SFT of teacher improves performance of distilled\nmodel when both models have same vocabulary, irrespective of algorithm and\nmetrics. Overall, SFT of both teacher and student results in better performance\nacross all metrics, although the statistical significance of the same depends\non the vocabulary of the teacher models."
                },
                "authors": [
                    {
                        "name": "Rishika Sen"
                    },
                    {
                        "name": "Sujoy Roychowdhury"
                    },
                    {
                        "name": "Sumit Soman"
                    },
                    {
                        "name": "H. G. Ranjani"
                    },
                    {
                        "name": "Srikhetra Mohanty"
                    }
                ],
                "author_detail": {
                    "name": "Srikhetra Mohanty"
                },
                "author": "Srikhetra Mohanty",
                "arxiv_comment": "10 pages, 4 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19989v1",
                "updated": "2025-04-28T17:06:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    6,
                    5,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T17:06:05Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    6,
                    5,
                    0,
                    118,
                    0
                ],
                "title": "HJRNO: Hamilton-Jacobi Reachability with Neural Operators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HJRNO: Hamilton-Jacobi Reachability with Neural Operators"
                },
                "summary": "Ensuring the safety of autonomous systems under uncertainty is a critical\nchallenge. Hamilton-Jacobi reachability (HJR) analysis is a widely used method\nfor guaranteeing safety under worst-case disturbances. Traditional HJR methods\nprovide safety guarantees but suffer from the curse of dimensionality, limiting\ntheir scalability to high-dimensional systems or varying environmental\nconditions. In this work, we propose HJRNO, a neural operator-based framework\nfor solving backward reachable tubes (BRTs) efficiently and accurately. By\nleveraging the Fourier Neural Operator (FNO), HJRNO learns a mapping between\nvalue functions, enabling fast inference with strong generalization across\ndifferent obstacle shapes, system configurations, and hyperparameters. We\ndemonstrate that HJRNO achieves low error on random obstacle scenarios and\ngeneralizes effectively across varying system dynamics. These results suggest\nthat HJRNO offers a promising foundation model approach for scalable, real-time\nsafety analysis in autonomous systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the safety of autonomous systems under uncertainty is a critical\nchallenge. Hamilton-Jacobi reachability (HJR) analysis is a widely used method\nfor guaranteeing safety under worst-case disturbances. Traditional HJR methods\nprovide safety guarantees but suffer from the curse of dimensionality, limiting\ntheir scalability to high-dimensional systems or varying environmental\nconditions. In this work, we propose HJRNO, a neural operator-based framework\nfor solving backward reachable tubes (BRTs) efficiently and accurately. By\nleveraging the Fourier Neural Operator (FNO), HJRNO learns a mapping between\nvalue functions, enabling fast inference with strong generalization across\ndifferent obstacle shapes, system configurations, and hyperparameters. We\ndemonstrate that HJRNO achieves low error on random obstacle scenarios and\ngeneralizes effectively across varying system dynamics. These results suggest\nthat HJRNO offers a promising foundation model approach for scalable, real-time\nsafety analysis in autonomous systems."
                },
                "authors": [
                    {
                        "name": "Yankai Li"
                    },
                    {
                        "name": "Mo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mo Chen"
                },
                "author": "Mo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19982v1",
                "updated": "2025-04-28T16:57:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    57,
                    17,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T16:57:17Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    57,
                    17,
                    0,
                    118,
                    0
                ],
                "title": "TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining\n  Turn-Level Precision with Dialogue-Level Comparisons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining\n  Turn-Level Precision with Dialogue-Level Comparisons"
                },
                "summary": "Task-oriented dialogue (TOD) systems are experiencing a revolution driven by\nLarge Language Models (LLMs), yet the evaluation methodologies for these\nsystems remain insufficient for their growing sophistication. While traditional\nautomatic metrics effectively assessed earlier modular systems, they focus\nsolely on the dialogue level and cannot detect critical intermediate errors\nthat can arise during user-agent interactions. In this paper, we introduce\nTD-EVAL (Turn and Dialogue-level Evaluation), a two-step evaluation framework\nthat unifies fine-grained turn-level analysis with holistic dialogue-level\ncomparisons. At turn level, we evaluate each response along three TOD-specific\ndimensions: conversation cohesion, backend knowledge consistency, and policy\ncompliance. Meanwhile, we design TOD Agent Arena that uses pairwise comparisons\nto provide a measure of dialogue-level quality. Through experiments on MultiWOZ\n2.4 and {\\tau}-Bench, we demonstrate that TD-EVAL effectively identifies the\nconversational errors that conventional metrics miss. Furthermore, TD-EVAL\nexhibits better alignment with human judgments than traditional and LLM-based\nmetrics. These findings demonstrate that TD-EVAL introduces a new paradigm for\nTOD system evaluation, efficiently assessing both turn and system levels with a\nplug-and-play framework for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-oriented dialogue (TOD) systems are experiencing a revolution driven by\nLarge Language Models (LLMs), yet the evaluation methodologies for these\nsystems remain insufficient for their growing sophistication. While traditional\nautomatic metrics effectively assessed earlier modular systems, they focus\nsolely on the dialogue level and cannot detect critical intermediate errors\nthat can arise during user-agent interactions. In this paper, we introduce\nTD-EVAL (Turn and Dialogue-level Evaluation), a two-step evaluation framework\nthat unifies fine-grained turn-level analysis with holistic dialogue-level\ncomparisons. At turn level, we evaluate each response along three TOD-specific\ndimensions: conversation cohesion, backend knowledge consistency, and policy\ncompliance. Meanwhile, we design TOD Agent Arena that uses pairwise comparisons\nto provide a measure of dialogue-level quality. Through experiments on MultiWOZ\n2.4 and {\\tau}-Bench, we demonstrate that TD-EVAL effectively identifies the\nconversational errors that conventional metrics miss. Furthermore, TD-EVAL\nexhibits better alignment with human judgments than traditional and LLM-based\nmetrics. These findings demonstrate that TD-EVAL introduces a new paradigm for\nTOD system evaluation, efficiently assessing both turn and system levels with a\nplug-and-play framework for future research."
                },
                "authors": [
                    {
                        "name": "Emre Can Acikgoz"
                    },
                    {
                        "name": "Carl Guo"
                    },
                    {
                        "name": "Suvodip Dey"
                    },
                    {
                        "name": "Akul Datta"
                    },
                    {
                        "name": "Takyoung Kim"
                    },
                    {
                        "name": "Gokhan Tur"
                    },
                    {
                        "name": "Dilek Hakkani-Tr"
                    }
                ],
                "author_detail": {
                    "name": "Dilek Hakkani-Tr"
                },
                "author": "Dilek Hakkani-Tr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19981v1",
                "updated": "2025-04-28T16:56:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    56,
                    41,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T16:56:41Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    56,
                    41,
                    0,
                    118,
                    0
                ],
                "title": "Accurate and Diverse LLM Mathematical Reasoning via Automated PRM-Guided\n  GFlowNets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and Diverse LLM Mathematical Reasoning via Automated PRM-Guided\n  GFlowNets"
                },
                "summary": "Achieving both accuracy and diverse reasoning remains challenging for Large\nLanguage Models (LLMs) in complex domains like mathematics. A key bottleneck is\nevaluating intermediate reasoning steps to guide generation without costly\nhuman annotations. To address this, we first introduce a novel Process Reward\nModel (PRM) trained automatically using Monte Carlo Tree Search coupled with a\nsimilarity-based data augmentation technique, effectively capturing step-level\nreasoning quality. Leveraging this PRM, we then adapt Generative Flow Networks\n(GFlowNets) to operate at the reasoning step level. Unlike traditional\nreinforcement learning focused on maximizing a single reward, GFlowNets\nnaturally sample diverse, high-quality solutions proportional to their rewards,\nas measured by our PRM. Empirical evaluation shows strong improvements in both\naccuracy and solution diversity on challenging mathematical benchmarks (e.g.,\n+2.59% absolute accuracy on MATH Level 5 for Llama3.2-3B), with effective\ngeneralization to unseen datasets (+9.4% absolute on SAT MATH). Our work\ndemonstrates the potential of PRM-guided, step-level GFlowNets for developing\nmore robust and versatile mathematical reasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving both accuracy and diverse reasoning remains challenging for Large\nLanguage Models (LLMs) in complex domains like mathematics. A key bottleneck is\nevaluating intermediate reasoning steps to guide generation without costly\nhuman annotations. To address this, we first introduce a novel Process Reward\nModel (PRM) trained automatically using Monte Carlo Tree Search coupled with a\nsimilarity-based data augmentation technique, effectively capturing step-level\nreasoning quality. Leveraging this PRM, we then adapt Generative Flow Networks\n(GFlowNets) to operate at the reasoning step level. Unlike traditional\nreinforcement learning focused on maximizing a single reward, GFlowNets\nnaturally sample diverse, high-quality solutions proportional to their rewards,\nas measured by our PRM. Empirical evaluation shows strong improvements in both\naccuracy and solution diversity on challenging mathematical benchmarks (e.g.,\n+2.59% absolute accuracy on MATH Level 5 for Llama3.2-3B), with effective\ngeneralization to unseen datasets (+9.4% absolute on SAT MATH). Our work\ndemonstrates the potential of PRM-guided, step-level GFlowNets for developing\nmore robust and versatile mathematical reasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Adam Younsi"
                    },
                    {
                        "name": "Abdalgader Abubaker"
                    },
                    {
                        "name": "Mohamed El Amine Seddik"
                    },
                    {
                        "name": "Hakim Hacid"
                    },
                    {
                        "name": "Salem Lahlou"
                    }
                ],
                "author_detail": {
                    "name": "Salem Lahlou"
                },
                "author": "Salem Lahlou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19962v1",
                "updated": "2025-04-28T16:33:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    33,
                    55,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T16:33:55Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    33,
                    55,
                    0,
                    118,
                    0
                ],
                "title": "Approximating neutron-star radii using gravitational-wave only\n  measurements with symbolic regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximating neutron-star radii using gravitational-wave only\n  measurements with symbolic regression"
                },
                "summary": "Binary neutron star inspirals detected as gravitational waves carry\ninformation on components' masses and tidal deformabilities, but not radii,\nwhich are measured by electromagnetic observations of neutron stars. An\nexpression for neutron-star radii as a function of gravitational-wave only data\nwould be advantageous for the multi-messenger astronomy. Using pySR, a symbolic\nregression method trained on TOV solutions to piecewise polytropic EOS input,\nan approximate symbolic expression for neutron-star radius as a function of\nmass and tidal deformability is obtained. The approximation is tested on\npiecewise polytropic EOS NS data, as well as on NS sequences based on various\nnon-polytropic EOSs based on realistic theories of dense matter, achieving\nconsistent agreement between the ground truth values and the approximation for\na broad range of NS parameters covering current astrophysical observations,\nwith average radii differences of few hundred meters. Additionally, the\napproximation is applied to GW170817 gravitational-wave mass and tidal\ndeformability posteriors, and compared to reported inferred radius\ndistributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary neutron star inspirals detected as gravitational waves carry\ninformation on components' masses and tidal deformabilities, but not radii,\nwhich are measured by electromagnetic observations of neutron stars. An\nexpression for neutron-star radii as a function of gravitational-wave only data\nwould be advantageous for the multi-messenger astronomy. Using pySR, a symbolic\nregression method trained on TOV solutions to piecewise polytropic EOS input,\nan approximate symbolic expression for neutron-star radius as a function of\nmass and tidal deformability is obtained. The approximation is tested on\npiecewise polytropic EOS NS data, as well as on NS sequences based on various\nnon-polytropic EOSs based on realistic theories of dense matter, achieving\nconsistent agreement between the ground truth values and the approximation for\na broad range of NS parameters covering current astrophysical observations,\nwith average radii differences of few hundred meters. Additionally, the\napproximation is applied to GW170817 gravitational-wave mass and tidal\ndeformability posteriors, and compared to reported inferred radius\ndistributions."
                },
                "authors": [
                    {
                        "name": "Micha Bejger"
                    }
                ],
                "author_detail": {
                    "name": "Micha Bejger"
                },
                "author": "Micha Bejger",
                "arxiv_comment": "6 pages, 6 figures; comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19959v2",
                "updated": "2025-04-29T02:05:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    2,
                    5,
                    45,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-28T16:33:08Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    33,
                    8,
                    0,
                    118,
                    0
                ],
                "title": "From Concept to Practice: an Automated LLM-aided UVM Machine for RTL\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Concept to Practice: an Automated LLM-aided UVM Machine for RTL\n  Verification"
                },
                "summary": "Verification presents a major bottleneck in Integrated Circuit (IC)\ndevelopment, consuming nearly 70% of the total development effort. While the\nUniversal Verification Methodology (UVM) is widely used in industry to improve\nverification efficiency through structured and reusable testbenches,\nconstructing these testbenches and generating sufficient stimuli remain\nchallenging. These challenges arise from the considerable manual coding effort\nrequired, repetitive manual execution of multiple EDA tools, and the need for\nin-depth domain expertise to navigate complex designs.Here, we present UVM^2,\nan automated verification framework that leverages Large Language Models (LLMs)\nto generate UVM testbenches and iteratively refine them using coverage\nfeedback, significantly reducing manual effort while maintaining rigorous\nverification standards.To evaluate UVM^2, we introduce a benchmark suite\ncomprising Register Transfer Level (RTL) designs of up to 1.6K lines of\ncode.The results show that UVM^2 reduces testbench setup time by up to UVM^2\ncompared to experienced engineers, and achieve average code and function\ncoverage of 87.44% and 89.58%, outperforming state-of-the-art solutions by\n20.96% and 23.51%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verification presents a major bottleneck in Integrated Circuit (IC)\ndevelopment, consuming nearly 70% of the total development effort. While the\nUniversal Verification Methodology (UVM) is widely used in industry to improve\nverification efficiency through structured and reusable testbenches,\nconstructing these testbenches and generating sufficient stimuli remain\nchallenging. These challenges arise from the considerable manual coding effort\nrequired, repetitive manual execution of multiple EDA tools, and the need for\nin-depth domain expertise to navigate complex designs.Here, we present UVM^2,\nan automated verification framework that leverages Large Language Models (LLMs)\nto generate UVM testbenches and iteratively refine them using coverage\nfeedback, significantly reducing manual effort while maintaining rigorous\nverification standards.To evaluate UVM^2, we introduce a benchmark suite\ncomprising Register Transfer Level (RTL) designs of up to 1.6K lines of\ncode.The results show that UVM^2 reduces testbench setup time by up to UVM^2\ncompared to experienced engineers, and achieve average code and function\ncoverage of 87.44% and 89.58%, outperforming state-of-the-art solutions by\n20.96% and 23.51%, respectively."
                },
                "authors": [
                    {
                        "name": "Junhao Ye"
                    },
                    {
                        "name": "Yuchen Hu"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Dingrong Pan"
                    },
                    {
                        "name": "Qichun Chen"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Xinwei Fang"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Nan Guan"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19956v1",
                "updated": "2025-04-28T16:29:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    29,
                    24,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T16:29:24Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    29,
                    24,
                    0,
                    118,
                    0
                ],
                "title": "Securing Agentic AI: A Comprehensive Threat Model and Mitigation\n  Framework for Generative AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Securing Agentic AI: A Comprehensive Threat Model and Mitigation\n  Framework for Generative AI Agents"
                },
                "summary": "As generative AI (GenAI) agents become more common in enterprise settings,\nthey introduce security challenges that differ significantly from those posed\nby traditional systems. These agents are not just LLMs; they reason, remember,\nand act, often with minimal human oversight. This paper introduces a\ncomprehensive threat model tailored specifically for GenAI agents, focusing on\nhow their autonomy, persistent memory access, complex reasoning, and tool\nintegration create novel risks. This research work identifies 9 primary threats\nand organizes them across five key domains: cognitive architecture\nvulnerabilities, temporal persistence threats, operational execution\nvulnerabilities, trust boundary violations, and governance circumvention. These\nthreats are not just theoretical they bring practical challenges such as\ndelayed exploitability, cross-system propagation, cross system lateral\nmovement, and subtle goal misalignments that are hard to detect with existing\nframeworks and standard approaches. To help address this, the research work\npresent two complementary frameworks: ATFAA - Advanced Threat Framework for\nAutonomous AI Agents, which organizes agent-specific risks, and SHIELD, a\nframework proposing practical mitigation strategies designed to reduce\nenterprise exposure. While this work builds on existing work in LLM and AI\nsecurity, the focus is squarely on what makes agents different and why those\ndifferences matter. Ultimately, this research argues that GenAI agents require\na new lens for security. If we fail to adapt our threat models and defenses to\naccount for their unique architecture and behavior, we risk turning a powerful\nnew tool into a serious enterprise liability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As generative AI (GenAI) agents become more common in enterprise settings,\nthey introduce security challenges that differ significantly from those posed\nby traditional systems. These agents are not just LLMs; they reason, remember,\nand act, often with minimal human oversight. This paper introduces a\ncomprehensive threat model tailored specifically for GenAI agents, focusing on\nhow their autonomy, persistent memory access, complex reasoning, and tool\nintegration create novel risks. This research work identifies 9 primary threats\nand organizes them across five key domains: cognitive architecture\nvulnerabilities, temporal persistence threats, operational execution\nvulnerabilities, trust boundary violations, and governance circumvention. These\nthreats are not just theoretical they bring practical challenges such as\ndelayed exploitability, cross-system propagation, cross system lateral\nmovement, and subtle goal misalignments that are hard to detect with existing\nframeworks and standard approaches. To help address this, the research work\npresent two complementary frameworks: ATFAA - Advanced Threat Framework for\nAutonomous AI Agents, which organizes agent-specific risks, and SHIELD, a\nframework proposing practical mitigation strategies designed to reduce\nenterprise exposure. While this work builds on existing work in LLM and AI\nsecurity, the focus is squarely on what makes agents different and why those\ndifferences matter. Ultimately, this research argues that GenAI agents require\na new lens for security. If we fail to adapt our threat models and defenses to\naccount for their unique architecture and behavior, we risk turning a powerful\nnew tool into a serious enterprise liability."
                },
                "authors": [
                    {
                        "name": "Vineeth Sai Narajala"
                    },
                    {
                        "name": "Om Narayan"
                    }
                ],
                "author_detail": {
                    "name": "Om Narayan"
                },
                "author": "Om Narayan",
                "arxiv_comment": "12 pages, 2 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08775v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08775v3",
                "updated": "2025-04-28T16:07:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    7,
                    5,
                    0,
                    118,
                    0
                ],
                "published": "2024-09-13T12:34:14Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    12,
                    34,
                    14,
                    4,
                    257,
                    0
                ],
                "title": "What Should We Engineer in Prompts? Training Humans in\n  Requirement-Driven LLM Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Should We Engineer in Prompts? Training Humans in\n  Requirement-Driven LLM Use"
                },
                "summary": "Prompting LLMs for complex tasks (e.g., building a trip advisor chatbot)\nneeds humans to clearly articulate customized requirements (e.g., \"start the\nresponse with a tl;dr\"). However, existing prompt engineering instructions\noften lack focused training on requirement articulation and instead tend to\nemphasize increasingly automatable strategies (e.g., tricks like adding\nrole-plays and \"think step-by-step\"). To address the gap, we introduce\nRequirement-Oriented Prompt Engineering (ROPE), a paradigm that focuses human\nattention on generating clear, complete requirements during prompting. We\nimplement ROPE through an assessment and training suite that provides\ndeliberate practice with LLM-generated feedback. In a randomized controlled\nexperiment with 30 novices, ROPE significantly outperforms conventional prompt\nengineering training (20% vs. 1% gains), a gap that automatic prompt\noptimization cannot close. Furthermore, we demonstrate a direct correlation\nbetween the quality of input requirements and LLM outputs. Our work paves the\nway to empower more end-users to build complex LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting LLMs for complex tasks (e.g., building a trip advisor chatbot)\nneeds humans to clearly articulate customized requirements (e.g., \"start the\nresponse with a tl;dr\"). However, existing prompt engineering instructions\noften lack focused training on requirement articulation and instead tend to\nemphasize increasingly automatable strategies (e.g., tricks like adding\nrole-plays and \"think step-by-step\"). To address the gap, we introduce\nRequirement-Oriented Prompt Engineering (ROPE), a paradigm that focuses human\nattention on generating clear, complete requirements during prompting. We\nimplement ROPE through an assessment and training suite that provides\ndeliberate practice with LLM-generated feedback. In a randomized controlled\nexperiment with 30 novices, ROPE significantly outperforms conventional prompt\nengineering training (20% vs. 1% gains), a gap that automatic prompt\noptimization cannot close. Furthermore, we demonstrate a direct correlation\nbetween the quality of input requirements and LLM outputs. Our work paves the\nway to empower more end-users to build complex LLM applications."
                },
                "authors": [
                    {
                        "name": "Qianou Ma"
                    },
                    {
                        "name": "Weirui Peng"
                    },
                    {
                        "name": "Chenyang Yang"
                    },
                    {
                        "name": "Hua Shen"
                    },
                    {
                        "name": "Kenneth Koedinger"
                    },
                    {
                        "name": "Tongshuang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Tongshuang Wu"
                },
                "author": "Tongshuang Wu",
                "arxiv_doi": "10.1145/3731756",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731756",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.08775v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08775v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages; TOCHI 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19924v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19924v2",
                "updated": "2025-04-29T02:15:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    2,
                    15,
                    36,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-28T15:58:38Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    58,
                    38,
                    0,
                    118,
                    0
                ],
                "title": "Collaborative Inference for Sparse High-Dimensional Models with\n  Non-Shared Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Inference for Sparse High-Dimensional Models with\n  Non-Shared Data"
                },
                "summary": "In modern data analysis, statistical efficiency improvement is expected via\neffective collaboration among multiple data holders with non-shared data. In\nthis article, we propose a collaborative score-type test (CST) for testing\nlinear hypotheses, which accommodates potentially high-dimensional nuisance\nparameters and a diverging number of constraints and target parameters. Through\na careful decomposition of the Kiefer-Bahadur representation for the\ntraditional score statistic, we identify and approximate the key components\nusing aggregated local gradient information from each data source. In addition,\nwe employ a two-stage partial penalization strategy to shrink the approximation\nerror and mitigate the bias from the high-dimensional nuisance parameters.\nUnlike existing methods, the CST procedure involves constrained optimization\nunder non-shared and high-dimensional data settings, which requires novel\ntheoretical developments. We derive the limiting distributions for the CST\nstatistic under the null hypothesis and the local alternatives. Besides, the\nCST exhibits an oracle property and achieves the global statistical efficiency.\nMoreover, it relaxes the stringent restrictions on the number of data sources\nrequired in the current literature. Extensive numerical studies and a real\nexample demonstrate the effectiveness and validity of our proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern data analysis, statistical efficiency improvement is expected via\neffective collaboration among multiple data holders with non-shared data. In\nthis article, we propose a collaborative score-type test (CST) for testing\nlinear hypotheses, which accommodates potentially high-dimensional nuisance\nparameters and a diverging number of constraints and target parameters. Through\na careful decomposition of the Kiefer-Bahadur representation for the\ntraditional score statistic, we identify and approximate the key components\nusing aggregated local gradient information from each data source. In addition,\nwe employ a two-stage partial penalization strategy to shrink the approximation\nerror and mitigate the bias from the high-dimensional nuisance parameters.\nUnlike existing methods, the CST procedure involves constrained optimization\nunder non-shared and high-dimensional data settings, which requires novel\ntheoretical developments. We derive the limiting distributions for the CST\nstatistic under the null hypothesis and the local alternatives. Besides, the\nCST exhibits an oracle property and achieves the global statistical efficiency.\nMoreover, it relaxes the stringent restrictions on the number of data sources\nrequired in the current literature. Extensive numerical studies and a real\nexample demonstrate the effectiveness and validity of our proposed method."
                },
                "authors": [
                    {
                        "name": "Yifan Gu"
                    },
                    {
                        "name": "Hanfang Yang"
                    },
                    {
                        "name": "Songshan Yang"
                    },
                    {
                        "name": "Hui Zou"
                    }
                ],
                "author_detail": {
                    "name": "Hui Zou"
                },
                "author": "Hui Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19924v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19924v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17965v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17965v2",
                "updated": "2025-04-28T15:54:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    54,
                    16,
                    0,
                    118,
                    0
                ],
                "published": "2024-12-23T20:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    20,
                    28,
                    22,
                    0,
                    358,
                    0
                ],
                "title": "LMV-RPA: Large Model Voting-based Robotic Process Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMV-RPA: Large Model Voting-based Robotic Process Automation"
                },
                "summary": "Automating high-volume unstructured data processing is essential for\noperational efficiency. Optical Character Recognition (OCR) is critical but\noften struggles with accuracy and efficiency in complex layouts and ambiguous\ntext. These challenges are especially pronounced in large-scale tasks requiring\nboth speed and precision. This paper introduces LMV-RPA, a Large Model\nVoting-based Robotic Process Automation system to enhance OCR workflows.\nLMV-RPA integrates outputs from OCR engines such as Paddle OCR, Tesseract OCR,\nEasy OCR, and DocTR with Large Language Models (LLMs) like LLaMA 3 and\nGemini-1.5-pro. Using a majority voting mechanism, it processes OCR outputs\ninto structured JSON formats, improving accuracy, particularly in complex\nlayouts. The multi-phase pipeline processes text extracted by OCR engines\nthrough LLMs, combining results to ensure the most accurate outputs. LMV-RPA\nachieves 99 percent accuracy in OCR tasks, surpassing baseline models with 94\npercent, while reducing processing time by 80 percent. Benchmark evaluations\nconfirm its scalability and demonstrate that LMV-RPA offers a faster, more\nreliable, and efficient solution for automating large-scale document processing\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating high-volume unstructured data processing is essential for\noperational efficiency. Optical Character Recognition (OCR) is critical but\noften struggles with accuracy and efficiency in complex layouts and ambiguous\ntext. These challenges are especially pronounced in large-scale tasks requiring\nboth speed and precision. This paper introduces LMV-RPA, a Large Model\nVoting-based Robotic Process Automation system to enhance OCR workflows.\nLMV-RPA integrates outputs from OCR engines such as Paddle OCR, Tesseract OCR,\nEasy OCR, and DocTR with Large Language Models (LLMs) like LLaMA 3 and\nGemini-1.5-pro. Using a majority voting mechanism, it processes OCR outputs\ninto structured JSON formats, improving accuracy, particularly in complex\nlayouts. The multi-phase pipeline processes text extracted by OCR engines\nthrough LLMs, combining results to ensure the most accurate outputs. LMV-RPA\nachieves 99 percent accuracy in OCR tasks, surpassing baseline models with 94\npercent, while reducing processing time by 80 percent. Benchmark evaluations\nconfirm its scalability and demonstrate that LMV-RPA offers a faster, more\nreliable, and efficient solution for automating large-scale document processing\ntasks."
                },
                "authors": [
                    {
                        "name": "Osama Abdellatif"
                    },
                    {
                        "name": "Ahmed Ayman"
                    },
                    {
                        "name": "Ali Hamdi"
                    }
                ],
                "author_detail": {
                    "name": "Ali Hamdi"
                },
                "author": "Ali Hamdi",
                "arxiv_comment": "12 pages, 1 figures, 1 algorithm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17965v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17965v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11209v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11209v2",
                "updated": "2025-04-28T15:53:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    53,
                    28,
                    0,
                    118,
                    0
                ],
                "published": "2025-02-16T17:15:28Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    17,
                    15,
                    28,
                    6,
                    47,
                    0
                ],
                "title": "Black-hole spectroscopy from a giant quantum vortex",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-hole spectroscopy from a giant quantum vortex"
                },
                "summary": "Black-hole spectroscopy aims to infer physical properties of black holes by\ndetecting the spectrum of quasinormal modes (QNMs) they emit while settling\ntowards equilibrium. Unlike normal modes, which are resonances of\nenergy-conserving systems, QNMs are damped oscillations arising when a system\nloses energy due to open boundaries or via dissipation. The detection of the\nfull QNM spectrum of black holes is challenging due to rapidly decaying\namplitudes of these resonances, limiting observations only to the longest-lived\nmode. Theoretical and numerical studies suggest that environmental confinement\ndue to surrounding plasma or dark matter modify the QNM spectrum. Here, we\nemploy black-hole spectroscopy to show how spatial confinement similarly\naffects the spectrum of nanometre-scale interface waves surrounding a giant\nquantum vortex in superfluid helium-4, an experimentally accessible quantum\nsystem that emulates dynamics in rotating curved spacetime. In the available\nparameter space, we observe regimes in which multiple QNMs emerge from the\ninterface noise spectrum. In agreement with theoretical predictions, their real\nand imaginary frequencies are shifted with respect to those expected in the\nunbounded system. Our results demonstrate the critical role of spatial\nconfinement in shaping the QNM spectrum, highlighting the importance of\nenvironmental effects on spectral stability of astrophysical compact objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-hole spectroscopy aims to infer physical properties of black holes by\ndetecting the spectrum of quasinormal modes (QNMs) they emit while settling\ntowards equilibrium. Unlike normal modes, which are resonances of\nenergy-conserving systems, QNMs are damped oscillations arising when a system\nloses energy due to open boundaries or via dissipation. The detection of the\nfull QNM spectrum of black holes is challenging due to rapidly decaying\namplitudes of these resonances, limiting observations only to the longest-lived\nmode. Theoretical and numerical studies suggest that environmental confinement\ndue to surrounding plasma or dark matter modify the QNM spectrum. Here, we\nemploy black-hole spectroscopy to show how spatial confinement similarly\naffects the spectrum of nanometre-scale interface waves surrounding a giant\nquantum vortex in superfluid helium-4, an experimentally accessible quantum\nsystem that emulates dynamics in rotating curved spacetime. In the available\nparameter space, we observe regimes in which multiple QNMs emerge from the\ninterface noise spectrum. In agreement with theoretical predictions, their real\nand imaginary frequencies are shifted with respect to those expected in the\nunbounded system. Our results demonstrate the critical role of spatial\nconfinement in shaping the QNM spectrum, highlighting the importance of\nenvironmental effects on spectral stability of astrophysical compact objects."
                },
                "authors": [
                    {
                        "name": "Pietro Smaniotto"
                    },
                    {
                        "name": "Leonardo Solidoro"
                    },
                    {
                        "name": "Patrik vanara"
                    },
                    {
                        "name": "Sam Patrick"
                    },
                    {
                        "name": "Maurcio Richartz"
                    },
                    {
                        "name": "Carlo F. Barenghi"
                    },
                    {
                        "name": "Ruth Gregory"
                    },
                    {
                        "name": "Silke Weinfurtner"
                    }
                ],
                "author_detail": {
                    "name": "Silke Weinfurtner"
                },
                "author": "Silke Weinfurtner",
                "arxiv_comment": "10 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11209v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11209v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19918v1",
                "updated": "2025-04-28T15:46:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    46,
                    2,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:46:02Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    46,
                    2,
                    0,
                    118,
                    0
                ],
                "title": "Enhancing Surgical Documentation through Multimodal Visual-Temporal\n  Transformers and Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Surgical Documentation through Multimodal Visual-Temporal\n  Transformers and Generative AI"
                },
                "summary": "The automatic summarization of surgical videos is essential for enhancing\nprocedural documentation, supporting surgical training, and facilitating\npost-operative analysis. This paper presents a novel method at the intersection\nof artificial intelligence and medicine, aiming to develop machine learning\nmodels with direct real-world applications in surgical contexts. We propose a\nmulti-modal framework that leverages recent advancements in computer vision and\nlarge language models to generate comprehensive video summaries. % The approach\nis structured in three key stages. First, surgical videos are divided into\nclips, and visual features are extracted at the frame level using visual\ntransformers. This step focuses on detecting tools, tissues, organs, and\nsurgical actions. Second, the extracted features are transformed into\nframe-level captions via large language models. These are then combined with\ntemporal features, captured using a ViViT-based encoder, to produce clip-level\nsummaries that reflect the broader context of each video segment. Finally, the\nclip-level descriptions are aggregated into a full surgical report using a\ndedicated LLM tailored for the summarization task. % We evaluate our method on\nthe CholecT50 dataset, using instrument and action annotations from 50\nlaparoscopic videos. The results show strong performance, achieving 96\\%\nprecision in tool detection and a BERT score of 0.74 for temporal context\nsummarization. This work contributes to the advancement of AI-assisted tools\nfor surgical reporting, offering a step toward more intelligent and reliable\nclinical documentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automatic summarization of surgical videos is essential for enhancing\nprocedural documentation, supporting surgical training, and facilitating\npost-operative analysis. This paper presents a novel method at the intersection\nof artificial intelligence and medicine, aiming to develop machine learning\nmodels with direct real-world applications in surgical contexts. We propose a\nmulti-modal framework that leverages recent advancements in computer vision and\nlarge language models to generate comprehensive video summaries. % The approach\nis structured in three key stages. First, surgical videos are divided into\nclips, and visual features are extracted at the frame level using visual\ntransformers. This step focuses on detecting tools, tissues, organs, and\nsurgical actions. Second, the extracted features are transformed into\nframe-level captions via large language models. These are then combined with\ntemporal features, captured using a ViViT-based encoder, to produce clip-level\nsummaries that reflect the broader context of each video segment. Finally, the\nclip-level descriptions are aggregated into a full surgical report using a\ndedicated LLM tailored for the summarization task. % We evaluate our method on\nthe CholecT50 dataset, using instrument and action annotations from 50\nlaparoscopic videos. The results show strong performance, achieving 96\\%\nprecision in tool detection and a BERT score of 0.74 for temporal context\nsummarization. This work contributes to the advancement of AI-assisted tools\nfor surgical reporting, offering a step toward more intelligent and reliable\nclinical documentation."
                },
                "authors": [
                    {
                        "name": "Hugo Georgenthum"
                    },
                    {
                        "name": "Cristian Cosentino"
                    },
                    {
                        "name": "Fabrizio Marozzo"
                    },
                    {
                        "name": "Pietro Li"
                    }
                ],
                "author_detail": {
                    "name": "Pietro Li"
                },
                "author": "Pietro Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19912v1",
                "updated": "2025-04-28T15:41:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    41,
                    28,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:41:28Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    41,
                    28,
                    0,
                    118,
                    0
                ],
                "title": "Can AI Agents Design and Implement Drug Discovery Pipelines?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can AI Agents Design and Implement Drug Discovery Pipelines?"
                },
                "summary": "The rapid advancement of artificial intelligence, particularly autonomous\nagentic systems based on Large Language Models (LLMs), presents new\nopportunities to accelerate drug discovery by improving in-silico modeling and\nreducing dependence on costly experimental trials. Current AI agent-based\nsystems demonstrate proficiency in solving programming challenges and\nconducting research, indicating an emerging potential to develop software\ncapable of addressing complex problems such as pharmaceutical design and drug\ndiscovery. This paper introduces DO Challenge, a benchmark designed to evaluate\nthe decision-making abilities of AI agents in a single, complex problem\nresembling virtual screening scenarios. The benchmark challenges systems to\nindependently develop, implement, and execute efficient strategies for\nidentifying promising molecular structures from extensive datasets, while\nnavigating chemical space, selecting models, and managing limited resources in\na multi-objective context. We also discuss insights from the DO Challenge 2025,\na competition based on the proposed benchmark, which showcased diverse\nstrategies explored by human participants. Furthermore, we present the Deep\nThought multi-agent system, which demonstrated strong performance on the\nbenchmark, outperforming most human teams. Among the language models tested,\nClaude 3.7 Sonnet, Gemini 2.5 Pro and o3 performed best in primary agent roles,\nand GPT-4o, Gemini 2.0 Flash were effective in auxiliary roles. While\npromising, the system's performance still fell short of expert-designed\nsolutions and showed high instability, highlighting both the potential and\ncurrent limitations of AI-driven methodologies in transforming drug discovery\nand broader scientific research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of artificial intelligence, particularly autonomous\nagentic systems based on Large Language Models (LLMs), presents new\nopportunities to accelerate drug discovery by improving in-silico modeling and\nreducing dependence on costly experimental trials. Current AI agent-based\nsystems demonstrate proficiency in solving programming challenges and\nconducting research, indicating an emerging potential to develop software\ncapable of addressing complex problems such as pharmaceutical design and drug\ndiscovery. This paper introduces DO Challenge, a benchmark designed to evaluate\nthe decision-making abilities of AI agents in a single, complex problem\nresembling virtual screening scenarios. The benchmark challenges systems to\nindependently develop, implement, and execute efficient strategies for\nidentifying promising molecular structures from extensive datasets, while\nnavigating chemical space, selecting models, and managing limited resources in\na multi-objective context. We also discuss insights from the DO Challenge 2025,\na competition based on the proposed benchmark, which showcased diverse\nstrategies explored by human participants. Furthermore, we present the Deep\nThought multi-agent system, which demonstrated strong performance on the\nbenchmark, outperforming most human teams. Among the language models tested,\nClaude 3.7 Sonnet, Gemini 2.5 Pro and o3 performed best in primary agent roles,\nand GPT-4o, Gemini 2.0 Flash were effective in auxiliary roles. While\npromising, the system's performance still fell short of expert-designed\nsolutions and showed high instability, highlighting both the potential and\ncurrent limitations of AI-driven methodologies in transforming drug discovery\nand broader scientific research."
                },
                "authors": [
                    {
                        "name": "Khachik Smbatyan"
                    },
                    {
                        "name": "Tsolak Ghukasyan"
                    },
                    {
                        "name": "Tigran Aghajanyan"
                    },
                    {
                        "name": "Hovhannes Dabaghyan"
                    },
                    {
                        "name": "Sergey Adamyan"
                    },
                    {
                        "name": "Aram Bughdaryan"
                    },
                    {
                        "name": "Vahagn Altunyan"
                    },
                    {
                        "name": "Gagik Navasardyan"
                    },
                    {
                        "name": "Aram Davtyan"
                    },
                    {
                        "name": "Anush Hakobyan"
                    },
                    {
                        "name": "Aram Gharibyan"
                    },
                    {
                        "name": "Arman Fahradyan"
                    },
                    {
                        "name": "Artur Hakobyan"
                    },
                    {
                        "name": "Hasmik Mnatsakanyan"
                    },
                    {
                        "name": "Narek Ginoyan"
                    },
                    {
                        "name": "Garik Petrosyan"
                    }
                ],
                "author_detail": {
                    "name": "Garik Petrosyan"
                },
                "author": "Garik Petrosyan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11690v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11690v3",
                "updated": "2025-04-28T15:39:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    39,
                    28,
                    0,
                    118,
                    0
                ],
                "published": "2024-09-18T04:10:44Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    10,
                    44,
                    2,
                    262,
                    0
                ],
                "title": "ID-Free Not Risk-Free: LLM-Powered Agents Unveil Risks in ID-Free\n  Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ID-Free Not Risk-Free: LLM-Powered Agents Unveil Risks in ID-Free\n  Recommender Systems"
                },
                "summary": "Recent advances in ID-free recommender systems have attracted significant\nattention for effectively addressing the cold start problem. However, their\nvulnerability to malicious attacks remains largely unexplored. In this paper,\nwe unveil a critical yet overlooked risk: LLM-powered agents can be\nstrategically deployed to attack ID-free recommenders, stealthily promoting\nlow-quality items in black-box settings. This attack exploits a novel\nrewriting-based deception strategy, where malicious agents synthesize deceptive\ntextual descriptions by simulating the characteristics of popular items. To\nachieve this, the attack mechanism integrates two primary components: (1) a\npopularity extraction component that captures essential characteristics of\npopular items and (2) a multi-agent collaboration mechanism that enables\niterative refinement of promotional textual descriptions through independent\nthinking and team discussion. To counter this risk, we further introduce a\ndetection method to identify suspicious text generated by our discovered\nattack. By unveiling this risk, our work aims to underscore the urgent need to\nenhance the security of ID-free recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in ID-free recommender systems have attracted significant\nattention for effectively addressing the cold start problem. However, their\nvulnerability to malicious attacks remains largely unexplored. In this paper,\nwe unveil a critical yet overlooked risk: LLM-powered agents can be\nstrategically deployed to attack ID-free recommenders, stealthily promoting\nlow-quality items in black-box settings. This attack exploits a novel\nrewriting-based deception strategy, where malicious agents synthesize deceptive\ntextual descriptions by simulating the characteristics of popular items. To\nachieve this, the attack mechanism integrates two primary components: (1) a\npopularity extraction component that captures essential characteristics of\npopular items and (2) a multi-agent collaboration mechanism that enables\niterative refinement of promotional textual descriptions through independent\nthinking and team discussion. To counter this risk, we further introduce a\ndetection method to identify suspicious text generated by our discovered\nattack. By unveiling this risk, our work aims to underscore the urgent need to\nenhance the security of ID-free recommender systems."
                },
                "authors": [
                    {
                        "name": "Zongwei Wang"
                    },
                    {
                        "name": "Min Gao"
                    },
                    {
                        "name": "Junliang Yu"
                    },
                    {
                        "name": "Xinyi Gao"
                    },
                    {
                        "name": "Quoc Viet Hung Nguyen"
                    },
                    {
                        "name": "Shazia Sadiq"
                    },
                    {
                        "name": "Hongzhi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Yin"
                },
                "author": "Hongzhi Yin",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11690v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11690v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19902v1",
                "updated": "2025-04-28T15:33:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    33,
                    15,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:33:15Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    33,
                    15,
                    0,
                    118,
                    0
                ],
                "title": "exoALMA V: Gaseous Emission Surfaces and Temperature Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "exoALMA V: Gaseous Emission Surfaces and Temperature Structures"
                },
                "summary": "Analysis of the gaseous component in protoplanetary disks can inform us about\ntheir thermal and physical structure, chemical composition, and kinematic\nproperties, all of which are crucial for understanding various processes within\nthe disks. By exploiting the asymmetry of the line emission, or via line\nprofile analysis, we can locate the emitting surfaces. Here, we present the\nemission surfaces of the exoALMA sources in $^{12}$CO $J=3-2$, $^{13}$CO\n$J=3-2$, and CS $J=7-6$. We find that $^{12}$CO traces the upper disk\natmosphere, with mean <$z/r$> values of $\\approx$ 0.28, while $^{13}$CO and CS\ntrace lower regions of the disk with mean <z/r> values of $\\approx$ 0.16 and\n$\\approx$ 0.18, respectively. We find that $^{12}$CO <$z/r$> and the disk mass\nare positively correlated with each other; this relationship offers a\nstraightforward way to infer the disk mass. We derive 2-D $r-z$ temperature\ndistributions of the disks. Additionally, we search for substructure in the\nsurfaces and radial intensity profiles; we find evidence of localized\nsubstructure in the emission surfaces and peak intensity profiles of nearly\nevery disk, with this substructure often being co-incident between molecular\ntracers, intensity profiles, and kinematic perturbations. Four disks display\nevidence of potential photo-desorption, implying that this effect may be common\neven in low FUV star-forming regions. For most disks, we find that the physical\nand thermal structure is more complex than analytical models can account for,\nhighlighting a need for more theoretical work and a better understanding of the\nrole of projection effects on our observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of the gaseous component in protoplanetary disks can inform us about\ntheir thermal and physical structure, chemical composition, and kinematic\nproperties, all of which are crucial for understanding various processes within\nthe disks. By exploiting the asymmetry of the line emission, or via line\nprofile analysis, we can locate the emitting surfaces. Here, we present the\nemission surfaces of the exoALMA sources in $^{12}$CO $J=3-2$, $^{13}$CO\n$J=3-2$, and CS $J=7-6$. We find that $^{12}$CO traces the upper disk\natmosphere, with mean <$z/r$> values of $\\approx$ 0.28, while $^{13}$CO and CS\ntrace lower regions of the disk with mean <z/r> values of $\\approx$ 0.16 and\n$\\approx$ 0.18, respectively. We find that $^{12}$CO <$z/r$> and the disk mass\nare positively correlated with each other; this relationship offers a\nstraightforward way to infer the disk mass. We derive 2-D $r-z$ temperature\ndistributions of the disks. Additionally, we search for substructure in the\nsurfaces and radial intensity profiles; we find evidence of localized\nsubstructure in the emission surfaces and peak intensity profiles of nearly\nevery disk, with this substructure often being co-incident between molecular\ntracers, intensity profiles, and kinematic perturbations. Four disks display\nevidence of potential photo-desorption, implying that this effect may be common\neven in low FUV star-forming regions. For most disks, we find that the physical\nand thermal structure is more complex than analytical models can account for,\nhighlighting a need for more theoretical work and a better understanding of the\nrole of projection effects on our observations."
                },
                "authors": [
                    {
                        "name": "Maria Galloway-Sprietsma"
                    },
                    {
                        "name": "Jaehan Bae"
                    },
                    {
                        "name": "Andrs F. Izquierdo"
                    },
                    {
                        "name": "Jochen Stadler"
                    },
                    {
                        "name": "Cristiano Longarini"
                    },
                    {
                        "name": "Richard Teague"
                    },
                    {
                        "name": "Sean M. Andrews"
                    },
                    {
                        "name": "Andrew J. Winter"
                    },
                    {
                        "name": "Myriam Benisty"
                    },
                    {
                        "name": "Stefano Facchini"
                    },
                    {
                        "name": "Giovanni Rosotti"
                    },
                    {
                        "name": "Brianna Zawadzki"
                    },
                    {
                        "name": "Christophe Pinte"
                    },
                    {
                        "name": "Daniele Fasano"
                    },
                    {
                        "name": "Marcelo Barraza-Alfaro"
                    },
                    {
                        "name": "Gianni Cataldi"
                    },
                    {
                        "name": "Nicols Cuello"
                    },
                    {
                        "name": "Pietro Curone"
                    },
                    {
                        "name": "Ian Czekala"
                    },
                    {
                        "name": "Mario Flock"
                    },
                    {
                        "name": "Misato Fukagawa"
                    },
                    {
                        "name": "Charles H. Gardner"
                    },
                    {
                        "name": "Himanshi Garg"
                    },
                    {
                        "name": "Cassandra Hall"
                    },
                    {
                        "name": "Jane Huang"
                    },
                    {
                        "name": "John D. Ilee"
                    },
                    {
                        "name": "Kazuhiro Kanagawa"
                    },
                    {
                        "name": "Geoffroy Lesur"
                    },
                    {
                        "name": "Giuseppe Lodato"
                    },
                    {
                        "name": "Ryan A. Loomis"
                    },
                    {
                        "name": "Francois Menard"
                    },
                    {
                        "name": "Ryuta Orihara"
                    },
                    {
                        "name": "Daniel J. Price"
                    },
                    {
                        "name": "Gaylor Wafflard-Fernandez"
                    },
                    {
                        "name": "David J. Wilner"
                    },
                    {
                        "name": "Lisa Wlfer"
                    },
                    {
                        "name": "Hsi-Wei Yen"
                    },
                    {
                        "name": "Tomohiro C. Yoshida"
                    }
                ],
                "author_detail": {
                    "name": "Tomohiro C. Yoshida"
                },
                "author": "Tomohiro C. Yoshida",
                "arxiv_doi": "10.3847/2041-8213/adc437",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/2041-8213/adc437",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.19902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper is part of the exoALMA Focus Issue of The Astrophysical\n  Journal Letters",
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19898v1",
                "updated": "2025-04-28T15:30:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    30,
                    58,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:30:58Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    30,
                    58,
                    0,
                    118,
                    0
                ],
                "title": "GenCLS++: Pushing the Boundaries of Generative Classification in LLMs\n  Through Comprehensive SFT and RL Studies Across Diverse Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenCLS++: Pushing the Boundaries of Generative Classification in LLMs\n  Through Comprehensive SFT and RL Studies Across Diverse Datasets"
                },
                "summary": "As a fundamental task in machine learning, text classification plays a\ncrucial role in many areas. With the rapid scaling of Large Language Models\n(LLMs), particularly through reinforcement learning (RL), there is a growing\nneed for more capable discriminators. Consequently, advances in classification\nare becoming increasingly vital for enhancing the overall capabilities of LLMs.\nTraditional discriminative methods map text to labels but overlook LLMs'\nintrinsic generative strengths. Generative classification addresses this by\nprompting the model to directly output labels. However, existing studies still\nrely on simple SFT alone, seldom probing the interplay between training and\ninference prompts, and no work has systematically leveraged RL for generative\ntext classifiers and unified SFT, RL, and inference-time prompting in one\nframework. We bridge this gap with GenCLS++, a framework that jointly optimizes\nSFT and RL while systematically exploring five high-level strategy\ndimensions-in-context learning variants, category definitions, explicit\nuncertainty labels, semantically irrelevant numeric labels, and\nperplexity-based decoding-during both training and inference. After an SFT\n\"policy warm-up,\" we apply RL with a simple rule-based reward, yielding sizable\nextra gains. Across seven datasets, GenCLS++ achieves an average accuracy\nimprovement of 3.46% relative to the naive SFT baseline; on public datasets,\nthis improvement rises to 4.00%. Notably, unlike reasoning-intensive tasks that\nbenefit from explicit thinking processes, we find that classification tasks\nperform better without such reasoning steps. These insights into the role of\nexplicit reasoning provide valuable guidance for future LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a fundamental task in machine learning, text classification plays a\ncrucial role in many areas. With the rapid scaling of Large Language Models\n(LLMs), particularly through reinforcement learning (RL), there is a growing\nneed for more capable discriminators. Consequently, advances in classification\nare becoming increasingly vital for enhancing the overall capabilities of LLMs.\nTraditional discriminative methods map text to labels but overlook LLMs'\nintrinsic generative strengths. Generative classification addresses this by\nprompting the model to directly output labels. However, existing studies still\nrely on simple SFT alone, seldom probing the interplay between training and\ninference prompts, and no work has systematically leveraged RL for generative\ntext classifiers and unified SFT, RL, and inference-time prompting in one\nframework. We bridge this gap with GenCLS++, a framework that jointly optimizes\nSFT and RL while systematically exploring five high-level strategy\ndimensions-in-context learning variants, category definitions, explicit\nuncertainty labels, semantically irrelevant numeric labels, and\nperplexity-based decoding-during both training and inference. After an SFT\n\"policy warm-up,\" we apply RL with a simple rule-based reward, yielding sizable\nextra gains. Across seven datasets, GenCLS++ achieves an average accuracy\nimprovement of 3.46% relative to the naive SFT baseline; on public datasets,\nthis improvement rises to 4.00%. Notably, unlike reasoning-intensive tasks that\nbenefit from explicit thinking processes, we find that classification tasks\nperform better without such reasoning steps. These insights into the role of\nexplicit reasoning provide valuable guidance for future LLM applications."
                },
                "authors": [
                    {
                        "name": "Mingqian He"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Chonggang Lu"
                    },
                    {
                        "name": "Ziyan Liu"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Haofu Qian"
                    }
                ],
                "author_detail": {
                    "name": "Haofu Qian"
                },
                "author": "Haofu Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19894v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19894v1",
                "updated": "2025-04-28T15:28:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    28,
                    14,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:28:14Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    28,
                    14,
                    0,
                    118,
                    0
                ],
                "title": "CineVerse: Consistent Keyframe Synthesis for Cinematic Scene Composition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CineVerse: Consistent Keyframe Synthesis for Cinematic Scene Composition"
                },
                "summary": "We present CineVerse, a novel framework for the task of cinematic scene\ncomposition. Similar to traditional multi-shot generation, our task emphasizes\nthe need for consistency and continuity across frames. However, our task also\nfocuses on addressing challenges inherent to filmmaking, such as multiple\ncharacters, complex interactions, and visual cinematic effects. In order to\nlearn to generate such content, we first create the CineVerse dataset. We use\nthis dataset to train our proposed two-stage approach. First, we prompt a large\nlanguage model (LLM) with task-specific instructions to take in a high-level\nscene description and generate a detailed plan for the overall setting and\ncharacters, as well as the individual shots. Then, we fine-tune a text-to-image\ngeneration model to synthesize high-quality visual keyframes. Experimental\nresults demonstrate that CineVerse yields promising improvements in generating\nvisually coherent and contextually rich movie scenes, paving the way for\nfurther exploration in cinematic video synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present CineVerse, a novel framework for the task of cinematic scene\ncomposition. Similar to traditional multi-shot generation, our task emphasizes\nthe need for consistency and continuity across frames. However, our task also\nfocuses on addressing challenges inherent to filmmaking, such as multiple\ncharacters, complex interactions, and visual cinematic effects. In order to\nlearn to generate such content, we first create the CineVerse dataset. We use\nthis dataset to train our proposed two-stage approach. First, we prompt a large\nlanguage model (LLM) with task-specific instructions to take in a high-level\nscene description and generate a detailed plan for the overall setting and\ncharacters, as well as the individual shots. Then, we fine-tune a text-to-image\ngeneration model to synthesize high-quality visual keyframes. Experimental\nresults demonstrate that CineVerse yields promising improvements in generating\nvisually coherent and contextually rich movie scenes, paving the way for\nfurther exploration in cinematic video synthesis."
                },
                "authors": [
                    {
                        "name": "Quynh Phung"
                    },
                    {
                        "name": "Long Mai"
                    },
                    {
                        "name": "Fabian David Caba Heilbron"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Jia-Bin Huang"
                    },
                    {
                        "name": "Cusuh Ham"
                    }
                ],
                "author_detail": {
                    "name": "Cusuh Ham"
                },
                "author": "Cusuh Ham",
                "arxiv_comment": "link website: https://cinevers.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19894v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19894v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14020v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14020v4",
                "updated": "2025-04-28T15:15:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    15,
                    17,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-18T18:18:45Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    18,
                    18,
                    45,
                    4,
                    108,
                    0
                ],
                "title": "HyDra: SOT-CAM Based Vector Symbolic Macro for Hyperdimensional\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyDra: SOT-CAM Based Vector Symbolic Macro for Hyperdimensional\n  Computing"
                },
                "summary": "Hyperdimensional computing (HDC) is a brain-inspired paradigm valued for its\nnoise robustness, parallelism, energy efficiency, and low computational\noverhead. Hardware accelerators are being explored to further enhance their\nperformance, but current solutions are often limited by application specificity\nand the latency of encoding and similarity search. This paper presents a\ngeneralized, reconfigurable on-chip training and inference architecture for\nHDC, utilizing spin-orbit-torque magnetic random access memory (SOT-MRAM) based\ncontent-addressable memory (SOT-CAM). The proposed SOT-CAM array integrates\nstorage and computation, enabling in-memory execution of key HDC operations:\nbinding (bitwise multiplication), permutation (bit shfiting), and efficient\nsimilarity search. Furthermore, a novel bit drop method-based permutation\nbacked by holographic information representation of HDC is proposed which\nreplaces conventional permutation execution in hardware resulting in a 6x\nlatency improvement, and an HDC-specific adder reduces energy and area by 1.51X\nand 1.43x, respectively. To mitigate the parasitic effect of interconnects in\nthe similarity search, a four-stage voltage scaling scheme has been proposed to\nensure an accurate representation of the Hamming distance. Benchmarked at 7nm,\nthe architecture achieves energy reductions of 21.5x, 552.74x, 1.45x, and\n282.57x for addition, permutation, multiplication, and search operations,\nrespectively, compared to CMOS-based HDC. Against state-of-the-art HDC\naccelerators, it achieves a 2.27x lower energy consumption and outperforms CPU\nand eGPU implementations by 2702x and 23161x, respectively, with less than 3%\ndrop in accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperdimensional computing (HDC) is a brain-inspired paradigm valued for its\nnoise robustness, parallelism, energy efficiency, and low computational\noverhead. Hardware accelerators are being explored to further enhance their\nperformance, but current solutions are often limited by application specificity\nand the latency of encoding and similarity search. This paper presents a\ngeneralized, reconfigurable on-chip training and inference architecture for\nHDC, utilizing spin-orbit-torque magnetic random access memory (SOT-MRAM) based\ncontent-addressable memory (SOT-CAM). The proposed SOT-CAM array integrates\nstorage and computation, enabling in-memory execution of key HDC operations:\nbinding (bitwise multiplication), permutation (bit shfiting), and efficient\nsimilarity search. Furthermore, a novel bit drop method-based permutation\nbacked by holographic information representation of HDC is proposed which\nreplaces conventional permutation execution in hardware resulting in a 6x\nlatency improvement, and an HDC-specific adder reduces energy and area by 1.51X\nand 1.43x, respectively. To mitigate the parasitic effect of interconnects in\nthe similarity search, a four-stage voltage scaling scheme has been proposed to\nensure an accurate representation of the Hamming distance. Benchmarked at 7nm,\nthe architecture achieves energy reductions of 21.5x, 552.74x, 1.45x, and\n282.57x for addition, permutation, multiplication, and search operations,\nrespectively, compared to CMOS-based HDC. Against state-of-the-art HDC\naccelerators, it achieves a 2.27x lower energy consumption and outperforms CPU\nand eGPU implementations by 2702x and 23161x, respectively, with less than 3%\ndrop in accuracy."
                },
                "authors": [
                    {
                        "name": "Md Mizanur Rahaman Nayan"
                    },
                    {
                        "name": "Che-Kai Liu"
                    },
                    {
                        "name": "Zishen Wan"
                    },
                    {
                        "name": "Arijit Raychowdhury"
                    },
                    {
                        "name": "Azad J Naeemi"
                    }
                ],
                "author_detail": {
                    "name": "Azad J Naeemi"
                },
                "author": "Azad J Naeemi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14020v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14020v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19867v1",
                "updated": "2025-04-28T15:00:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    0,
                    3,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:00:03Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    0,
                    3,
                    0,
                    118,
                    0
                ],
                "title": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\n  Computation and Unified Storage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\n  Computation and Unified Storage"
                },
                "summary": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models."
                },
                "authors": [
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Lufang Chen"
                    },
                    {
                        "name": "Zhong Wang"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Qiuli Mao"
                    },
                    {
                        "name": "Jianping Ma"
                    },
                    {
                        "name": "Chao Xiong"
                    },
                    {
                        "name": "Guanyu Wu"
                    },
                    {
                        "name": "Buhe Han"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yun Liang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "18 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19863v1",
                "updated": "2025-04-28T14:55:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    55,
                    12,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T14:55:12Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    55,
                    12,
                    0,
                    118,
                    0
                ],
                "title": "Towards Ball Spin and Trajectory Analysis in Table Tennis Broadcast\n  Videos via Physically Grounded Synthetic-to-Real Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Ball Spin and Trajectory Analysis in Table Tennis Broadcast\n  Videos via Physically Grounded Synthetic-to-Real Transfer"
                },
                "summary": "Analyzing a player's technique in table tennis requires knowledge of the\nball's 3D trajectory and spin. While, the spin is not directly observable in\nstandard broadcasting videos, we show that it can be inferred from the ball's\ntrajectory in the video. We present a novel method to infer the initial spin\nand 3D trajectory from the corresponding 2D trajectory in a video. Without\nground truth labels for broadcast videos, we train a neural network solely on\nsynthetic data. Due to the choice of our input data representation, physically\ncorrect synthetic training data, and using targeted augmentations, the network\nnaturally generalizes to real data. Notably, these simple techniques are\nsufficient to achieve generalization. No real data at all is required for\ntraining. To the best of our knowledge, we are the first to present a method\nfor spin and trajectory prediction in simple monocular broadcast videos,\nachieving an accuracy of 92.0% in spin classification and a 2D reprojection\nerror of 0.19% of the image diagonal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing a player's technique in table tennis requires knowledge of the\nball's 3D trajectory and spin. While, the spin is not directly observable in\nstandard broadcasting videos, we show that it can be inferred from the ball's\ntrajectory in the video. We present a novel method to infer the initial spin\nand 3D trajectory from the corresponding 2D trajectory in a video. Without\nground truth labels for broadcast videos, we train a neural network solely on\nsynthetic data. Due to the choice of our input data representation, physically\ncorrect synthetic training data, and using targeted augmentations, the network\nnaturally generalizes to real data. Notably, these simple techniques are\nsufficient to achieve generalization. No real data at all is required for\ntraining. To the best of our knowledge, we are the first to present a method\nfor spin and trajectory prediction in simple monocular broadcast videos,\nachieving an accuracy of 92.0% in spin classification and a 2D reprojection\nerror of 0.19% of the image diagonal."
                },
                "authors": [
                    {
                        "name": "Daniel Kienzle"
                    },
                    {
                        "name": "Robin Schn"
                    },
                    {
                        "name": "Rainer Lienhart"
                    },
                    {
                        "name": "Shin'Ichi Satoh"
                    }
                ],
                "author_detail": {
                    "name": "Shin'Ichi Satoh"
                },
                "author": "Shin'Ichi Satoh",
                "arxiv_comment": "To be published in 2025 IEEE/CVF International Conference on Computer\n  Vision and Pattern Recognition Workshops (CVPRW)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19860v1",
                "updated": "2025-04-28T14:50:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    50,
                    45,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T14:50:45Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    50,
                    45,
                    0,
                    118,
                    0
                ],
                "title": "CoherenDream: Boosting Holistic Text Coherence in 3D Generation via\n  Multimodal Large Language Models Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoherenDream: Boosting Holistic Text Coherence in 3D Generation via\n  Multimodal Large Language Models Feedback"
                },
                "summary": "Score Distillation Sampling (SDS) has achieved remarkable success in\ntext-to-3D content generation. However, SDS-based methods struggle to maintain\nsemantic fidelity for user prompts, particularly when involving multiple\nobjects with intricate interactions. While existing approaches often address 3D\nconsistency through multiview diffusion model fine-tuning on 3D datasets, this\nstrategy inadvertently exacerbates text-3D alignment degradation. The\nlimitation stems from SDS's inherent accumulation of view-independent biases\nduring optimization, which progressively diverges from the ideal text alignment\ndirection. To alleviate this limitation, we propose a novel SDS objective,\ndubbed as Textual Coherent Score Distillation (TCSD), which integrates\nalignment feedback from multimodal large language models (MLLMs). Our TCSD\nleverages cross-modal understanding capabilities of MLLMs to assess and guide\nthe text-3D correspondence during the optimization. We further develop\n3DLLaVA-CRITIC - a fine-tuned MLLM specialized for evaluating multiview text\nalignment in 3D generations. Additionally, we introduce an LLM-layout\ninitialization that significantly accelerates optimization convergence through\nsemantic-aware spatial configuration. Comprehensive evaluations demonstrate\nthat our framework, CoherenDream, establishes state-of-the-art performance in\ntext-aligned 3D generation across multiple benchmarks, including T$^3$Bench and\nTIFA subset. Qualitative results showcase the superior performance of\nCoherenDream in preserving textual consistency and semantic interactions. As\nthe first study to incorporate MLLMs into SDS optimization, we also conduct\nextensive ablation studies to explore optimal MLLM adaptations for 3D\ngeneration tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Score Distillation Sampling (SDS) has achieved remarkable success in\ntext-to-3D content generation. However, SDS-based methods struggle to maintain\nsemantic fidelity for user prompts, particularly when involving multiple\nobjects with intricate interactions. While existing approaches often address 3D\nconsistency through multiview diffusion model fine-tuning on 3D datasets, this\nstrategy inadvertently exacerbates text-3D alignment degradation. The\nlimitation stems from SDS's inherent accumulation of view-independent biases\nduring optimization, which progressively diverges from the ideal text alignment\ndirection. To alleviate this limitation, we propose a novel SDS objective,\ndubbed as Textual Coherent Score Distillation (TCSD), which integrates\nalignment feedback from multimodal large language models (MLLMs). Our TCSD\nleverages cross-modal understanding capabilities of MLLMs to assess and guide\nthe text-3D correspondence during the optimization. We further develop\n3DLLaVA-CRITIC - a fine-tuned MLLM specialized for evaluating multiview text\nalignment in 3D generations. Additionally, we introduce an LLM-layout\ninitialization that significantly accelerates optimization convergence through\nsemantic-aware spatial configuration. Comprehensive evaluations demonstrate\nthat our framework, CoherenDream, establishes state-of-the-art performance in\ntext-aligned 3D generation across multiple benchmarks, including T$^3$Bench and\nTIFA subset. Qualitative results showcase the superior performance of\nCoherenDream in preserving textual consistency and semantic interactions. As\nthe first study to incorporate MLLMs into SDS optimization, we also conduct\nextensive ablation studies to explore optimal MLLM adaptations for 3D\ngeneration tasks."
                },
                "authors": [
                    {
                        "name": "Chenhan Jiang"
                    },
                    {
                        "name": "Yihan Zeng"
                    },
                    {
                        "name": "Hang Xu"
                    },
                    {
                        "name": "Dit-Yan Yeung"
                    }
                ],
                "author_detail": {
                    "name": "Dit-Yan Yeung"
                },
                "author": "Dit-Yan Yeung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19855v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19855v2",
                "updated": "2025-04-29T02:52:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    2,
                    52,
                    54,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-28T14:48:00Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    48,
                    0,
                    0,
                    118,
                    0
                ],
                "title": "The Automation Advantage in AI Red Teaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Automation Advantage in AI Red Teaming"
                },
                "summary": "This paper analyzes Large Language Model (LLM) security vulnerabilities based\non data from Crucible, encompassing 214,271 attack attempts by 1,674 users\nacross 30 LLM challenges. Our findings reveal automated approaches\nsignificantly outperform manual techniques (69.5% vs 47.6% success rate),\ndespite only 5.2% of users employing automation. We demonstrate that automated\napproaches excel in systematic exploration and pattern matching challenges,\nwhile manual approaches retain speed advantages in certain creative reasoning\nscenarios, often solving problems 5x faster when successful. Challenge\ncategories requiring systematic exploration are most effectively targeted\nthrough automation, while intuitive challenges sometimes favor manual\ntechniques for time-to-solve metrics. These results illuminate how algorithmic\ntesting is transforming AI red-teaming practices, with implications for both\noffensive security research and defensive measures. Our analysis suggests\noptimal security testing combines human creativity for strategy development\nwith programmatic execution for thorough exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper analyzes Large Language Model (LLM) security vulnerabilities based\non data from Crucible, encompassing 214,271 attack attempts by 1,674 users\nacross 30 LLM challenges. Our findings reveal automated approaches\nsignificantly outperform manual techniques (69.5% vs 47.6% success rate),\ndespite only 5.2% of users employing automation. We demonstrate that automated\napproaches excel in systematic exploration and pattern matching challenges,\nwhile manual approaches retain speed advantages in certain creative reasoning\nscenarios, often solving problems 5x faster when successful. Challenge\ncategories requiring systematic exploration are most effectively targeted\nthrough automation, while intuitive challenges sometimes favor manual\ntechniques for time-to-solve metrics. These results illuminate how algorithmic\ntesting is transforming AI red-teaming practices, with implications for both\noffensive security research and defensive measures. Our analysis suggests\noptimal security testing combines human creativity for strategy development\nwith programmatic execution for thorough exploration."
                },
                "authors": [
                    {
                        "name": "Rob Mulla"
                    },
                    {
                        "name": "Ads Dawson"
                    },
                    {
                        "name": "Vincent Abruzzon"
                    },
                    {
                        "name": "Brian Greunke"
                    },
                    {
                        "name": "Nick Landers"
                    },
                    {
                        "name": "Brad Palm"
                    },
                    {
                        "name": "Will Pearce"
                    }
                ],
                "author_detail": {
                    "name": "Will Pearce"
                },
                "author": "Will Pearce",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19855v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19855v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5; D.4.6; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19841v1",
                "updated": "2025-04-28T14:41:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    41,
                    13,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T14:41:13Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    41,
                    13,
                    0,
                    118,
                    0
                ],
                "title": "Inference with few treated units",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with few treated units"
                },
                "summary": "In many causal inference applications, only one or a few units (or clusters\nof units) are treated. An important challenge in such settings is that standard\ninference methods that rely on asymptotic theory may be unreliable, even when\nthe total number of units is large. This survey reviews and categorizes\ninference methods that are designed to accommodate few treated units,\nconsidering both cross-sectional and panel data methods. We discuss trade-offs\nand connections between different approaches. In doing so, we propose slight\nmodifications to improve the finite-sample validity of some methods, and we\nalso provide theoretical justifications for existing heuristic approaches that\nhave been proposed in the literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many causal inference applications, only one or a few units (or clusters\nof units) are treated. An important challenge in such settings is that standard\ninference methods that rely on asymptotic theory may be unreliable, even when\nthe total number of units is large. This survey reviews and categorizes\ninference methods that are designed to accommodate few treated units,\nconsidering both cross-sectional and panel data methods. We discuss trade-offs\nand connections between different approaches. In doing so, we propose slight\nmodifications to improve the finite-sample validity of some methods, and we\nalso provide theoretical justifications for existing heuristic approaches that\nhave been proposed in the literature."
                },
                "authors": [
                    {
                        "name": "Luis Alvarez"
                    },
                    {
                        "name": "Bruno Ferman"
                    },
                    {
                        "name": "Kaspar Wthrich"
                    }
                ],
                "author_detail": {
                    "name": "Kaspar Wthrich"
                },
                "author": "Kaspar Wthrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19838v1",
                "updated": "2025-04-28T14:39:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    39,
                    25,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T14:39:25Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    39,
                    25,
                    0,
                    118,
                    0
                ],
                "title": "LLM-Powered GUI Agents in Phone Automation: Surveying Progress and\n  Prospects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Powered GUI Agents in Phone Automation: Surveying Progress and\n  Prospects"
                },
                "summary": "With the rapid rise of large language models (LLMs), phone automation has\nundergone transformative changes. This paper systematically reviews LLM-driven\nphone GUI agents, highlighting their evolution from script-based automation to\nintelligent, adaptive systems. We first contextualize key challenges, (i)\nlimited generality, (ii) high maintenance overhead, and (iii) weak intent\ncomprehension, and show how LLMs address these issues through advanced language\nunderstanding, multimodal perception, and robust decision-making. We then\npropose a taxonomy covering fundamental agent frameworks (single-agent,\nmulti-agent, plan-then-act), modeling approaches (prompt engineering,\ntraining-based), and essential datasets and benchmarks. Furthermore, we detail\ntask-specific architectures, supervised fine-tuning, and reinforcement learning\nstrategies that bridge user intent and GUI operations. Finally, we discuss open\nchallenges such as dataset diversity, on-device deployment efficiency,\nuser-centric adaptation, and security concerns, offering forward-looking\ninsights into this rapidly evolving field. By providing a structured overview\nand identifying pressing research gaps, this paper serves as a definitive\nreference for researchers and practitioners seeking to harness LLMs in\ndesigning scalable, user-friendly phone GUI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid rise of large language models (LLMs), phone automation has\nundergone transformative changes. This paper systematically reviews LLM-driven\nphone GUI agents, highlighting their evolution from script-based automation to\nintelligent, adaptive systems. We first contextualize key challenges, (i)\nlimited generality, (ii) high maintenance overhead, and (iii) weak intent\ncomprehension, and show how LLMs address these issues through advanced language\nunderstanding, multimodal perception, and robust decision-making. We then\npropose a taxonomy covering fundamental agent frameworks (single-agent,\nmulti-agent, plan-then-act), modeling approaches (prompt engineering,\ntraining-based), and essential datasets and benchmarks. Furthermore, we detail\ntask-specific architectures, supervised fine-tuning, and reinforcement learning\nstrategies that bridge user intent and GUI operations. Finally, we discuss open\nchallenges such as dataset diversity, on-device deployment efficiency,\nuser-centric adaptation, and security concerns, offering forward-looking\ninsights into this rapidly evolving field. By providing a structured overview\nand identifying pressing research gaps, this paper serves as a definitive\nreference for researchers and practitioners seeking to harness LLMs in\ndesigning scalable, user-friendly phone GUI agents."
                },
                "authors": [
                    {
                        "name": "Guangyi Liu"
                    },
                    {
                        "name": "Pengxiang Zhao"
                    },
                    {
                        "name": "Liang Liu"
                    },
                    {
                        "name": "Yaxuan Guo"
                    },
                    {
                        "name": "Han Xiao"
                    },
                    {
                        "name": "Weifeng Lin"
                    },
                    {
                        "name": "Yuxiang Chai"
                    },
                    {
                        "name": "Yue Han"
                    },
                    {
                        "name": "Shuai Ren"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Xiaoyu Liang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Tianze Wu"
                    },
                    {
                        "name": "Linghao Li"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Guanjing Xiong"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "arxiv_comment": "37 pages, 10 figures, 7 tables, Project Homepage:\n  https://github.com/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15622v2",
                "updated": "2025-04-28T14:28:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    28,
                    14,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-22T06:28:08Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    6,
                    28,
                    8,
                    1,
                    112,
                    0
                ],
                "title": "Exploring the Role of Large Language Models in Cybersecurity: A\n  Systematic Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Role of Large Language Models in Cybersecurity: A\n  Systematic Survey"
                },
                "summary": "With the rapid development of technology and the acceleration of\ndigitalisation, the frequency and complexity of cyber security threats are\nincreasing. Traditional cybersecurity approaches, often based on static rules\nand predefined scenarios, are struggling to adapt to the rapidly evolving\nnature of modern cyberattacks. There is an urgent need for more adaptive and\nintelligent defence strategies. The emergence of Large Language Model (LLM)\nprovides an innovative solution to cope with the increasingly severe cyber\nthreats, and its potential in analysing complex attack patterns, predicting\nthreats and assisting real-time response has attracted a lot of attention in\nthe field of cybersecurity, and exploring how to effectively use LLM to defend\nagainst cyberattacks has become a hot topic in the current research field. This\nsurvey examines the applications of LLM from the perspective of the cyber\nattack lifecycle, focusing on the three phases of defense reconnaissance,\nfoothold establishment, and lateral movement, and it analyzes the potential of\nLLMs in Cyber Threat Intelligence (CTI) tasks. Meanwhile, we investigate how\nLLM-based security solutions are deployed and applied in different network\nscenarios. It also summarizes the internal and external risk issues faced by\nLLM during its application. Finally, this survey also points out the facing\nrisk issues and possible future research directions in this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of technology and the acceleration of\ndigitalisation, the frequency and complexity of cyber security threats are\nincreasing. Traditional cybersecurity approaches, often based on static rules\nand predefined scenarios, are struggling to adapt to the rapidly evolving\nnature of modern cyberattacks. There is an urgent need for more adaptive and\nintelligent defence strategies. The emergence of Large Language Model (LLM)\nprovides an innovative solution to cope with the increasingly severe cyber\nthreats, and its potential in analysing complex attack patterns, predicting\nthreats and assisting real-time response has attracted a lot of attention in\nthe field of cybersecurity, and exploring how to effectively use LLM to defend\nagainst cyberattacks has become a hot topic in the current research field. This\nsurvey examines the applications of LLM from the perspective of the cyber\nattack lifecycle, focusing on the three phases of defense reconnaissance,\nfoothold establishment, and lateral movement, and it analyzes the potential of\nLLMs in Cyber Threat Intelligence (CTI) tasks. Meanwhile, we investigate how\nLLM-based security solutions are deployed and applied in different network\nscenarios. It also summarizes the internal and external risk issues faced by\nLLM during its application. Finally, this survey also points out the facing\nrisk issues and possible future research directions in this domain."
                },
                "authors": [
                    {
                        "name": "Shuang Tian"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Jiqiang Liu"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Xuangou Wu"
                    },
                    {
                        "name": "Xiaoqiang Zhu"
                    },
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Weiting Zhang"
                    },
                    {
                        "name": "Zhenhui Yuan"
                    },
                    {
                        "name": "Shiwen Mao"
                    },
                    {
                        "name": "Dong In Kim"
                    }
                ],
                "author_detail": {
                    "name": "Dong In Kim"
                },
                "author": "Dong In Kim",
                "arxiv_comment": "20 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19811v1",
                "updated": "2025-04-28T14:08:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    8,
                    45,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T14:08:45Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    8,
                    45,
                    0,
                    118,
                    0
                ],
                "title": "Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language\n  Model Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language\n  Model Performance"
                },
                "summary": "Accurately forecasting the performance of Large Language Models (LLMs) before\nextensive fine-tuning or merging can substantially reduce both computational\nexpense and development time. Although prior approaches like scaling laws\naccount for global factors such as parameter size or training tokens, they\noften overlook explicit lineage relationships - i.e., which models are derived\nor merged from which parents. In this work, we propose a novel\nLineage-Regularized Matrix Factorization (LRMF) framework that encodes\nancestral ties among LLMs via a graph Laplacian regularizer. By leveraging\nmulti-hop parent-child connections, LRMF consistently outperforms conventional\nmatrix factorization and collaborative filtering methods in both instance-level\nand benchmark-level performance prediction. Our large-scale study includes\n2,934 publicly available Hugging Face models and 21,000+ instances across 6\nmajor benchmarks, showing that lineage constraints yield up to 7-10 percentage\npoints higher correlation with actual performance compared to baselines.\nMoreover, LRMF effectively addresses the cold-start problem, providing accurate\nestimates for newly derived or merged models even with minimal data. This\nlineage-guided strategy thus offers a resource-efficient way to inform\nhyperparameter tuning, data selection, and model combination in modern LLM\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately forecasting the performance of Large Language Models (LLMs) before\nextensive fine-tuning or merging can substantially reduce both computational\nexpense and development time. Although prior approaches like scaling laws\naccount for global factors such as parameter size or training tokens, they\noften overlook explicit lineage relationships - i.e., which models are derived\nor merged from which parents. In this work, we propose a novel\nLineage-Regularized Matrix Factorization (LRMF) framework that encodes\nancestral ties among LLMs via a graph Laplacian regularizer. By leveraging\nmulti-hop parent-child connections, LRMF consistently outperforms conventional\nmatrix factorization and collaborative filtering methods in both instance-level\nand benchmark-level performance prediction. Our large-scale study includes\n2,934 publicly available Hugging Face models and 21,000+ instances across 6\nmajor benchmarks, showing that lineage constraints yield up to 7-10 percentage\npoints higher correlation with actual performance compared to baselines.\nMoreover, LRMF effectively addresses the cold-start problem, providing accurate\nestimates for newly derived or merged models even with minimal data. This\nlineage-guided strategy thus offers a resource-efficient way to inform\nhyperparameter tuning, data selection, and model combination in modern LLM\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Takuya Tamura"
                    },
                    {
                        "name": "Taro Yano"
                    },
                    {
                        "name": "Masafumi Enomoto"
                    },
                    {
                        "name": "Masafumi Oyamada"
                    }
                ],
                "author_detail": {
                    "name": "Masafumi Oyamada"
                },
                "author": "Masafumi Oyamada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17827v2",
                "updated": "2025-04-28T13:44:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    13,
                    44,
                    19,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-24T03:09:04Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    3,
                    9,
                    4,
                    3,
                    114,
                    0
                ],
                "title": "Evolution Meets Diffusion: Efficient Neural Architecture Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolution Meets Diffusion: Efficient Neural Architecture Generation"
                },
                "summary": "Neural Architecture Search (NAS) has gained widespread attention for its\ntransformative potential in deep learning model design. However, the vast and\ncomplex search space of NAS leads to significant computational and time costs.\nNeural Architecture Generation (NAG) addresses this by reframing NAS as a\ngeneration problem, enabling the precise generation of optimal architectures\nfor specific tasks. Despite its promise, mainstream methods like diffusion\nmodels face limitations in global search capabilities and are still hindered by\nhigh computational and time demands. To overcome these challenges, we propose\nEvolutionary Diffusion-based Neural Architecture Generation (EDNAG), a novel\napproach that achieves efficient and training-free architecture generation.\nEDNAG leverages evolutionary algorithms to simulate the denoising process in\ndiffusion models, using fitness to guide the transition from random Gaussian\ndistributions to optimal architecture distributions. This approach combines the\nstrengths of evolutionary strategies and diffusion models, enabling rapid and\neffective architecture generation. Extensive experiments demonstrate that EDNAG\nachieves state-of-the-art (SOTA) performance in architecture optimization, with\nan improvement in accuracy of up to 10.45%. Furthermore, it eliminates the need\nfor time-consuming training and boosts inference speed by an average of 50\ntimes, showcasing its exceptional efficiency and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Architecture Search (NAS) has gained widespread attention for its\ntransformative potential in deep learning model design. However, the vast and\ncomplex search space of NAS leads to significant computational and time costs.\nNeural Architecture Generation (NAG) addresses this by reframing NAS as a\ngeneration problem, enabling the precise generation of optimal architectures\nfor specific tasks. Despite its promise, mainstream methods like diffusion\nmodels face limitations in global search capabilities and are still hindered by\nhigh computational and time demands. To overcome these challenges, we propose\nEvolutionary Diffusion-based Neural Architecture Generation (EDNAG), a novel\napproach that achieves efficient and training-free architecture generation.\nEDNAG leverages evolutionary algorithms to simulate the denoising process in\ndiffusion models, using fitness to guide the transition from random Gaussian\ndistributions to optimal architecture distributions. This approach combines the\nstrengths of evolutionary strategies and diffusion models, enabling rapid and\neffective architecture generation. Extensive experiments demonstrate that EDNAG\nachieves state-of-the-art (SOTA) performance in architecture optimization, with\nan improvement in accuracy of up to 10.45%. Furthermore, it eliminates the need\nfor time-consuming training and boosts inference speed by an average of 50\ntimes, showcasing its exceptional efficiency and effectiveness."
                },
                "authors": [
                    {
                        "name": "Bingye Zhou"
                    },
                    {
                        "name": "Caiyang Yu"
                    }
                ],
                "author_detail": {
                    "name": "Caiyang Yu"
                },
                "author": "Caiyang Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11563v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11563v3",
                "updated": "2025-04-28T13:42:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    13,
                    42,
                    0,
                    0,
                    118,
                    0
                ],
                "published": "2024-01-21T18:43:55Z",
                "published_parsed": [
                    2024,
                    1,
                    21,
                    18,
                    43,
                    55,
                    6,
                    21,
                    0
                ],
                "title": "Distributed Multi-Task Learning for Stochastic Bandits with Context\n  Distribution and Stage-wise Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Multi-Task Learning for Stochastic Bandits with Context\n  Distribution and Stage-wise Constraints"
                },
                "summary": "We present conservative distributed multi-task learning in stochastic linear\ncontextual bandits with heterogeneous agents. This extends conservative linear\nbandits to a distributed setting where M agents tackle different but related\ntasks while adhering to stage-wise performance constraints. The exact context\nis unknown, and only a context distribution is available to the agents as in\nmany practical applications that involve a prediction mechanism to infer\ncontext, such as stock market prediction and weather forecast. We propose a\ndistributed upper confidence bound (UCB) algorithm, DiSC-UCB. Our algorithm\nconstructs a pruned action set during each round to ensure the constraints are\nmet. Additionally, it includes synchronized sharing of estimates among agents\nvia a central server using well-structured synchronization steps. We prove the\nregret and communication bounds on the algorithm. We extend the problem to a\nsetting where the agents are unaware of the baseline reward. For this setting,\nwe provide a modified algorithm, DiSC-UCB2, and we show that the modified\nalgorithm achieves the same regret and communication bounds. We empirically\nvalidated the performance of our algorithm on synthetic data and real-world\nMovielens-100K data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present conservative distributed multi-task learning in stochastic linear\ncontextual bandits with heterogeneous agents. This extends conservative linear\nbandits to a distributed setting where M agents tackle different but related\ntasks while adhering to stage-wise performance constraints. The exact context\nis unknown, and only a context distribution is available to the agents as in\nmany practical applications that involve a prediction mechanism to infer\ncontext, such as stock market prediction and weather forecast. We propose a\ndistributed upper confidence bound (UCB) algorithm, DiSC-UCB. Our algorithm\nconstructs a pruned action set during each round to ensure the constraints are\nmet. Additionally, it includes synchronized sharing of estimates among agents\nvia a central server using well-structured synchronization steps. We prove the\nregret and communication bounds on the algorithm. We extend the problem to a\nsetting where the agents are unaware of the baseline reward. For this setting,\nwe provide a modified algorithm, DiSC-UCB2, and we show that the modified\nalgorithm achieves the same regret and communication bounds. We empirically\nvalidated the performance of our algorithm on synthetic data and real-world\nMovielens-100K data."
                },
                "authors": [
                    {
                        "name": "Jiabin Lin"
                    },
                    {
                        "name": "Shana Moothedath"
                    }
                ],
                "author_detail": {
                    "name": "Shana Moothedath"
                },
                "author": "Shana Moothedath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.11563v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11563v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19797v1",
                "updated": "2025-04-28T13:38:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    13,
                    38,
                    53,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T13:38:53Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    13,
                    38,
                    53,
                    0,
                    118,
                    0
                ],
                "title": "Dynamic Tsetlin Machine Accelerators for On-Chip Training at the Edge\n  using FPGAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Tsetlin Machine Accelerators for On-Chip Training at the Edge\n  using FPGAs"
                },
                "summary": "The increased demand for data privacy and security in machine learning (ML)\napplications has put impetus on effective edge training on Internet-of-Things\n(IoT) nodes. Edge training aims to leverage speed, energy efficiency and\nadaptability within the resource constraints of the nodes. Deploying and\ntraining Deep Neural Networks (DNNs)-based models at the edge, although\naccurate, posit significant challenges from the back-propagation algorithm's\ncomplexity, bit precision trade-offs, and heterogeneity of DNN layers. This\npaper presents a Dynamic Tsetlin Machine (DTM) training accelerator as an\nalternative to DNN implementations. DTM utilizes logic-based on-chip inference\nwith finite-state automata-driven learning within the same Field Programmable\nGate Array (FPGA) package. Underpinned on the Vanilla and Coalesced Tsetlin\nMachine algorithms, the dynamic aspect of the accelerator design allows for a\nrun-time reconfiguration targeting different datasets, model architectures, and\nmodel sizes without resynthesis. This makes the DTM suitable for targeting\nmultivariate sensor-based edge tasks. Compared to DNNs, DTM trains with fewer\nmultiply-accumulates, devoid of derivative computation. It is a data-centric ML\nalgorithm that learns by aligning Tsetlin automata with input data to form\nlogical propositions enabling efficient Look-up-Table (LUT) mapping and frugal\nBlock RAM usage in FPGA training implementations. The proposed accelerator\noffers 2.54x more Giga operations per second per Watt (GOP/s per W) and uses 6x\nless power than the next-best comparable design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increased demand for data privacy and security in machine learning (ML)\napplications has put impetus on effective edge training on Internet-of-Things\n(IoT) nodes. Edge training aims to leverage speed, energy efficiency and\nadaptability within the resource constraints of the nodes. Deploying and\ntraining Deep Neural Networks (DNNs)-based models at the edge, although\naccurate, posit significant challenges from the back-propagation algorithm's\ncomplexity, bit precision trade-offs, and heterogeneity of DNN layers. This\npaper presents a Dynamic Tsetlin Machine (DTM) training accelerator as an\nalternative to DNN implementations. DTM utilizes logic-based on-chip inference\nwith finite-state automata-driven learning within the same Field Programmable\nGate Array (FPGA) package. Underpinned on the Vanilla and Coalesced Tsetlin\nMachine algorithms, the dynamic aspect of the accelerator design allows for a\nrun-time reconfiguration targeting different datasets, model architectures, and\nmodel sizes without resynthesis. This makes the DTM suitable for targeting\nmultivariate sensor-based edge tasks. Compared to DNNs, DTM trains with fewer\nmultiply-accumulates, devoid of derivative computation. It is a data-centric ML\nalgorithm that learns by aligning Tsetlin automata with input data to form\nlogical propositions enabling efficient Look-up-Table (LUT) mapping and frugal\nBlock RAM usage in FPGA training implementations. The proposed accelerator\noffers 2.54x more Giga operations per second per Watt (GOP/s per W) and uses 6x\nless power than the next-best comparable design."
                },
                "authors": [
                    {
                        "name": "Gang Mao"
                    },
                    {
                        "name": "Tousif Rahman"
                    },
                    {
                        "name": "Sidharth Maheshwari"
                    },
                    {
                        "name": "Bob Pattison"
                    },
                    {
                        "name": "Zhuang Shao"
                    },
                    {
                        "name": "Rishad Shafik"
                    },
                    {
                        "name": "Alex Yakovlev"
                    }
                ],
                "author_detail": {
                    "name": "Alex Yakovlev"
                },
                "author": "Alex Yakovlev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19793v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19793v1",
                "updated": "2025-04-28T13:36:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    13,
                    36,
                    43,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T13:36:43Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    13,
                    36,
                    43,
                    0,
                    118,
                    0
                ],
                "title": "Prompt Injection Attack to Tool Selection in LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Injection Attack to Tool Selection in LLM Agents"
                },
                "summary": "Tool selection is a key component of LLM agents. The process operates through\na two-step mechanism - \\emph{retrieval} and \\emph{selection} - to pick the most\nappropriate tool from a tool library for a given task. In this work, we\nintroduce \\textit{ToolHijacker}, a novel prompt injection attack targeting tool\nselection in no-box scenarios. ToolHijacker injects a malicious tool document\ninto the tool library to manipulate the LLM agent's tool selection process,\ncompelling it to consistently choose the attacker's malicious tool for an\nattacker-chosen target task. Specifically, we formulate the crafting of such\ntool documents as an optimization problem and propose a two-phase optimization\nstrategy to solve it. Our extensive experimental evaluation shows that\nToolHijacker is highly effective, significantly outperforming existing\nmanual-based and automated prompt injection attacks when applied to tool\nselection. Moreover, we explore various defenses, including prevention-based\ndefenses (StruQ and SecAlign) and detection-based defenses (known-answer\ndetection, perplexity detection, and perplexity windowed detection). Our\nexperimental results indicate that these defenses are insufficient,\nhighlighting the urgent need for developing new defense strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool selection is a key component of LLM agents. The process operates through\na two-step mechanism - \\emph{retrieval} and \\emph{selection} - to pick the most\nappropriate tool from a tool library for a given task. In this work, we\nintroduce \\textit{ToolHijacker}, a novel prompt injection attack targeting tool\nselection in no-box scenarios. ToolHijacker injects a malicious tool document\ninto the tool library to manipulate the LLM agent's tool selection process,\ncompelling it to consistently choose the attacker's malicious tool for an\nattacker-chosen target task. Specifically, we formulate the crafting of such\ntool documents as an optimization problem and propose a two-phase optimization\nstrategy to solve it. Our extensive experimental evaluation shows that\nToolHijacker is highly effective, significantly outperforming existing\nmanual-based and automated prompt injection attacks when applied to tool\nselection. Moreover, we explore various defenses, including prevention-based\ndefenses (StruQ and SecAlign) and detection-based defenses (known-answer\ndetection, perplexity detection, and perplexity windowed detection). Our\nexperimental results indicate that these defenses are insufficient,\nhighlighting the urgent need for developing new defense strategies."
                },
                "authors": [
                    {
                        "name": "Jiawen Shi"
                    },
                    {
                        "name": "Zenghui Yuan"
                    },
                    {
                        "name": "Guiyao Tie"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Neil Zhenqiang Gong"
                    },
                    {
                        "name": "Lichao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lichao Sun"
                },
                "author": "Lichao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19793v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19793v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05474v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05474v2",
                "updated": "2025-04-28T13:22:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    13,
                    22,
                    11,
                    0,
                    118,
                    0
                ],
                "published": "2024-11-08T11:00:05Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    11,
                    0,
                    5,
                    4,
                    313,
                    0
                ],
                "title": "Enhancing Robustness in Language-Driven Robotics: A Modular Approach to\n  Failure Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Robustness in Language-Driven Robotics: A Modular Approach to\n  Failure Reduction"
                },
                "summary": "Recent advances in large language models (LLMs) have led to significant\nprogress in robotics, enabling embodied agents to better understand and execute\nopen-ended tasks. However, existing approaches using LLMs face limitations in\ngrounding their outputs within the physical environment and aligning with the\ncapabilities of the robot. This challenge becomes even more pronounced with\nsmaller language models, which are more computationally efficient but less\nrobust in task planning and execution. In this paper, we present a novel\nmodular architecture designed to enhance the robustness of LLM-driven robotics\nby addressing these grounding and alignment issues. We formalize the task\nplanning problem within a goal-conditioned POMDP framework, identify key\nfailure modes in LLM-driven planning, and propose targeted design principles to\nmitigate these issues. Our architecture introduces an ``expected outcomes''\nmodule to prevent mischaracterization of subgoals and a feedback mechanism to\nenable real-time error recovery. Experimental results, both in simulation and\non physical robots, demonstrate that our approach significantly improves task\nsuccess rates for pick-and-place and manipulation tasks compared to both larger\nLLMs and standard baselines. Through hardware experiments, we also demonstrate\nhow our architecture can be run efficiently and locally. This work highlights\nthe potential of smaller, locally-executable LLMs in robotics and provides a\nscalable, efficient solution for robust task execution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have led to significant\nprogress in robotics, enabling embodied agents to better understand and execute\nopen-ended tasks. However, existing approaches using LLMs face limitations in\ngrounding their outputs within the physical environment and aligning with the\ncapabilities of the robot. This challenge becomes even more pronounced with\nsmaller language models, which are more computationally efficient but less\nrobust in task planning and execution. In this paper, we present a novel\nmodular architecture designed to enhance the robustness of LLM-driven robotics\nby addressing these grounding and alignment issues. We formalize the task\nplanning problem within a goal-conditioned POMDP framework, identify key\nfailure modes in LLM-driven planning, and propose targeted design principles to\nmitigate these issues. Our architecture introduces an ``expected outcomes''\nmodule to prevent mischaracterization of subgoals and a feedback mechanism to\nenable real-time error recovery. Experimental results, both in simulation and\non physical robots, demonstrate that our approach significantly improves task\nsuccess rates for pick-and-place and manipulation tasks compared to both larger\nLLMs and standard baselines. Through hardware experiments, we also demonstrate\nhow our architecture can be run efficiently and locally. This work highlights\nthe potential of smaller, locally-executable LLMs in robotics and provides a\nscalable, efficient solution for robust task execution."
                },
                "authors": [
                    {
                        "name": "miland Garrab"
                    },
                    {
                        "name": "Pierre Teixeira"
                    },
                    {
                        "name": "Mahdi Khoramshahi"
                    },
                    {
                        "name": "Stphane Doncieux"
                    }
                ],
                "author_detail": {
                    "name": "Stphane Doncieux"
                },
                "author": "Stphane Doncieux",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05474v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05474v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T40 (Primary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14758v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14758v3",
                "updated": "2025-04-28T13:14:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    13,
                    14,
                    58,
                    0,
                    118,
                    0
                ],
                "published": "2024-12-19T11:39:10Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    11,
                    39,
                    10,
                    3,
                    354,
                    0
                ],
                "title": "Semantic Foundations of Reductive Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Foundations of Reductive Reasoning"
                },
                "summary": "The development of logic has largely been through the 'deductive' paradigm:\nconclusions are inferred from established premisses. However, the use of logic\nin the context of both human and machine reasoning is typically through the\ndual 'reductive' perspective: collections of sufficient premisses are generated\nfrom putative conclusions. We call this paradigm, 'reductive logic'. This\nexpression of logic encompass as diverse reasoning activities as proving a\nformula in a formal system to seeking to meet a friend before noon on Saturday.\nThis paper is a semantical analysis of reductive logic. In particular, we\nprovide mathematical foundations for representing and reasoning about\n'reduction operators'. Heuristically, reduction operators may be thought of as\n`backwards' inference rules. In this paper, we address their mathematical\nrepresentation, how they are used in the context of reductive reasoning, and,\ncrucially, what makes them 'valid'.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of logic has largely been through the 'deductive' paradigm:\nconclusions are inferred from established premisses. However, the use of logic\nin the context of both human and machine reasoning is typically through the\ndual 'reductive' perspective: collections of sufficient premisses are generated\nfrom putative conclusions. We call this paradigm, 'reductive logic'. This\nexpression of logic encompass as diverse reasoning activities as proving a\nformula in a formal system to seeking to meet a friend before noon on Saturday.\nThis paper is a semantical analysis of reductive logic. In particular, we\nprovide mathematical foundations for representing and reasoning about\n'reduction operators'. Heuristically, reduction operators may be thought of as\n`backwards' inference rules. In this paper, we address their mathematical\nrepresentation, how they are used in the context of reductive reasoning, and,\ncrucially, what makes them 'valid'."
                },
                "authors": [
                    {
                        "name": "Alexander V. Gheorghiu"
                    },
                    {
                        "name": "David J. Pym"
                    }
                ],
                "author_detail": {
                    "name": "David J. Pym"
                },
                "author": "David J. Pym",
                "arxiv_journal_ref": "Topoi 2024, Special issue 'Meaning and Understanding via Proofs'",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14758v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14758v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03111v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03111v2",
                "updated": "2025-04-28T13:07:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    13,
                    7,
                    40,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-04T01:41:06Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    1,
                    41,
                    6,
                    4,
                    94,
                    0
                ],
                "title": "Les Dissonances: Cross-Tool Harvesting and Polluting in Multi-Tool\n  Empowered LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Les Dissonances: Cross-Tool Harvesting and Polluting in Multi-Tool\n  Empowered LLM Agents"
                },
                "summary": "Large Language Model (LLM) agents are autonomous systems powered by LLMs,\ncapable of reasoning and planning to solve problems by leveraging a set of\ntools. However, the integration of multi-tool capabilities in LLM agents\nintroduces challenges in securely managing tools, ensuring their compatibility,\nhandling dependency relationships, and protecting control flows within LLM\nagent workflows. In this paper, we present the first systematic security\nanalysis of task control flows in multi-tool-enabled LLM agents. We identify a\nnovel threat, Cross-Tool Harvesting and Polluting (XTHP), which includes\nmultiple attack vectors to first hijack the normal control flows of agent\ntasks, and then collect and pollute confidential or private information within\nLLM agent systems. To understand the impact of this threat, we developed Chord,\na dynamic scanning tool designed to automatically detect real-world agent tools\nsusceptible to XTHP attacks. Our evaluation of 66 real-world tools from the\nrepositories of two major LLM agent development frameworks, LangChain and\nLlamaIndex, revealed a significant security concern: 75\\% are vulnerable to\nXTHP attacks, highlighting the prevalence of this threat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents are autonomous systems powered by LLMs,\ncapable of reasoning and planning to solve problems by leveraging a set of\ntools. However, the integration of multi-tool capabilities in LLM agents\nintroduces challenges in securely managing tools, ensuring their compatibility,\nhandling dependency relationships, and protecting control flows within LLM\nagent workflows. In this paper, we present the first systematic security\nanalysis of task control flows in multi-tool-enabled LLM agents. We identify a\nnovel threat, Cross-Tool Harvesting and Polluting (XTHP), which includes\nmultiple attack vectors to first hijack the normal control flows of agent\ntasks, and then collect and pollute confidential or private information within\nLLM agent systems. To understand the impact of this threat, we developed Chord,\na dynamic scanning tool designed to automatically detect real-world agent tools\nsusceptible to XTHP attacks. Our evaluation of 66 real-world tools from the\nrepositories of two major LLM agent development frameworks, LangChain and\nLlamaIndex, revealed a significant security concern: 75\\% are vulnerable to\nXTHP attacks, highlighting the prevalence of this threat."
                },
                "authors": [
                    {
                        "name": "Zichuan Li"
                    },
                    {
                        "name": "Jian Cui"
                    },
                    {
                        "name": "Xiaojing Liao"
                    },
                    {
                        "name": "Luyi Xing"
                    }
                ],
                "author_detail": {
                    "name": "Luyi Xing"
                },
                "author": "Luyi Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03111v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03111v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05360v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05360v2",
                "updated": "2025-04-28T13:05:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    13,
                    5,
                    39,
                    0,
                    118,
                    0
                ],
                "published": "2025-03-07T12:03:43Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    12,
                    3,
                    43,
                    4,
                    66,
                    0
                ],
                "title": "On an Inferential Semantics for Intuitionistic Sentential Logic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On an Inferential Semantics for Intuitionistic Sentential Logic"
                },
                "summary": "Sandqvist's base-extension semantics (B-eS) for intuitionistic sentential\nlogic grounds meaning relative to bases (rather than, say, models), which are\narbitrary sets of permitted inferences over sentences. While his soundness\nproof is standard, his completeness proof, is quite unusual. It closely\nparallels a method introduced much earlier by Mints, who developed a\nresolution-based approach to intuitionistic logic using a systematic\ntranslation of formulas into sentential counterparts. In this short note, we\nhighlight the connection between these two approaches and show that the\nsoundness and completeness of B-eS follow directly from Mints' theorem. While\nthe result is modest, it reinforces the relevance of proof-search to\nproof-theoretic semantics and suggests that resolution methods have a deeper\nconceptual role in constructive reasoning than is often acknowledged.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sandqvist's base-extension semantics (B-eS) for intuitionistic sentential\nlogic grounds meaning relative to bases (rather than, say, models), which are\narbitrary sets of permitted inferences over sentences. While his soundness\nproof is standard, his completeness proof, is quite unusual. It closely\nparallels a method introduced much earlier by Mints, who developed a\nresolution-based approach to intuitionistic logic using a systematic\ntranslation of formulas into sentential counterparts. In this short note, we\nhighlight the connection between these two approaches and show that the\nsoundness and completeness of B-eS follow directly from Mints' theorem. While\nthe result is modest, it reinforces the relevance of proof-search to\nproof-theoretic semantics and suggests that resolution methods have a deeper\nconceptual role in constructive reasoning than is often acknowledged."
                },
                "authors": [
                    {
                        "name": "Alexander V. Gheorghiu"
                    }
                ],
                "author_detail": {
                    "name": "Alexander V. Gheorghiu"
                },
                "author": "Alexander V. Gheorghiu",
                "arxiv_journal_ref": "Journal of Logic and Computation 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05360v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05360v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19759v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19759v1",
                "updated": "2025-04-28T12:56:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    56,
                    36,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T12:56:36Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    56,
                    36,
                    0,
                    118,
                    0
                ],
                "title": "Moral Reasoning Across Languages: The Critical Role of Low-Resource\n  Languages in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moral Reasoning Across Languages: The Critical Role of Low-Resource\n  Languages in LLMs"
                },
                "summary": "In this paper, we introduce the Multilingual Moral Reasoning Benchmark (MMRB)\nto evaluate the moral reasoning abilities of large language models (LLMs)\nacross five typologically diverse languages and three levels of contextual\ncomplexity: sentence, paragraph, and document. Our results show moral reasoning\nperformance degrades with increasing context complexity, particularly for\nlow-resource languages such as Vietnamese. We further fine-tune the open-source\nLLaMA-3-8B model using curated monolingual data for alignment and poisoning.\nSurprisingly, low-resource languages have a stronger impact on multilingual\nreasoning than high-resource ones, highlighting their critical role in\nmultilingual NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce the Multilingual Moral Reasoning Benchmark (MMRB)\nto evaluate the moral reasoning abilities of large language models (LLMs)\nacross five typologically diverse languages and three levels of contextual\ncomplexity: sentence, paragraph, and document. Our results show moral reasoning\nperformance degrades with increasing context complexity, particularly for\nlow-resource languages such as Vietnamese. We further fine-tune the open-source\nLLaMA-3-8B model using curated monolingual data for alignment and poisoning.\nSurprisingly, low-resource languages have a stronger impact on multilingual\nreasoning than high-resource ones, highlighting their critical role in\nmultilingual NLP."
                },
                "authors": [
                    {
                        "name": "Huichi Zhou"
                    },
                    {
                        "name": "Zehao Xu"
                    },
                    {
                        "name": "Munan Zhao"
                    },
                    {
                        "name": "Kaihong Li"
                    },
                    {
                        "name": "Yiqiang Li"
                    },
                    {
                        "name": "Hongtao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongtao Wang"
                },
                "author": "Hongtao Wang",
                "arxiv_comment": "5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19759v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19759v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04178v2",
                "updated": "2025-04-28T12:53:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    53,
                    23,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-05T13:48:33Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    13,
                    48,
                    33,
                    5,
                    95,
                    0
                ],
                "title": "MSL: Not All Tokens Are What You Need for Tuning LLM as a Recommender",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSL: Not All Tokens Are What You Need for Tuning LLM as a Recommender"
                },
                "summary": "Large language models (LLMs), known for their comprehension capabilities and\nextensive knowledge, have been increasingly applied to recommendation systems\n(RS). Given the fundamental gap between the mechanism of LLMs and the\nrequirement of RS, researchers have focused on fine-tuning LLMs with\nrecommendation-specific data to enhance their performance. Language Modeling\nLoss (LML), originally designed for language generation tasks, is commonly\nadopted. However, we identify two critical limitations of LML: 1) it exhibits\nsignificant divergence from the recommendation objective; 2) it erroneously\ntreats all fictitious item descriptions as negative samples, introducing\nmisleading training signals.\n  To address these limitations, we propose a novel Masked Softmax Loss (MSL)\ntailored for fine-tuning LLMs on recommendation. MSL improves LML by\nidentifying and masking invalid tokens that could lead to fictitious item\ndescriptions during loss computation. This strategy can effectively avoid the\ninterference from erroneous negative signals and ensure well alignment with the\nrecommendation objective supported by theoretical guarantees. During\nimplementation, we identify a potential challenge related to gradient vanishing\nof MSL. To overcome this, we further introduce the temperature coefficient and\npropose an Adaptive Temperature Strategy (ATS) that adaptively adjusts the\ntemperature without requiring extensive hyperparameter tuning. Extensive\nexperiments conducted on four public datasets further validate the\neffectiveness of MSL, achieving an average improvement of 42.24% in NDCG@10.\nThe code is available at https://github.com/WANGBohaO-jpg/MSL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), known for their comprehension capabilities and\nextensive knowledge, have been increasingly applied to recommendation systems\n(RS). Given the fundamental gap between the mechanism of LLMs and the\nrequirement of RS, researchers have focused on fine-tuning LLMs with\nrecommendation-specific data to enhance their performance. Language Modeling\nLoss (LML), originally designed for language generation tasks, is commonly\nadopted. However, we identify two critical limitations of LML: 1) it exhibits\nsignificant divergence from the recommendation objective; 2) it erroneously\ntreats all fictitious item descriptions as negative samples, introducing\nmisleading training signals.\n  To address these limitations, we propose a novel Masked Softmax Loss (MSL)\ntailored for fine-tuning LLMs on recommendation. MSL improves LML by\nidentifying and masking invalid tokens that could lead to fictitious item\ndescriptions during loss computation. This strategy can effectively avoid the\ninterference from erroneous negative signals and ensure well alignment with the\nrecommendation objective supported by theoretical guarantees. During\nimplementation, we identify a potential challenge related to gradient vanishing\nof MSL. To overcome this, we further introduce the temperature coefficient and\npropose an Adaptive Temperature Strategy (ATS) that adaptively adjusts the\ntemperature without requiring extensive hyperparameter tuning. Extensive\nexperiments conducted on four public datasets further validate the\neffectiveness of MSL, achieving an average improvement of 42.24% in NDCG@10.\nThe code is available at https://github.com/WANGBohaO-jpg/MSL."
                },
                "authors": [
                    {
                        "name": "Bohao Wang"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Xingyu Lou"
                    },
                    {
                        "name": "Changwang Zhang"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Yuegang Sun"
                    },
                    {
                        "name": "Yan Feng"
                    },
                    {
                        "name": "Chun Chen"
                    },
                    {
                        "name": "Can Wang"
                    }
                ],
                "author_detail": {
                    "name": "Can Wang"
                },
                "author": "Can Wang",
                "arxiv_comment": "Accepted by SIGIR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19754v1",
                "updated": "2025-04-28T12:52:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    52,
                    5,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T12:52:05Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    52,
                    5,
                    0,
                    118,
                    0
                ],
                "title": "Reconstructing Context: Evaluating Advanced Chunking Strategies for\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing Context: Evaluating Advanced Chunking Strategies for\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) has become a transformative approach for\nenhancing large language models (LLMs) by grounding their outputs in external\nknowledge sources. Yet, a critical question persists: how can vast volumes of\nexternal knowledge be managed effectively within the input constraints of LLMs?\nTraditional methods address this by chunking external documents into smaller,\nfixed-size segments. While this approach alleviates input limitations, it often\nfragments context, resulting in incomplete retrieval and diminished coherence\nin generation. To overcome these shortcomings, two advanced techniques, late\nchunking and contextual retrieval, have been introduced, both aiming to\npreserve global context. Despite their potential, their comparative strengths\nand limitations remain unclear. This study presents a rigorous analysis of late\nchunking and contextual retrieval, evaluating their effectiveness and\nefficiency in optimizing RAG systems. Our results indicate that contextual\nretrieval preserves semantic coherence more effectively but requires greater\ncomputational resources. In contrast, late chunking offers higher efficiency\nbut tends to sacrifice relevance and completeness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has become a transformative approach for\nenhancing large language models (LLMs) by grounding their outputs in external\nknowledge sources. Yet, a critical question persists: how can vast volumes of\nexternal knowledge be managed effectively within the input constraints of LLMs?\nTraditional methods address this by chunking external documents into smaller,\nfixed-size segments. While this approach alleviates input limitations, it often\nfragments context, resulting in incomplete retrieval and diminished coherence\nin generation. To overcome these shortcomings, two advanced techniques, late\nchunking and contextual retrieval, have been introduced, both aiming to\npreserve global context. Despite their potential, their comparative strengths\nand limitations remain unclear. This study presents a rigorous analysis of late\nchunking and contextual retrieval, evaluating their effectiveness and\nefficiency in optimizing RAG systems. Our results indicate that contextual\nretrieval preserves semantic coherence more effectively but requires greater\ncomputational resources. In contrast, late chunking offers higher efficiency\nbut tends to sacrifice relevance and completeness."
                },
                "authors": [
                    {
                        "name": "Carlo Merola"
                    },
                    {
                        "name": "Jaspinder Singh"
                    }
                ],
                "author_detail": {
                    "name": "Jaspinder Singh"
                },
                "author": "Jaspinder Singh",
                "arxiv_comment": "13 pages, 2 figures, Second Workshop on Knowledge-Enhanced\n  Information Retrieval, ECIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19746v1",
                "updated": "2025-04-28T12:47:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    47,
                    23,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T12:47:23Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    47,
                    23,
                    0,
                    118,
                    0
                ],
                "title": "FineQ: Software-Hardware Co-Design for Low-Bit Fine-Grained\n  Mixed-Precision Quantization of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineQ: Software-Hardware Co-Design for Low-Bit Fine-Grained\n  Mixed-Precision Quantization of LLMs"
                },
                "summary": "Large language models (LLMs) have significantly advanced the natural language\nprocessing paradigm but impose substantial demands on memory and computational\nresources. Quantization is one of the most effective ways to reduce memory\nconsumption of LLMs. However, advanced single-precision quantization methods\nexperience significant accuracy degradation when quantizing to ultra-low bits.\nExisting mixed-precision quantization methods are quantized by groups with\ncoarse granularity. Employing high precision for group data leads to\nsubstantial memory overhead, whereas low precision severely impacts model\naccuracy. To address this issue, we propose FineQ, software-hardware co-design\nfor low-bit fine-grained mixed-precision quantization of LLMs. First, FineQ\npartitions the weights into finer-grained clusters and considers the\ndistribution of outliers within these clusters, thus achieving a balance\nbetween model accuracy and memory overhead. Then, we propose an outlier\nprotection mechanism within clusters that uses 3 bits to represent outliers and\nintroduce an encoding scheme for index and data concatenation to enable aligned\nmemory access. Finally, we introduce an accelerator utilizing temporal coding\nthat effectively supports the quantization algorithm while simplifying the\nmultipliers in the systolic array. FineQ achieves higher model accuracy\ncompared to the SOTA mixed-precision quantization algorithm at a close average\nbit-width. Meanwhile, the accelerator achieves up to 1.79x energy efficiency\nand reduces the area of the systolic array by 61.2%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly advanced the natural language\nprocessing paradigm but impose substantial demands on memory and computational\nresources. Quantization is one of the most effective ways to reduce memory\nconsumption of LLMs. However, advanced single-precision quantization methods\nexperience significant accuracy degradation when quantizing to ultra-low bits.\nExisting mixed-precision quantization methods are quantized by groups with\ncoarse granularity. Employing high precision for group data leads to\nsubstantial memory overhead, whereas low precision severely impacts model\naccuracy. To address this issue, we propose FineQ, software-hardware co-design\nfor low-bit fine-grained mixed-precision quantization of LLMs. First, FineQ\npartitions the weights into finer-grained clusters and considers the\ndistribution of outliers within these clusters, thus achieving a balance\nbetween model accuracy and memory overhead. Then, we propose an outlier\nprotection mechanism within clusters that uses 3 bits to represent outliers and\nintroduce an encoding scheme for index and data concatenation to enable aligned\nmemory access. Finally, we introduce an accelerator utilizing temporal coding\nthat effectively supports the quantization algorithm while simplifying the\nmultipliers in the systolic array. FineQ achieves higher model accuracy\ncompared to the SOTA mixed-precision quantization algorithm at a close average\nbit-width. Meanwhile, the accelerator achieves up to 1.79x energy efficiency\nand reduces the area of the systolic array by 61.2%."
                },
                "authors": [
                    {
                        "name": "Xilong Xie"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Limin Xiao"
                    },
                    {
                        "name": "Meng Han"
                    },
                    {
                        "name": "Lin Sun"
                    },
                    {
                        "name": "Shuai Zheng"
                    },
                    {
                        "name": "Xiangrong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangrong Xu"
                },
                "author": "Xiangrong Xu",
                "arxiv_comment": "DATE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19739v1",
                "updated": "2025-04-28T12:36:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    36,
                    14,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T12:36:14Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    36,
                    14,
                    0,
                    118,
                    0
                ],
                "title": "Contrastive Language-Image Learning with Augmented Textual Prompts for\n  3D/4D FER Using Vision-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Language-Image Learning with Augmented Textual Prompts for\n  3D/4D FER Using Vision-Language Model"
                },
                "summary": "In this paper, we introduce AffectVLM, a vision-language model designed to\nintegrate multiviews for a semantically rich and visually comprehensive\nunderstanding of facial emotions from 3D/4D data. To effectively capture visual\nfeatures, we propose a joint representation learning framework paired with a\nnovel gradient-friendly loss function that accelerates model convergence\ntowards optimal feature representation. Additionally, we introduce augmented\ntextual prompts to enhance the model's linguistic capabilities and employ mixed\nview augmentation to expand the visual dataset. We also develop a Streamlit app\nfor a real-time interactive inference and enable the model for distributed\nlearning. Extensive experiments validate the superior performance of AffectVLM\nacross multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce AffectVLM, a vision-language model designed to\nintegrate multiviews for a semantically rich and visually comprehensive\nunderstanding of facial emotions from 3D/4D data. To effectively capture visual\nfeatures, we propose a joint representation learning framework paired with a\nnovel gradient-friendly loss function that accelerates model convergence\ntowards optimal feature representation. Additionally, we introduce augmented\ntextual prompts to enhance the model's linguistic capabilities and employ mixed\nview augmentation to expand the visual dataset. We also develop a Streamlit app\nfor a real-time interactive inference and enable the model for distributed\nlearning. Extensive experiments validate the superior performance of AffectVLM\nacross multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Muzammil Behzad"
                    },
                    {
                        "name": "Guoying Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Guoying Zhao"
                },
                "author": "Guoying Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19734v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19734v1",
                "updated": "2025-04-28T12:31:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    31,
                    38,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T12:31:38Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    31,
                    38,
                    0,
                    118,
                    0
                ],
                "title": "LLM-Assisted Automated Deductive Coding of Dialogue Data: Leveraging\n  Dialogue-Specific Characteristics to Enhance Contextual Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Assisted Automated Deductive Coding of Dialogue Data: Leveraging\n  Dialogue-Specific Characteristics to Enhance Contextual Understanding"
                },
                "summary": "Dialogue data has been a key source for understanding learning processes,\noffering critical insights into how students engage in collaborative\ndiscussions and how these interactions shape their knowledge construction. The\nadvent of Large Language Models (LLMs) has introduced promising opportunities\nfor advancing qualitative research, particularly in the automated coding of\ndialogue data. However, the inherent contextual complexity of dialogue presents\nunique challenges for these models, especially in understanding and\ninterpreting complex contextual information. This study addresses these\nchallenges by developing a novel LLM-assisted automated coding approach for\ndialogue data. The novelty of our proposed framework is threefold: 1) We\npredict the code for an utterance based on dialogue-specific characteristics --\ncommunicative acts and communicative events -- using separate prompts following\nthe role prompts and chain-of-thoughts methods; 2) We engaged multiple LLMs\nincluding GPT-4-turbo, GPT-4o, DeepSeek in collaborative code prediction; 3) We\nleveraged the interrelation between events and acts to implement consistency\nchecking using GPT-4o. In particular, our contextual consistency checking\nprovided a substantial accuracy improvement. We also found the accuracy of act\npredictions was consistently higher than that of event predictions. This study\ncontributes a new methodological framework for enhancing the precision of\nautomated coding of dialogue data as well as offers a scalable solution for\naddressing the contextual challenges inherent in dialogue analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue data has been a key source for understanding learning processes,\noffering critical insights into how students engage in collaborative\ndiscussions and how these interactions shape their knowledge construction. The\nadvent of Large Language Models (LLMs) has introduced promising opportunities\nfor advancing qualitative research, particularly in the automated coding of\ndialogue data. However, the inherent contextual complexity of dialogue presents\nunique challenges for these models, especially in understanding and\ninterpreting complex contextual information. This study addresses these\nchallenges by developing a novel LLM-assisted automated coding approach for\ndialogue data. The novelty of our proposed framework is threefold: 1) We\npredict the code for an utterance based on dialogue-specific characteristics --\ncommunicative acts and communicative events -- using separate prompts following\nthe role prompts and chain-of-thoughts methods; 2) We engaged multiple LLMs\nincluding GPT-4-turbo, GPT-4o, DeepSeek in collaborative code prediction; 3) We\nleveraged the interrelation between events and acts to implement consistency\nchecking using GPT-4o. In particular, our contextual consistency checking\nprovided a substantial accuracy improvement. We also found the accuracy of act\npredictions was consistently higher than that of event predictions. This study\ncontributes a new methodological framework for enhancing the precision of\nautomated coding of dialogue data as well as offers a scalable solution for\naddressing the contextual challenges inherent in dialogue analysis."
                },
                "authors": [
                    {
                        "name": "Ying Na"
                    },
                    {
                        "name": "Shihui Feng"
                    }
                ],
                "author_detail": {
                    "name": "Shihui Feng"
                },
                "author": "Shihui Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19734v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19730v1",
                "updated": "2025-04-28T12:28:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    28,
                    55,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T12:28:55Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    28,
                    55,
                    0,
                    118,
                    0
                ],
                "title": "Evaluate-and-Purify: Fortifying Code Language Models Against Adversarial\n  Attacks Using LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluate-and-Purify: Fortifying Code Language Models Against Adversarial\n  Attacks Using LLM-as-a-Judge"
                },
                "summary": "The widespread adoption of code language models in software engineering tasks\nhas exposed vulnerabilities to adversarial attacks, especially the identifier\nsubstitution attacks. Although existing identifier substitution attackers\ndemonstrate high success rates, they often produce adversarial examples with\nunnatural code patterns. In this paper, we systematically assess the quality of\nadversarial examples using LLM-as-a-Judge. Our analysis reveals that over 80%\nof adversarial examples generated by state-of-the-art identifier substitution\nattackers (e.g., ALERT) are actually detectable. Based on this insight, we\npropose EP-Shield, a unified framework for evaluating and purifying identifier\nsubstitution attacks via naturalness-aware reasoning. Specifically, we first\nevaluate the naturalness of code and identify the perturbed adversarial code,\nthen purify it so that the victim model can restore correct prediction.\nExtensive experiments demonstrate the superiority of EP-Shield over adversarial\nfine-tuning (up to 83.36% improvement) and its lightweight design 7B\nparameters) with GPT-4-level performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of code language models in software engineering tasks\nhas exposed vulnerabilities to adversarial attacks, especially the identifier\nsubstitution attacks. Although existing identifier substitution attackers\ndemonstrate high success rates, they often produce adversarial examples with\nunnatural code patterns. In this paper, we systematically assess the quality of\nadversarial examples using LLM-as-a-Judge. Our analysis reveals that over 80%\nof adversarial examples generated by state-of-the-art identifier substitution\nattackers (e.g., ALERT) are actually detectable. Based on this insight, we\npropose EP-Shield, a unified framework for evaluating and purifying identifier\nsubstitution attacks via naturalness-aware reasoning. Specifically, we first\nevaluate the naturalness of code and identify the perturbed adversarial code,\nthen purify it so that the victim model can restore correct prediction.\nExtensive experiments demonstrate the superiority of EP-Shield over adversarial\nfine-tuning (up to 83.36% improvement) and its lightweight design 7B\nparameters) with GPT-4-level performance."
                },
                "authors": [
                    {
                        "name": "Wenhan Mu"
                    },
                    {
                        "name": "Ling Xu"
                    },
                    {
                        "name": "Shuren Pei"
                    },
                    {
                        "name": "Le Mi"
                    },
                    {
                        "name": "Huichi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Huichi Zhou"
                },
                "author": "Huichi Zhou",
                "arxiv_comment": "25 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03108v2",
                "updated": "2025-04-28T12:27:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    27,
                    25,
                    0,
                    118,
                    0
                ],
                "published": "2025-03-05T02:08:12Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    2,
                    8,
                    12,
                    2,
                    64,
                    0
                ],
                "title": "SoK: Knowledge is All You Need: Accelerating Last Mile Delivery for\n  Automated Provenance-based Intrusion Detection with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: Knowledge is All You Need: Accelerating Last Mile Delivery for\n  Automated Provenance-based Intrusion Detection with LLMs"
                },
                "summary": "Recently, provenance-based intrusion detection systems (PIDSes) have been\nwidely proposed for endpoint threat analysis. However, due to the lack of\nsystematic integration and utilization of knowledge, existing PIDSes still\nrequire significant manual intervention for practical deployment, making full\nautomation challenging. This paper presents a disruptive innovation by\ncategorizing PIDSes according to the types of knowledge they utilize. In\nresponse to the prevalent issue of ``knowledge silos problem'' in existing\nresearch, we introduce a novel knowledge-driven provenance-based intrusion\ndetection framework, powered by large language models (LLMs). We also present\nOmniSec, a best practice system built upon this framework. By integrating\nattack representation knowledge, threat intelligence knowledge, and benign\nbehavior knowledge, OmniSec outperforms the state-of-the-art approaches on\npublic benchmark datasets. OmniSec is available online at\nhttps://anonymous.4open.science/r/PIDS-with-LLM-613B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, provenance-based intrusion detection systems (PIDSes) have been\nwidely proposed for endpoint threat analysis. However, due to the lack of\nsystematic integration and utilization of knowledge, existing PIDSes still\nrequire significant manual intervention for practical deployment, making full\nautomation challenging. This paper presents a disruptive innovation by\ncategorizing PIDSes according to the types of knowledge they utilize. In\nresponse to the prevalent issue of ``knowledge silos problem'' in existing\nresearch, we introduce a novel knowledge-driven provenance-based intrusion\ndetection framework, powered by large language models (LLMs). We also present\nOmniSec, a best practice system built upon this framework. By integrating\nattack representation knowledge, threat intelligence knowledge, and benign\nbehavior knowledge, OmniSec outperforms the state-of-the-art approaches on\npublic benchmark datasets. OmniSec is available online at\nhttps://anonymous.4open.science/r/PIDS-with-LLM-613B."
                },
                "authors": [
                    {
                        "name": "Wenrui Cheng"
                    },
                    {
                        "name": "Tiantian Zhu"
                    },
                    {
                        "name": "Chunlin Xiong"
                    },
                    {
                        "name": "Haofei Sun"
                    },
                    {
                        "name": "Zijun Wang"
                    },
                    {
                        "name": "Shunan Jing"
                    },
                    {
                        "name": "Mingqi Lv"
                    },
                    {
                        "name": "Yan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yan Chen"
                },
                "author": "Yan Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17109v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17109v4",
                "updated": "2025-04-28T12:27:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    27,
                    12,
                    0,
                    118,
                    0
                ],
                "published": "2024-05-27T12:25:34Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    12,
                    25,
                    34,
                    0,
                    148,
                    0
                ],
                "title": "Left-Linear Completion with AC Axioms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Left-Linear Completion with AC Axioms"
                },
                "summary": "We revisit completion modulo equational theories for left-linear term rewrite\nsystems where unification modulo the theory is avoided and the normal rewrite\nrelation can be used in order to decide validity questions. To that end, we\ngive a new correctness proof for finite runs and establish a simulation result\nbetween the two inference systems known from the literature. Given a concrete\nreduction order, novel canonicity results show that the resulting complete\nsystems are unique up to the representation of their rules' right-hand sides.\nFurthermore, we show how left-linear AC completion can be simulated by general\nAC completion. In particular, this result allows us to switch from the former\nto the latter at any point during a completion process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We revisit completion modulo equational theories for left-linear term rewrite\nsystems where unification modulo the theory is avoided and the normal rewrite\nrelation can be used in order to decide validity questions. To that end, we\ngive a new correctness proof for finite runs and establish a simulation result\nbetween the two inference systems known from the literature. Given a concrete\nreduction order, novel canonicity results show that the resulting complete\nsystems are unique up to the representation of their rules' right-hand sides.\nFurthermore, we show how left-linear AC completion can be simulated by general\nAC completion. In particular, this result allows us to switch from the former\nto the latter at any point during a completion process."
                },
                "authors": [
                    {
                        "name": "Johannes Niederhauser"
                    },
                    {
                        "name": "Nao Hirokawa"
                    },
                    {
                        "name": "Aart Middeldorp"
                    }
                ],
                "author_detail": {
                    "name": "Aart Middeldorp"
                },
                "author": "Aart Middeldorp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17109v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17109v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19724v1",
                "updated": "2025-04-28T12:19:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    19,
                    53,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T12:19:53Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    19,
                    53,
                    0,
                    118,
                    0
                ],
                "title": "RepText: Rendering Visual Text via Replicating",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepText: Rendering Visual Text via Replicating"
                },
                "summary": "Although contemporary text-to-image generation models have achieved\nremarkable breakthroughs in producing visually appealing images, their capacity\nto generate precise and flexible typographic elements, especially non-Latin\nalphabets, remains constrained. To address these limitations, we start from an\nnaive assumption that text understanding is only a sufficient condition for\ntext rendering, but not a necessary condition. Based on this, we present\nRepText, which aims to empower pre-trained monolingual text-to-image generation\nmodels with the ability to accurately render, or more precisely, replicate,\nmultilingual visual text in user-specified fonts, without the need to really\nunderstand them. Specifically, we adopt the setting from ControlNet and\nadditionally integrate language agnostic glyph and position of rendered text to\nenable generating harmonized visual text, allowing users to customize text\ncontent, font and position on their needs. To improve accuracy, a text\nperceptual loss is employed along with the diffusion loss. Furthermore, to\nstabilize rendering process, at the inference phase, we directly initialize\nwith noisy glyph latent instead of random initialization, and adopt region\nmasks to restrict the feature injection to only the text region to avoid\ndistortion of the background. We conducted extensive experiments to verify the\neffectiveness of our RepText relative to existing works, our approach\noutperforms existing open-source methods and achieves comparable results to\nnative multi-language closed-source models. To be more fair, we also\nexhaustively discuss its limitations in the end.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although contemporary text-to-image generation models have achieved\nremarkable breakthroughs in producing visually appealing images, their capacity\nto generate precise and flexible typographic elements, especially non-Latin\nalphabets, remains constrained. To address these limitations, we start from an\nnaive assumption that text understanding is only a sufficient condition for\ntext rendering, but not a necessary condition. Based on this, we present\nRepText, which aims to empower pre-trained monolingual text-to-image generation\nmodels with the ability to accurately render, or more precisely, replicate,\nmultilingual visual text in user-specified fonts, without the need to really\nunderstand them. Specifically, we adopt the setting from ControlNet and\nadditionally integrate language agnostic glyph and position of rendered text to\nenable generating harmonized visual text, allowing users to customize text\ncontent, font and position on their needs. To improve accuracy, a text\nperceptual loss is employed along with the diffusion loss. Furthermore, to\nstabilize rendering process, at the inference phase, we directly initialize\nwith noisy glyph latent instead of random initialization, and adopt region\nmasks to restrict the feature injection to only the text region to avoid\ndistortion of the background. We conducted extensive experiments to verify the\neffectiveness of our RepText relative to existing works, our approach\noutperforms existing open-source methods and achieves comparable results to\nnative multi-language closed-source models. To be more fair, we also\nexhaustively discuss its limitations in the end."
                },
                "authors": [
                    {
                        "name": "Haofan Wang"
                    },
                    {
                        "name": "Yujia Xu"
                    },
                    {
                        "name": "Yimeng Li"
                    },
                    {
                        "name": "Junchen Li"
                    },
                    {
                        "name": "Chaowei Zhang"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Kejia Yang"
                    },
                    {
                        "name": "Zhibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Chen"
                },
                "author": "Zhibo Chen",
                "arxiv_comment": "Technical Report. https://reptext.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11983v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11983v2",
                "updated": "2025-04-28T12:17:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    17,
                    25,
                    0,
                    118,
                    0
                ],
                "published": "2024-12-16T17:04:40Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    4,
                    40,
                    0,
                    351,
                    0
                ],
                "title": "Leveraging Large Language Models for Effective Label-free Node\n  Classification in Text-Attributed Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Effective Label-free Node\n  Classification in Text-Attributed Graphs"
                },
                "summary": "Graph neural networks (GNNs) have become the preferred models for node\nclassification in graph data due to their robust capabilities in integrating\ngraph structures and attributes. However, these models heavily depend on a\nsubstantial amount of high-quality labeled data for training, which is often\ncostly to obtain. With the rise of large language models (LLMs), a promising\napproach is to utilize their exceptional zero-shot capabilities and extensive\nknowledge for node labeling. Despite encouraging results, this approach either\nrequires numerous queries to LLMs or suffers from reduced performance due to\nnoisy labels generated by LLMs. To address these challenges, we introduce\nLocle, an active self-training framework that does Label-free node\nClassification with LLMs cost-Effectively. Locle iteratively identifies small\nsets of \"critical\" samples using GNNs and extracts informative pseudo-labels\nfor them with both LLMs and GNNs, serving as additional supervision signals to\nenhance model training. Specifically, Locle comprises three key components: (i)\nan effective active node selection strategy for initial annotations; (ii) a\ncareful sample selection scheme to identify \"critical\" nodes based on label\ndisharmonicity and entropy; and (iii) a label refinement module that combines\nLLMs and GNNs with a rewired topology. Extensive experiments on five benchmark\ntext-attributed graph datasets demonstrate that Locle significantly outperforms\nstate-of-the-art methods under the same query budget to LLMs in terms of\nlabel-free node classification. Notably, on the DBLP dataset with 14.3k nodes,\nLocle achieves an 8.08% improvement in accuracy over the state-of-the-art at a\ncost of less than one cent. Our code is available at\nhttps://github.com/HKBU-LAGAS/Locle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) have become the preferred models for node\nclassification in graph data due to their robust capabilities in integrating\ngraph structures and attributes. However, these models heavily depend on a\nsubstantial amount of high-quality labeled data for training, which is often\ncostly to obtain. With the rise of large language models (LLMs), a promising\napproach is to utilize their exceptional zero-shot capabilities and extensive\nknowledge for node labeling. Despite encouraging results, this approach either\nrequires numerous queries to LLMs or suffers from reduced performance due to\nnoisy labels generated by LLMs. To address these challenges, we introduce\nLocle, an active self-training framework that does Label-free node\nClassification with LLMs cost-Effectively. Locle iteratively identifies small\nsets of \"critical\" samples using GNNs and extracts informative pseudo-labels\nfor them with both LLMs and GNNs, serving as additional supervision signals to\nenhance model training. Specifically, Locle comprises three key components: (i)\nan effective active node selection strategy for initial annotations; (ii) a\ncareful sample selection scheme to identify \"critical\" nodes based on label\ndisharmonicity and entropy; and (iii) a label refinement module that combines\nLLMs and GNNs with a rewired topology. Extensive experiments on five benchmark\ntext-attributed graph datasets demonstrate that Locle significantly outperforms\nstate-of-the-art methods under the same query budget to LLMs in terms of\nlabel-free node classification. Notably, on the DBLP dataset with 14.3k nodes,\nLocle achieves an 8.08% improvement in accuracy over the state-of-the-art at a\ncost of less than one cent. Our code is available at\nhttps://github.com/HKBU-LAGAS/Locle."
                },
                "authors": [
                    {
                        "name": "Taiyan Zhang"
                    },
                    {
                        "name": "Renchi Yang"
                    },
                    {
                        "name": "Yurui Lai"
                    },
                    {
                        "name": "Mingyu Yan"
                    },
                    {
                        "name": "Xiaochun Ye"
                    },
                    {
                        "name": "Dongrui Fan"
                    }
                ],
                "author_detail": {
                    "name": "Dongrui Fan"
                },
                "author": "Dongrui Fan",
                "arxiv_comment": "14 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11983v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11983v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19720v1",
                "updated": "2025-04-28T12:14:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    14,
                    2,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T12:14:02Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    14,
                    2,
                    0,
                    118,
                    0
                ],
                "title": "Taming the Titans: A Survey of Efficient LLM Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming the Titans: A Survey of Efficient LLM Inference Serving"
                },
                "summary": "Large Language Models (LLMs) for Generative AI have achieved remarkable\nprogress, evolving into sophisticated and versatile tools widely adopted across\nvarious domains and applications. However, the substantial memory overhead\ncaused by their vast number of parameters, combined with the high computational\ndemands of the attention mechanism, poses significant challenges in achieving\nlow latency and high throughput for LLM inference services. Recent\nadvancements, driven by groundbreaking research, have significantly accelerated\nprogress in this field. This paper provides a comprehensive survey of these\nmethods, covering fundamental instance-level approaches, in-depth cluster-level\nstrategies, emerging scenario directions, and other miscellaneous but important\nareas. At the instance level, we review model placement, request scheduling,\ndecoding length prediction, storage management, and the disaggregation\nparadigm. At the cluster level, we explore GPU cluster deployment,\nmulti-instance load balancing, and cloud service solutions. For emerging\nscenarios, we organize the discussion around specific tasks, modules, and\nauxiliary methods. To ensure a holistic overview, we also highlight several\nniche yet critical areas. Finally, we outline potential research directions to\nfurther advance the field of LLM inference serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) for Generative AI have achieved remarkable\nprogress, evolving into sophisticated and versatile tools widely adopted across\nvarious domains and applications. However, the substantial memory overhead\ncaused by their vast number of parameters, combined with the high computational\ndemands of the attention mechanism, poses significant challenges in achieving\nlow latency and high throughput for LLM inference services. Recent\nadvancements, driven by groundbreaking research, have significantly accelerated\nprogress in this field. This paper provides a comprehensive survey of these\nmethods, covering fundamental instance-level approaches, in-depth cluster-level\nstrategies, emerging scenario directions, and other miscellaneous but important\nareas. At the instance level, we review model placement, request scheduling,\ndecoding length prediction, storage management, and the disaggregation\nparadigm. At the cluster level, we explore GPU cluster deployment,\nmulti-instance load balancing, and cloud service solutions. For emerging\nscenarios, we organize the discussion around specific tasks, modules, and\nauxiliary methods. To ensure a holistic overview, we also highlight several\nniche yet critical areas. Finally, we outline potential research directions to\nfurther advance the field of LLM inference serving."
                },
                "authors": [
                    {
                        "name": "Ranran Zhen"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Yixin Ji"
                    },
                    {
                        "name": "Zhenlin Yang"
                    },
                    {
                        "name": "Tong Liu"
                    },
                    {
                        "name": "Qingrong Xia"
                    },
                    {
                        "name": "Xinyu Duan"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Baoxing Huai"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "work in progress;11 pages of main paper with 7 main figures, overall\n  20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19716v1",
                "updated": "2025-04-28T12:09:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    9,
                    10,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T12:09:10Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    9,
                    10,
                    0,
                    118,
                    0
                ],
                "title": "QuickGrasp: Lightweight Antipodal Grasp Planning with Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuickGrasp: Lightweight Antipodal Grasp Planning with Point Clouds"
                },
                "summary": "Grasping has been a long-standing challenge in facilitating the final\ninterface between a robot and the environment. As environments and tasks become\ncomplicated, the need to embed higher intelligence to infer from the\nsurroundings and act on them has become necessary. Although most methods\nutilize techniques to estimate grasp pose by treating the problem via pure\nsampling-based approaches in the six-degree-of-freedom space or as a learning\nproblem, they usually fail in real-life settings owing to poor generalization\nacross domains. In addition, the time taken to generate the grasp plan and the\nlack of repeatability, owing to sampling inefficiency and the probabilistic\nnature of existing grasp planning approaches, severely limits their application\nin real-world tasks. This paper presents a lightweight analytical approach\ntowards robotic grasp planning, particularly antipodal grasps, with little to\nno sampling in the six-degree-of-freedom space. The proposed grasp planning\nalgorithm is formulated as an optimization problem towards estimating grasp\npoints on the object surface instead of directly estimating the end-effector\npose. To this extent, a soft-region-growing algorithm is presented for\neffective plane segmentation, even in the case of curved surfaces. An\noptimization-based quality metric is then used for the evaluation of grasp\npoints to ensure indirect force closure. The proposed grasp framework is\ncompared with the existing state-of-the-art grasp planning approach, Grasp pose\ndetection (GPD), as a baseline over multiple simulated objects. The\neffectiveness of the proposed approach in comparison to GPD is also evaluated\nin a real-world setting using image and point-cloud data, with the planned\ngrasps being executed using a ROBOTIQ gripper and UR5 manipulator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grasping has been a long-standing challenge in facilitating the final\ninterface between a robot and the environment. As environments and tasks become\ncomplicated, the need to embed higher intelligence to infer from the\nsurroundings and act on them has become necessary. Although most methods\nutilize techniques to estimate grasp pose by treating the problem via pure\nsampling-based approaches in the six-degree-of-freedom space or as a learning\nproblem, they usually fail in real-life settings owing to poor generalization\nacross domains. In addition, the time taken to generate the grasp plan and the\nlack of repeatability, owing to sampling inefficiency and the probabilistic\nnature of existing grasp planning approaches, severely limits their application\nin real-world tasks. This paper presents a lightweight analytical approach\ntowards robotic grasp planning, particularly antipodal grasps, with little to\nno sampling in the six-degree-of-freedom space. The proposed grasp planning\nalgorithm is formulated as an optimization problem towards estimating grasp\npoints on the object surface instead of directly estimating the end-effector\npose. To this extent, a soft-region-growing algorithm is presented for\neffective plane segmentation, even in the case of curved surfaces. An\noptimization-based quality metric is then used for the evaluation of grasp\npoints to ensure indirect force closure. The proposed grasp framework is\ncompared with the existing state-of-the-art grasp planning approach, Grasp pose\ndetection (GPD), as a baseline over multiple simulated objects. The\neffectiveness of the proposed approach in comparison to GPD is also evaluated\nin a real-world setting using image and point-cloud data, with the planned\ngrasps being executed using a ROBOTIQ gripper and UR5 manipulator."
                },
                "authors": [
                    {
                        "name": "Navin Sriram Ravie"
                    },
                    {
                        "name": "Keerthi Vasan M"
                    },
                    {
                        "name": "Asokan Thondiyath"
                    },
                    {
                        "name": "Bijo Sebastian"
                    }
                ],
                "author_detail": {
                    "name": "Bijo Sebastian"
                },
                "author": "Bijo Sebastian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15452v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15452v2",
                "updated": "2025-04-28T12:08:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    8,
                    33,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-21T21:37:03Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    21,
                    37,
                    3,
                    0,
                    111,
                    0
                ],
                "title": "A positive correlation between broad HI Ly$$ absorptions and local\n  overdensities of galaxies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A positive correlation between broad HI Ly$$ absorptions and local\n  overdensities of galaxies"
                },
                "summary": "A large fraction of the baryon budget at $z<1$ resides in large-scale\nfilaments in the form of diffuse intergalactic gas, and numerous studies have\nreported a significant correlation between the strength of the absorptions\nproduced by this gas in the spectra of bright background sources, and impact\nparameter to cosmic filaments intersected by these sightlines. However, a\nsimilar relation is harder to determine for the warm-hot phase of the\nintergalactic gas, since its higher Doppler parameter and significantly lower\nneutral gas fraction makes this gas difficult to detect in absorption. We use a\nsample of 13 broad Ly$\\alpha$ absorbers (BLAs) detected in the HST/COS spectrum\nof a single QSO ($z\\sim0.27$), whose sightline intersects several inter-cluster\naxes, to study the relation between BLAs and the large-scale structure of the\nUniverse. Given their Doppler parameters of $b>40$ km s$^{-1}$, BLAs are good\ntracers of warm-hot intergalactic gas. We use VLT/MUSE and VLT/VIMOS data to\ninfer local overdensities of galaxies at the redshifts of the BLAs, and to\nassess the potential association of the BLAs with nearby galaxies. We find that\nout of the 13 BLAs in our sample, four are associated with a strong overdensity\nof galaxies, and four with tentative overdensities. The remaining five are\nlocated at redshifts where we do not identify any excess of galaxies. We find\nthat these overdensities of galaxies at the redshift of BLAs are local, and\nthey vanish when larger cosmic volumes are considered, in terms of a larger\nvelocity offset to the BLA or larger impact parameter to the QSO sightline.\nFinally, we find a positive correlation between the total hydrogen column\ndensities inferred from the BLAs, and the relative excess of galaxies at the\nsame redshifts, consistent with the picture where warm-hot gas resides deep\nwithin the gravitational potential well of cosmic filaments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A large fraction of the baryon budget at $z<1$ resides in large-scale\nfilaments in the form of diffuse intergalactic gas, and numerous studies have\nreported a significant correlation between the strength of the absorptions\nproduced by this gas in the spectra of bright background sources, and impact\nparameter to cosmic filaments intersected by these sightlines. However, a\nsimilar relation is harder to determine for the warm-hot phase of the\nintergalactic gas, since its higher Doppler parameter and significantly lower\nneutral gas fraction makes this gas difficult to detect in absorption. We use a\nsample of 13 broad Ly$\\alpha$ absorbers (BLAs) detected in the HST/COS spectrum\nof a single QSO ($z\\sim0.27$), whose sightline intersects several inter-cluster\naxes, to study the relation between BLAs and the large-scale structure of the\nUniverse. Given their Doppler parameters of $b>40$ km s$^{-1}$, BLAs are good\ntracers of warm-hot intergalactic gas. We use VLT/MUSE and VLT/VIMOS data to\ninfer local overdensities of galaxies at the redshifts of the BLAs, and to\nassess the potential association of the BLAs with nearby galaxies. We find that\nout of the 13 BLAs in our sample, four are associated with a strong overdensity\nof galaxies, and four with tentative overdensities. The remaining five are\nlocated at redshifts where we do not identify any excess of galaxies. We find\nthat these overdensities of galaxies at the redshift of BLAs are local, and\nthey vanish when larger cosmic volumes are considered, in terms of a larger\nvelocity offset to the BLA or larger impact parameter to the QSO sightline.\nFinally, we find a positive correlation between the total hydrogen column\ndensities inferred from the BLAs, and the relative excess of galaxies at the\nsame redshifts, consistent with the picture where warm-hot gas resides deep\nwithin the gravitational potential well of cosmic filaments."
                },
                "authors": [
                    {
                        "name": "Ismael Pessa"
                    },
                    {
                        "name": "Nicolas Tejos"
                    },
                    {
                        "name": "Karen Martinez-Acosta"
                    },
                    {
                        "name": "Sebastian Lopez"
                    },
                    {
                        "name": "Jessica Werk"
                    },
                    {
                        "name": "J. Xavier Prochaska"
                    }
                ],
                "author_detail": {
                    "name": "J. Xavier Prochaska"
                },
                "author": "J. Xavier Prochaska",
                "arxiv_comment": "Accepted for publication in A&A. 22 pages and 14 figures + 14 pages\n  appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15452v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15452v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16286v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16286v2",
                "updated": "2025-04-28T11:53:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    11,
                    53,
                    26,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-22T21:48:05Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    21,
                    48,
                    5,
                    1,
                    112,
                    0
                ],
                "title": "The Paradox of Poetic Intent in Back-Translation: Evaluating the Quality\n  of Large Language Models in Chinese Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Paradox of Poetic Intent in Back-Translation: Evaluating the Quality\n  of Large Language Models in Chinese Translation"
                },
                "summary": "The rapid advancement of large language models (LLMs) has reshaped the\nlandscape of machine translation, yet challenges persist in preserving poetic\nintent, cultural heritage, and handling specialized terminology in\nChinese-English translation. This study constructs a diverse corpus\nencompassing Chinese scientific terminology, historical translation paradoxes,\nand literary metaphors. Utilizing a back-translation and Friedman test-based\nevaluation system (BT-Fried), we evaluate BLEU, CHRF, TER, and semantic\nsimilarity metrics across six major LLMs (e.g., GPT-4.5, DeepSeek V3) and three\ntraditional translation tools. Key findings include: (1) Scientific abstracts\noften benefit from back-translation, while traditional tools outperform LLMs in\nlinguistically distinct texts; (2) LLMs struggle with cultural and literary\nretention, exemplifying the \"paradox of poetic intent\"; (3) Some models exhibit\n\"verbatim back-translation\", reflecting emergent memory behavior; (4) A novel\nBLEU variant using Jieba segmentation and n-gram weighting is proposed. The\nstudy contributes to the empirical evaluation of Chinese NLP performance and\nadvances understanding of cultural fidelity in AI-mediated translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has reshaped the\nlandscape of machine translation, yet challenges persist in preserving poetic\nintent, cultural heritage, and handling specialized terminology in\nChinese-English translation. This study constructs a diverse corpus\nencompassing Chinese scientific terminology, historical translation paradoxes,\nand literary metaphors. Utilizing a back-translation and Friedman test-based\nevaluation system (BT-Fried), we evaluate BLEU, CHRF, TER, and semantic\nsimilarity metrics across six major LLMs (e.g., GPT-4.5, DeepSeek V3) and three\ntraditional translation tools. Key findings include: (1) Scientific abstracts\noften benefit from back-translation, while traditional tools outperform LLMs in\nlinguistically distinct texts; (2) LLMs struggle with cultural and literary\nretention, exemplifying the \"paradox of poetic intent\"; (3) Some models exhibit\n\"verbatim back-translation\", reflecting emergent memory behavior; (4) A novel\nBLEU variant using Jieba segmentation and n-gram weighting is proposed. The\nstudy contributes to the empirical evaluation of Chinese NLP performance and\nadvances understanding of cultural fidelity in AI-mediated translation."
                },
                "authors": [
                    {
                        "name": "Li Weigang"
                    },
                    {
                        "name": "Pedro Carvalho Brom"
                    }
                ],
                "author_detail": {
                    "name": "Pedro Carvalho Brom"
                },
                "author": "Pedro Carvalho Brom",
                "arxiv_comment": "24 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16286v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16286v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19698v1",
                "updated": "2025-04-28T11:52:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    11,
                    52,
                    27,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T11:52:27Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    11,
                    52,
                    27,
                    0,
                    118,
                    0
                ],
                "title": "Advances in Approximate Bayesian Inference for Models in Epidemiology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in Approximate Bayesian Inference for Models in Epidemiology"
                },
                "summary": "Bayesian inference methods are useful in infectious diseases modeling due to\ntheir capability to propagate uncertainty, manage sparse data, incorporate\nlatent structures, and address high-dimensional parameter spaces. However,\nparameter inference through assimilation of observational data in these models\nremains challenging. While asymptotically exact Bayesian methods offer\ntheoretical guarantees for accurate inference, they can be computationally\ndemanding and impractical for real-time outbreak analysis. This review\nsynthesizes recent advances in approximate Bayesian inference methods that aim\nto balance inferential accuracy with scalability. We focus on four prominent\nfamilies: Approximate Bayesian Computation, Bayesian Synthetic Likelihood,\nIntegrated Nested Laplace Approximation, and Variational Inference. For each\nmethod, we evaluate its relevance to epidemiological applications, emphasizing\ninnovations that improve both computational efficiency and inference accuracy.\nWe also offer practical guidance on method selection across a range of modeling\nscenarios. Finally, we identify hybrid exact approximate inference as a\npromising frontier that combines methodological rigor with the scalability\nneeded for the response to outbreaks. This review provides epidemiologists with\na conceptual framework to navigate the trade-off between statistical accuracy\nand computational feasibility in contemporary disease modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference methods are useful in infectious diseases modeling due to\ntheir capability to propagate uncertainty, manage sparse data, incorporate\nlatent structures, and address high-dimensional parameter spaces. However,\nparameter inference through assimilation of observational data in these models\nremains challenging. While asymptotically exact Bayesian methods offer\ntheoretical guarantees for accurate inference, they can be computationally\ndemanding and impractical for real-time outbreak analysis. This review\nsynthesizes recent advances in approximate Bayesian inference methods that aim\nto balance inferential accuracy with scalability. We focus on four prominent\nfamilies: Approximate Bayesian Computation, Bayesian Synthetic Likelihood,\nIntegrated Nested Laplace Approximation, and Variational Inference. For each\nmethod, we evaluate its relevance to epidemiological applications, emphasizing\ninnovations that improve both computational efficiency and inference accuracy.\nWe also offer practical guidance on method selection across a range of modeling\nscenarios. Finally, we identify hybrid exact approximate inference as a\npromising frontier that combines methodological rigor with the scalability\nneeded for the response to outbreaks. This review provides epidemiologists with\na conceptual framework to navigate the trade-off between statistical accuracy\nand computational feasibility in contemporary disease modeling."
                },
                "authors": [
                    {
                        "name": "Xiahui Li"
                    },
                    {
                        "name": "Fergus Chadwick"
                    },
                    {
                        "name": "Ben Swallow"
                    }
                ],
                "author_detail": {
                    "name": "Ben Swallow"
                },
                "author": "Ben Swallow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05999v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05999v2",
                "updated": "2025-04-28T11:42:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    11,
                    42,
                    10,
                    0,
                    118,
                    0
                ],
                "published": "2024-05-09T09:37:22Z",
                "published_parsed": [
                    2024,
                    5,
                    9,
                    9,
                    37,
                    22,
                    3,
                    130,
                    0
                ],
                "title": "LLMPot: Dynamically Configured LLM-based Honeypot for Industrial\n  Protocol and Physical Process Emulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMPot: Dynamically Configured LLM-based Honeypot for Industrial\n  Protocol and Physical Process Emulation"
                },
                "summary": "Industrial Control Systems (ICS) are extensively used in critical\ninfrastructures ensuring efficient, reliable, and continuous operations.\nHowever, their increasing connectivity and addition of advanced features make\nthem vulnerable to cyber threats, potentially leading to severe disruptions in\nessential services. In this context, honeypots play a vital role by acting as\ndecoy targets within ICS networks, or on the Internet, helping to detect, log,\nanalyze, and develop mitigations for ICS-specific cyber threats. Deploying ICS\nhoneypots, however, is challenging due to the necessity of accurately\nreplicating industrial protocols and device characteristics, a crucial\nrequirement for effectively mimicking the unique operational behavior of\ndifferent industrial systems. Moreover, this challenge is compounded by the\nsignificant manual effort required in also mimicking the control logic the PLC\nwould execute, in order to capture attacker traffic aiming to disrupt critical\ninfrastructure operations. In this paper, we propose LLMPot, a novel approach\nfor designing honeypots in ICS networks harnessing the potency of Large\nLanguage Models (LLMs). LLMPot aims to automate and optimize the creation of\nrealistic honeypots with vendor-agnostic configurations, and for any control\nlogic, aiming to eliminate the manual effort and specialized knowledge\ntraditionally required in this domain. We conducted extensive experiments\nfocusing on a wide array of parameters, demonstrating that our LLM-based\napproach can effectively create honeypot devices implementing different\nindustrial protocols and diverse control logic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial Control Systems (ICS) are extensively used in critical\ninfrastructures ensuring efficient, reliable, and continuous operations.\nHowever, their increasing connectivity and addition of advanced features make\nthem vulnerable to cyber threats, potentially leading to severe disruptions in\nessential services. In this context, honeypots play a vital role by acting as\ndecoy targets within ICS networks, or on the Internet, helping to detect, log,\nanalyze, and develop mitigations for ICS-specific cyber threats. Deploying ICS\nhoneypots, however, is challenging due to the necessity of accurately\nreplicating industrial protocols and device characteristics, a crucial\nrequirement for effectively mimicking the unique operational behavior of\ndifferent industrial systems. Moreover, this challenge is compounded by the\nsignificant manual effort required in also mimicking the control logic the PLC\nwould execute, in order to capture attacker traffic aiming to disrupt critical\ninfrastructure operations. In this paper, we propose LLMPot, a novel approach\nfor designing honeypots in ICS networks harnessing the potency of Large\nLanguage Models (LLMs). LLMPot aims to automate and optimize the creation of\nrealistic honeypots with vendor-agnostic configurations, and for any control\nlogic, aiming to eliminate the manual effort and specialized knowledge\ntraditionally required in this domain. We conducted extensive experiments\nfocusing on a wide array of parameters, demonstrating that our LLM-based\napproach can effectively create honeypot devices implementing different\nindustrial protocols and diverse control logic."
                },
                "authors": [
                    {
                        "name": "Christoforos Vasilatos"
                    },
                    {
                        "name": "Dunia J. Mahboobeh"
                    },
                    {
                        "name": "Hithem Lamri"
                    },
                    {
                        "name": "Manaar Alam"
                    },
                    {
                        "name": "Michail Maniatakos"
                    }
                ],
                "author_detail": {
                    "name": "Michail Maniatakos"
                },
                "author": "Michail Maniatakos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05999v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05999v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14926v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14926v3",
                "updated": "2025-04-28T11:41:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    11,
                    41,
                    23,
                    0,
                    118,
                    0
                ],
                "published": "2024-09-23T11:25:13Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    11,
                    25,
                    13,
                    0,
                    267,
                    0
                ],
                "title": "Early and Late Buzzards: Comparing Different Approaches for\n  Quantile-based Multiple Testing in Heavy-Tailed Wildlife Research Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early and Late Buzzards: Comparing Different Approaches for\n  Quantile-based Multiple Testing in Heavy-Tailed Wildlife Research Data"
                },
                "summary": "In medical, ecological and psychological research, there is a need for\nmethods to handle multiple testing, for example to consider group comparisons\nwith more than two groups. Typical approaches that deal with multiple testing\nare mean or variance based which can be less effective in the context of\nheavy-tailed and skewed data. Here, the median is the preferred measure of\nlocation and the interquartile range (IQR) is an adequate alternative to the\nvariance. Therefore, it may be fruitful to formulate research questions of\ninterest in terms of the median or the IQR. For this reason, we compare\ndifferent inference approaches for two-sided and non-inferiority hypotheses\nformulated in terms of medians or IQRs in an extensive simulation study. We\nconsider multiple contrast testing procedures combined with a bootstrap method\nas well as testing procedures with Bonferroni correction. As an example of a\nmultiple testing problem based on heavy-tailed data we analyse an ecological\ntrait variation in early and late breeding in a medium-sized bird of prey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In medical, ecological and psychological research, there is a need for\nmethods to handle multiple testing, for example to consider group comparisons\nwith more than two groups. Typical approaches that deal with multiple testing\nare mean or variance based which can be less effective in the context of\nheavy-tailed and skewed data. Here, the median is the preferred measure of\nlocation and the interquartile range (IQR) is an adequate alternative to the\nvariance. Therefore, it may be fruitful to formulate research questions of\ninterest in terms of the median or the IQR. For this reason, we compare\ndifferent inference approaches for two-sided and non-inferiority hypotheses\nformulated in terms of medians or IQRs in an extensive simulation study. We\nconsider multiple contrast testing procedures combined with a bootstrap method\nas well as testing procedures with Bonferroni correction. As an example of a\nmultiple testing problem based on heavy-tailed data we analyse an ecological\ntrait variation in early and late breeding in a medium-sized bird of prey."
                },
                "authors": [
                    {
                        "name": "Marlne Baumeister"
                    },
                    {
                        "name": "Merle Munko"
                    },
                    {
                        "name": "Kai-Philipp Gladow"
                    },
                    {
                        "name": "Marc Ditzhaus"
                    },
                    {
                        "name": "Nayden Chakarov"
                    },
                    {
                        "name": "Markus Pauly"
                    }
                ],
                "author_detail": {
                    "name": "Markus Pauly"
                },
                "author": "Markus Pauly",
                "arxiv_comment": "DOI for supplementary code: doi:10.17877/TUDODATA-2025-M6TDKFDE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14926v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14926v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19683v1",
                "updated": "2025-04-28T11:20:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    11,
                    20,
                    51,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T11:20:51Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    11,
                    20,
                    51,
                    0,
                    118,
                    0
                ],
                "title": "GPA-RAM: Grasp-Pretraining Augmented Robotic Attention Mamba for Spatial\n  Task Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPA-RAM: Grasp-Pretraining Augmented Robotic Attention Mamba for Spatial\n  Task Learning"
                },
                "summary": "Most existing robot manipulation methods prioritize task learning by\nenhancing perception through complex deep network architectures. However, they\nface challenges in real-time collision-free planning. Hence, Robotic Attention\nMamba (RAM) is designed for refined planning. Specifically, by integrating\nMamba and parallel single-view attention, RAM aligns multi-view vision and\ntask-related language features, ensuring efficient fine-grained task planning\nwith linear complexity and robust real-time performance. Nevertheless, it has\nthe potential for further improvement in high-precision grasping and\nmanipulation. Thus, Grasp-Pretraining Augmentation (GPA) is devised, with a\ngrasp pose feature extractor pretrained utilizing object grasp poses directly\ninherited from whole-task demonstrations. Subsequently, the extracted grasp\nfeatures are fused with the spatially aligned planning features from RAM\nthrough attention-based Pre-trained Location Fusion, preserving high-resolution\ngrasping cues overshadowed by an overemphasis on global planning. To summarize,\nwe propose Grasp-Pretraining Augmented Robotic Attention Mamba (GPA-RAM),\ndividing spatial task learning into RAM for planning skill learning and GPA for\ngrasping skill learning. GPA-RAM demonstrates superior performance across three\nrobot systems with distinct camera configurations in simulation and the real\nworld. Compared with previous state-of-the-art methods, it improves the\nabsolute success rate by 8.2% (from 79.3% to 87.5%) on the RLBench multi-task\nbenchmark and 40\\% (from 16% to 56%), 12% (from 86% to 98%) on the ALOHA\nbimanual manipulation tasks, while delivering notably faster inference.\nFurthermore, experimental results demonstrate that both RAM and GPA enhance\ntask learning, with GPA proving robust to different architectures of pretrained\ngrasp pose feature extractors. The website is:\nhttps://logssim.github.io/GPA\\_RAM\\_website/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most existing robot manipulation methods prioritize task learning by\nenhancing perception through complex deep network architectures. However, they\nface challenges in real-time collision-free planning. Hence, Robotic Attention\nMamba (RAM) is designed for refined planning. Specifically, by integrating\nMamba and parallel single-view attention, RAM aligns multi-view vision and\ntask-related language features, ensuring efficient fine-grained task planning\nwith linear complexity and robust real-time performance. Nevertheless, it has\nthe potential for further improvement in high-precision grasping and\nmanipulation. Thus, Grasp-Pretraining Augmentation (GPA) is devised, with a\ngrasp pose feature extractor pretrained utilizing object grasp poses directly\ninherited from whole-task demonstrations. Subsequently, the extracted grasp\nfeatures are fused with the spatially aligned planning features from RAM\nthrough attention-based Pre-trained Location Fusion, preserving high-resolution\ngrasping cues overshadowed by an overemphasis on global planning. To summarize,\nwe propose Grasp-Pretraining Augmented Robotic Attention Mamba (GPA-RAM),\ndividing spatial task learning into RAM for planning skill learning and GPA for\ngrasping skill learning. GPA-RAM demonstrates superior performance across three\nrobot systems with distinct camera configurations in simulation and the real\nworld. Compared with previous state-of-the-art methods, it improves the\nabsolute success rate by 8.2% (from 79.3% to 87.5%) on the RLBench multi-task\nbenchmark and 40\\% (from 16% to 56%), 12% (from 86% to 98%) on the ALOHA\nbimanual manipulation tasks, while delivering notably faster inference.\nFurthermore, experimental results demonstrate that both RAM and GPA enhance\ntask learning, with GPA proving robust to different architectures of pretrained\ngrasp pose feature extractors. The website is:\nhttps://logssim.github.io/GPA\\_RAM\\_website/."
                },
                "authors": [
                    {
                        "name": "Juyi Sheng"
                    },
                    {
                        "name": "Yangjun Liu"
                    },
                    {
                        "name": "Sheng Xu"
                    },
                    {
                        "name": "Zhixin Yang"
                    },
                    {
                        "name": "Mengyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Mengyuan Liu"
                },
                "author": "Mengyuan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19678v1",
                "updated": "2025-04-28T11:08:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    11,
                    8,
                    22,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T11:08:22Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    11,
                    8,
                    22,
                    0,
                    118,
                    0
                ],
                "title": "From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review"
                },
                "summary": "Large language models and autonomous AI agents have evolved rapidly,\nresulting in a diverse array of evaluation benchmarks, frameworks, and\ncollaboration protocols. However, the landscape remains fragmented and lacks a\nunified taxonomy or comprehensive survey. Therefore, we present a side-by-side\ncomparison of benchmarks developed between 2019 and 2025 that evaluate these\nmodels and agents across multiple domains. In addition, we propose a taxonomy\nof approximately 60 benchmarks that cover general and academic knowledge\nreasoning, mathematical problem-solving, code generation and software\nengineering, factual grounding and retrieval, domain-specific evaluations,\nmultimodal and embodied tasks, task orchestration, and interactive assessments.\nFurthermore, we review AI-agent frameworks introduced between 2023 and 2025\nthat integrate large language models with modular toolkits to enable autonomous\ndecision-making and multi-step reasoning. Moreover, we present real-world\napplications of autonomous AI agents in materials science, biomedical research,\nacademic ideation, software engineering, synthetic data generation, chemical\nreasoning, mathematical problem-solving, geographic information systems,\nmultimedia, healthcare, and finance. We then survey key agent-to-agent\ncollaboration protocols, namely the Agent Communication Protocol (ACP), the\nModel Context Protocol (MCP), and the Agent-to-Agent Protocol (A2A). Finally,\nwe discuss recommendations for future research, focusing on advanced reasoning\nstrategies, failure modes in multi-agent LLM systems, automated scientific\ndiscovery, dynamic tool integration via reinforcement learning, integrated\nsearch capabilities, and security vulnerabilities in agent protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models and autonomous AI agents have evolved rapidly,\nresulting in a diverse array of evaluation benchmarks, frameworks, and\ncollaboration protocols. However, the landscape remains fragmented and lacks a\nunified taxonomy or comprehensive survey. Therefore, we present a side-by-side\ncomparison of benchmarks developed between 2019 and 2025 that evaluate these\nmodels and agents across multiple domains. In addition, we propose a taxonomy\nof approximately 60 benchmarks that cover general and academic knowledge\nreasoning, mathematical problem-solving, code generation and software\nengineering, factual grounding and retrieval, domain-specific evaluations,\nmultimodal and embodied tasks, task orchestration, and interactive assessments.\nFurthermore, we review AI-agent frameworks introduced between 2023 and 2025\nthat integrate large language models with modular toolkits to enable autonomous\ndecision-making and multi-step reasoning. Moreover, we present real-world\napplications of autonomous AI agents in materials science, biomedical research,\nacademic ideation, software engineering, synthetic data generation, chemical\nreasoning, mathematical problem-solving, geographic information systems,\nmultimedia, healthcare, and finance. We then survey key agent-to-agent\ncollaboration protocols, namely the Agent Communication Protocol (ACP), the\nModel Context Protocol (MCP), and the Agent-to-Agent Protocol (A2A). Finally,\nwe discuss recommendations for future research, focusing on advanced reasoning\nstrategies, failure modes in multi-agent LLM systems, automated scientific\ndiscovery, dynamic tool integration via reinforcement learning, integrated\nsearch capabilities, and security vulnerabilities in agent protocols."
                },
                "authors": [
                    {
                        "name": "Mohamed Amine Ferrag"
                    },
                    {
                        "name": "Norbert Tihanyi"
                    },
                    {
                        "name": "Merouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Merouane Debbah"
                },
                "author": "Merouane Debbah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19675v1",
                "updated": "2025-04-28T11:04:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    11,
                    4,
                    23,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T11:04:23Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    11,
                    4,
                    23,
                    0,
                    118,
                    0
                ],
                "title": "Annif at SemEval-2025 Task 5: Traditional XMTC augmented by LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Annif at SemEval-2025 Task 5: Traditional XMTC augmented by LLMs"
                },
                "summary": "This paper presents the Annif system in SemEval-2025 Task 5 (LLMs4Subjects),\nwhich focussed on subject indexing using large language models (LLMs). The task\nrequired creating subject predictions for bibliographic records from the\nbilingual TIBKAT database using the GND subject vocabulary. Our approach\ncombines traditional natural language processing and machine learning\ntechniques implemented in the Annif toolkit with innovative LLM-based methods\nfor translation and synthetic data generation, and merging predictions from\nmonolingual models. The system ranked first in the all-subjects category and\nsecond in the tib-core-subjects category in the quantitative evaluation, and\nfourth in qualitative evaluations. These findings demonstrate the potential of\ncombining traditional XMTC algorithms with modern LLM techniques to improve the\naccuracy and efficiency of subject indexing in multilingual contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the Annif system in SemEval-2025 Task 5 (LLMs4Subjects),\nwhich focussed on subject indexing using large language models (LLMs). The task\nrequired creating subject predictions for bibliographic records from the\nbilingual TIBKAT database using the GND subject vocabulary. Our approach\ncombines traditional natural language processing and machine learning\ntechniques implemented in the Annif toolkit with innovative LLM-based methods\nfor translation and synthetic data generation, and merging predictions from\nmonolingual models. The system ranked first in the all-subjects category and\nsecond in the tib-core-subjects category in the quantitative evaluation, and\nfourth in qualitative evaluations. These findings demonstrate the potential of\ncombining traditional XMTC algorithms with modern LLM techniques to improve the\naccuracy and efficiency of subject indexing in multilingual contexts."
                },
                "authors": [
                    {
                        "name": "Osma Suominen"
                    },
                    {
                        "name": "Juho Inkinen"
                    },
                    {
                        "name": "Mona Lehtinen"
                    }
                ],
                "author_detail": {
                    "name": "Mona Lehtinen"
                },
                "author": "Mona Lehtinen",
                "arxiv_comment": "6 pages, 4 figures, submitted to SemEval-2025 workshop Task 5:\n  LLMs4Subjects",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19674v1",
                "updated": "2025-04-28T11:01:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    11,
                    1,
                    8,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T11:01:08Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    11,
                    1,
                    8,
                    0,
                    118,
                    0
                ],
                "title": "$\\texttt{SAGE}$: A Generic Framework for LLM Safety Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\texttt{SAGE}$: A Generic Framework for LLM Safety Evaluation"
                },
                "summary": "Safety evaluation of Large Language Models (LLMs) has made progress and\nattracted academic interest, but it remains challenging to keep pace with the\nrapid integration of LLMs across diverse applications. Different applications\nexpose users to various harms, necessitating application-specific safety\nevaluations with tailored harms and policies. Another major gap is the lack of\nfocus on the dynamic and conversational nature of LLM systems. Such potential\noversights can lead to harms that go unnoticed in standard safety benchmarks.\nThis paper identifies the above as key requirements for robust LLM safety\nevaluation and recognizing that current evaluation methodologies do not satisfy\nthese, we introduce the $\\texttt{SAGE}$ (Safety AI Generic Evaluation)\nframework. $\\texttt{SAGE}$ is an automated modular framework designed for\ncustomized and dynamic harm evaluations. It utilizes adversarial user models\nthat are system-aware and have unique personalities, enabling a holistic\nred-teaming evaluation. We demonstrate $\\texttt{SAGE}$'s effectiveness by\nevaluating seven state-of-the-art LLMs across three applications and harm\npolicies. Our experiments with multi-turn conversational evaluations revealed a\nconcerning finding that harm steadily increases with conversation length.\nFurthermore, we observe significant disparities in model behavior when exposed\nto different user personalities and scenarios. Our findings also reveal that\nsome models minimize harmful outputs by employing severe refusal tactics that\ncan hinder their usefulness. These insights highlight the necessity of adaptive\nand context-specific testing to ensure better safety alignment and safer\ndeployment of LLMs in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety evaluation of Large Language Models (LLMs) has made progress and\nattracted academic interest, but it remains challenging to keep pace with the\nrapid integration of LLMs across diverse applications. Different applications\nexpose users to various harms, necessitating application-specific safety\nevaluations with tailored harms and policies. Another major gap is the lack of\nfocus on the dynamic and conversational nature of LLM systems. Such potential\noversights can lead to harms that go unnoticed in standard safety benchmarks.\nThis paper identifies the above as key requirements for robust LLM safety\nevaluation and recognizing that current evaluation methodologies do not satisfy\nthese, we introduce the $\\texttt{SAGE}$ (Safety AI Generic Evaluation)\nframework. $\\texttt{SAGE}$ is an automated modular framework designed for\ncustomized and dynamic harm evaluations. It utilizes adversarial user models\nthat are system-aware and have unique personalities, enabling a holistic\nred-teaming evaluation. We demonstrate $\\texttt{SAGE}$'s effectiveness by\nevaluating seven state-of-the-art LLMs across three applications and harm\npolicies. Our experiments with multi-turn conversational evaluations revealed a\nconcerning finding that harm steadily increases with conversation length.\nFurthermore, we observe significant disparities in model behavior when exposed\nto different user personalities and scenarios. Our findings also reveal that\nsome models minimize harmful outputs by employing severe refusal tactics that\ncan hinder their usefulness. These insights highlight the necessity of adaptive\nand context-specific testing to ensure better safety alignment and safer\ndeployment of LLMs in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Madhur Jindal"
                    },
                    {
                        "name": "Hari Shrawgi"
                    },
                    {
                        "name": "Parag Agrawal"
                    },
                    {
                        "name": "Sandipan Dandapat"
                    }
                ],
                "author_detail": {
                    "name": "Sandipan Dandapat"
                },
                "author": "Sandipan Dandapat",
                "arxiv_comment": "24 pages, 9 main pages excluding references and appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14653v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14653v2",
                "updated": "2025-04-28T10:44:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    10,
                    44,
                    46,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-20T15:25:58Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    15,
                    25,
                    58,
                    6,
                    110,
                    0
                ],
                "title": "Wireless Large AI Model: Shaping the AI-Native Future of 6G and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Large AI Model: Shaping the AI-Native Future of 6G and Beyond"
                },
                "summary": "The emergence of sixth-generation and beyond communication systems is\nexpected to fundamentally transform digital experiences through introducing\nunparalleled levels of intelligence, efficiency, and connectivity. A promising\ntechnology poised to enable this revolutionary vision is the wireless large AI\nmodel (WLAM), characterized by its exceptional capabilities in data processing,\ninference, and decision-making. In light of these remarkable capabilities, this\npaper provides a comprehensive survey of WLAM, elucidating its fundamental\nprinciples, diverse applications, critical challenges, and future research\nopportunities. We begin by introducing the background of WLAM and analyzing the\nkey synergies with wireless networks, emphasizing the mutual benefits.\nSubsequently, we explore the foundational characteristics of WLAM, delving into\ntheir unique relevance in wireless environments. Then, the role of WLAM in\noptimizing wireless communication systems across various use cases and the\nreciprocal benefits are systematically investigated. Furthermore, we discuss\nthe integration of WLAM with emerging technologies, highlighting their\npotential to enable transformative capabilities and breakthroughs in wireless\ncommunication. Finally, we thoroughly examine the high-level challenges\nhindering the practical implementation of WLAM and discuss pivotal future\nresearch directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of sixth-generation and beyond communication systems is\nexpected to fundamentally transform digital experiences through introducing\nunparalleled levels of intelligence, efficiency, and connectivity. A promising\ntechnology poised to enable this revolutionary vision is the wireless large AI\nmodel (WLAM), characterized by its exceptional capabilities in data processing,\ninference, and decision-making. In light of these remarkable capabilities, this\npaper provides a comprehensive survey of WLAM, elucidating its fundamental\nprinciples, diverse applications, critical challenges, and future research\nopportunities. We begin by introducing the background of WLAM and analyzing the\nkey synergies with wireless networks, emphasizing the mutual benefits.\nSubsequently, we explore the foundational characteristics of WLAM, delving into\ntheir unique relevance in wireless environments. Then, the role of WLAM in\noptimizing wireless communication systems across various use cases and the\nreciprocal benefits are systematically investigated. Furthermore, we discuss\nthe integration of WLAM with emerging technologies, highlighting their\npotential to enable transformative capabilities and breakthroughs in wireless\ncommunication. Finally, we thoroughly examine the high-level challenges\nhindering the practical implementation of WLAM and discuss pivotal future\nresearch directions."
                },
                "authors": [
                    {
                        "name": "Fenghao Zhu"
                    },
                    {
                        "name": "Xinquan Wang"
                    },
                    {
                        "name": "Xinyi Li"
                    },
                    {
                        "name": "Maojun Zhang"
                    },
                    {
                        "name": "Yixuan Chen"
                    },
                    {
                        "name": "Chongwen Huang"
                    },
                    {
                        "name": "Zhaohui Yang"
                    },
                    {
                        "name": "Xiaoming Chen"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Richeng Jin"
                    },
                    {
                        "name": "Yongming Huang"
                    },
                    {
                        "name": "Wei Feng"
                    },
                    {
                        "name": "Tingting Yang"
                    },
                    {
                        "name": "Baoming Bai"
                    },
                    {
                        "name": "Feifei Gao"
                    },
                    {
                        "name": "Kun Yang"
                    },
                    {
                        "name": "Yuanwei Liu"
                    },
                    {
                        "name": "Sami Muhaidat"
                    },
                    {
                        "name": "Chau Yuen"
                    },
                    {
                        "name": "Kaibin Huang"
                    },
                    {
                        "name": "Kai-Kit Wong"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Mrouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Mrouane Debbah"
                },
                "author": "Mrouane Debbah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14653v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14653v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19667v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19667v1",
                "updated": "2025-04-28T10:43:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    10,
                    43,
                    35,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T10:43:35Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    10,
                    43,
                    35,
                    0,
                    118,
                    0
                ],
                "title": "A Tripartite Perspective on GraphRAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Tripartite Perspective on GraphRAG"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious domains, yet they struggle with knowledge-intensive tasks in areas that\ndemand factual accuracy, e.g. industrial automation and healthcare. Key\nlimitations include their tendency to hallucinate, lack of source traceability\n(provenance), and challenges in timely knowledge updates. Combining language\nmodels with knowledge graphs (GraphRAG) offers promising avenues for overcoming\nthese deficits. However, a major challenge lies in creating such a knowledge\ngraph in the first place. Here, we propose a novel approach that combines LLMs\nwith a tripartite knowledge graph representation, which is constructed by\nconnecting complex, domain-specific objects via a curated ontology of\ncorresponding, domain-specific concepts to relevant sections within chunks of\ntext through a concept-anchored pre-analysis of source documents starting from\nan initial lexical graph. As a consequence, our Tripartite-GraphRAG approach\nimplements: i) a concept-specific, information-preserving pre-compression of\ntextual chunks; ii) allows for the formation of a concept-specific relevance\nestimation of embedding similarities grounded in statistics; and iii) avoids\ncommon challenges w.r.t. continuous extendability, such as the need for entity\nresolution and deduplication. By applying a transformation to the knowledge\ngraph, we formulate LLM prompt creation as an unsupervised node classification\nproblem, drawing on ideas from Markov Random Fields. We evaluate our approach\non a healthcare use case, involving multi-faceted analyses of patient anamneses\ngiven a set of medical concepts as well as clinical literature. Experiments\nindicate that it can optimize information density, coverage, and arrangement of\nLLM prompts while reducing their lengths, which may lead to reduced costs and\nmore consistent and reliable LLM outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious domains, yet they struggle with knowledge-intensive tasks in areas that\ndemand factual accuracy, e.g. industrial automation and healthcare. Key\nlimitations include their tendency to hallucinate, lack of source traceability\n(provenance), and challenges in timely knowledge updates. Combining language\nmodels with knowledge graphs (GraphRAG) offers promising avenues for overcoming\nthese deficits. However, a major challenge lies in creating such a knowledge\ngraph in the first place. Here, we propose a novel approach that combines LLMs\nwith a tripartite knowledge graph representation, which is constructed by\nconnecting complex, domain-specific objects via a curated ontology of\ncorresponding, domain-specific concepts to relevant sections within chunks of\ntext through a concept-anchored pre-analysis of source documents starting from\nan initial lexical graph. As a consequence, our Tripartite-GraphRAG approach\nimplements: i) a concept-specific, information-preserving pre-compression of\ntextual chunks; ii) allows for the formation of a concept-specific relevance\nestimation of embedding similarities grounded in statistics; and iii) avoids\ncommon challenges w.r.t. continuous extendability, such as the need for entity\nresolution and deduplication. By applying a transformation to the knowledge\ngraph, we formulate LLM prompt creation as an unsupervised node classification\nproblem, drawing on ideas from Markov Random Fields. We evaluate our approach\non a healthcare use case, involving multi-faceted analyses of patient anamneses\ngiven a set of medical concepts as well as clinical literature. Experiments\nindicate that it can optimize information density, coverage, and arrangement of\nLLM prompts while reducing their lengths, which may lead to reduced costs and\nmore consistent and reliable LLM outputs."
                },
                "authors": [
                    {
                        "name": "Michael Banf"
                    },
                    {
                        "name": "Johannes Kuhn"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Kuhn"
                },
                "author": "Johannes Kuhn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19667v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19667v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21862v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21862v3",
                "updated": "2025-04-28T10:27:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    10,
                    27,
                    31,
                    0,
                    118,
                    0
                ],
                "published": "2024-10-29T08:56:29Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    8,
                    56,
                    29,
                    1,
                    303,
                    0
                ],
                "title": "Hierarchical mixtures of Unigram models for short text clustering: The\n  role of Beta-Liouville priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical mixtures of Unigram models for short text clustering: The\n  role of Beta-Liouville priors"
                },
                "summary": "This paper presents a variant of the Multinomial mixture model tailored to\nthe unsupervised classification of short text data. While the Multinomial\nprobability vector is traditionally assigned a Dirichlet prior distribution,\nthis work explores an alternative formulation based on the Beta-Liouville\ndistribution, which offers a more flexible correlation structure than the\nDirichlet. We examine the theoretical properties of the Beta-Liouville\ndistribution, with particular focus on its conjugacy with the Multinomial\nlikelihood. This property enables the derivation of update equations for a CAVI\n(Coordinate Ascent Variational Inference) algorithm, facilitating approximate\nposterior inference of the model parameters. In addition, we introduce a\nstochastic variant of the CAVI algorithm to enhance scalability. The paper\nconcludes with empirical examples demonstrating effective strategies for\nselecting the Beta-Liouville hyperparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a variant of the Multinomial mixture model tailored to\nthe unsupervised classification of short text data. While the Multinomial\nprobability vector is traditionally assigned a Dirichlet prior distribution,\nthis work explores an alternative formulation based on the Beta-Liouville\ndistribution, which offers a more flexible correlation structure than the\nDirichlet. We examine the theoretical properties of the Beta-Liouville\ndistribution, with particular focus on its conjugacy with the Multinomial\nlikelihood. This property enables the derivation of update equations for a CAVI\n(Coordinate Ascent Variational Inference) algorithm, facilitating approximate\nposterior inference of the model parameters. In addition, we introduce a\nstochastic variant of the CAVI algorithm to enhance scalability. The paper\nconcludes with empirical examples demonstrating effective strategies for\nselecting the Beta-Liouville hyperparameters."
                },
                "authors": [
                    {
                        "name": "Massimo Bilancia"
                    },
                    {
                        "name": "Samuele Magro"
                    }
                ],
                "author_detail": {
                    "name": "Samuele Magro"
                },
                "author": "Samuele Magro",
                "arxiv_comment": "28 pages, 6 figures. Submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21862v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21862v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17425v2",
                "updated": "2025-04-28T10:21:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    10,
                    21,
                    54,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-24T10:26:47Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    10,
                    26,
                    47,
                    3,
                    114,
                    0
                ],
                "title": "MeerKAT discovers a jet-driven bow shock near GRS 1915+105. How an\n  invisible large-scale jet sculpts a microquasar's environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeerKAT discovers a jet-driven bow shock near GRS 1915+105. How an\n  invisible large-scale jet sculpts a microquasar's environment"
                },
                "summary": "Black holes, both supermassive and stellar-mass, impact the evolution of\ntheir surroundings on a large range of scales. While the role of supermassive\nblack holes is well studied, the effects of stellar-mass black holes on their\nsurroundings, particularly in inducing structures in the interstellar medium\n(ISM), remain under explored.\n  This study focuses on the black hole X-ray binary GRS 1915+105, renowned for\nits active jets, and the primary aim is to unveil and characterise the impact\nof GRS 1915+105 on its environment by identifying structures induced by jet-ISM\ninteraction. Methods: We observed GRS 1915+105 with MeerKAT for a total\nexposure time of 14~hr, and we obtained the deepest image of GRS 1915+105 to\ndate. Using a previously proposed self-similar model for large-scale jets, we\ninferred the properties of both the jets and the ISM, providing insights into\nthe jet-ISM interaction site.\n  Our observations revealed a bow shock structure near GRS 1915+105, likely\ninduced by a jet interacting with the ISM and blowing an overpressured cavity\nin the medium. We constrained the ISM density to 100--160 particles\\,cm$^{-3}$\nwhile assuming a temperature range of 10$^4$--10$^6$\\,K, which implies a bow\nshock expansion velocity of $20\\,{\\rm km\\,s}^{-1}<\\dot{L} <\\,360\\,{\\rm\nkm\\,s}^{-1}$. We estimate that the jet responsible for the formation of the bow\nshock has an age between 0.09 and 0.22 Myr, and the time-averaged energy rate\nConclusions: Our results confirm that in stellar-mass black holes, the energy\ndissipated through jets can be comparable to the accretion energy, and through\nthe interaction of the jet with the ISM, such energy is transferred back to the\nenvironment.\n  This feedback mechanism mirrors the powerful influence of supermassive black\nholes on their environments, underscoring the significant role a black hole's\nactivity has in shaping its surroundings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black holes, both supermassive and stellar-mass, impact the evolution of\ntheir surroundings on a large range of scales. While the role of supermassive\nblack holes is well studied, the effects of stellar-mass black holes on their\nsurroundings, particularly in inducing structures in the interstellar medium\n(ISM), remain under explored.\n  This study focuses on the black hole X-ray binary GRS 1915+105, renowned for\nits active jets, and the primary aim is to unveil and characterise the impact\nof GRS 1915+105 on its environment by identifying structures induced by jet-ISM\ninteraction. Methods: We observed GRS 1915+105 with MeerKAT for a total\nexposure time of 14~hr, and we obtained the deepest image of GRS 1915+105 to\ndate. Using a previously proposed self-similar model for large-scale jets, we\ninferred the properties of both the jets and the ISM, providing insights into\nthe jet-ISM interaction site.\n  Our observations revealed a bow shock structure near GRS 1915+105, likely\ninduced by a jet interacting with the ISM and blowing an overpressured cavity\nin the medium. We constrained the ISM density to 100--160 particles\\,cm$^{-3}$\nwhile assuming a temperature range of 10$^4$--10$^6$\\,K, which implies a bow\nshock expansion velocity of $20\\,{\\rm km\\,s}^{-1}<\\dot{L} <\\,360\\,{\\rm\nkm\\,s}^{-1}$. We estimate that the jet responsible for the formation of the bow\nshock has an age between 0.09 and 0.22 Myr, and the time-averaged energy rate\nConclusions: Our results confirm that in stellar-mass black holes, the energy\ndissipated through jets can be comparable to the accretion energy, and through\nthe interaction of the jet with the ISM, such energy is transferred back to the\nenvironment.\n  This feedback mechanism mirrors the powerful influence of supermassive black\nholes on their environments, underscoring the significant role a black hole's\nactivity has in shaping its surroundings."
                },
                "authors": [
                    {
                        "name": "S. E. Motta"
                    },
                    {
                        "name": "P. Atri"
                    },
                    {
                        "name": "James H. Matthews"
                    },
                    {
                        "name": "Jakob van den Eijnden"
                    },
                    {
                        "name": "Rob P. Fender"
                    },
                    {
                        "name": "James C. A. Miller-Jones"
                    },
                    {
                        "name": "Ian Heywood"
                    },
                    {
                        "name": "Patrick Woudt"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Woudt"
                },
                "author": "Patrick Woudt",
                "arxiv_doi": "10.1051/0004-6361/202452838",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202452838",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.17425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 3 figures, 2 tables, published in A&A. Acknowledgments\n  updated",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19660v1",
                "updated": "2025-04-28T10:20:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    10,
                    20,
                    4,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T10:20:04Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    10,
                    20,
                    4,
                    0,
                    118,
                    0
                ],
                "title": "Decentralization of Generative AI via Mixture of Experts for Wireless\n  Networks: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralization of Generative AI via Mixture of Experts for Wireless\n  Networks: A Comprehensive Survey"
                },
                "summary": "Mixture of Experts (MoE) has emerged as a promising paradigm for scaling\nmodel capacity while preserving computational efficiency, particularly in\nlarge-scale machine learning architectures such as large language models\n(LLMs). Recent advances in MoE have facilitated its adoption in wireless\nnetworks to address the increasing complexity and heterogeneity of modern\ncommunication systems. This paper presents a comprehensive survey of the MoE\nframework in wireless networks, highlighting its potential in optimizing\nresource efficiency, improving scalability, and enhancing adaptability across\ndiverse network tasks. We first introduce the fundamental concepts of MoE,\nincluding various gating mechanisms and the integration with generative AI\n(GenAI) and reinforcement learning (RL). Subsequently, we discuss the extensive\napplications of MoE across critical wireless communication scenarios, such as\nvehicular networks, unmanned aerial vehicles (UAVs), satellite communications,\nheterogeneous networks, integrated sensing and communication (ISAC), and mobile\nedge networks. Furthermore, key applications in channel prediction, physical\nlayer signal processing, radio resource management, network optimization, and\nsecurity are thoroughly examined. Additionally, we present a detailed overview\nof open-source datasets that are widely used in MoE-based models to support\ndiverse machine learning tasks. Finally, this survey identifies crucial future\nresearch directions for MoE, emphasizing the importance of advanced training\ntechniques, resource-aware gating strategies, and deeper integration with\nemerging 6G technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) has emerged as a promising paradigm for scaling\nmodel capacity while preserving computational efficiency, particularly in\nlarge-scale machine learning architectures such as large language models\n(LLMs). Recent advances in MoE have facilitated its adoption in wireless\nnetworks to address the increasing complexity and heterogeneity of modern\ncommunication systems. This paper presents a comprehensive survey of the MoE\nframework in wireless networks, highlighting its potential in optimizing\nresource efficiency, improving scalability, and enhancing adaptability across\ndiverse network tasks. We first introduce the fundamental concepts of MoE,\nincluding various gating mechanisms and the integration with generative AI\n(GenAI) and reinforcement learning (RL). Subsequently, we discuss the extensive\napplications of MoE across critical wireless communication scenarios, such as\nvehicular networks, unmanned aerial vehicles (UAVs), satellite communications,\nheterogeneous networks, integrated sensing and communication (ISAC), and mobile\nedge networks. Furthermore, key applications in channel prediction, physical\nlayer signal processing, radio resource management, network optimization, and\nsecurity are thoroughly examined. Additionally, we present a detailed overview\nof open-source datasets that are widely used in MoE-based models to support\ndiverse machine learning tasks. Finally, this survey identifies crucial future\nresearch directions for MoE, emphasizing the importance of advanced training\ntechniques, resource-aware gating strategies, and deeper integration with\nemerging 6G technologies."
                },
                "authors": [
                    {
                        "name": "Yunting Xu"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Changyuan Zhao"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Jiawen Kang"
                    },
                    {
                        "name": "Zehui Xiong"
                    },
                    {
                        "name": "Bo Qian"
                    },
                    {
                        "name": "Haibo Zhou"
                    },
                    {
                        "name": "Shiwen Mao"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Xuemin Shen"
                    },
                    {
                        "name": "Dong In Kim"
                    }
                ],
                "author_detail": {
                    "name": "Dong In Kim"
                },
                "author": "Dong In Kim",
                "arxiv_comment": "Survey paper, 30 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19656v1",
                "updated": "2025-04-28T10:17:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    10,
                    17,
                    23,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T10:17:23Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    10,
                    17,
                    23,
                    0,
                    118,
                    0
                ],
                "title": "Investigating the Period-Luminosity Relations of delta Scuti Stars: A\n  Pathway to Distance and 3-D Dust Map Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating the Period-Luminosity Relations of delta Scuti Stars: A\n  Pathway to Distance and 3-D Dust Map Inference"
                },
                "summary": "While delta Scuti stars are the most numerous class of kappa-mechanism\npulsators in the instability strip, the short periods and small peak-to-peak\namplitudes have left them understudied and underutilized. Recently, large-scale\ntime-domain surveys have significantly increased the number of identified delta\nScuti stars. Notably, the Tsinghua University-Ma Huateng Telescopes for Survey\n(TMTS), with its high-cadence observations at 1-minute intervals, has\nidentified thousands of delta Scuti stars, greatly expanding the sample of\nthese short-period pulsating variables. Using the delta Scuti stars from the\nTMTS catalogs of Periodic Variable Stars, we cross-matched the dataset with\nPan-STARRS1, 2MASS, and WISE to obtain photometric measurements across optical\nand infrared bands. Parallax data, used as Bayesian priors, were retrieved from\nGaia DR3, and line-of-sight dust extinction priors were estimated from a\nthree-dimensional dust map. Using PyMC, we performed a simultaneous\ndetermination of the 11-band P-L relations of delta Scuti stars, which not only\nyields precise measurements of these relations, but also greatly improves\nconstraints on the distance moduli and color excesses, as evidenced by the\nreduced uncertainties in the posterior distributions. Furthermore, our\nmethodology enables an independent estimation of the color excess through the\nP-L relations, offering a potential complement to existing 3-D dust maps.\nMoreover, by cross-matching with LAMOST DR7, we investigated the influence of\nmetallicity on the P-L relations. Our analysis reveals that incorporating\nmetallicity might reduce the intrinsic scatter at longer wavelengths. However,\nthis result does not achieve 3 sigma significance, leaving open the possibility\nthat the observed reduction is attributable to statistical fluctuations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While delta Scuti stars are the most numerous class of kappa-mechanism\npulsators in the instability strip, the short periods and small peak-to-peak\namplitudes have left them understudied and underutilized. Recently, large-scale\ntime-domain surveys have significantly increased the number of identified delta\nScuti stars. Notably, the Tsinghua University-Ma Huateng Telescopes for Survey\n(TMTS), with its high-cadence observations at 1-minute intervals, has\nidentified thousands of delta Scuti stars, greatly expanding the sample of\nthese short-period pulsating variables. Using the delta Scuti stars from the\nTMTS catalogs of Periodic Variable Stars, we cross-matched the dataset with\nPan-STARRS1, 2MASS, and WISE to obtain photometric measurements across optical\nand infrared bands. Parallax data, used as Bayesian priors, were retrieved from\nGaia DR3, and line-of-sight dust extinction priors were estimated from a\nthree-dimensional dust map. Using PyMC, we performed a simultaneous\ndetermination of the 11-band P-L relations of delta Scuti stars, which not only\nyields precise measurements of these relations, but also greatly improves\nconstraints on the distance moduli and color excesses, as evidenced by the\nreduced uncertainties in the posterior distributions. Furthermore, our\nmethodology enables an independent estimation of the color excess through the\nP-L relations, offering a potential complement to existing 3-D dust maps.\nMoreover, by cross-matching with LAMOST DR7, we investigated the influence of\nmetallicity on the P-L relations. Our analysis reveals that incorporating\nmetallicity might reduce the intrinsic scatter at longer wavelengths. However,\nthis result does not achieve 3 sigma significance, leaving open the possibility\nthat the observed reduction is attributable to statistical fluctuations."
                },
                "authors": [
                    {
                        "name": "Fangzhou Guo"
                    },
                    {
                        "name": "Joshua S. Bloom"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Liyang Chen"
                    },
                    {
                        "name": "Jie Lin"
                    },
                    {
                        "name": "Xiaodian Chen"
                    },
                    {
                        "name": "Jun Mo"
                    },
                    {
                        "name": "Jicheng Zhang"
                    },
                    {
                        "name": "Shengyu Yan"
                    },
                    {
                        "name": "Qichun Liu"
                    },
                    {
                        "name": "Haowei Peng"
                    },
                    {
                        "name": "Xiaojun Jiang"
                    },
                    {
                        "name": "Xiaoran Ma"
                    },
                    {
                        "name": "Danfeng Xiang"
                    },
                    {
                        "name": "Wenxiong Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenxiong Li"
                },
                "author": "Wenxiong Li",
                "arxiv_comment": "17 pages, 17 figures, accepted to Astronomy & Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17761v2",
                "updated": "2025-04-28T09:56:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    56,
                    8,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-24T17:25:12Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    17,
                    25,
                    12,
                    3,
                    114,
                    0
                ],
                "title": "Step1X-Edit: A Practical Framework for General Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step1X-Edit: A Practical Framework for General Image Editing"
                },
                "summary": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing."
                },
                "authors": [
                    {
                        "name": "Shiyu Liu"
                    },
                    {
                        "name": "Yucheng Han"
                    },
                    {
                        "name": "Peng Xing"
                    },
                    {
                        "name": "Fukun Yin"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Jiaqi Liao"
                    },
                    {
                        "name": "Yingming Wang"
                    },
                    {
                        "name": "Honghao Fu"
                    },
                    {
                        "name": "Chunrui Han"
                    },
                    {
                        "name": "Guopeng Li"
                    },
                    {
                        "name": "Yuang Peng"
                    },
                    {
                        "name": "Quan Sun"
                    },
                    {
                        "name": "Jingwei Wu"
                    },
                    {
                        "name": "Yan Cai"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Ranchen Ming"
                    },
                    {
                        "name": "Lei Xia"
                    },
                    {
                        "name": "Xianfang Zeng"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Gang Yu"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "arxiv_comment": "code: https://github.com/stepfun-ai/Step1X-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19636v1",
                "updated": "2025-04-28T09:52:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    52,
                    41,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T09:52:41Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    52,
                    41,
                    0,
                    118,
                    0
                ],
                "title": "Fitness Landscape of Large Language Model-Assisted Automated Algorithm\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fitness Landscape of Large Language Model-Assisted Automated Algorithm\n  Search"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant potential in\nalgorithm design. However, when integrated into search frameworks for iterative\nalgorithm search, the underlying fitness landscape--critical for understanding\nsearch behaviou--remains underexplored. In this paper, we illustrate and\nanalyze the fitness landscape of LLM-assisted Algorithm Search (LAS) using a\ngraph-based approach, where nodes represent algorithms and edges denote\ntransitions between them. We conduct extensive evaluations across six algorithm\ndesign tasks and six commonly used LLMs. Our findings reveal that LAS\nlandscapes are highly multimodal and rugged, particularly in combinatorial\noptimization tasks, with distinct structural variations across tasks and LLMs.\nFor instance, heuristic design tasks exhibit dense clusters of high-performing\nalgorithms, while symbolic regression tasks show sparse, scattered\ndistributions. Additionally, we demonstrate how population size influences\nexploration-exploitation trade-offs and the evolving trajectory of elite\nalgorithms. These insights not only advance our understanding of LAS landscapes\nbut also provide practical guidance for designing more effective LAS methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant potential in\nalgorithm design. However, when integrated into search frameworks for iterative\nalgorithm search, the underlying fitness landscape--critical for understanding\nsearch behaviou--remains underexplored. In this paper, we illustrate and\nanalyze the fitness landscape of LLM-assisted Algorithm Search (LAS) using a\ngraph-based approach, where nodes represent algorithms and edges denote\ntransitions between them. We conduct extensive evaluations across six algorithm\ndesign tasks and six commonly used LLMs. Our findings reveal that LAS\nlandscapes are highly multimodal and rugged, particularly in combinatorial\noptimization tasks, with distinct structural variations across tasks and LLMs.\nFor instance, heuristic design tasks exhibit dense clusters of high-performing\nalgorithms, while symbolic regression tasks show sparse, scattered\ndistributions. Additionally, we demonstrate how population size influences\nexploration-exploitation trade-offs and the evolving trajectory of elite\nalgorithms. These insights not only advance our understanding of LAS landscapes\nbut also provide practical guidance for designing more effective LAS methods."
                },
                "authors": [
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Qingfu Zhang"
                    },
                    {
                        "name": "Xialiang Tong"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Kun Mao"
                    }
                ],
                "author_detail": {
                    "name": "Kun Mao"
                },
                "author": "Kun Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19635v1",
                "updated": "2025-04-28T09:49:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    49,
                    54,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T09:49:54Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    49,
                    54,
                    0,
                    118,
                    0
                ],
                "title": "Diffusion Stochastic Learning Over Adaptive Competing Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Stochastic Learning Over Adaptive Competing Networks"
                },
                "summary": "This paper studies a stochastic dynamic game between two competing teams,\neach consisting of a network of collaborating agents. Unlike fully cooperative\nsettings, where all agents share a common objective, each team in this game\naims to minimize its own distinct objective. In the adversarial setting, their\nobjectives could be conflicting as in zero-sum games. Throughout the\ncompetition, agents share strategic information within their own team while\nsimultaneously inferring and adapting to the strategies of the opposing team.\nWe propose diffusion learning algorithms to address two important classes of\nthis network game: i) a zero-sum game characterized by weak cross-team subgraph\ninteractions, and ii) a general non-zero-sum game exhibiting strong cross-team\nsubgraph interactions. We analyze the stability performance of the proposed\nalgorithms under reasonable assumptions and illustrate the theoretical results\nthrough experiments on Cournot team competition and decentralized GAN training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies a stochastic dynamic game between two competing teams,\neach consisting of a network of collaborating agents. Unlike fully cooperative\nsettings, where all agents share a common objective, each team in this game\naims to minimize its own distinct objective. In the adversarial setting, their\nobjectives could be conflicting as in zero-sum games. Throughout the\ncompetition, agents share strategic information within their own team while\nsimultaneously inferring and adapting to the strategies of the opposing team.\nWe propose diffusion learning algorithms to address two important classes of\nthis network game: i) a zero-sum game characterized by weak cross-team subgraph\ninteractions, and ii) a general non-zero-sum game exhibiting strong cross-team\nsubgraph interactions. We analyze the stability performance of the proposed\nalgorithms under reasonable assumptions and illustrate the theoretical results\nthrough experiments on Cournot team competition and decentralized GAN training."
                },
                "authors": [
                    {
                        "name": "Yike Zhao"
                    },
                    {
                        "name": "Haoyuan Cai"
                    },
                    {
                        "name": "Ali H. Sayed"
                    }
                ],
                "author_detail": {
                    "name": "Ali H. Sayed"
                },
                "author": "Ali H. Sayed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19623v1",
                "updated": "2025-04-28T09:32:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    32,
                    10,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T09:32:10Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    32,
                    10,
                    0,
                    118,
                    0
                ],
                "title": "Multi-Horizon Echo State Network Prediction of Intraday Stock Returns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Horizon Echo State Network Prediction of Intraday Stock Returns"
                },
                "summary": "Stock return prediction is a problem that has received much attention in the\nfinance literature. In recent years, sophisticated machine learning methods\nhave been shown to perform significantly better than ''classical'' prediction\ntechniques. One downside of these approaches is that they are often very\nexpensive to implement, for both training and inference, because of their high\ncomplexity. We propose a return prediction framework for intraday returns at\nmultiple horizons based on Echo State Network (ESN) models, wherein a large\nportion of parameters are drawn at random and never trained. We show that this\napproach enjoys the benefits of recurrent neural network expressivity,\ninherently efficient implementation, and strong forecasting performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stock return prediction is a problem that has received much attention in the\nfinance literature. In recent years, sophisticated machine learning methods\nhave been shown to perform significantly better than ''classical'' prediction\ntechniques. One downside of these approaches is that they are often very\nexpensive to implement, for both training and inference, because of their high\ncomplexity. We propose a return prediction framework for intraday returns at\nmultiple horizons based on Echo State Network (ESN) models, wherein a large\nportion of parameters are drawn at random and never trained. We show that this\napproach enjoys the benefits of recurrent neural network expressivity,\ninherently efficient implementation, and strong forecasting performance."
                },
                "authors": [
                    {
                        "name": "Giovanni Ballarin"
                    },
                    {
                        "name": "Jacopo Capra"
                    },
                    {
                        "name": "Petros Dellaportas"
                    }
                ],
                "author_detail": {
                    "name": "Petros Dellaportas"
                },
                "author": "Petros Dellaportas",
                "arxiv_comment": "27 pages, 3 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.CP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13918v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13918v4",
                "updated": "2025-04-28T09:22:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    22,
                    21,
                    0,
                    118,
                    0
                ],
                "published": "2024-08-25T19:03:46Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    19,
                    3,
                    46,
                    6,
                    238,
                    0
                ],
                "title": "Geo-Llama: Leveraging LLMs for Human Mobility Trajectory Generation with\n  Spatiotemporal Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geo-Llama: Leveraging LLMs for Human Mobility Trajectory Generation with\n  Spatiotemporal Constraints"
                },
                "summary": "Generating realistic human mobility data is essential for various application\ndomains, including transportation, urban planning, and epidemic control, as\nreal data is often inaccessible to researchers due to high costs and privacy\nconcerns. Existing deep generative models learn from real trajectories to\ngenerate synthetic ones. Despite the progress, most of them suffer from\ntraining stability issues and scale poorly with increasing data size. More\nimportantly, they often lack control mechanisms to guide the generated\ntrajectories under constraints such as enforcing specific visits. To address\nthese limitations, we formally define the controlled trajectory generation\nproblem for effectively handling multiple spatiotemporal constraints. We\nintroduce Geo-Llama, a novel LLM finetuning framework that can enforce multiple\nexplicit visit constraints while maintaining contextual coherence of the\ngenerated trajectories. In this approach, pre-trained LLMs are fine-tuned on\ntrajectory data with a visit-wise permutation strategy where each visit\ncorresponds to a specific time and location. This strategy enables the model to\ncapture spatiotemporal patterns regardless of visit orders while maintaining\nflexible and in-context constraint integration through prompts during\ngeneration. Extensive experiments on real-world and synthetic datasets validate\nthe effectiveness of Geo-Llama, demonstrating its versatility and robustness in\nhandling a broad range of constraints to generate more realistic trajectories\ncompared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating realistic human mobility data is essential for various application\ndomains, including transportation, urban planning, and epidemic control, as\nreal data is often inaccessible to researchers due to high costs and privacy\nconcerns. Existing deep generative models learn from real trajectories to\ngenerate synthetic ones. Despite the progress, most of them suffer from\ntraining stability issues and scale poorly with increasing data size. More\nimportantly, they often lack control mechanisms to guide the generated\ntrajectories under constraints such as enforcing specific visits. To address\nthese limitations, we formally define the controlled trajectory generation\nproblem for effectively handling multiple spatiotemporal constraints. We\nintroduce Geo-Llama, a novel LLM finetuning framework that can enforce multiple\nexplicit visit constraints while maintaining contextual coherence of the\ngenerated trajectories. In this approach, pre-trained LLMs are fine-tuned on\ntrajectory data with a visit-wise permutation strategy where each visit\ncorresponds to a specific time and location. This strategy enables the model to\ncapture spatiotemporal patterns regardless of visit orders while maintaining\nflexible and in-context constraint integration through prompts during\ngeneration. Extensive experiments on real-world and synthetic datasets validate\nthe effectiveness of Geo-Llama, demonstrating its versatility and robustness in\nhandling a broad range of constraints to generate more realistic trajectories\ncompared to existing methods."
                },
                "authors": [
                    {
                        "name": "Siyu Li"
                    },
                    {
                        "name": "Toan Tran"
                    },
                    {
                        "name": "Haowen Lin"
                    },
                    {
                        "name": "John Krumm"
                    },
                    {
                        "name": "Cyrus Shahabi"
                    },
                    {
                        "name": "Lingyi Zhao"
                    },
                    {
                        "name": "Khurram Shafique"
                    },
                    {
                        "name": "Li Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiong"
                },
                "author": "Li Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13918v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13918v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19611v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19611v1",
                "updated": "2025-04-28T09:18:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    18,
                    44,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T09:18:44Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    18,
                    44,
                    0,
                    118,
                    0
                ],
                "title": "Scene2Hap: Combining LLMs and Physical Modeling for Automatically\n  Generating Vibrotactile Signals for Full VR Scenes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scene2Hap: Combining LLMs and Physical Modeling for Automatically\n  Generating Vibrotactile Signals for Full VR Scenes"
                },
                "summary": "Haptic feedback contributes to immersive virtual reality (VR) experiences.\nDesigning such feedback at scale, for all objects within a VR scene and their\nrespective arrangements, remains a time-consuming task. We present Scene2Hap,\nan LLM-centered system that automatically designs object-level vibrotactile\nfeedback for entire VR scenes based on the objects' semantic attributes and\nphysical context. Scene2Hap employs a multimodal large language model to\nestimate the semantics and physical context of each object, including its\nmaterial properties and vibration behavior, from the multimodal information\npresent in the VR scene. This semantic and physical context is then used to\ncreate plausible vibrotactile signals by generating or retrieving audio signals\nand converting them to vibrotactile signals. For the more realistic spatial\nrendering of haptics in VR, Scene2Hap estimates the propagation and attenuation\nof vibration signals from their source across objects in the scene, considering\nthe estimated material properties and physical context, such as the distance\nand contact between virtual objects. Results from two user studies confirm that\nScene2Hap successfully estimates the semantics and physical context of VR\nscenes, and the physical modeling of vibration propagation improves usability,\nperceived materiality, and spatial awareness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Haptic feedback contributes to immersive virtual reality (VR) experiences.\nDesigning such feedback at scale, for all objects within a VR scene and their\nrespective arrangements, remains a time-consuming task. We present Scene2Hap,\nan LLM-centered system that automatically designs object-level vibrotactile\nfeedback for entire VR scenes based on the objects' semantic attributes and\nphysical context. Scene2Hap employs a multimodal large language model to\nestimate the semantics and physical context of each object, including its\nmaterial properties and vibration behavior, from the multimodal information\npresent in the VR scene. This semantic and physical context is then used to\ncreate plausible vibrotactile signals by generating or retrieving audio signals\nand converting them to vibrotactile signals. For the more realistic spatial\nrendering of haptics in VR, Scene2Hap estimates the propagation and attenuation\nof vibration signals from their source across objects in the scene, considering\nthe estimated material properties and physical context, such as the distance\nand contact between virtual objects. Results from two user studies confirm that\nScene2Hap successfully estimates the semantics and physical context of VR\nscenes, and the physical modeling of vibration propagation improves usability,\nperceived materiality, and spatial awareness."
                },
                "authors": [
                    {
                        "name": "Arata Jingu"
                    },
                    {
                        "name": "Easa AliAbbasi"
                    },
                    {
                        "name": "Paul Strohmeier"
                    },
                    {
                        "name": "Jrgen Steimle"
                    }
                ],
                "author_detail": {
                    "name": "Jrgen Steimle"
                },
                "author": "Jrgen Steimle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19611v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19606v1",
                "updated": "2025-04-28T09:10:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    10,
                    41,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T09:10:41Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    10,
                    41,
                    0,
                    118,
                    0
                ],
                "title": "Coreference Resolution for Vietnamese Narrative Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coreference Resolution for Vietnamese Narrative Texts"
                },
                "summary": "Coreference resolution is a vital task in natural language processing (NLP)\nthat involves identifying and linking different expressions in a text that\nrefer to the same entity. This task is particularly challenging for Vietnamese,\na low-resource language with limited annotated datasets. To address these\nchallenges, we developed a comprehensive annotated dataset using narrative\ntexts from VnExpress, a widely-read Vietnamese online news platform. We\nestablished detailed guidelines for annotating entities, focusing on ensuring\nconsistency and accuracy. Additionally, we evaluated the performance of large\nlanguage models (LLMs), specifically GPT-3.5-Turbo and GPT-4, on this dataset.\nOur results demonstrate that GPT-4 significantly outperforms GPT-3.5-Turbo in\nterms of both accuracy and response consistency, making it a more reliable tool\nfor coreference resolution in Vietnamese.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coreference resolution is a vital task in natural language processing (NLP)\nthat involves identifying and linking different expressions in a text that\nrefer to the same entity. This task is particularly challenging for Vietnamese,\na low-resource language with limited annotated datasets. To address these\nchallenges, we developed a comprehensive annotated dataset using narrative\ntexts from VnExpress, a widely-read Vietnamese online news platform. We\nestablished detailed guidelines for annotating entities, focusing on ensuring\nconsistency and accuracy. Additionally, we evaluated the performance of large\nlanguage models (LLMs), specifically GPT-3.5-Turbo and GPT-4, on this dataset.\nOur results demonstrate that GPT-4 significantly outperforms GPT-3.5-Turbo in\nterms of both accuracy and response consistency, making it a more reliable tool\nfor coreference resolution in Vietnamese."
                },
                "authors": [
                    {
                        "name": "Hieu-Dai Tran"
                    },
                    {
                        "name": "Duc-Vu Nguyen"
                    },
                    {
                        "name": "Ngan Luu-Thuy Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Ngan Luu-Thuy Nguyen"
                },
                "author": "Ngan Luu-Thuy Nguyen",
                "arxiv_comment": "Accepted at PACLIC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19599v1",
                "updated": "2025-04-28T09:02:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    2,
                    24,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T09:02:24Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    2,
                    24,
                    0,
                    118,
                    0
                ],
                "title": "GVPO: Group Variance Policy Optimization for Large Language Model\n  Post-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GVPO: Group Variance Policy Optimization for Large Language Model\n  Post-Training"
                },
                "summary": "Post-training plays a crucial role in refining and aligning large language\nmodels to meet specific tasks and human preferences. While recent advancements\nin post-training techniques, such as Group Relative Policy Optimization (GRPO),\nleverage increased sampling with relative reward scoring to achieve superior\nperformance, these methods often suffer from training instability that limits\ntheir practical adoption. To address this challenge, we present Group Variance\nPolicy Optimization (GVPO). GVPO incorporates the analytical solution to\nKL-constrained reward maximization directly into its gradient weights, ensuring\nalignment with the optimal policy. The method provides intuitive physical\ninterpretations: its gradient mirrors the mean squared error between the\ncentral distance of implicit rewards and that of actual rewards. GVPO offers\ntwo key advantages: (1) it guarantees a unique optimal solution, exactly the\nKL-constrained reward maximization objective, (2) it supports flexible sampling\ndistributions that avoids on-policy and importance sampling limitations. By\nunifying theoretical guarantees with practical adaptability, GVPO establishes a\nnew paradigm for reliable and versatile LLM post-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training plays a crucial role in refining and aligning large language\nmodels to meet specific tasks and human preferences. While recent advancements\nin post-training techniques, such as Group Relative Policy Optimization (GRPO),\nleverage increased sampling with relative reward scoring to achieve superior\nperformance, these methods often suffer from training instability that limits\ntheir practical adoption. To address this challenge, we present Group Variance\nPolicy Optimization (GVPO). GVPO incorporates the analytical solution to\nKL-constrained reward maximization directly into its gradient weights, ensuring\nalignment with the optimal policy. The method provides intuitive physical\ninterpretations: its gradient mirrors the mean squared error between the\ncentral distance of implicit rewards and that of actual rewards. GVPO offers\ntwo key advantages: (1) it guarantees a unique optimal solution, exactly the\nKL-constrained reward maximization objective, (2) it supports flexible sampling\ndistributions that avoids on-policy and importance sampling limitations. By\nunifying theoretical guarantees with practical adaptability, GVPO establishes a\nnew paradigm for reliable and versatile LLM post-training."
                },
                "authors": [
                    {
                        "name": "Kaichen Zhang"
                    },
                    {
                        "name": "Yuzhong Hong"
                    },
                    {
                        "name": "Junwei Bao"
                    },
                    {
                        "name": "Hongfei Jiang"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Dingqian Hong"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19596v1",
                "updated": "2025-04-28T09:00:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    0,
                    4,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T09:00:04Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    0,
                    4,
                    0,
                    118,
                    0
                ],
                "title": "Towards Robust Multimodal Physiological Foundation Models: Handling\n  Arbitrary Missing Modalities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Robust Multimodal Physiological Foundation Models: Handling\n  Arbitrary Missing Modalities"
                },
                "summary": "Multimodal physiological signals, such as EEG, ECG, EOG, and EMG, are crucial\nfor healthcare and brain-computer interfaces. While existing methods rely on\nspecialized architectures and dataset-specific fusion strategies, they struggle\nto learn universal representations that generalize across datasets and handle\nmissing modalities at inference time. To address these issues, we propose\nPhysioOmni, a foundation model for multimodal physiological signal analysis\nthat models both homogeneous and heterogeneous features to decouple multimodal\nsignals and extract generic representations while maintaining compatibility\nwith arbitrary missing modalities. PhysioOmni trains a decoupled multimodal\ntokenizer, enabling masked signal pre-training via modality-invariant and\nmodality-specific objectives. To ensure adaptability to diverse and incomplete\nmodality combinations, the pre-trained encoders undergo resilient fine-tuning\nwith prototype alignment on downstream datasets. Extensive experiments on four\ndownstream tasks, emotion recognition, sleep stage classification, motor\nprediction, and mental workload detection, demonstrate that PhysioOmni achieves\nstate-of-the-art performance while maintaining strong robustness to missing\nmodalities. Our code and model weights will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal physiological signals, such as EEG, ECG, EOG, and EMG, are crucial\nfor healthcare and brain-computer interfaces. While existing methods rely on\nspecialized architectures and dataset-specific fusion strategies, they struggle\nto learn universal representations that generalize across datasets and handle\nmissing modalities at inference time. To address these issues, we propose\nPhysioOmni, a foundation model for multimodal physiological signal analysis\nthat models both homogeneous and heterogeneous features to decouple multimodal\nsignals and extract generic representations while maintaining compatibility\nwith arbitrary missing modalities. PhysioOmni trains a decoupled multimodal\ntokenizer, enabling masked signal pre-training via modality-invariant and\nmodality-specific objectives. To ensure adaptability to diverse and incomplete\nmodality combinations, the pre-trained encoders undergo resilient fine-tuning\nwith prototype alignment on downstream datasets. Extensive experiments on four\ndownstream tasks, emotion recognition, sleep stage classification, motor\nprediction, and mental workload detection, demonstrate that PhysioOmni achieves\nstate-of-the-art performance while maintaining strong robustness to missing\nmodalities. Our code and model weights will be released."
                },
                "authors": [
                    {
                        "name": "Xi Fu"
                    },
                    {
                        "name": "Wei-Bang Jiang"
                    },
                    {
                        "name": "Yi Ding"
                    },
                    {
                        "name": "Cuntai Guan"
                    }
                ],
                "author_detail": {
                    "name": "Cuntai Guan"
                },
                "author": "Cuntai Guan",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19591v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19591v1",
                "updated": "2025-04-28T08:53:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    53,
                    39,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T08:53:39Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    53,
                    39,
                    0,
                    118,
                    0
                ],
                "title": "Semantic Packet Aggregation for Token Communication via Genetic Beam\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Packet Aggregation for Token Communication via Genetic Beam\n  Search"
                },
                "summary": "Token communication (TC) is poised to play a pivotal role in emerging\nlanguage-driven applications such as AI-generated content (AIGC) and wireless\nlanguage models (LLMs). However, token loss caused by channel noise can\nseverely degrade task performance. To address this, in this article, we focus\non the problem of semantics-aware packetization and develop a novel algorithm,\ntermed semantic packet aggregation with genetic beam search (SemPA-GBeam),\nwhich aims to maximize the average token similarity (ATS) over erasure\nchannels. Inspired from the genetic algorithm (GA) and the beam search\nalgorithm, SemPA-GBeam iteratively optimizes token grouping for packetization\nwithin a fixed number of groups (i.e., fixed beam width in beam search) while\nrandomly swapping a fraction of tokens (i.e., mutation in GA). Experiments on\nthe MS-COCO dataset demonstrate that SemPA-GBeam achieves ATS and LPIPS scores\ncomparable to exhaustive search while reducing complexity by more than 20x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token communication (TC) is poised to play a pivotal role in emerging\nlanguage-driven applications such as AI-generated content (AIGC) and wireless\nlanguage models (LLMs). However, token loss caused by channel noise can\nseverely degrade task performance. To address this, in this article, we focus\non the problem of semantics-aware packetization and develop a novel algorithm,\ntermed semantic packet aggregation with genetic beam search (SemPA-GBeam),\nwhich aims to maximize the average token similarity (ATS) over erasure\nchannels. Inspired from the genetic algorithm (GA) and the beam search\nalgorithm, SemPA-GBeam iteratively optimizes token grouping for packetization\nwithin a fixed number of groups (i.e., fixed beam width in beam search) while\nrandomly swapping a fraction of tokens (i.e., mutation in GA). Experiments on\nthe MS-COCO dataset demonstrate that SemPA-GBeam achieves ATS and LPIPS scores\ncomparable to exhaustive search while reducing complexity by more than 20x."
                },
                "authors": [
                    {
                        "name": "Seunghun Lee"
                    },
                    {
                        "name": "Jihong Park"
                    },
                    {
                        "name": "Jinho Choi"
                    },
                    {
                        "name": "Hyuncheol Park"
                    }
                ],
                "author_detail": {
                    "name": "Hyuncheol Park"
                },
                "author": "Hyuncheol Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19591v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19591v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08800v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08800v2",
                "updated": "2025-04-28T08:45:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    45,
                    38,
                    0,
                    118,
                    0
                ],
                "published": "2024-10-11T13:34:24Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    34,
                    24,
                    4,
                    285,
                    0
                ],
                "title": "Data Processing for the OpenGPT-X Model Family",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Processing for the OpenGPT-X Model Family"
                },
                "summary": "This paper presents a comprehensive overview of the data preparation pipeline\ndeveloped for the OpenGPT-X project, a large-scale initiative aimed at creating\nopen and high-performance multilingual large language models (LLMs). The\nproject goal is to deliver models that cover all major European languages, with\na particular focus on real-world applications within the European Union. We\nexplain all data processing steps, starting with the data selection and\nrequirement definition to the preparation of the final datasets for model\ntraining. We distinguish between curated data and web data, as each of these\ncategories is handled by distinct pipelines, with curated data undergoing\nminimal filtering and web data requiring extensive filtering and deduplication.\nThis distinction guided the development of specialized algorithmic solutions\nfor both pipelines. In addition to describing the processing methodologies, we\nprovide an in-depth analysis of the datasets, increasing transparency and\nalignment with European data regulations. Finally, we share key insights and\nchallenges faced during the project, offering recommendations for future\nendeavors in large-scale multilingual data preparation for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive overview of the data preparation pipeline\ndeveloped for the OpenGPT-X project, a large-scale initiative aimed at creating\nopen and high-performance multilingual large language models (LLMs). The\nproject goal is to deliver models that cover all major European languages, with\na particular focus on real-world applications within the European Union. We\nexplain all data processing steps, starting with the data selection and\nrequirement definition to the preparation of the final datasets for model\ntraining. We distinguish between curated data and web data, as each of these\ncategories is handled by distinct pipelines, with curated data undergoing\nminimal filtering and web data requiring extensive filtering and deduplication.\nThis distinction guided the development of specialized algorithmic solutions\nfor both pipelines. In addition to describing the processing methodologies, we\nprovide an in-depth analysis of the datasets, increasing transparency and\nalignment with European data regulations. Finally, we share key insights and\nchallenges faced during the project, offering recommendations for future\nendeavors in large-scale multilingual data preparation for LLMs."
                },
                "authors": [
                    {
                        "name": "Nicolo' Brandizzi"
                    },
                    {
                        "name": "Hammam Abdelwahab"
                    },
                    {
                        "name": "Anirban Bhowmick"
                    },
                    {
                        "name": "Lennard Helmer"
                    },
                    {
                        "name": "Benny Jrg Stein"
                    },
                    {
                        "name": "Pavel Denisov"
                    },
                    {
                        "name": "Qasid Saleem"
                    },
                    {
                        "name": "Michael Fromm"
                    },
                    {
                        "name": "Mehdi Ali"
                    },
                    {
                        "name": "Richard Rutmann"
                    },
                    {
                        "name": "Farzad Naderi"
                    },
                    {
                        "name": "Mohamad Saif Agy"
                    },
                    {
                        "name": "Alexander Schwirjow"
                    },
                    {
                        "name": "Fabian Kch"
                    },
                    {
                        "name": "Luzian Hahn"
                    },
                    {
                        "name": "Malte Ostendorff"
                    },
                    {
                        "name": "Pedro Ortiz Suarez"
                    },
                    {
                        "name": "Georg Rehm"
                    },
                    {
                        "name": "Dennis Wegener"
                    },
                    {
                        "name": "Nicolas Flores-Herr"
                    },
                    {
                        "name": "Joachim Khler"
                    },
                    {
                        "name": "Johannes Leveling"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Leveling"
                },
                "author": "Johannes Leveling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08800v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.1; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19580v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19580v1",
                "updated": "2025-04-28T08:41:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    41,
                    8,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T08:41:08Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    41,
                    8,
                    0,
                    118,
                    0
                ],
                "title": "ARTEMIS: Autoregressive End-to-End Trajectory Planning with Mixture of\n  Experts for Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARTEMIS: Autoregressive End-to-End Trajectory Planning with Mixture of\n  Experts for Autonomous Driving"
                },
                "summary": "This paper presents ARTEMIS, an end-to-end autonomous driving framework that\ncombines autoregressive trajectory planning with Mixture-of-Experts (MoE).\nTraditional modular methods suffer from error propagation, while existing\nend-to-end models typically employ static one-shot inference paradigms that\ninadequately capture the dynamic changes of the environment. ARTEMIS takes a\ndifferent method by generating trajectory waypoints sequentially, preserves\ncritical temporal dependencies while dynamically routing scene-specific queries\nto specialized expert networks. It effectively relieves trajectory quality\ndegradation issues encountered when guidance information is ambiguous, and\novercomes the inherent representational limitations of singular network\narchitectures when processing diverse driving scenarios. Additionally, we use a\nlightweight batch reallocation strategy that significantly improves the\ntraining speed of the Mixture-of-Experts model. Through experiments on the\nNAVSIM dataset, ARTEMIS exhibits superior competitive performance, achieving\n87.0 PDMS and 83.1 EPDMS with ResNet-34 backbone, demonstrates state-of-the-art\nperformance on multiple metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents ARTEMIS, an end-to-end autonomous driving framework that\ncombines autoregressive trajectory planning with Mixture-of-Experts (MoE).\nTraditional modular methods suffer from error propagation, while existing\nend-to-end models typically employ static one-shot inference paradigms that\ninadequately capture the dynamic changes of the environment. ARTEMIS takes a\ndifferent method by generating trajectory waypoints sequentially, preserves\ncritical temporal dependencies while dynamically routing scene-specific queries\nto specialized expert networks. It effectively relieves trajectory quality\ndegradation issues encountered when guidance information is ambiguous, and\novercomes the inherent representational limitations of singular network\narchitectures when processing diverse driving scenarios. Additionally, we use a\nlightweight batch reallocation strategy that significantly improves the\ntraining speed of the Mixture-of-Experts model. Through experiments on the\nNAVSIM dataset, ARTEMIS exhibits superior competitive performance, achieving\n87.0 PDMS and 83.1 EPDMS with ResNet-34 backbone, demonstrates state-of-the-art\nperformance on multiple metrics."
                },
                "authors": [
                    {
                        "name": "Renju Feng"
                    },
                    {
                        "name": "Ning Xi"
                    },
                    {
                        "name": "Duanfeng Chu"
                    },
                    {
                        "name": "Rukang Wang"
                    },
                    {
                        "name": "Zejian Deng"
                    },
                    {
                        "name": "Anzheng Wang"
                    },
                    {
                        "name": "Liping Lu"
                    },
                    {
                        "name": "Jinxiang Wang"
                    },
                    {
                        "name": "Yanjun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yanjun Huang"
                },
                "author": "Yanjun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19580v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18986v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18986v2",
                "updated": "2025-04-28T08:24:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    24,
                    16,
                    0,
                    118,
                    0
                ],
                "published": "2025-02-26T09:47:16Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    9,
                    47,
                    16,
                    2,
                    57,
                    0
                ],
                "title": "Evaluating Membership Inference Attacks in heterogeneous-data setups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Membership Inference Attacks in heterogeneous-data setups"
                },
                "summary": "Among all privacy attacks against Machine Learning (ML), membership inference\nattacks (MIA) attracted the most attention. In these attacks, the attacker is\ngiven an ML model and a data point, and they must infer whether the data point\nwas used for training. The attacker also has an auxiliary dataset to tune their\ninference algorithm.\n  Attack papers commonly simulate setups in which the attacker's and the\ntarget's datasets are sampled from the same distribution. This setting is\nconvenient to perform experiments, but it rarely holds in practice. ML\nliterature commonly starts with similar simplifying assumptions (i.e., \"i.i.d.\"\ndatasets), and later generalizes the results to support heterogeneous data\ndistributions. Similarly, our work makes a first step in the generalization of\nthe MIA evaluation to heterogeneous data.\n  First, we design a metric to measure the heterogeneity between any pair of\ntabular data distributions. This metric provides a continuous scale to analyze\nthe phenomenon. Second, we compare two methodologies to simulate a data\nheterogeneity between the target and the attacker. These setups provide\nopposite performances: 90% attack accuracy vs. 50% (i.e., random guessing). Our\nresults show that the MIA accuracy depends on the experimental setup; and even\nif research on MIA considers heterogeneous data setups, we have no standardized\nbaseline of how to simulate it. The lack of such a baseline for MIA experiments\nposes a significant challenge to risk assessments in real-world machine\nlearning scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Among all privacy attacks against Machine Learning (ML), membership inference\nattacks (MIA) attracted the most attention. In these attacks, the attacker is\ngiven an ML model and a data point, and they must infer whether the data point\nwas used for training. The attacker also has an auxiliary dataset to tune their\ninference algorithm.\n  Attack papers commonly simulate setups in which the attacker's and the\ntarget's datasets are sampled from the same distribution. This setting is\nconvenient to perform experiments, but it rarely holds in practice. ML\nliterature commonly starts with similar simplifying assumptions (i.e., \"i.i.d.\"\ndatasets), and later generalizes the results to support heterogeneous data\ndistributions. Similarly, our work makes a first step in the generalization of\nthe MIA evaluation to heterogeneous data.\n  First, we design a metric to measure the heterogeneity between any pair of\ntabular data distributions. This metric provides a continuous scale to analyze\nthe phenomenon. Second, we compare two methodologies to simulate a data\nheterogeneity between the target and the attacker. These setups provide\nopposite performances: 90% attack accuracy vs. 50% (i.e., random guessing). Our\nresults show that the MIA accuracy depends on the experimental setup; and even\nif research on MIA considers heterogeneous data setups, we have no standardized\nbaseline of how to simulate it. The lack of such a baseline for MIA experiments\nposes a significant challenge to risk assessments in real-world machine\nlearning scenarios."
                },
                "authors": [
                    {
                        "name": "Bram van Dartel"
                    },
                    {
                        "name": "Marc Damie"
                    },
                    {
                        "name": "Florian Hahn"
                    }
                ],
                "author_detail": {
                    "name": "Florian Hahn"
                },
                "author": "Florian Hahn",
                "arxiv_comment": "Accepted in SiMLA workshop 2025 (co-located with ACNS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18986v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18986v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19565v1",
                "updated": "2025-04-28T08:18:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    18,
                    24,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T08:18:24Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    18,
                    24,
                    0,
                    118,
                    0
                ],
                "title": "m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation\n  Framework for Biomedical Large Language Models Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation\n  Framework for Biomedical Large Language Models Training"
                },
                "summary": "The rapid progress of large language models (LLMs) in biomedical research has\nunderscored the limitations of existing open-source annotated scientific\ncorpora, which are often insufficient in quantity and quality. Addressing the\nchallenge posed by the complex hierarchy of biomedical knowledge, we propose a\nknowledge-driven, multi-agent framework for scientific corpus distillation\ntailored for LLM training in the biomedical domain. Central to our approach is\na collaborative multi-agent architecture, where specialized agents, each guided\nby the Medical Subject Headings (MeSH) hierarchy, work in concert to\nautonomously extract, synthesize, and self-evaluate high-quality textual data\nfrom vast scientific literature. These agents collectively generate and refine\ndomain-specific question-answer pairs, ensuring comprehensive coverage and\nconsistency with biomedical ontologies while minimizing manual involvement.\nExtensive experimental results show that language models trained on our\nmulti-agent distilled datasets achieve notable improvements in biomedical\nquestion-answering tasks, outperforming both strong life sciences LLM baselines\nand advanced proprietary models. Notably, our AI-Ready dataset enables\nLlama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2, despite their larger\nscale. Detailed ablation studies and case analyses further validate the\neffectiveness and synergy of each agent within the framework, highlighting the\npotential of multi-agent collaboration in biomedical LLM training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress of large language models (LLMs) in biomedical research has\nunderscored the limitations of existing open-source annotated scientific\ncorpora, which are often insufficient in quantity and quality. Addressing the\nchallenge posed by the complex hierarchy of biomedical knowledge, we propose a\nknowledge-driven, multi-agent framework for scientific corpus distillation\ntailored for LLM training in the biomedical domain. Central to our approach is\na collaborative multi-agent architecture, where specialized agents, each guided\nby the Medical Subject Headings (MeSH) hierarchy, work in concert to\nautonomously extract, synthesize, and self-evaluate high-quality textual data\nfrom vast scientific literature. These agents collectively generate and refine\ndomain-specific question-answer pairs, ensuring comprehensive coverage and\nconsistency with biomedical ontologies while minimizing manual involvement.\nExtensive experimental results show that language models trained on our\nmulti-agent distilled datasets achieve notable improvements in biomedical\nquestion-answering tasks, outperforming both strong life sciences LLM baselines\nand advanced proprietary models. Notably, our AI-Ready dataset enables\nLlama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2, despite their larger\nscale. Detailed ablation studies and case analyses further validate the\neffectiveness and synergy of each agent within the framework, highlighting the\npotential of multi-agent collaboration in biomedical LLM training."
                },
                "authors": [
                    {
                        "name": "Meng Xiao"
                    },
                    {
                        "name": "Xunxin Cai"
                    },
                    {
                        "name": "Chengrui Wang"
                    },
                    {
                        "name": "Yuanchun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yuanchun Zhou"
                },
                "author": "Yuanchun Zhou",
                "arxiv_comment": "22 pages, Large Language Model, Agentic AI, Dataset Distillation,\n  Multi-agent Collaboration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19557v1",
                "updated": "2025-04-28T08:02:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    2,
                    2,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T08:02:02Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    2,
                    2,
                    0,
                    118,
                    0
                ],
                "title": "CE-NPBG: Connectivity Enhanced Neural Point-Based Graphics for Novel\n  View Synthesis in Autonomous Driving Scenes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CE-NPBG: Connectivity Enhanced Neural Point-Based Graphics for Novel\n  View Synthesis in Autonomous Driving Scenes"
                },
                "summary": "Current point-based approaches encounter limitations in scalability and\nrendering quality when using large 3D point cloud maps because using them\ndirectly for novel view synthesis (NVS) leads to degraded visualizations. We\nidentify the primary issue behind these low-quality renderings as a visibility\nmismatch between geometry and appearance, stemming from using these two\nmodalities together. To address this problem, we present CE-NPBG, a new\napproach for novel view synthesis (NVS) in large-scale autonomous driving\nscenes. Our method is a neural point-based technique that leverages two\nmodalities: posed images (cameras) and synchronized raw 3D point clouds\n(LiDAR). We first employ a connectivity relationship graph between appearance\nand geometry, which retrieves points from a large 3D point cloud map observed\nfrom the current camera perspective and uses them for rendering. By leveraging\nthis connectivity, our method significantly improves rendering quality and\nenhances run-time and scalability by using only a small subset of points from\nthe large 3D point cloud map. Our approach associates neural descriptors with\nthe points and uses them to synthesize views. To enhance the encoding of these\ndescriptors and elevate rendering quality, we propose a joint adversarial and\npoint rasterization training. During training, we pair an image-synthesizer\nnetwork with a multi-resolution discriminator. At inference, we decouple them\nand use the image-synthesizer to generate novel views. We also integrate our\nproposal into the recent 3D Gaussian Splatting work to highlight its benefits\nfor improved rendering and scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current point-based approaches encounter limitations in scalability and\nrendering quality when using large 3D point cloud maps because using them\ndirectly for novel view synthesis (NVS) leads to degraded visualizations. We\nidentify the primary issue behind these low-quality renderings as a visibility\nmismatch between geometry and appearance, stemming from using these two\nmodalities together. To address this problem, we present CE-NPBG, a new\napproach for novel view synthesis (NVS) in large-scale autonomous driving\nscenes. Our method is a neural point-based technique that leverages two\nmodalities: posed images (cameras) and synchronized raw 3D point clouds\n(LiDAR). We first employ a connectivity relationship graph between appearance\nand geometry, which retrieves points from a large 3D point cloud map observed\nfrom the current camera perspective and uses them for rendering. By leveraging\nthis connectivity, our method significantly improves rendering quality and\nenhances run-time and scalability by using only a small subset of points from\nthe large 3D point cloud map. Our approach associates neural descriptors with\nthe points and uses them to synthesize views. To enhance the encoding of these\ndescriptors and elevate rendering quality, we propose a joint adversarial and\npoint rasterization training. During training, we pair an image-synthesizer\nnetwork with a multi-resolution discriminator. At inference, we decouple them\nand use the image-synthesizer to generate novel views. We also integrate our\nproposal into the recent 3D Gaussian Splatting work to highlight its benefits\nfor improved rendering and scalability."
                },
                "authors": [
                    {
                        "name": "Mohammad Altillawi"
                    },
                    {
                        "name": "Fengyi Shen"
                    },
                    {
                        "name": "Liudi Yang"
                    },
                    {
                        "name": "Sai Manoj Prakhya"
                    },
                    {
                        "name": "Ziyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ziyuan Liu"
                },
                "author": "Ziyuan Liu",
                "arxiv_comment": "Accepted in 2025 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition Workshops (CVPRW)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10652v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10652v2",
                "updated": "2025-04-28T07:58:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    7,
                    58,
                    34,
                    0,
                    118,
                    0
                ],
                "published": "2024-07-15T12:13:53Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    12,
                    13,
                    53,
                    0,
                    197,
                    0
                ],
                "title": "Cutting Through the Clutter: The Potential of LLMs for Efficient\n  Filtration in Systematic Literature Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cutting Through the Clutter: The Potential of LLMs for Efficient\n  Filtration in Systematic Literature Reviews"
                },
                "summary": "Systematic literature reviews (SLRs) are essential but labor-intensive due to\nhigh publication volumes and inefficient keyword-based filtering. To streamline\nthis process, we evaluate Large Language Models (LLMs) for enhancing efficiency\nand accuracy in corpus filtration while minimizing manual effort. Our\nopen-source tool LLMSurver presents a visual interface to utilize LLMs for\nliterature filtration, evaluate the results, and refine queries in an\ninteractive way. We assess the real-world performance of our approach in\nfiltering over 8.3k articles during a recent survey construction, comparing\nresults with human efforts. The findings show that recent LLM models can reduce\nfiltering time from weeks to minutes. A consensus scheme ensures recall rates\n>98.8%, surpassing typical human error thresholds and improving selection\naccuracy. This work advances literature review methodologies and highlights the\npotential of responsible human-AI collaboration in academic research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic literature reviews (SLRs) are essential but labor-intensive due to\nhigh publication volumes and inefficient keyword-based filtering. To streamline\nthis process, we evaluate Large Language Models (LLMs) for enhancing efficiency\nand accuracy in corpus filtration while minimizing manual effort. Our\nopen-source tool LLMSurver presents a visual interface to utilize LLMs for\nliterature filtration, evaluate the results, and refine queries in an\ninteractive way. We assess the real-world performance of our approach in\nfiltering over 8.3k articles during a recent survey construction, comparing\nresults with human efforts. The findings show that recent LLM models can reduce\nfiltering time from weeks to minutes. A consensus scheme ensures recall rates\n>98.8%, surpassing typical human error thresholds and improving selection\naccuracy. This work advances literature review methodologies and highlights the\npotential of responsible human-AI collaboration in academic research."
                },
                "authors": [
                    {
                        "name": "Lucas Joos"
                    },
                    {
                        "name": "Daniel A. Keim"
                    },
                    {
                        "name": "Maximilian T. Fischer"
                    }
                ],
                "author_detail": {
                    "name": "Maximilian T. Fischer"
                },
                "author": "Maximilian T. Fischer",
                "arxiv_comment": "6 pages, 5 figures, 1 table",
                "arxiv_journal_ref": "16th International EuroVis Workshop on Visual Analytics\n  (EuroVA'25), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10652v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10652v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19645v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19645v2",
                "updated": "2025-04-28T07:49:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    7,
                    49,
                    39,
                    0,
                    118,
                    0
                ],
                "published": "2025-02-27T00:30:29Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    0,
                    30,
                    29,
                    3,
                    58,
                    0
                ],
                "title": "Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success"
                },
                "summary": "Recent vision-language-action models (VLAs) build upon pretrained\nvision-language models and leverage diverse robot datasets to demonstrate\nstrong task execution, language following ability, and semantic generalization.\nDespite these successes, VLAs struggle with novel robot setups and require\nfine-tuning to achieve good performance, yet how to most effectively fine-tune\nthem is unclear given many possible strategies. In this work, we study key VLA\nadaptation design choices such as different action decoding schemes, action\nrepresentations, and learning objectives for fine-tuning, using OpenVLA as our\nrepresentative base model. Our empirical analysis informs an Optimized\nFine-Tuning (OFT) recipe that integrates parallel decoding, action chunking, a\ncontinuous action representation, and a simple L1 regression-based learning\nobjective to altogether improve inference efficiency, policy performance, and\nflexibility in the model's input-output specifications. We propose OpenVLA-OFT,\nan instantiation of this recipe, which sets a new state of the art on the\nLIBERO simulation benchmark, significantly boosting OpenVLA's average success\nrate across four task suites from 76.5% to 97.1% while increasing action\ngeneration throughput by 26$\\times$. In real-world evaluations, our fine-tuning\nrecipe enables OpenVLA to successfully execute dexterous, high-frequency\ncontrol tasks on a bimanual ALOHA robot and outperform other VLAs ($\\pi_0$ and\nRDT-1B) fine-tuned using their default recipes, as well as strong imitation\nlearning policies trained from scratch (Diffusion Policy and ACT) by up to 15%\n(absolute) in average success rate. We release code for OFT and pretrained\nmodel checkpoints at https://openvla-oft.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent vision-language-action models (VLAs) build upon pretrained\nvision-language models and leverage diverse robot datasets to demonstrate\nstrong task execution, language following ability, and semantic generalization.\nDespite these successes, VLAs struggle with novel robot setups and require\nfine-tuning to achieve good performance, yet how to most effectively fine-tune\nthem is unclear given many possible strategies. In this work, we study key VLA\nadaptation design choices such as different action decoding schemes, action\nrepresentations, and learning objectives for fine-tuning, using OpenVLA as our\nrepresentative base model. Our empirical analysis informs an Optimized\nFine-Tuning (OFT) recipe that integrates parallel decoding, action chunking, a\ncontinuous action representation, and a simple L1 regression-based learning\nobjective to altogether improve inference efficiency, policy performance, and\nflexibility in the model's input-output specifications. We propose OpenVLA-OFT,\nan instantiation of this recipe, which sets a new state of the art on the\nLIBERO simulation benchmark, significantly boosting OpenVLA's average success\nrate across four task suites from 76.5% to 97.1% while increasing action\ngeneration throughput by 26$\\times$. In real-world evaluations, our fine-tuning\nrecipe enables OpenVLA to successfully execute dexterous, high-frequency\ncontrol tasks on a bimanual ALOHA robot and outperform other VLAs ($\\pi_0$ and\nRDT-1B) fine-tuned using their default recipes, as well as strong imitation\nlearning policies trained from scratch (Diffusion Policy and ACT) by up to 15%\n(absolute) in average success rate. We release code for OFT and pretrained\nmodel checkpoints at https://openvla-oft.github.io/."
                },
                "authors": [
                    {
                        "name": "Moo Jin Kim"
                    },
                    {
                        "name": "Chelsea Finn"
                    },
                    {
                        "name": "Percy Liang"
                    }
                ],
                "author_detail": {
                    "name": "Percy Liang"
                },
                "author": "Percy Liang",
                "arxiv_comment": "Accepted to Robotics: Science and Systems (RSS) 2025. Project\n  website: https://openvla-oft.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19645v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19645v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.01090v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.01090v4",
                "updated": "2025-04-28T07:47:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    7,
                    47,
                    49,
                    0,
                    118,
                    0
                ],
                "published": "2023-11-02T08:55:11Z",
                "published_parsed": [
                    2023,
                    11,
                    2,
                    8,
                    55,
                    11,
                    3,
                    306,
                    0
                ],
                "title": "Infusion: internal diffusion for inpainting of dynamic textures and\n  complex motion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infusion: internal diffusion for inpainting of dynamic textures and\n  complex motion"
                },
                "summary": "Video inpainting is the task of filling a region in a video in a visually\nconvincing manner. It is very challenging due to the high dimensionality of the\ndata and the temporal consistency required for obtaining convincing results.\nRecently, diffusion models have shown impressive results in modeling complex\ndata distributions, including images and videos. Such models remain nonetheless\nvery expensive to train and to perform inference with, which strongly reduce\ntheir applicability to videos, and yields unreasonable computational loads. We\nshow that in the case of video inpainting, thanks to the highly auto-similar\nnature of videos, the training data of a diffusion model can be restricted to\nthe input video and still produce very satisfying results. With this internal\nlearning approach, where the training data is limited to a single video, our\nlightweight models perform very well with only half a million parameters, in\ncontrast to the very large networks with billions of parameters typically found\nin the literature. We also introduce a new method for efficient training and\ninference of diffusion models in the context of internal learning, by splitting\nthe diffusion process into different learning intervals corresponding to\ndifferent noise levels of the diffusion process. We show qualitative and\nquantitative results, demonstrating that our method reaches or exceeds state of\nthe art performance in the case of dynamic textures and complex dynamic\nbackgrounds",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video inpainting is the task of filling a region in a video in a visually\nconvincing manner. It is very challenging due to the high dimensionality of the\ndata and the temporal consistency required for obtaining convincing results.\nRecently, diffusion models have shown impressive results in modeling complex\ndata distributions, including images and videos. Such models remain nonetheless\nvery expensive to train and to perform inference with, which strongly reduce\ntheir applicability to videos, and yields unreasonable computational loads. We\nshow that in the case of video inpainting, thanks to the highly auto-similar\nnature of videos, the training data of a diffusion model can be restricted to\nthe input video and still produce very satisfying results. With this internal\nlearning approach, where the training data is limited to a single video, our\nlightweight models perform very well with only half a million parameters, in\ncontrast to the very large networks with billions of parameters typically found\nin the literature. We also introduce a new method for efficient training and\ninference of diffusion models in the context of internal learning, by splitting\nthe diffusion process into different learning intervals corresponding to\ndifferent noise levels of the diffusion process. We show qualitative and\nquantitative results, demonstrating that our method reaches or exceeds state of\nthe art performance in the case of dynamic textures and complex dynamic\nbackgrounds"
                },
                "authors": [
                    {
                        "name": "Nicolas Cherel"
                    },
                    {
                        "name": "Andrs Almansa"
                    },
                    {
                        "name": "Yann Gousseau"
                    },
                    {
                        "name": "Alasdair Newson"
                    }
                ],
                "author_detail": {
                    "name": "Alasdair Newson"
                },
                "author": "Alasdair Newson",
                "arxiv_doi": "10.1111/cgf.70070",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1111/cgf.70070",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2311.01090v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.01090v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 11 figures. Published in Eurographics 2025",
                "arxiv_journal_ref": "Computer Graphics Forum e70070 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19538v1",
                "updated": "2025-04-28T07:41:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    7,
                    41,
                    3,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T07:41:03Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    7,
                    41,
                    3,
                    0,
                    118,
                    0
                ],
                "title": "Towards Faster and More Compact Foundation Models for Molecular Property\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Faster and More Compact Foundation Models for Molecular Property\n  Prediction"
                },
                "summary": "Advancements in machine learning for molecular property prediction have\nimproved accuracy but at the expense of higher computational cost and longer\ntraining times. Recently, the Joint Multi-domain Pre-training (JMP) foundation\nmodel has demonstrated strong performance across various downstream tasks with\nreduced training time over previous models. Despite JMP's advantages,\nfine-tuning it on molecular datasets ranging from small-scale to large-scale\nrequires considerable time and computational resources. In this work, we\ninvestigate strategies to enhance efficiency by reducing model size while\npreserving performance. To better understand the model's efficiency, we analyze\nthe layer contributions of JMP and find that later interaction blocks provide\ndiminishing returns, suggesting an opportunity for model compression. We\nexplore block reduction strategies by pruning the pre-trained model and\nevaluating its impact on efficiency and accuracy during fine-tuning. Our\nanalysis reveals that removing two interaction blocks results in a minimal\nperformance drop, reducing the model size by 32% while increasing inference\nthroughput by 1.3x. These results suggest that JMP-L is over-parameterized and\nthat a smaller, more efficient variant can achieve comparable performance with\nlower computational cost. Our study provides insights for developing lighter,\nfaster, and more scalable foundation models for molecular and materials\ndiscovery. The code is publicly available at:\nhttps://github.com/Yasir-Ghunaim/efficient-jmp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in machine learning for molecular property prediction have\nimproved accuracy but at the expense of higher computational cost and longer\ntraining times. Recently, the Joint Multi-domain Pre-training (JMP) foundation\nmodel has demonstrated strong performance across various downstream tasks with\nreduced training time over previous models. Despite JMP's advantages,\nfine-tuning it on molecular datasets ranging from small-scale to large-scale\nrequires considerable time and computational resources. In this work, we\ninvestigate strategies to enhance efficiency by reducing model size while\npreserving performance. To better understand the model's efficiency, we analyze\nthe layer contributions of JMP and find that later interaction blocks provide\ndiminishing returns, suggesting an opportunity for model compression. We\nexplore block reduction strategies by pruning the pre-trained model and\nevaluating its impact on efficiency and accuracy during fine-tuning. Our\nanalysis reveals that removing two interaction blocks results in a minimal\nperformance drop, reducing the model size by 32% while increasing inference\nthroughput by 1.3x. These results suggest that JMP-L is over-parameterized and\nthat a smaller, more efficient variant can achieve comparable performance with\nlower computational cost. Our study provides insights for developing lighter,\nfaster, and more scalable foundation models for molecular and materials\ndiscovery. The code is publicly available at:\nhttps://github.com/Yasir-Ghunaim/efficient-jmp."
                },
                "authors": [
                    {
                        "name": "Yasir Ghunaim"
                    },
                    {
                        "name": "Andrs Villa"
                    },
                    {
                        "name": "Gergo Ignacz"
                    },
                    {
                        "name": "Gyorgy Szekely"
                    },
                    {
                        "name": "Motasem Alfarra"
                    },
                    {
                        "name": "Bernard Ghanem"
                    }
                ],
                "author_detail": {
                    "name": "Bernard Ghanem"
                },
                "author": "Bernard Ghanem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08539v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08539v2",
                "updated": "2025-04-28T07:32:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    7,
                    32,
                    42,
                    0,
                    118,
                    0
                ],
                "published": "2025-03-11T15:28:09Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    28,
                    9,
                    1,
                    70,
                    0
                ],
                "title": "Desirable Unfamiliarity: Insights from Eye Movements on Engagement and\n  Readability of Dictation Interfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Desirable Unfamiliarity: Insights from Eye Movements on Engagement and\n  Readability of Dictation Interfaces"
                },
                "summary": "Dictation interfaces support efficient text input, but the transcribed text\ncan be hard to read. To understand how users read and review dictated text, we\nconducted a controlled eye-tracking experiment with 20 participants to compare\nfive dictation interfaces: PLAIN (real-time transcription), AOC (periodic\ncorrections), RAKE (keyword highlights), GP-TSM (grammar-preserving\nhighlights), and SUMMARY (LLM-generated abstraction summary). The study\nanalyzed participants' gaze patterns during their speech composition and\nreviewing processes. The findings show that during composition, participants\nspent only 7--11% of their time actively reading, and they favored real-time\nfeedback and avoided distracting interface changes. During reviewing, although\nSUMMARY introduced unfamiliar words (requiring longer and more frequent\nfixation), they were easier to read (requiring fewer regressions). Participants\npreferred SUMMARY for the polished text that preserved fidelity to original\nmeanings. RAKE guided the reading of self-produced text better than GP-TSM.\nRAKE guides the reading of self-produced text better than GP-TSM. These\nsurprising findings suggest that dictation interfaces could consider showing\nsummaries or key information to support recall instead of raw transcripts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dictation interfaces support efficient text input, but the transcribed text\ncan be hard to read. To understand how users read and review dictated text, we\nconducted a controlled eye-tracking experiment with 20 participants to compare\nfive dictation interfaces: PLAIN (real-time transcription), AOC (periodic\ncorrections), RAKE (keyword highlights), GP-TSM (grammar-preserving\nhighlights), and SUMMARY (LLM-generated abstraction summary). The study\nanalyzed participants' gaze patterns during their speech composition and\nreviewing processes. The findings show that during composition, participants\nspent only 7--11% of their time actively reading, and they favored real-time\nfeedback and avoided distracting interface changes. During reviewing, although\nSUMMARY introduced unfamiliar words (requiring longer and more frequent\nfixation), they were easier to read (requiring fewer regressions). Participants\npreferred SUMMARY for the polished text that preserved fidelity to original\nmeanings. RAKE guided the reading of self-produced text better than GP-TSM.\nRAKE guides the reading of self-produced text better than GP-TSM. These\nsurprising findings suggest that dictation interfaces could consider showing\nsummaries or key information to support recall instead of raw transcripts."
                },
                "authors": [
                    {
                        "name": "Zhaohui Liang"
                    },
                    {
                        "name": "Yonglin Chen"
                    },
                    {
                        "name": "Naser Al Madi"
                    },
                    {
                        "name": "Can Liu"
                    }
                ],
                "author_detail": {
                    "name": "Can Liu"
                },
                "author": "Can Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08539v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08539v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19522v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19522v1",
                "updated": "2025-04-28T06:40:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    6,
                    40,
                    41,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T06:40:41Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    6,
                    40,
                    41,
                    0,
                    118,
                    0
                ],
                "title": "A Model-based DNN for Learning HMIMO Beamforming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Model-based DNN for Learning HMIMO Beamforming"
                },
                "summary": "Holographic MIMO (HMIMO) is a promising technique for large-scale MIMO\nsystems to enhance spectral efficiency while maintaining low hardware cost and\npower consumption. Existing alternating optimization algorithms can effectively\noptimize the hybrid beamforming of HMIMO to improve the system performance,\nwhile their high computational complexity hinders real-time application. In\nthis paper, we propose a model-based deep neural network (MB-DNN), which\nleverages permutation equivalent properties and the optimal beamforming\nstructure to jointly optimize the holographic and digital beamforming.\nSimulation results demonstrate that the proposed MB-DNN outperforms benchmark\nschemes and requires much less inference time than existing alternating\noptimization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Holographic MIMO (HMIMO) is a promising technique for large-scale MIMO\nsystems to enhance spectral efficiency while maintaining low hardware cost and\npower consumption. Existing alternating optimization algorithms can effectively\noptimize the hybrid beamforming of HMIMO to improve the system performance,\nwhile their high computational complexity hinders real-time application. In\nthis paper, we propose a model-based deep neural network (MB-DNN), which\nleverages permutation equivalent properties and the optimal beamforming\nstructure to jointly optimize the holographic and digital beamforming.\nSimulation results demonstrate that the proposed MB-DNN outperforms benchmark\nschemes and requires much less inference time than existing alternating\noptimization algorithms."
                },
                "authors": [
                    {
                        "name": "Shiyong Chen"
                    },
                    {
                        "name": "Shengqian Han"
                    }
                ],
                "author_detail": {
                    "name": "Shengqian Han"
                },
                "author": "Shengqian Han",
                "arxiv_comment": "5 pages,4figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19522v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19522v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 90B18, 94A05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19521v1",
                "updated": "2025-04-28T06:40:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    6,
                    40,
                    1,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T06:40:01Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    6,
                    40,
                    1,
                    0,
                    118,
                    0
                ],
                "title": "Security Steerability is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security Steerability is All You Need"
                },
                "summary": "The adoption of Generative AI (GenAI) in various applications inevitably\ncomes with expanding the attack surface, combining new security threats along\nwith the traditional ones. Consequently, numerous research and industrial\ninitiatives aim to mitigate these security threats in GenAI by developing\nmetrics and designing defenses. However, while most of the GenAI security work\nfocuses on universal threats (e.g. manipulating the LLM to generate forbidden\ncontent), there is significantly less discussion on application-level security\nand how to mitigate it.\n  Thus, in this work we adopt an application-centric approach to GenAI\nsecurity, and show that while LLMs cannot protect against ad-hoc application\nspecific threats, they can provide the framework for applications to protect\nthemselves against such threats. Our first contribution is defining Security\nSteerability - a novel security measure for LLMs, assessing the model's\ncapability to adhere to strict guardrails that are defined in the system prompt\n('Refrain from discussing about politics'). These guardrails, in case\neffective, can stop threats in the presence of malicious users who attempt to\ncircumvent the application and cause harm to its providers.\n  Our second contribution is a methodology to measure the security steerability\nof LLMs, utilizing two newly-developed datasets: VeganRibs assesses the LLM\nbehavior in forcing specific guardrails that are not security per se in the\npresence of malicious user that uses attack boosters (jailbreaks and\nperturbations), and ReverseText takes this approach further and measures the\nLLM ability to force specific treatment of the user input as plain text while\ndo user try to give it additional meanings...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of Generative AI (GenAI) in various applications inevitably\ncomes with expanding the attack surface, combining new security threats along\nwith the traditional ones. Consequently, numerous research and industrial\ninitiatives aim to mitigate these security threats in GenAI by developing\nmetrics and designing defenses. However, while most of the GenAI security work\nfocuses on universal threats (e.g. manipulating the LLM to generate forbidden\ncontent), there is significantly less discussion on application-level security\nand how to mitigate it.\n  Thus, in this work we adopt an application-centric approach to GenAI\nsecurity, and show that while LLMs cannot protect against ad-hoc application\nspecific threats, they can provide the framework for applications to protect\nthemselves against such threats. Our first contribution is defining Security\nSteerability - a novel security measure for LLMs, assessing the model's\ncapability to adhere to strict guardrails that are defined in the system prompt\n('Refrain from discussing about politics'). These guardrails, in case\neffective, can stop threats in the presence of malicious users who attempt to\ncircumvent the application and cause harm to its providers.\n  Our second contribution is a methodology to measure the security steerability\nof LLMs, utilizing two newly-developed datasets: VeganRibs assesses the LLM\nbehavior in forcing specific guardrails that are not security per se in the\npresence of malicious user that uses attack boosters (jailbreaks and\nperturbations), and ReverseText takes this approach further and measures the\nLLM ability to force specific treatment of the user input as plain text while\ndo user try to give it additional meanings..."
                },
                "authors": [
                    {
                        "name": "Itay Hazan"
                    },
                    {
                        "name": "Idan Habler"
                    },
                    {
                        "name": "Ron Bitton"
                    },
                    {
                        "name": "Itsik Mantin"
                    }
                ],
                "author_detail": {
                    "name": "Itsik Mantin"
                },
                "author": "Itsik Mantin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16070v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16070v2",
                "updated": "2025-04-28T06:37:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    6,
                    37,
                    4,
                    0,
                    118,
                    0
                ],
                "published": "2025-01-27T14:19:56Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    19,
                    56,
                    0,
                    27,
                    0
                ],
                "title": "Generalizing Egocentric Temporal Neighborhoods to probe for spatial\n  correlations in temporal networks and infer their topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizing Egocentric Temporal Neighborhoods to probe for spatial\n  correlations in temporal networks and infer their topology"
                },
                "summary": "Motifs are thought to be some fundamental components of social face-to-face\ninteraction temporal networks. However, the motifs previously considered are\neither limited to a handful of nodes and edges, or do not include triangles,\nwhich are thought to be of critical relevance to understand the dynamics of\nsocial systems. Thus, we introduce a new class of motifs, that include these\ntriangles, are not limited in their number of nodes or edges, and yet can be\nmined efficiently in any temporal network. Referring to these motifs as the\nedge-centered motifs, we show analytically how they subsume the Egocentric\nTemporal Neighborhoods motifs of the literature. We also confirm in empirical\ndata that the edge-centered motifs bring relevant information with respect to\nthe Egocentric motifs by using a principle of maximum entropy. Then, we show\nhow mining for the edge-centered motifs in a network can be used to probe for\nspatial correlations in the underlying dynamics that have produced that\nnetwork. We deduce an approximate formula for the distribution of the\nedge-centered motifs in empirical networks of social face-to-face interactions.\nIn the last section of this paper, we explore how the statistics of the\nedge-centered motifs can be used to infer the complete topology of the network\nthey were sampled from. This leads to the needs of mathematical development,\nthat we inaugurate here under the name of graph tiling theory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motifs are thought to be some fundamental components of social face-to-face\ninteraction temporal networks. However, the motifs previously considered are\neither limited to a handful of nodes and edges, or do not include triangles,\nwhich are thought to be of critical relevance to understand the dynamics of\nsocial systems. Thus, we introduce a new class of motifs, that include these\ntriangles, are not limited in their number of nodes or edges, and yet can be\nmined efficiently in any temporal network. Referring to these motifs as the\nedge-centered motifs, we show analytically how they subsume the Egocentric\nTemporal Neighborhoods motifs of the literature. We also confirm in empirical\ndata that the edge-centered motifs bring relevant information with respect to\nthe Egocentric motifs by using a principle of maximum entropy. Then, we show\nhow mining for the edge-centered motifs in a network can be used to probe for\nspatial correlations in the underlying dynamics that have produced that\nnetwork. We deduce an approximate formula for the distribution of the\nedge-centered motifs in empirical networks of social face-to-face interactions.\nIn the last section of this paper, we explore how the statistics of the\nedge-centered motifs can be used to infer the complete topology of the network\nthey were sampled from. This leads to the needs of mathematical development,\nthat we inaugurate here under the name of graph tiling theory."
                },
                "authors": [
                    {
                        "name": "Didier Le Bail"
                    }
                ],
                "author_detail": {
                    "name": "Didier Le Bail"
                },
                "author": "Didier Le Bail",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16070v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16070v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19516v1",
                "updated": "2025-04-28T06:26:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    6,
                    26,
                    21,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T06:26:21Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    6,
                    26,
                    21,
                    0,
                    118,
                    0
                ],
                "title": "Bullet: Boosting GPU Utilization for LLM Serving via Dynamic\n  Spatial-Temporal Orchestration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bullet: Boosting GPU Utilization for LLM Serving via Dynamic\n  Spatial-Temporal Orchestration"
                },
                "summary": "Modern LLM serving systems confront inefficient GPU utilization due to the\nfundamental mismatch between compute-intensive prefill and memory-bound decode\nphases. While current practices attempt to address this by organizing these\nphases into hybrid batches, such solutions create an inefficient tradeoff that\nsacrifices either throughput or latency, leaving substantial GPU resources\nunderutilized. We identify two key root causes: 1) the prefill phase suffers\nfrom suboptimal compute utilization due to wave quantization and attention\nbottlenecks. 2) hybrid batches disproportionately prioritize latency over\nthroughput, resulting in wasted compute and memory bandwidth. To mitigate the\nissues, we present Bullet, a novel spatial-temporal orchestration system that\neliminates these inefficiencies through precise phase coordination. Bullet\nenables concurrent execution of prefill and decode phases, while dynamically\nprovisioning GPU resources using real-time performance modeling. By integrating\nSLO-aware scheduling and adaptive resource allocation, Bullet maximizes\nutilization without compromising latency targets. Experimental evaluations on\nreal-world workloads demonstrate that Bullet delivers 1.26x average throughput\ngains (up to 1.55x) over state-of-the-arts, while consistently meeting latency\nconstraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM serving systems confront inefficient GPU utilization due to the\nfundamental mismatch between compute-intensive prefill and memory-bound decode\nphases. While current practices attempt to address this by organizing these\nphases into hybrid batches, such solutions create an inefficient tradeoff that\nsacrifices either throughput or latency, leaving substantial GPU resources\nunderutilized. We identify two key root causes: 1) the prefill phase suffers\nfrom suboptimal compute utilization due to wave quantization and attention\nbottlenecks. 2) hybrid batches disproportionately prioritize latency over\nthroughput, resulting in wasted compute and memory bandwidth. To mitigate the\nissues, we present Bullet, a novel spatial-temporal orchestration system that\neliminates these inefficiencies through precise phase coordination. Bullet\nenables concurrent execution of prefill and decode phases, while dynamically\nprovisioning GPU resources using real-time performance modeling. By integrating\nSLO-aware scheduling and adaptive resource allocation, Bullet maximizes\nutilization without compromising latency targets. Experimental evaluations on\nreal-world workloads demonstrate that Bullet delivers 1.26x average throughput\ngains (up to 1.55x) over state-of-the-arts, while consistently meeting latency\nconstraints."
                },
                "authors": [
                    {
                        "name": "Zejia Lin"
                    },
                    {
                        "name": "Hongxin Xu"
                    },
                    {
                        "name": "Guanyi Chen"
                    },
                    {
                        "name": "Xianwei Zhang"
                    },
                    {
                        "name": "Yutong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Lu"
                },
                "author": "Yutong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11975v2",
                "updated": "2025-04-28T06:19:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    6,
                    19,
                    32,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-16T11:15:26Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    11,
                    15,
                    26,
                    2,
                    106,
                    0
                ],
                "title": "SemEval-2025 Task 3: Mu-SHROOM, the Multilingual Shared Task on\n  Hallucinations and Related Observable Overgeneration Mistakes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemEval-2025 Task 3: Mu-SHROOM, the Multilingual Shared Task on\n  Hallucinations and Related Observable Overgeneration Mistakes"
                },
                "summary": "We present the Mu-SHROOM shared task which is focused on detecting\nhallucinations and other overgeneration mistakes in the output of\ninstruction-tuned large language models (LLMs). Mu-SHROOM addresses\ngeneral-purpose LLMs in 14 languages, and frames the hallucination detection\nproblem as a span-labeling task. We received 2,618 submissions from 43\nparticipating teams employing diverse methodologies. The large number of\nsubmissions underscores the interest of the community in hallucination\ndetection. We present the results of the participating systems and conduct an\nempirical analysis to identify key factors contributing to strong performance\nin this task. We also emphasize relevant current challenges, notably the\nvarying degree of hallucinations across languages and the high annotator\ndisagreement when labeling hallucination spans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the Mu-SHROOM shared task which is focused on detecting\nhallucinations and other overgeneration mistakes in the output of\ninstruction-tuned large language models (LLMs). Mu-SHROOM addresses\ngeneral-purpose LLMs in 14 languages, and frames the hallucination detection\nproblem as a span-labeling task. We received 2,618 submissions from 43\nparticipating teams employing diverse methodologies. The large number of\nsubmissions underscores the interest of the community in hallucination\ndetection. We present the results of the participating systems and conduct an\nempirical analysis to identify key factors contributing to strong performance\nin this task. We also emphasize relevant current challenges, notably the\nvarying degree of hallucinations across languages and the high annotator\ndisagreement when labeling hallucination spans."
                },
                "authors": [
                    {
                        "name": "Ral Vzquez"
                    },
                    {
                        "name": "Timothee Mickus"
                    },
                    {
                        "name": "Elaine Zosa"
                    },
                    {
                        "name": "Teemu Vahtola"
                    },
                    {
                        "name": "Jrg Tiedemann"
                    },
                    {
                        "name": "Aman Sinha"
                    },
                    {
                        "name": "Vincent Segonne"
                    },
                    {
                        "name": "Fernando Snchez-Vega"
                    },
                    {
                        "name": "Alessandro Raganato"
                    },
                    {
                        "name": "Jindich Libovick"
                    },
                    {
                        "name": "Jussi Karlgren"
                    },
                    {
                        "name": "Shaoxiong Ji"
                    },
                    {
                        "name": "Jindich Helcl"
                    },
                    {
                        "name": "Liane Guillou"
                    },
                    {
                        "name": "Ona de Gibert"
                    },
                    {
                        "name": "Jaione Bengoetxea"
                    },
                    {
                        "name": "Joseph Attieh"
                    },
                    {
                        "name": "Marianna Apidianaki"
                    }
                ],
                "author_detail": {
                    "name": "Marianna Apidianaki"
                },
                "author": "Marianna Apidianaki",
                "arxiv_comment": "Mu-SHROOM is part of SemEval-2025 (Task 3). TBP: Proceedings of the\n  19th International Workshop on Semantic Evaluation (SemEval-2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03686v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03686v2",
                "updated": "2025-04-28T06:14:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    6,
                    14,
                    26,
                    0,
                    118,
                    0
                ],
                "published": "2025-03-22T13:10:27Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    13,
                    10,
                    27,
                    5,
                    81,
                    0
                ],
                "title": "Revisiting Outage for Edge Inference Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Outage for Edge Inference Systems"
                },
                "summary": "One of the key missions of sixth-generation (6G) mobile networks is to deploy\nlarge-scale artificial intelligence (AI) models at the network edge to provide\nremote-inference services for edge devices. The resultant platform, known as\nedge inference, will support a wide range of Internet-of-Things applications,\nsuch as autonomous driving, industrial automation, and augmented reality. Given\nthe mission-critical and time-sensitive nature of these tasks, it is essential\nto design edge inference systems that are both reliable and capable of meeting\nstringent end-to-end (E2E) latency constraints. Existing studies, which\nprimarily focus on communication reliability as characterized by channel outage\nprobability, may fail to guarantee E2E performance, specifically in terms of\nE2E inference accuracy and latency. To address this limitation, we propose a\ntheoretical framework that introduces and mathematically characterizes the\ninference outage (InfOut) probability, which quantifies the likelihood that the\nE2E inference accuracy falls below a target threshold. Under an E2E latency\nconstraint, this framework establishes a fundamental tradeoff between\ncommunication overhead (i.e., uploading more sensor observations) and inference\nreliability as quantified by the InfOut probability. To find a tractable way to\noptimize this tradeoff, we derive accurate surrogate functions for InfOut\nprobability by applying a Gaussian approximation to the distribution of the\nreceived discriminant gain. Experimental results demonstrate the superiority of\nthe proposed design over conventional communication-centric approaches in terms\nof E2E inference reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the key missions of sixth-generation (6G) mobile networks is to deploy\nlarge-scale artificial intelligence (AI) models at the network edge to provide\nremote-inference services for edge devices. The resultant platform, known as\nedge inference, will support a wide range of Internet-of-Things applications,\nsuch as autonomous driving, industrial automation, and augmented reality. Given\nthe mission-critical and time-sensitive nature of these tasks, it is essential\nto design edge inference systems that are both reliable and capable of meeting\nstringent end-to-end (E2E) latency constraints. Existing studies, which\nprimarily focus on communication reliability as characterized by channel outage\nprobability, may fail to guarantee E2E performance, specifically in terms of\nE2E inference accuracy and latency. To address this limitation, we propose a\ntheoretical framework that introduces and mathematically characterizes the\ninference outage (InfOut) probability, which quantifies the likelihood that the\nE2E inference accuracy falls below a target threshold. Under an E2E latency\nconstraint, this framework establishes a fundamental tradeoff between\ncommunication overhead (i.e., uploading more sensor observations) and inference\nreliability as quantified by the InfOut probability. To find a tractable way to\noptimize this tradeoff, we derive accurate surrogate functions for InfOut\nprobability by applying a Gaussian approximation to the distribution of the\nreceived discriminant gain. Experimental results demonstrate the superiority of\nthe proposed design over conventional communication-centric approaches in terms\nof E2E inference reliability."
                },
                "authors": [
                    {
                        "name": "Zhanwei Wang"
                    },
                    {
                        "name": "Qunsong Zeng"
                    },
                    {
                        "name": "Haotian Zheng"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03686v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03686v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.20039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20039v1",
                "updated": "2025-04-28T17:59:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    59,
                    28,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T17:59:28Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    59,
                    28,
                    0,
                    118,
                    0
                ],
                "title": "AutoJudge: Judge Decoding Without Manual Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoJudge: Judge Decoding Without Manual Annotation"
                },
                "summary": "We introduce AutoJudge, a framework that accelerates large language model\n(LLM) inference with task-specific lossy speculative decoding. Instead of\nmatching the original model output distribution token-by-token, we identify\nwhich of the generated tokens affect the downstream quality of the generated\nresponse, relaxing the guarantee so that the \"unimportant\" tokens can be\ngenerated faster. Our approach relies on a semi-greedy search algorithm to test\nwhich of the mismatches between target and draft model should be corrected to\npreserve quality, and which ones may be skipped. We then train a lightweight\nclassifier based on existing LLM embeddings to predict, at inference time,\nwhich mismatching tokens can be safely accepted without compromising the final\nanswer quality. We test our approach with Llama 3.2 1B (draft) and Llama 3.1 8B\n(target) models on zero-shot GSM8K reasoning, where it achieves up to 1.5x more\naccepted tokens per verification cycle with under 1% degradation in answer\naccuracy compared to standard speculative decoding and over 2x with small loss\nin accuracy. When applied to the LiveCodeBench benchmark, our approach\nautomatically detects other, programming-specific important tokens and shows\nsimilar speedups, demonstrating its ability to generalize across tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce AutoJudge, a framework that accelerates large language model\n(LLM) inference with task-specific lossy speculative decoding. Instead of\nmatching the original model output distribution token-by-token, we identify\nwhich of the generated tokens affect the downstream quality of the generated\nresponse, relaxing the guarantee so that the \"unimportant\" tokens can be\ngenerated faster. Our approach relies on a semi-greedy search algorithm to test\nwhich of the mismatches between target and draft model should be corrected to\npreserve quality, and which ones may be skipped. We then train a lightweight\nclassifier based on existing LLM embeddings to predict, at inference time,\nwhich mismatching tokens can be safely accepted without compromising the final\nanswer quality. We test our approach with Llama 3.2 1B (draft) and Llama 3.1 8B\n(target) models on zero-shot GSM8K reasoning, where it achieves up to 1.5x more\naccepted tokens per verification cycle with under 1% degradation in answer\naccuracy compared to standard speculative decoding and over 2x with small loss\nin accuracy. When applied to the LiveCodeBench benchmark, our approach\nautomatically detects other, programming-specific important tokens and shows\nsimilar speedups, demonstrating its ability to generalize across tasks."
                },
                "authors": [
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Fedor Velikonivtsev"
                    },
                    {
                        "name": "Ruslan Svirschevski"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Max Ryabinin"
                    }
                ],
                "author_detail": {
                    "name": "Max Ryabinin"
                },
                "author": "Max Ryabinin",
                "arxiv_comment": "Preprint, Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20022v1",
                "updated": "2025-04-28T17:48:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    48,
                    13,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T17:48:13Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    48,
                    13,
                    0,
                    118,
                    0
                ],
                "title": "Better To Ask in English? Evaluating Factual Accuracy of Multilingual\n  LLMs in English and Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better To Ask in English? Evaluating Factual Accuracy of Multilingual\n  LLMs in English and Low-Resource Languages"
                },
                "summary": "Multilingual Large Language Models (LLMs) have demonstrated significant\neffectiveness across various languages, particularly in high-resource languages\nsuch as English. However, their performance in terms of factual accuracy across\nother low-resource languages, especially Indic languages, remains an area of\ninvestigation. In this study, we assess the factual accuracy of LLMs - GPT-4o,\nGemma-2-9B, Gemma-2-2B, and Llama-3.1-8B - by comparing their performance in\nEnglish and Indic languages using the IndicQuest dataset, which contains\nquestion-answer pairs in English and 19 Indic languages. By asking the same\nquestions in English and their respective Indic translations, we analyze\nwhether the models are more reliable for regional context questions in Indic\nlanguages or when operating in English. Our findings reveal that LLMs often\nperform better in English, even for questions rooted in Indic contexts.\nNotably, we observe a higher tendency for hallucination in responses generated\nin low-resource Indic languages, highlighting challenges in the multilingual\nunderstanding capabilities of current LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Large Language Models (LLMs) have demonstrated significant\neffectiveness across various languages, particularly in high-resource languages\nsuch as English. However, their performance in terms of factual accuracy across\nother low-resource languages, especially Indic languages, remains an area of\ninvestigation. In this study, we assess the factual accuracy of LLMs - GPT-4o,\nGemma-2-9B, Gemma-2-2B, and Llama-3.1-8B - by comparing their performance in\nEnglish and Indic languages using the IndicQuest dataset, which contains\nquestion-answer pairs in English and 19 Indic languages. By asking the same\nquestions in English and their respective Indic translations, we analyze\nwhether the models are more reliable for regional context questions in Indic\nlanguages or when operating in English. Our findings reveal that LLMs often\nperform better in English, even for questions rooted in Indic contexts.\nNotably, we observe a higher tendency for hallucination in responses generated\nin low-resource Indic languages, highlighting challenges in the multilingual\nunderstanding capabilities of current LLMs."
                },
                "authors": [
                    {
                        "name": "Pritika Rohera"
                    },
                    {
                        "name": "Chaitrali Ginimav"
                    },
                    {
                        "name": "Gayatri Sawant"
                    },
                    {
                        "name": "Raviraj Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Raviraj Joshi"
                },
                "author": "Raviraj Joshi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20020v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20020v1",
                "updated": "2025-04-28T17:42:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    42,
                    2,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T17:42:02Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    42,
                    2,
                    0,
                    118,
                    0
                ],
                "title": "Modular Machine Learning: An Indispensable Path towards New-Generation\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modular Machine Learning: An Indispensable Path towards New-Generation\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) have dramatically advanced machine learning\nresearch including natural language processing, computer vision, data mining,\netc., yet they still exhibit critical limitations in reasoning, factual\nconsistency, and interpretability. In this paper, we introduce a novel learning\nparadigm -- Modular Machine Learning (MML) -- as an essential approach toward\nnew-generation LLMs. MML decomposes the complex structure of LLMs into three\ninterdependent components: modular representation, modular model, and modular\nreasoning, aiming to enhance LLMs' capability of counterfactual reasoning,\nmitigating hallucinations, as well as promoting fairness, safety, and\ntransparency. Specifically, the proposed MML paradigm can: i) clarify the\ninternal working mechanism of LLMs through the disentanglement of semantic\ncomponents; ii) allow for flexible and task-adaptive model design; iii) enable\ninterpretable and logic-driven decision-making process. We present a feasible\nimplementation of MML-based LLMs via leveraging advanced techniques such as\ndisentangled representation learning, neural architecture search and\nneuro-symbolic learning. We critically identify key challenges, such as the\nintegration of continuous neural and discrete symbolic processes, joint\noptimization, and computational scalability, present promising future research\ndirections that deserve further exploration. Ultimately, the integration of the\nMML paradigm with LLMs has the potential to bridge the gap between statistical\n(deep) learning and formal (logical) reasoning, thereby paving the way for\nrobust, adaptable, and trustworthy AI systems across a wide range of real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have dramatically advanced machine learning\nresearch including natural language processing, computer vision, data mining,\netc., yet they still exhibit critical limitations in reasoning, factual\nconsistency, and interpretability. In this paper, we introduce a novel learning\nparadigm -- Modular Machine Learning (MML) -- as an essential approach toward\nnew-generation LLMs. MML decomposes the complex structure of LLMs into three\ninterdependent components: modular representation, modular model, and modular\nreasoning, aiming to enhance LLMs' capability of counterfactual reasoning,\nmitigating hallucinations, as well as promoting fairness, safety, and\ntransparency. Specifically, the proposed MML paradigm can: i) clarify the\ninternal working mechanism of LLMs through the disentanglement of semantic\ncomponents; ii) allow for flexible and task-adaptive model design; iii) enable\ninterpretable and logic-driven decision-making process. We present a feasible\nimplementation of MML-based LLMs via leveraging advanced techniques such as\ndisentangled representation learning, neural architecture search and\nneuro-symbolic learning. We critically identify key challenges, such as the\nintegration of continuous neural and discrete symbolic processes, joint\noptimization, and computational scalability, present promising future research\ndirections that deserve further exploration. Ultimately, the integration of the\nMML paradigm with LLMs has the potential to bridge the gap between statistical\n(deep) learning and formal (logical) reasoning, thereby paving the way for\nrobust, adaptable, and trustworthy AI systems across a wide range of real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Zeyang Zhang"
                    },
                    {
                        "name": "Haibo Chen"
                    },
                    {
                        "name": "Wenwu Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Wenwu Zhu"
                },
                "author": "Wenwu Zhu",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20020v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05779v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05779v3",
                "updated": "2025-04-28T17:36:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    36,
                    27,
                    0,
                    118,
                    0
                ],
                "published": "2024-10-08T08:00:12Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    8,
                    0,
                    12,
                    1,
                    282,
                    0
                ],
                "title": "LightRAG: Simple and Fast Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightRAG: Simple and Fast Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge sources, enabling more accurate and\ncontextually relevant responses tailored to user needs. However, existing RAG\nsystems have significant limitations, including reliance on flat data\nrepresentations and inadequate contextual awareness, which can lead to\nfragmented answers that fail to capture complex inter-dependencies. To address\nthese challenges, we propose LightRAG, which incorporates graph structures into\ntext indexing and retrieval processes. This innovative framework employs a\ndual-level retrieval system that enhances comprehensive information retrieval\nfrom both low-level and high-level knowledge discovery. Additionally, the\nintegration of graph structures with vector representations facilitates\nefficient retrieval of related entities and their relationships, significantly\nimproving response times while maintaining contextual relevance. This\ncapability is further enhanced by an incremental update algorithm that ensures\nthe timely integration of new data, allowing the system to remain effective and\nresponsive in rapidly changing data environments. Extensive experimental\nvalidation demonstrates considerable improvements in retrieval accuracy and\nefficiency compared to existing approaches. We have made our LightRAG\nopen-source and available at the link: https://github.com/HKUDS/LightRAG",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge sources, enabling more accurate and\ncontextually relevant responses tailored to user needs. However, existing RAG\nsystems have significant limitations, including reliance on flat data\nrepresentations and inadequate contextual awareness, which can lead to\nfragmented answers that fail to capture complex inter-dependencies. To address\nthese challenges, we propose LightRAG, which incorporates graph structures into\ntext indexing and retrieval processes. This innovative framework employs a\ndual-level retrieval system that enhances comprehensive information retrieval\nfrom both low-level and high-level knowledge discovery. Additionally, the\nintegration of graph structures with vector representations facilitates\nefficient retrieval of related entities and their relationships, significantly\nimproving response times while maintaining contextual relevance. This\ncapability is further enhanced by an incremental update algorithm that ensures\nthe timely integration of new data, allowing the system to remain effective and\nresponsive in rapidly changing data environments. Extensive experimental\nvalidation demonstrates considerable improvements in retrieval accuracy and\nefficiency compared to existing approaches. We have made our LightRAG\nopen-source and available at the link: https://github.com/HKUDS/LightRAG"
                },
                "authors": [
                    {
                        "name": "Zirui Guo"
                    },
                    {
                        "name": "Lianghao Xia"
                    },
                    {
                        "name": "Yanhua Yu"
                    },
                    {
                        "name": "Tu Ao"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05779v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05779v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20016v1",
                "updated": "2025-04-28T17:35:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    35,
                    46,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T17:35:46Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    35,
                    46,
                    0,
                    118,
                    0
                ],
                "title": "Applying LLM-Powered Virtual Humans to Child Interviews in\n  Child-Centered Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applying LLM-Powered Virtual Humans to Child Interviews in\n  Child-Centered Design"
                },
                "summary": "In child-centered design, directly engaging children is crucial for deeply\nunderstanding their experiences. However, current research often prioritizes\nadult perspectives, as interviewing children involves unique challenges such as\nenvironmental sensitivities and the need for trust-building. AI-powered virtual\nhumans (VHs) offer a promising approach to facilitate engaging and multimodal\ninteractions with children. This study establishes key design guidelines for\nLLM-powered virtual humans tailored to child interviews, standardizing\nmultimodal elements including color schemes, voice characteristics, facial\nfeatures, expressions, head movements, and gestures. Using ChatGPT-based prompt\nengineering, we developed three distinct Human-AI workflows (LLM-Auto,\nLLM-Interview, and LLM-Analyze) and conducted a user study involving 15\nchildren aged 6 to 12. The results indicated that the LLM-Analyze workflow\noutperformed the others by eliciting longer responses, achieving higher user\nexperience ratings, and promoting more effective child engagement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In child-centered design, directly engaging children is crucial for deeply\nunderstanding their experiences. However, current research often prioritizes\nadult perspectives, as interviewing children involves unique challenges such as\nenvironmental sensitivities and the need for trust-building. AI-powered virtual\nhumans (VHs) offer a promising approach to facilitate engaging and multimodal\ninteractions with children. This study establishes key design guidelines for\nLLM-powered virtual humans tailored to child interviews, standardizing\nmultimodal elements including color schemes, voice characteristics, facial\nfeatures, expressions, head movements, and gestures. Using ChatGPT-based prompt\nengineering, we developed three distinct Human-AI workflows (LLM-Auto,\nLLM-Interview, and LLM-Analyze) and conducted a user study involving 15\nchildren aged 6 to 12. The results indicated that the LLM-Analyze workflow\noutperformed the others by eliciting longer responses, achieving higher user\nexperience ratings, and promoting more effective child engagement."
                },
                "authors": [
                    {
                        "name": "Linshi Li"
                    },
                    {
                        "name": "Hanlin Cai"
                    }
                ],
                "author_detail": {
                    "name": "Hanlin Cai"
                },
                "author": "Hanlin Cai",
                "arxiv_doi": "10.1145/3713043.3731551",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713043.3731551",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.20016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper has been accepted as a Work-in-Progress (WiP) paper in the\n  24th annual ACM Interaction Design and Children (IDC) Conference",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20013v1",
                "updated": "2025-04-28T17:32:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    32,
                    38,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T17:32:38Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    32,
                    38,
                    0,
                    118,
                    0
                ],
                "title": "LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case\n  Study on Neural News Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case\n  Study on Neural News Recommendation"
                },
                "summary": "Online fake news moderation now faces a new challenge brought by the\nmalicious use of large language models (LLMs) in fake news production. Though\nexisting works have shown LLM-generated fake news is hard to detect from an\nindividual aspect, it remains underexplored how its large-scale release will\nimpact the news ecosystem. In this study, we develop a simulation pipeline and\na dataset with ~56k generated news of diverse types to investigate the effects\nof LLM-generated fake news within neural news recommendation systems. Our\nfindings expose a truth decay phenomenon, where real news is gradually losing\nits advantageous position in news ranking against fake news as LLM-generated\nnews is involved in news recommendation. We further provide an explanation\nabout why truth decay occurs from a familiarity perspective and show the\npositive correlation between perplexity and news ranking. Finally, we discuss\nthe threats of LLM-generated fake news and provide possible countermeasures. We\nurge stakeholders to address this emerging challenge to preserve the integrity\nof news ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online fake news moderation now faces a new challenge brought by the\nmalicious use of large language models (LLMs) in fake news production. Though\nexisting works have shown LLM-generated fake news is hard to detect from an\nindividual aspect, it remains underexplored how its large-scale release will\nimpact the news ecosystem. In this study, we develop a simulation pipeline and\na dataset with ~56k generated news of diverse types to investigate the effects\nof LLM-generated fake news within neural news recommendation systems. Our\nfindings expose a truth decay phenomenon, where real news is gradually losing\nits advantageous position in news ranking against fake news as LLM-generated\nnews is involved in news recommendation. We further provide an explanation\nabout why truth decay occurs from a familiarity perspective and show the\npositive correlation between perplexity and news ranking. Finally, we discuss\nthe threats of LLM-generated fake news and provide possible countermeasures. We\nurge stakeholders to address this emerging challenge to preserve the integrity\nof news ecosystems."
                },
                "authors": [
                    {
                        "name": "Beizhe Hu"
                    },
                    {
                        "name": "Qiang Sheng"
                    },
                    {
                        "name": "Juan Cao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Danding Wang"
                    }
                ],
                "author_detail": {
                    "name": "Danding Wang"
                },
                "author": "Danding Wang",
                "arxiv_doi": "10.1145/3726302.3730027",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730027",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.20013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACM SIGIR 2025 Full Paper",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20010v1",
                "updated": "2025-04-28T17:29:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    29,
                    51,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T17:29:51Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    29,
                    51,
                    0,
                    118,
                    0
                ],
                "title": "Towards Automated Scoping of AI for Social Good Projects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Automated Scoping of AI for Social Good Projects"
                },
                "summary": "Artificial Intelligence for Social Good (AI4SG) is an emerging effort that\naims to address complex societal challenges with the powerful capabilities of\nAI systems. These challenges range from local issues with transit networks to\nglobal wildlife preservation. However, regardless of scale, a critical\nbottleneck for many AI4SG initiatives is the laborious process of problem\nscoping -- a complex and resource-intensive task -- due to a scarcity of\nprofessionals with both technical and domain expertise. Given the remarkable\napplications of large language models (LLM), we propose a Problem Scoping Agent\n(PSA) that uses an LLM to generate comprehensive project proposals grounded in\nscientific literature and real-world knowledge. We demonstrate that our PSA\nframework generates proposals comparable to those written by experts through a\nblind review and AI evaluations. Finally, we document the challenges of\nreal-world problem scoping and note several areas for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence for Social Good (AI4SG) is an emerging effort that\naims to address complex societal challenges with the powerful capabilities of\nAI systems. These challenges range from local issues with transit networks to\nglobal wildlife preservation. However, regardless of scale, a critical\nbottleneck for many AI4SG initiatives is the laborious process of problem\nscoping -- a complex and resource-intensive task -- due to a scarcity of\nprofessionals with both technical and domain expertise. Given the remarkable\napplications of large language models (LLM), we propose a Problem Scoping Agent\n(PSA) that uses an LLM to generate comprehensive project proposals grounded in\nscientific literature and real-world knowledge. We demonstrate that our PSA\nframework generates proposals comparable to those written by experts through a\nblind review and AI evaluations. Finally, we document the challenges of\nreal-world problem scoping and note several areas for future work."
                },
                "authors": [
                    {
                        "name": "Jacob Emmerson"
                    },
                    {
                        "name": "Rayid Ghani"
                    },
                    {
                        "name": "Zheyuan Ryan Shi"
                    }
                ],
                "author_detail": {
                    "name": "Zheyuan Ryan Shi"
                },
                "author": "Zheyuan Ryan Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20006v1",
                "updated": "2025-04-28T17:24:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    24,
                    36,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T17:24:36Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    24,
                    36,
                    0,
                    118,
                    0
                ],
                "title": "Chatbot Arena Meets Nuggets: Towards Explanations and Diagnostics in the\n  Evaluation of LLM Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chatbot Arena Meets Nuggets: Towards Explanations and Diagnostics in the\n  Evaluation of LLM Responses"
                },
                "summary": "Battles, or side-by-side comparisons in so called arenas that elicit human\npreferences, have emerged as a popular approach to assessing the output quality\nof LLMs. Recently, this idea has been extended to retrieval-augmented\ngeneration (RAG) systems. While undoubtedly representing an advance in\nevaluation, battles have at least two drawbacks, particularly in the context of\ncomplex information-seeking queries: they are neither explanatory nor\ndiagnostic. Recently, the nugget evaluation methodology has emerged as a\npromising approach to evaluate the quality of RAG answers. Nuggets decompose\nlong-form LLM-generated answers into atomic facts, highlighting important\npieces of information necessary in a \"good\" response. In this work, we apply\nour AutoNuggetizer framework to analyze data from roughly 7K Search Arena\nbattles provided by LMArena in a fully automatic manner. Our results show a\nsignificant correlation between nugget scores and human preferences, showcasing\npromise in our approach to explainable and diagnostic system evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Battles, or side-by-side comparisons in so called arenas that elicit human\npreferences, have emerged as a popular approach to assessing the output quality\nof LLMs. Recently, this idea has been extended to retrieval-augmented\ngeneration (RAG) systems. While undoubtedly representing an advance in\nevaluation, battles have at least two drawbacks, particularly in the context of\ncomplex information-seeking queries: they are neither explanatory nor\ndiagnostic. Recently, the nugget evaluation methodology has emerged as a\npromising approach to evaluate the quality of RAG answers. Nuggets decompose\nlong-form LLM-generated answers into atomic facts, highlighting important\npieces of information necessary in a \"good\" response. In this work, we apply\nour AutoNuggetizer framework to analyze data from roughly 7K Search Arena\nbattles provided by LMArena in a fully automatic manner. Our results show a\nsignificant correlation between nugget scores and human preferences, showcasing\npromise in our approach to explainable and diagnostic system evaluations."
                },
                "authors": [
                    {
                        "name": "Sahel Sharifymoghaddam"
                    },
                    {
                        "name": "Shivani Upadhyay"
                    },
                    {
                        "name": "Nandan Thakur"
                    },
                    {
                        "name": "Ronak Pradeep"
                    },
                    {
                        "name": "Jimmy Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Lin"
                },
                "author": "Jimmy Lin",
                "arxiv_comment": "10 pages, 8 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02239v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02239v2",
                "updated": "2025-04-28T17:19:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    19,
                    27,
                    0,
                    118,
                    0
                ],
                "published": "2024-08-05T05:15:17Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    5,
                    15,
                    17,
                    0,
                    218,
                    0
                ],
                "title": "Pula: Training Large Language Models for Setswana",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pula: Training Large Language Models for Setswana"
                },
                "summary": "In this work we present Pula, a suite of bilingual language models proficient\nin both Setswana and English. Leveraging recent advancements in data\navailability and efficient fine-tuning, Pula 8B and Pula 14B outperform GPT-4o\nand Gemini 1.5 Pro on English-Setswana translation tasks and achieve\nstate-of-the-art performance on Setswana reasoning tasks for their size. We\nrelease the weights for Pula 1B, 3B, 8B, and 14B as well as training logs and\ntraining and evaluation code. Alongside Pula, we release the largest-ever\nSetswana text corpus, Marothodi, and the first comprehensive Setswana\ninstruction-tuning dataset, Medupi, consisting of reformatted datasets,\ntranslated corpora, and synthetic LLM-generated text. To accompany this data,\nwe release the code used for dataset construction, formatting, filtering, and\nscraping. Last, we release two Setswana LLM-translated benchmarks, MMLU-tsn and\nGSM8K-tsn, to measure Setswana knowledge and reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we present Pula, a suite of bilingual language models proficient\nin both Setswana and English. Leveraging recent advancements in data\navailability and efficient fine-tuning, Pula 8B and Pula 14B outperform GPT-4o\nand Gemini 1.5 Pro on English-Setswana translation tasks and achieve\nstate-of-the-art performance on Setswana reasoning tasks for their size. We\nrelease the weights for Pula 1B, 3B, 8B, and 14B as well as training logs and\ntraining and evaluation code. Alongside Pula, we release the largest-ever\nSetswana text corpus, Marothodi, and the first comprehensive Setswana\ninstruction-tuning dataset, Medupi, consisting of reformatted datasets,\ntranslated corpora, and synthetic LLM-generated text. To accompany this data,\nwe release the code used for dataset construction, formatting, filtering, and\nscraping. Last, we release two Setswana LLM-translated benchmarks, MMLU-tsn and\nGSM8K-tsn, to measure Setswana knowledge and reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Nathan Brown"
                    },
                    {
                        "name": "Vukosi Marivate"
                    }
                ],
                "author_detail": {
                    "name": "Vukosi Marivate"
                },
                "author": "Vukosi Marivate",
                "arxiv_comment": "NAACL 2025. 10 pages, 5 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02239v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02239v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20000v1",
                "updated": "2025-04-28T17:19:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    19,
                    25,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T17:19:25Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    19,
                    25,
                    0,
                    118,
                    0
                ],
                "title": "Knowledge Distillation of Domain-adapted LLMs for Question-Answering in\n  Telecom",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Distillation of Domain-adapted LLMs for Question-Answering in\n  Telecom"
                },
                "summary": "Knowledge Distillation (KD) is one of the approaches to reduce the size of\nLarge Language Models (LLMs). A LLM with smaller number of model parameters\n(student) is trained to mimic the performance of a LLM of a larger size\n(teacher model) on a specific task. For domain-specific tasks, it is not clear\nif teacher or student model, or both, must be considered for domain adaptation.\nIn this work, we study this problem from perspective of telecom domain\nQuestion-Answering (QA) task. We systematically experiment with Supervised\nFine-tuning (SFT) of teacher only, SFT of student only and SFT of both prior to\nKD. We design experiments to study the impact of vocabulary (same and\ndifferent) and KD algorithms (vanilla KD and Dual Space KD, DSKD) on the\ndistilled model. Multi-faceted evaluation of the distillation using 14\ndifferent metrics (N-gram, embedding and LLM-based metrics) is considered.\nExperimental results show that SFT of teacher improves performance of distilled\nmodel when both models have same vocabulary, irrespective of algorithm and\nmetrics. Overall, SFT of both teacher and student results in better performance\nacross all metrics, although the statistical significance of the same depends\non the vocabulary of the teacher models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Distillation (KD) is one of the approaches to reduce the size of\nLarge Language Models (LLMs). A LLM with smaller number of model parameters\n(student) is trained to mimic the performance of a LLM of a larger size\n(teacher model) on a specific task. For domain-specific tasks, it is not clear\nif teacher or student model, or both, must be considered for domain adaptation.\nIn this work, we study this problem from perspective of telecom domain\nQuestion-Answering (QA) task. We systematically experiment with Supervised\nFine-tuning (SFT) of teacher only, SFT of student only and SFT of both prior to\nKD. We design experiments to study the impact of vocabulary (same and\ndifferent) and KD algorithms (vanilla KD and Dual Space KD, DSKD) on the\ndistilled model. Multi-faceted evaluation of the distillation using 14\ndifferent metrics (N-gram, embedding and LLM-based metrics) is considered.\nExperimental results show that SFT of teacher improves performance of distilled\nmodel when both models have same vocabulary, irrespective of algorithm and\nmetrics. Overall, SFT of both teacher and student results in better performance\nacross all metrics, although the statistical significance of the same depends\non the vocabulary of the teacher models."
                },
                "authors": [
                    {
                        "name": "Rishika Sen"
                    },
                    {
                        "name": "Sujoy Roychowdhury"
                    },
                    {
                        "name": "Sumit Soman"
                    },
                    {
                        "name": "H. G. Ranjani"
                    },
                    {
                        "name": "Srikhetra Mohanty"
                    }
                ],
                "author_detail": {
                    "name": "Srikhetra Mohanty"
                },
                "author": "Srikhetra Mohanty",
                "arxiv_comment": "10 pages, 4 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19982v1",
                "updated": "2025-04-28T16:57:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    57,
                    17,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T16:57:17Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    57,
                    17,
                    0,
                    118,
                    0
                ],
                "title": "TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining\n  Turn-Level Precision with Dialogue-Level Comparisons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining\n  Turn-Level Precision with Dialogue-Level Comparisons"
                },
                "summary": "Task-oriented dialogue (TOD) systems are experiencing a revolution driven by\nLarge Language Models (LLMs), yet the evaluation methodologies for these\nsystems remain insufficient for their growing sophistication. While traditional\nautomatic metrics effectively assessed earlier modular systems, they focus\nsolely on the dialogue level and cannot detect critical intermediate errors\nthat can arise during user-agent interactions. In this paper, we introduce\nTD-EVAL (Turn and Dialogue-level Evaluation), a two-step evaluation framework\nthat unifies fine-grained turn-level analysis with holistic dialogue-level\ncomparisons. At turn level, we evaluate each response along three TOD-specific\ndimensions: conversation cohesion, backend knowledge consistency, and policy\ncompliance. Meanwhile, we design TOD Agent Arena that uses pairwise comparisons\nto provide a measure of dialogue-level quality. Through experiments on MultiWOZ\n2.4 and {\\tau}-Bench, we demonstrate that TD-EVAL effectively identifies the\nconversational errors that conventional metrics miss. Furthermore, TD-EVAL\nexhibits better alignment with human judgments than traditional and LLM-based\nmetrics. These findings demonstrate that TD-EVAL introduces a new paradigm for\nTOD system evaluation, efficiently assessing both turn and system levels with a\nplug-and-play framework for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-oriented dialogue (TOD) systems are experiencing a revolution driven by\nLarge Language Models (LLMs), yet the evaluation methodologies for these\nsystems remain insufficient for their growing sophistication. While traditional\nautomatic metrics effectively assessed earlier modular systems, they focus\nsolely on the dialogue level and cannot detect critical intermediate errors\nthat can arise during user-agent interactions. In this paper, we introduce\nTD-EVAL (Turn and Dialogue-level Evaluation), a two-step evaluation framework\nthat unifies fine-grained turn-level analysis with holistic dialogue-level\ncomparisons. At turn level, we evaluate each response along three TOD-specific\ndimensions: conversation cohesion, backend knowledge consistency, and policy\ncompliance. Meanwhile, we design TOD Agent Arena that uses pairwise comparisons\nto provide a measure of dialogue-level quality. Through experiments on MultiWOZ\n2.4 and {\\tau}-Bench, we demonstrate that TD-EVAL effectively identifies the\nconversational errors that conventional metrics miss. Furthermore, TD-EVAL\nexhibits better alignment with human judgments than traditional and LLM-based\nmetrics. These findings demonstrate that TD-EVAL introduces a new paradigm for\nTOD system evaluation, efficiently assessing both turn and system levels with a\nplug-and-play framework for future research."
                },
                "authors": [
                    {
                        "name": "Emre Can Acikgoz"
                    },
                    {
                        "name": "Carl Guo"
                    },
                    {
                        "name": "Suvodip Dey"
                    },
                    {
                        "name": "Akul Datta"
                    },
                    {
                        "name": "Takyoung Kim"
                    },
                    {
                        "name": "Gokhan Tur"
                    },
                    {
                        "name": "Dilek Hakkani-Tr"
                    }
                ],
                "author_detail": {
                    "name": "Dilek Hakkani-Tr"
                },
                "author": "Dilek Hakkani-Tr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19981v1",
                "updated": "2025-04-28T16:56:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    56,
                    41,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T16:56:41Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    56,
                    41,
                    0,
                    118,
                    0
                ],
                "title": "Accurate and Diverse LLM Mathematical Reasoning via Automated PRM-Guided\n  GFlowNets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and Diverse LLM Mathematical Reasoning via Automated PRM-Guided\n  GFlowNets"
                },
                "summary": "Achieving both accuracy and diverse reasoning remains challenging for Large\nLanguage Models (LLMs) in complex domains like mathematics. A key bottleneck is\nevaluating intermediate reasoning steps to guide generation without costly\nhuman annotations. To address this, we first introduce a novel Process Reward\nModel (PRM) trained automatically using Monte Carlo Tree Search coupled with a\nsimilarity-based data augmentation technique, effectively capturing step-level\nreasoning quality. Leveraging this PRM, we then adapt Generative Flow Networks\n(GFlowNets) to operate at the reasoning step level. Unlike traditional\nreinforcement learning focused on maximizing a single reward, GFlowNets\nnaturally sample diverse, high-quality solutions proportional to their rewards,\nas measured by our PRM. Empirical evaluation shows strong improvements in both\naccuracy and solution diversity on challenging mathematical benchmarks (e.g.,\n+2.59% absolute accuracy on MATH Level 5 for Llama3.2-3B), with effective\ngeneralization to unseen datasets (+9.4% absolute on SAT MATH). Our work\ndemonstrates the potential of PRM-guided, step-level GFlowNets for developing\nmore robust and versatile mathematical reasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving both accuracy and diverse reasoning remains challenging for Large\nLanguage Models (LLMs) in complex domains like mathematics. A key bottleneck is\nevaluating intermediate reasoning steps to guide generation without costly\nhuman annotations. To address this, we first introduce a novel Process Reward\nModel (PRM) trained automatically using Monte Carlo Tree Search coupled with a\nsimilarity-based data augmentation technique, effectively capturing step-level\nreasoning quality. Leveraging this PRM, we then adapt Generative Flow Networks\n(GFlowNets) to operate at the reasoning step level. Unlike traditional\nreinforcement learning focused on maximizing a single reward, GFlowNets\nnaturally sample diverse, high-quality solutions proportional to their rewards,\nas measured by our PRM. Empirical evaluation shows strong improvements in both\naccuracy and solution diversity on challenging mathematical benchmarks (e.g.,\n+2.59% absolute accuracy on MATH Level 5 for Llama3.2-3B), with effective\ngeneralization to unseen datasets (+9.4% absolute on SAT MATH). Our work\ndemonstrates the potential of PRM-guided, step-level GFlowNets for developing\nmore robust and versatile mathematical reasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Adam Younsi"
                    },
                    {
                        "name": "Abdalgader Abubaker"
                    },
                    {
                        "name": "Mohamed El Amine Seddik"
                    },
                    {
                        "name": "Hakim Hacid"
                    },
                    {
                        "name": "Salem Lahlou"
                    }
                ],
                "author_detail": {
                    "name": "Salem Lahlou"
                },
                "author": "Salem Lahlou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19959v2",
                "updated": "2025-04-29T02:05:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    2,
                    5,
                    45,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-28T16:33:08Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    33,
                    8,
                    0,
                    118,
                    0
                ],
                "title": "From Concept to Practice: an Automated LLM-aided UVM Machine for RTL\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Concept to Practice: an Automated LLM-aided UVM Machine for RTL\n  Verification"
                },
                "summary": "Verification presents a major bottleneck in Integrated Circuit (IC)\ndevelopment, consuming nearly 70% of the total development effort. While the\nUniversal Verification Methodology (UVM) is widely used in industry to improve\nverification efficiency through structured and reusable testbenches,\nconstructing these testbenches and generating sufficient stimuli remain\nchallenging. These challenges arise from the considerable manual coding effort\nrequired, repetitive manual execution of multiple EDA tools, and the need for\nin-depth domain expertise to navigate complex designs.Here, we present UVM^2,\nan automated verification framework that leverages Large Language Models (LLMs)\nto generate UVM testbenches and iteratively refine them using coverage\nfeedback, significantly reducing manual effort while maintaining rigorous\nverification standards.To evaluate UVM^2, we introduce a benchmark suite\ncomprising Register Transfer Level (RTL) designs of up to 1.6K lines of\ncode.The results show that UVM^2 reduces testbench setup time by up to UVM^2\ncompared to experienced engineers, and achieve average code and function\ncoverage of 87.44% and 89.58%, outperforming state-of-the-art solutions by\n20.96% and 23.51%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verification presents a major bottleneck in Integrated Circuit (IC)\ndevelopment, consuming nearly 70% of the total development effort. While the\nUniversal Verification Methodology (UVM) is widely used in industry to improve\nverification efficiency through structured and reusable testbenches,\nconstructing these testbenches and generating sufficient stimuli remain\nchallenging. These challenges arise from the considerable manual coding effort\nrequired, repetitive manual execution of multiple EDA tools, and the need for\nin-depth domain expertise to navigate complex designs.Here, we present UVM^2,\nan automated verification framework that leverages Large Language Models (LLMs)\nto generate UVM testbenches and iteratively refine them using coverage\nfeedback, significantly reducing manual effort while maintaining rigorous\nverification standards.To evaluate UVM^2, we introduce a benchmark suite\ncomprising Register Transfer Level (RTL) designs of up to 1.6K lines of\ncode.The results show that UVM^2 reduces testbench setup time by up to UVM^2\ncompared to experienced engineers, and achieve average code and function\ncoverage of 87.44% and 89.58%, outperforming state-of-the-art solutions by\n20.96% and 23.51%, respectively."
                },
                "authors": [
                    {
                        "name": "Junhao Ye"
                    },
                    {
                        "name": "Yuchen Hu"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Dingrong Pan"
                    },
                    {
                        "name": "Qichun Chen"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Xinwei Fang"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Nan Guan"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19956v1",
                "updated": "2025-04-28T16:29:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    29,
                    24,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T16:29:24Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    29,
                    24,
                    0,
                    118,
                    0
                ],
                "title": "Securing Agentic AI: A Comprehensive Threat Model and Mitigation\n  Framework for Generative AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Securing Agentic AI: A Comprehensive Threat Model and Mitigation\n  Framework for Generative AI Agents"
                },
                "summary": "As generative AI (GenAI) agents become more common in enterprise settings,\nthey introduce security challenges that differ significantly from those posed\nby traditional systems. These agents are not just LLMs; they reason, remember,\nand act, often with minimal human oversight. This paper introduces a\ncomprehensive threat model tailored specifically for GenAI agents, focusing on\nhow their autonomy, persistent memory access, complex reasoning, and tool\nintegration create novel risks. This research work identifies 9 primary threats\nand organizes them across five key domains: cognitive architecture\nvulnerabilities, temporal persistence threats, operational execution\nvulnerabilities, trust boundary violations, and governance circumvention. These\nthreats are not just theoretical they bring practical challenges such as\ndelayed exploitability, cross-system propagation, cross system lateral\nmovement, and subtle goal misalignments that are hard to detect with existing\nframeworks and standard approaches. To help address this, the research work\npresent two complementary frameworks: ATFAA - Advanced Threat Framework for\nAutonomous AI Agents, which organizes agent-specific risks, and SHIELD, a\nframework proposing practical mitigation strategies designed to reduce\nenterprise exposure. While this work builds on existing work in LLM and AI\nsecurity, the focus is squarely on what makes agents different and why those\ndifferences matter. Ultimately, this research argues that GenAI agents require\na new lens for security. If we fail to adapt our threat models and defenses to\naccount for their unique architecture and behavior, we risk turning a powerful\nnew tool into a serious enterprise liability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As generative AI (GenAI) agents become more common in enterprise settings,\nthey introduce security challenges that differ significantly from those posed\nby traditional systems. These agents are not just LLMs; they reason, remember,\nand act, often with minimal human oversight. This paper introduces a\ncomprehensive threat model tailored specifically for GenAI agents, focusing on\nhow their autonomy, persistent memory access, complex reasoning, and tool\nintegration create novel risks. This research work identifies 9 primary threats\nand organizes them across five key domains: cognitive architecture\nvulnerabilities, temporal persistence threats, operational execution\nvulnerabilities, trust boundary violations, and governance circumvention. These\nthreats are not just theoretical they bring practical challenges such as\ndelayed exploitability, cross-system propagation, cross system lateral\nmovement, and subtle goal misalignments that are hard to detect with existing\nframeworks and standard approaches. To help address this, the research work\npresent two complementary frameworks: ATFAA - Advanced Threat Framework for\nAutonomous AI Agents, which organizes agent-specific risks, and SHIELD, a\nframework proposing practical mitigation strategies designed to reduce\nenterprise exposure. While this work builds on existing work in LLM and AI\nsecurity, the focus is squarely on what makes agents different and why those\ndifferences matter. Ultimately, this research argues that GenAI agents require\na new lens for security. If we fail to adapt our threat models and defenses to\naccount for their unique architecture and behavior, we risk turning a powerful\nnew tool into a serious enterprise liability."
                },
                "authors": [
                    {
                        "name": "Vineeth Sai Narajala"
                    },
                    {
                        "name": "Om Narayan"
                    }
                ],
                "author_detail": {
                    "name": "Om Narayan"
                },
                "author": "Om Narayan",
                "arxiv_comment": "12 pages, 2 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19949v1",
                "updated": "2025-04-28T16:21:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    21,
                    20,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T16:21:20Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    21,
                    20,
                    0,
                    118,
                    0
                ],
                "title": "Capturing Aerodynamic Characteristics of ATTAS Aircraft with Evolving\n  Intelligent System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capturing Aerodynamic Characteristics of ATTAS Aircraft with Evolving\n  Intelligent System"
                },
                "summary": "Accurate modeling of aerodynamic coefficients is crucial for understanding\nand optimizing the performance of modern aircraft systems. This paper presents\nthe novel deployment of an Evolving Type-2 Quantum Fuzzy Neural Network\n(eT2QFNN) for modeling the aerodynamic coefficients of the ATTAS aircraft to\nexpress the aerodynamic characteristics. eT2QFNN can represent the nonlinear\naircraft model by creating multiple linear submodels with its rule-based\nstructure through an incremental learning strategy rather than a traditional\nbatch learning approach. Moreover, it enhances robustness to uncertainties and\ndata noise through its quantum membership functions, as well as its automatic\nrule-learning and parameter-tuning capabilities. During the estimation of the\naerodynamic coefficients via the flight data of the ATTAS, two different\nstudies are conducted in the training phase: one with a large amount of data\nand the other with a limited amount of data. The results show that the modeling\nperformance of the eT2QFNN is superior in comparison to baseline counterparts.\nFurthermore, eT2QFNN estimated the aerodynamic model with fewer rules compared\nto Type-1 fuzzy counterparts. In addition, by applying the Delta method to the\nproposed approach, the stability and control derivatives of the aircraft are\nanalyzed. The results prove the superiority of the proposed eT2QFNN in\nrepresenting aerodynamic coefficients.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate modeling of aerodynamic coefficients is crucial for understanding\nand optimizing the performance of modern aircraft systems. This paper presents\nthe novel deployment of an Evolving Type-2 Quantum Fuzzy Neural Network\n(eT2QFNN) for modeling the aerodynamic coefficients of the ATTAS aircraft to\nexpress the aerodynamic characteristics. eT2QFNN can represent the nonlinear\naircraft model by creating multiple linear submodels with its rule-based\nstructure through an incremental learning strategy rather than a traditional\nbatch learning approach. Moreover, it enhances robustness to uncertainties and\ndata noise through its quantum membership functions, as well as its automatic\nrule-learning and parameter-tuning capabilities. During the estimation of the\naerodynamic coefficients via the flight data of the ATTAS, two different\nstudies are conducted in the training phase: one with a large amount of data\nand the other with a limited amount of data. The results show that the modeling\nperformance of the eT2QFNN is superior in comparison to baseline counterparts.\nFurthermore, eT2QFNN estimated the aerodynamic model with fewer rules compared\nto Type-1 fuzzy counterparts. In addition, by applying the Delta method to the\nproposed approach, the stability and control derivatives of the aircraft are\nanalyzed. The results prove the superiority of the proposed eT2QFNN in\nrepresenting aerodynamic coefficients."
                },
                "authors": [
                    {
                        "name": "Aydoan Soylu"
                    },
                    {
                        "name": "Tufan Kumbasar"
                    }
                ],
                "author_detail": {
                    "name": "Tufan Kumbasar"
                },
                "author": "Tufan Kumbasar",
                "arxiv_comment": "in International Congress on Human-Computer Interaction, Optimization\n  and Robotic Applications, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08775v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08775v3",
                "updated": "2025-04-28T16:07:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    7,
                    5,
                    0,
                    118,
                    0
                ],
                "published": "2024-09-13T12:34:14Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    12,
                    34,
                    14,
                    4,
                    257,
                    0
                ],
                "title": "What Should We Engineer in Prompts? Training Humans in\n  Requirement-Driven LLM Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Should We Engineer in Prompts? Training Humans in\n  Requirement-Driven LLM Use"
                },
                "summary": "Prompting LLMs for complex tasks (e.g., building a trip advisor chatbot)\nneeds humans to clearly articulate customized requirements (e.g., \"start the\nresponse with a tl;dr\"). However, existing prompt engineering instructions\noften lack focused training on requirement articulation and instead tend to\nemphasize increasingly automatable strategies (e.g., tricks like adding\nrole-plays and \"think step-by-step\"). To address the gap, we introduce\nRequirement-Oriented Prompt Engineering (ROPE), a paradigm that focuses human\nattention on generating clear, complete requirements during prompting. We\nimplement ROPE through an assessment and training suite that provides\ndeliberate practice with LLM-generated feedback. In a randomized controlled\nexperiment with 30 novices, ROPE significantly outperforms conventional prompt\nengineering training (20% vs. 1% gains), a gap that automatic prompt\noptimization cannot close. Furthermore, we demonstrate a direct correlation\nbetween the quality of input requirements and LLM outputs. Our work paves the\nway to empower more end-users to build complex LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting LLMs for complex tasks (e.g., building a trip advisor chatbot)\nneeds humans to clearly articulate customized requirements (e.g., \"start the\nresponse with a tl;dr\"). However, existing prompt engineering instructions\noften lack focused training on requirement articulation and instead tend to\nemphasize increasingly automatable strategies (e.g., tricks like adding\nrole-plays and \"think step-by-step\"). To address the gap, we introduce\nRequirement-Oriented Prompt Engineering (ROPE), a paradigm that focuses human\nattention on generating clear, complete requirements during prompting. We\nimplement ROPE through an assessment and training suite that provides\ndeliberate practice with LLM-generated feedback. In a randomized controlled\nexperiment with 30 novices, ROPE significantly outperforms conventional prompt\nengineering training (20% vs. 1% gains), a gap that automatic prompt\noptimization cannot close. Furthermore, we demonstrate a direct correlation\nbetween the quality of input requirements and LLM outputs. Our work paves the\nway to empower more end-users to build complex LLM applications."
                },
                "authors": [
                    {
                        "name": "Qianou Ma"
                    },
                    {
                        "name": "Weirui Peng"
                    },
                    {
                        "name": "Chenyang Yang"
                    },
                    {
                        "name": "Hua Shen"
                    },
                    {
                        "name": "Kenneth Koedinger"
                    },
                    {
                        "name": "Tongshuang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Tongshuang Wu"
                },
                "author": "Tongshuang Wu",
                "arxiv_doi": "10.1145/3731756",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731756",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.08775v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08775v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages; TOCHI 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17965v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17965v2",
                "updated": "2025-04-28T15:54:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    54,
                    16,
                    0,
                    118,
                    0
                ],
                "published": "2024-12-23T20:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    20,
                    28,
                    22,
                    0,
                    358,
                    0
                ],
                "title": "LMV-RPA: Large Model Voting-based Robotic Process Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMV-RPA: Large Model Voting-based Robotic Process Automation"
                },
                "summary": "Automating high-volume unstructured data processing is essential for\noperational efficiency. Optical Character Recognition (OCR) is critical but\noften struggles with accuracy and efficiency in complex layouts and ambiguous\ntext. These challenges are especially pronounced in large-scale tasks requiring\nboth speed and precision. This paper introduces LMV-RPA, a Large Model\nVoting-based Robotic Process Automation system to enhance OCR workflows.\nLMV-RPA integrates outputs from OCR engines such as Paddle OCR, Tesseract OCR,\nEasy OCR, and DocTR with Large Language Models (LLMs) like LLaMA 3 and\nGemini-1.5-pro. Using a majority voting mechanism, it processes OCR outputs\ninto structured JSON formats, improving accuracy, particularly in complex\nlayouts. The multi-phase pipeline processes text extracted by OCR engines\nthrough LLMs, combining results to ensure the most accurate outputs. LMV-RPA\nachieves 99 percent accuracy in OCR tasks, surpassing baseline models with 94\npercent, while reducing processing time by 80 percent. Benchmark evaluations\nconfirm its scalability and demonstrate that LMV-RPA offers a faster, more\nreliable, and efficient solution for automating large-scale document processing\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating high-volume unstructured data processing is essential for\noperational efficiency. Optical Character Recognition (OCR) is critical but\noften struggles with accuracy and efficiency in complex layouts and ambiguous\ntext. These challenges are especially pronounced in large-scale tasks requiring\nboth speed and precision. This paper introduces LMV-RPA, a Large Model\nVoting-based Robotic Process Automation system to enhance OCR workflows.\nLMV-RPA integrates outputs from OCR engines such as Paddle OCR, Tesseract OCR,\nEasy OCR, and DocTR with Large Language Models (LLMs) like LLaMA 3 and\nGemini-1.5-pro. Using a majority voting mechanism, it processes OCR outputs\ninto structured JSON formats, improving accuracy, particularly in complex\nlayouts. The multi-phase pipeline processes text extracted by OCR engines\nthrough LLMs, combining results to ensure the most accurate outputs. LMV-RPA\nachieves 99 percent accuracy in OCR tasks, surpassing baseline models with 94\npercent, while reducing processing time by 80 percent. Benchmark evaluations\nconfirm its scalability and demonstrate that LMV-RPA offers a faster, more\nreliable, and efficient solution for automating large-scale document processing\ntasks."
                },
                "authors": [
                    {
                        "name": "Osama Abdellatif"
                    },
                    {
                        "name": "Ahmed Ayman"
                    },
                    {
                        "name": "Ali Hamdi"
                    }
                ],
                "author_detail": {
                    "name": "Ali Hamdi"
                },
                "author": "Ali Hamdi",
                "arxiv_comment": "12 pages, 1 figures, 1 algorithm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17965v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17965v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19918v1",
                "updated": "2025-04-28T15:46:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    46,
                    2,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:46:02Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    46,
                    2,
                    0,
                    118,
                    0
                ],
                "title": "Enhancing Surgical Documentation through Multimodal Visual-Temporal\n  Transformers and Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Surgical Documentation through Multimodal Visual-Temporal\n  Transformers and Generative AI"
                },
                "summary": "The automatic summarization of surgical videos is essential for enhancing\nprocedural documentation, supporting surgical training, and facilitating\npost-operative analysis. This paper presents a novel method at the intersection\nof artificial intelligence and medicine, aiming to develop machine learning\nmodels with direct real-world applications in surgical contexts. We propose a\nmulti-modal framework that leverages recent advancements in computer vision and\nlarge language models to generate comprehensive video summaries. % The approach\nis structured in three key stages. First, surgical videos are divided into\nclips, and visual features are extracted at the frame level using visual\ntransformers. This step focuses on detecting tools, tissues, organs, and\nsurgical actions. Second, the extracted features are transformed into\nframe-level captions via large language models. These are then combined with\ntemporal features, captured using a ViViT-based encoder, to produce clip-level\nsummaries that reflect the broader context of each video segment. Finally, the\nclip-level descriptions are aggregated into a full surgical report using a\ndedicated LLM tailored for the summarization task. % We evaluate our method on\nthe CholecT50 dataset, using instrument and action annotations from 50\nlaparoscopic videos. The results show strong performance, achieving 96\\%\nprecision in tool detection and a BERT score of 0.74 for temporal context\nsummarization. This work contributes to the advancement of AI-assisted tools\nfor surgical reporting, offering a step toward more intelligent and reliable\nclinical documentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automatic summarization of surgical videos is essential for enhancing\nprocedural documentation, supporting surgical training, and facilitating\npost-operative analysis. This paper presents a novel method at the intersection\nof artificial intelligence and medicine, aiming to develop machine learning\nmodels with direct real-world applications in surgical contexts. We propose a\nmulti-modal framework that leverages recent advancements in computer vision and\nlarge language models to generate comprehensive video summaries. % The approach\nis structured in three key stages. First, surgical videos are divided into\nclips, and visual features are extracted at the frame level using visual\ntransformers. This step focuses on detecting tools, tissues, organs, and\nsurgical actions. Second, the extracted features are transformed into\nframe-level captions via large language models. These are then combined with\ntemporal features, captured using a ViViT-based encoder, to produce clip-level\nsummaries that reflect the broader context of each video segment. Finally, the\nclip-level descriptions are aggregated into a full surgical report using a\ndedicated LLM tailored for the summarization task. % We evaluate our method on\nthe CholecT50 dataset, using instrument and action annotations from 50\nlaparoscopic videos. The results show strong performance, achieving 96\\%\nprecision in tool detection and a BERT score of 0.74 for temporal context\nsummarization. This work contributes to the advancement of AI-assisted tools\nfor surgical reporting, offering a step toward more intelligent and reliable\nclinical documentation."
                },
                "authors": [
                    {
                        "name": "Hugo Georgenthum"
                    },
                    {
                        "name": "Cristian Cosentino"
                    },
                    {
                        "name": "Fabrizio Marozzo"
                    },
                    {
                        "name": "Pietro Li"
                    }
                ],
                "author_detail": {
                    "name": "Pietro Li"
                },
                "author": "Pietro Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19912v1",
                "updated": "2025-04-28T15:41:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    41,
                    28,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:41:28Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    41,
                    28,
                    0,
                    118,
                    0
                ],
                "title": "Can AI Agents Design and Implement Drug Discovery Pipelines?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can AI Agents Design and Implement Drug Discovery Pipelines?"
                },
                "summary": "The rapid advancement of artificial intelligence, particularly autonomous\nagentic systems based on Large Language Models (LLMs), presents new\nopportunities to accelerate drug discovery by improving in-silico modeling and\nreducing dependence on costly experimental trials. Current AI agent-based\nsystems demonstrate proficiency in solving programming challenges and\nconducting research, indicating an emerging potential to develop software\ncapable of addressing complex problems such as pharmaceutical design and drug\ndiscovery. This paper introduces DO Challenge, a benchmark designed to evaluate\nthe decision-making abilities of AI agents in a single, complex problem\nresembling virtual screening scenarios. The benchmark challenges systems to\nindependently develop, implement, and execute efficient strategies for\nidentifying promising molecular structures from extensive datasets, while\nnavigating chemical space, selecting models, and managing limited resources in\na multi-objective context. We also discuss insights from the DO Challenge 2025,\na competition based on the proposed benchmark, which showcased diverse\nstrategies explored by human participants. Furthermore, we present the Deep\nThought multi-agent system, which demonstrated strong performance on the\nbenchmark, outperforming most human teams. Among the language models tested,\nClaude 3.7 Sonnet, Gemini 2.5 Pro and o3 performed best in primary agent roles,\nand GPT-4o, Gemini 2.0 Flash were effective in auxiliary roles. While\npromising, the system's performance still fell short of expert-designed\nsolutions and showed high instability, highlighting both the potential and\ncurrent limitations of AI-driven methodologies in transforming drug discovery\nand broader scientific research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of artificial intelligence, particularly autonomous\nagentic systems based on Large Language Models (LLMs), presents new\nopportunities to accelerate drug discovery by improving in-silico modeling and\nreducing dependence on costly experimental trials. Current AI agent-based\nsystems demonstrate proficiency in solving programming challenges and\nconducting research, indicating an emerging potential to develop software\ncapable of addressing complex problems such as pharmaceutical design and drug\ndiscovery. This paper introduces DO Challenge, a benchmark designed to evaluate\nthe decision-making abilities of AI agents in a single, complex problem\nresembling virtual screening scenarios. The benchmark challenges systems to\nindependently develop, implement, and execute efficient strategies for\nidentifying promising molecular structures from extensive datasets, while\nnavigating chemical space, selecting models, and managing limited resources in\na multi-objective context. We also discuss insights from the DO Challenge 2025,\na competition based on the proposed benchmark, which showcased diverse\nstrategies explored by human participants. Furthermore, we present the Deep\nThought multi-agent system, which demonstrated strong performance on the\nbenchmark, outperforming most human teams. Among the language models tested,\nClaude 3.7 Sonnet, Gemini 2.5 Pro and o3 performed best in primary agent roles,\nand GPT-4o, Gemini 2.0 Flash were effective in auxiliary roles. While\npromising, the system's performance still fell short of expert-designed\nsolutions and showed high instability, highlighting both the potential and\ncurrent limitations of AI-driven methodologies in transforming drug discovery\nand broader scientific research."
                },
                "authors": [
                    {
                        "name": "Khachik Smbatyan"
                    },
                    {
                        "name": "Tsolak Ghukasyan"
                    },
                    {
                        "name": "Tigran Aghajanyan"
                    },
                    {
                        "name": "Hovhannes Dabaghyan"
                    },
                    {
                        "name": "Sergey Adamyan"
                    },
                    {
                        "name": "Aram Bughdaryan"
                    },
                    {
                        "name": "Vahagn Altunyan"
                    },
                    {
                        "name": "Gagik Navasardyan"
                    },
                    {
                        "name": "Aram Davtyan"
                    },
                    {
                        "name": "Anush Hakobyan"
                    },
                    {
                        "name": "Aram Gharibyan"
                    },
                    {
                        "name": "Arman Fahradyan"
                    },
                    {
                        "name": "Artur Hakobyan"
                    },
                    {
                        "name": "Hasmik Mnatsakanyan"
                    },
                    {
                        "name": "Narek Ginoyan"
                    },
                    {
                        "name": "Garik Petrosyan"
                    }
                ],
                "author_detail": {
                    "name": "Garik Petrosyan"
                },
                "author": "Garik Petrosyan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11690v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11690v3",
                "updated": "2025-04-28T15:39:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    39,
                    28,
                    0,
                    118,
                    0
                ],
                "published": "2024-09-18T04:10:44Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    10,
                    44,
                    2,
                    262,
                    0
                ],
                "title": "ID-Free Not Risk-Free: LLM-Powered Agents Unveil Risks in ID-Free\n  Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ID-Free Not Risk-Free: LLM-Powered Agents Unveil Risks in ID-Free\n  Recommender Systems"
                },
                "summary": "Recent advances in ID-free recommender systems have attracted significant\nattention for effectively addressing the cold start problem. However, their\nvulnerability to malicious attacks remains largely unexplored. In this paper,\nwe unveil a critical yet overlooked risk: LLM-powered agents can be\nstrategically deployed to attack ID-free recommenders, stealthily promoting\nlow-quality items in black-box settings. This attack exploits a novel\nrewriting-based deception strategy, where malicious agents synthesize deceptive\ntextual descriptions by simulating the characteristics of popular items. To\nachieve this, the attack mechanism integrates two primary components: (1) a\npopularity extraction component that captures essential characteristics of\npopular items and (2) a multi-agent collaboration mechanism that enables\niterative refinement of promotional textual descriptions through independent\nthinking and team discussion. To counter this risk, we further introduce a\ndetection method to identify suspicious text generated by our discovered\nattack. By unveiling this risk, our work aims to underscore the urgent need to\nenhance the security of ID-free recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in ID-free recommender systems have attracted significant\nattention for effectively addressing the cold start problem. However, their\nvulnerability to malicious attacks remains largely unexplored. In this paper,\nwe unveil a critical yet overlooked risk: LLM-powered agents can be\nstrategically deployed to attack ID-free recommenders, stealthily promoting\nlow-quality items in black-box settings. This attack exploits a novel\nrewriting-based deception strategy, where malicious agents synthesize deceptive\ntextual descriptions by simulating the characteristics of popular items. To\nachieve this, the attack mechanism integrates two primary components: (1) a\npopularity extraction component that captures essential characteristics of\npopular items and (2) a multi-agent collaboration mechanism that enables\niterative refinement of promotional textual descriptions through independent\nthinking and team discussion. To counter this risk, we further introduce a\ndetection method to identify suspicious text generated by our discovered\nattack. By unveiling this risk, our work aims to underscore the urgent need to\nenhance the security of ID-free recommender systems."
                },
                "authors": [
                    {
                        "name": "Zongwei Wang"
                    },
                    {
                        "name": "Min Gao"
                    },
                    {
                        "name": "Junliang Yu"
                    },
                    {
                        "name": "Xinyi Gao"
                    },
                    {
                        "name": "Quoc Viet Hung Nguyen"
                    },
                    {
                        "name": "Shazia Sadiq"
                    },
                    {
                        "name": "Hongzhi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Yin"
                },
                "author": "Hongzhi Yin",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11690v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11690v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14918v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14918v2",
                "updated": "2025-04-28T15:37:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    37,
                    12,
                    0,
                    118,
                    0
                ],
                "published": "2024-09-23T11:16:46Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    11,
                    16,
                    46,
                    0,
                    267,
                    0
                ],
                "title": "A Realistic Simulation Framework for Analog/Digital Neuromorphic\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Realistic Simulation Framework for Analog/Digital Neuromorphic\n  Architectures"
                },
                "summary": "Developing dedicated mixed-signal neuromorphic computing systems optimized\nfor real-time sensory-processing in extreme edge-computing applications\nrequires time-consuming design, fabrication, and deployment of full-custom\nneuromorphic processors. To ensure that initial prototyping efforts, exploring\nthe properties of different network architectures and parameter settings, lead\nto realistic results, it is important to use simulation frameworks that match\nas best as possible the properties of the final hardware. This is particularly\nchallenging for neuromorphic hardware platforms made using mixed-signal\nanalog/digital circuits, due to the variability and noise sensitivity of their\ncomponents. In this paper, we address this challenge by developing a software\nspiking neural network simulator explicitly designed to account for the\nproperties of mixed-signal neuromorphic circuits, including device mismatch\nvariability.\n  The simulator, called ARCANA (A Realistic Simulation Framework for\nAnalog/Digital Neuromorphic Architectures), is designed to reproduce the\ndynamics of mixed-signal synapse and neuron electronic circuits with\nautogradient differentiation for parameter optimization and GPU acceleration.\nWe demonstrate the effectiveness of this approach by matching software\nsimulation results with measurements made from an existing neuromorphic\nprocessor. We show how the results obtained provide a reliable estimate of the\nbehavior of the spiking neural network trained in software, once deployed in\nhardware. This framework enables the development and innovation of new learning\nrules and processing architectures in neuromorphic embedded systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing dedicated mixed-signal neuromorphic computing systems optimized\nfor real-time sensory-processing in extreme edge-computing applications\nrequires time-consuming design, fabrication, and deployment of full-custom\nneuromorphic processors. To ensure that initial prototyping efforts, exploring\nthe properties of different network architectures and parameter settings, lead\nto realistic results, it is important to use simulation frameworks that match\nas best as possible the properties of the final hardware. This is particularly\nchallenging for neuromorphic hardware platforms made using mixed-signal\nanalog/digital circuits, due to the variability and noise sensitivity of their\ncomponents. In this paper, we address this challenge by developing a software\nspiking neural network simulator explicitly designed to account for the\nproperties of mixed-signal neuromorphic circuits, including device mismatch\nvariability.\n  The simulator, called ARCANA (A Realistic Simulation Framework for\nAnalog/Digital Neuromorphic Architectures), is designed to reproduce the\ndynamics of mixed-signal synapse and neuron electronic circuits with\nautogradient differentiation for parameter optimization and GPU acceleration.\nWe demonstrate the effectiveness of this approach by matching software\nsimulation results with measurements made from an existing neuromorphic\nprocessor. We show how the results obtained provide a reliable estimate of the\nbehavior of the spiking neural network trained in software, once deployed in\nhardware. This framework enables the development and innovation of new learning\nrules and processing architectures in neuromorphic embedded systems."
                },
                "authors": [
                    {
                        "name": "Fernando M. Quintana"
                    },
                    {
                        "name": "Maryada"
                    },
                    {
                        "name": "Pedro L. Galindo"
                    },
                    {
                        "name": "Elisa Donati"
                    },
                    {
                        "name": "Giacomo Indiveri"
                    },
                    {
                        "name": "Fernando Perez-Pea"
                    }
                ],
                "author_detail": {
                    "name": "Fernando Perez-Pea"
                },
                "author": "Fernando Perez-Pea",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14918v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14918v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19898v1",
                "updated": "2025-04-28T15:30:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    30,
                    58,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:30:58Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    30,
                    58,
                    0,
                    118,
                    0
                ],
                "title": "GenCLS++: Pushing the Boundaries of Generative Classification in LLMs\n  Through Comprehensive SFT and RL Studies Across Diverse Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenCLS++: Pushing the Boundaries of Generative Classification in LLMs\n  Through Comprehensive SFT and RL Studies Across Diverse Datasets"
                },
                "summary": "As a fundamental task in machine learning, text classification plays a\ncrucial role in many areas. With the rapid scaling of Large Language Models\n(LLMs), particularly through reinforcement learning (RL), there is a growing\nneed for more capable discriminators. Consequently, advances in classification\nare becoming increasingly vital for enhancing the overall capabilities of LLMs.\nTraditional discriminative methods map text to labels but overlook LLMs'\nintrinsic generative strengths. Generative classification addresses this by\nprompting the model to directly output labels. However, existing studies still\nrely on simple SFT alone, seldom probing the interplay between training and\ninference prompts, and no work has systematically leveraged RL for generative\ntext classifiers and unified SFT, RL, and inference-time prompting in one\nframework. We bridge this gap with GenCLS++, a framework that jointly optimizes\nSFT and RL while systematically exploring five high-level strategy\ndimensions-in-context learning variants, category definitions, explicit\nuncertainty labels, semantically irrelevant numeric labels, and\nperplexity-based decoding-during both training and inference. After an SFT\n\"policy warm-up,\" we apply RL with a simple rule-based reward, yielding sizable\nextra gains. Across seven datasets, GenCLS++ achieves an average accuracy\nimprovement of 3.46% relative to the naive SFT baseline; on public datasets,\nthis improvement rises to 4.00%. Notably, unlike reasoning-intensive tasks that\nbenefit from explicit thinking processes, we find that classification tasks\nperform better without such reasoning steps. These insights into the role of\nexplicit reasoning provide valuable guidance for future LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a fundamental task in machine learning, text classification plays a\ncrucial role in many areas. With the rapid scaling of Large Language Models\n(LLMs), particularly through reinforcement learning (RL), there is a growing\nneed for more capable discriminators. Consequently, advances in classification\nare becoming increasingly vital for enhancing the overall capabilities of LLMs.\nTraditional discriminative methods map text to labels but overlook LLMs'\nintrinsic generative strengths. Generative classification addresses this by\nprompting the model to directly output labels. However, existing studies still\nrely on simple SFT alone, seldom probing the interplay between training and\ninference prompts, and no work has systematically leveraged RL for generative\ntext classifiers and unified SFT, RL, and inference-time prompting in one\nframework. We bridge this gap with GenCLS++, a framework that jointly optimizes\nSFT and RL while systematically exploring five high-level strategy\ndimensions-in-context learning variants, category definitions, explicit\nuncertainty labels, semantically irrelevant numeric labels, and\nperplexity-based decoding-during both training and inference. After an SFT\n\"policy warm-up,\" we apply RL with a simple rule-based reward, yielding sizable\nextra gains. Across seven datasets, GenCLS++ achieves an average accuracy\nimprovement of 3.46% relative to the naive SFT baseline; on public datasets,\nthis improvement rises to 4.00%. Notably, unlike reasoning-intensive tasks that\nbenefit from explicit thinking processes, we find that classification tasks\nperform better without such reasoning steps. These insights into the role of\nexplicit reasoning provide valuable guidance for future LLM applications."
                },
                "authors": [
                    {
                        "name": "Mingqian He"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Chonggang Lu"
                    },
                    {
                        "name": "Ziyan Liu"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Haofu Qian"
                    }
                ],
                "author_detail": {
                    "name": "Haofu Qian"
                },
                "author": "Haofu Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19894v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19894v1",
                "updated": "2025-04-28T15:28:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    28,
                    14,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:28:14Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    28,
                    14,
                    0,
                    118,
                    0
                ],
                "title": "CineVerse: Consistent Keyframe Synthesis for Cinematic Scene Composition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CineVerse: Consistent Keyframe Synthesis for Cinematic Scene Composition"
                },
                "summary": "We present CineVerse, a novel framework for the task of cinematic scene\ncomposition. Similar to traditional multi-shot generation, our task emphasizes\nthe need for consistency and continuity across frames. However, our task also\nfocuses on addressing challenges inherent to filmmaking, such as multiple\ncharacters, complex interactions, and visual cinematic effects. In order to\nlearn to generate such content, we first create the CineVerse dataset. We use\nthis dataset to train our proposed two-stage approach. First, we prompt a large\nlanguage model (LLM) with task-specific instructions to take in a high-level\nscene description and generate a detailed plan for the overall setting and\ncharacters, as well as the individual shots. Then, we fine-tune a text-to-image\ngeneration model to synthesize high-quality visual keyframes. Experimental\nresults demonstrate that CineVerse yields promising improvements in generating\nvisually coherent and contextually rich movie scenes, paving the way for\nfurther exploration in cinematic video synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present CineVerse, a novel framework for the task of cinematic scene\ncomposition. Similar to traditional multi-shot generation, our task emphasizes\nthe need for consistency and continuity across frames. However, our task also\nfocuses on addressing challenges inherent to filmmaking, such as multiple\ncharacters, complex interactions, and visual cinematic effects. In order to\nlearn to generate such content, we first create the CineVerse dataset. We use\nthis dataset to train our proposed two-stage approach. First, we prompt a large\nlanguage model (LLM) with task-specific instructions to take in a high-level\nscene description and generate a detailed plan for the overall setting and\ncharacters, as well as the individual shots. Then, we fine-tune a text-to-image\ngeneration model to synthesize high-quality visual keyframes. Experimental\nresults demonstrate that CineVerse yields promising improvements in generating\nvisually coherent and contextually rich movie scenes, paving the way for\nfurther exploration in cinematic video synthesis."
                },
                "authors": [
                    {
                        "name": "Quynh Phung"
                    },
                    {
                        "name": "Long Mai"
                    },
                    {
                        "name": "Fabian David Caba Heilbron"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Jia-Bin Huang"
                    },
                    {
                        "name": "Cusuh Ham"
                    }
                ],
                "author_detail": {
                    "name": "Cusuh Ham"
                },
                "author": "Cusuh Ham",
                "arxiv_comment": "link website: https://cinevers.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19894v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19894v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19867v1",
                "updated": "2025-04-28T15:00:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    0,
                    3,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:00:03Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    0,
                    3,
                    0,
                    118,
                    0
                ],
                "title": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\n  Computation and Unified Storage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\n  Computation and Unified Storage"
                },
                "summary": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models."
                },
                "authors": [
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Lufang Chen"
                    },
                    {
                        "name": "Zhong Wang"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Qiuli Mao"
                    },
                    {
                        "name": "Jianping Ma"
                    },
                    {
                        "name": "Chao Xiong"
                    },
                    {
                        "name": "Guanyu Wu"
                    },
                    {
                        "name": "Buhe Han"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yun Liang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "18 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19860v1",
                "updated": "2025-04-28T14:50:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    50,
                    45,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T14:50:45Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    50,
                    45,
                    0,
                    118,
                    0
                ],
                "title": "CoherenDream: Boosting Holistic Text Coherence in 3D Generation via\n  Multimodal Large Language Models Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoherenDream: Boosting Holistic Text Coherence in 3D Generation via\n  Multimodal Large Language Models Feedback"
                },
                "summary": "Score Distillation Sampling (SDS) has achieved remarkable success in\ntext-to-3D content generation. However, SDS-based methods struggle to maintain\nsemantic fidelity for user prompts, particularly when involving multiple\nobjects with intricate interactions. While existing approaches often address 3D\nconsistency through multiview diffusion model fine-tuning on 3D datasets, this\nstrategy inadvertently exacerbates text-3D alignment degradation. The\nlimitation stems from SDS's inherent accumulation of view-independent biases\nduring optimization, which progressively diverges from the ideal text alignment\ndirection. To alleviate this limitation, we propose a novel SDS objective,\ndubbed as Textual Coherent Score Distillation (TCSD), which integrates\nalignment feedback from multimodal large language models (MLLMs). Our TCSD\nleverages cross-modal understanding capabilities of MLLMs to assess and guide\nthe text-3D correspondence during the optimization. We further develop\n3DLLaVA-CRITIC - a fine-tuned MLLM specialized for evaluating multiview text\nalignment in 3D generations. Additionally, we introduce an LLM-layout\ninitialization that significantly accelerates optimization convergence through\nsemantic-aware spatial configuration. Comprehensive evaluations demonstrate\nthat our framework, CoherenDream, establishes state-of-the-art performance in\ntext-aligned 3D generation across multiple benchmarks, including T$^3$Bench and\nTIFA subset. Qualitative results showcase the superior performance of\nCoherenDream in preserving textual consistency and semantic interactions. As\nthe first study to incorporate MLLMs into SDS optimization, we also conduct\nextensive ablation studies to explore optimal MLLM adaptations for 3D\ngeneration tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Score Distillation Sampling (SDS) has achieved remarkable success in\ntext-to-3D content generation. However, SDS-based methods struggle to maintain\nsemantic fidelity for user prompts, particularly when involving multiple\nobjects with intricate interactions. While existing approaches often address 3D\nconsistency through multiview diffusion model fine-tuning on 3D datasets, this\nstrategy inadvertently exacerbates text-3D alignment degradation. The\nlimitation stems from SDS's inherent accumulation of view-independent biases\nduring optimization, which progressively diverges from the ideal text alignment\ndirection. To alleviate this limitation, we propose a novel SDS objective,\ndubbed as Textual Coherent Score Distillation (TCSD), which integrates\nalignment feedback from multimodal large language models (MLLMs). Our TCSD\nleverages cross-modal understanding capabilities of MLLMs to assess and guide\nthe text-3D correspondence during the optimization. We further develop\n3DLLaVA-CRITIC - a fine-tuned MLLM specialized for evaluating multiview text\nalignment in 3D generations. Additionally, we introduce an LLM-layout\ninitialization that significantly accelerates optimization convergence through\nsemantic-aware spatial configuration. Comprehensive evaluations demonstrate\nthat our framework, CoherenDream, establishes state-of-the-art performance in\ntext-aligned 3D generation across multiple benchmarks, including T$^3$Bench and\nTIFA subset. Qualitative results showcase the superior performance of\nCoherenDream in preserving textual consistency and semantic interactions. As\nthe first study to incorporate MLLMs into SDS optimization, we also conduct\nextensive ablation studies to explore optimal MLLM adaptations for 3D\ngeneration tasks."
                },
                "authors": [
                    {
                        "name": "Chenhan Jiang"
                    },
                    {
                        "name": "Yihan Zeng"
                    },
                    {
                        "name": "Hang Xu"
                    },
                    {
                        "name": "Dit-Yan Yeung"
                    }
                ],
                "author_detail": {
                    "name": "Dit-Yan Yeung"
                },
                "author": "Dit-Yan Yeung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19855v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19855v2",
                "updated": "2025-04-29T02:52:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    2,
                    52,
                    54,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-28T14:48:00Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    48,
                    0,
                    0,
                    118,
                    0
                ],
                "title": "The Automation Advantage in AI Red Teaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Automation Advantage in AI Red Teaming"
                },
                "summary": "This paper analyzes Large Language Model (LLM) security vulnerabilities based\non data from Crucible, encompassing 214,271 attack attempts by 1,674 users\nacross 30 LLM challenges. Our findings reveal automated approaches\nsignificantly outperform manual techniques (69.5% vs 47.6% success rate),\ndespite only 5.2% of users employing automation. We demonstrate that automated\napproaches excel in systematic exploration and pattern matching challenges,\nwhile manual approaches retain speed advantages in certain creative reasoning\nscenarios, often solving problems 5x faster when successful. Challenge\ncategories requiring systematic exploration are most effectively targeted\nthrough automation, while intuitive challenges sometimes favor manual\ntechniques for time-to-solve metrics. These results illuminate how algorithmic\ntesting is transforming AI red-teaming practices, with implications for both\noffensive security research and defensive measures. Our analysis suggests\noptimal security testing combines human creativity for strategy development\nwith programmatic execution for thorough exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper analyzes Large Language Model (LLM) security vulnerabilities based\non data from Crucible, encompassing 214,271 attack attempts by 1,674 users\nacross 30 LLM challenges. Our findings reveal automated approaches\nsignificantly outperform manual techniques (69.5% vs 47.6% success rate),\ndespite only 5.2% of users employing automation. We demonstrate that automated\napproaches excel in systematic exploration and pattern matching challenges,\nwhile manual approaches retain speed advantages in certain creative reasoning\nscenarios, often solving problems 5x faster when successful. Challenge\ncategories requiring systematic exploration are most effectively targeted\nthrough automation, while intuitive challenges sometimes favor manual\ntechniques for time-to-solve metrics. These results illuminate how algorithmic\ntesting is transforming AI red-teaming practices, with implications for both\noffensive security research and defensive measures. Our analysis suggests\noptimal security testing combines human creativity for strategy development\nwith programmatic execution for thorough exploration."
                },
                "authors": [
                    {
                        "name": "Rob Mulla"
                    },
                    {
                        "name": "Ads Dawson"
                    },
                    {
                        "name": "Vincent Abruzzon"
                    },
                    {
                        "name": "Brian Greunke"
                    },
                    {
                        "name": "Nick Landers"
                    },
                    {
                        "name": "Brad Palm"
                    },
                    {
                        "name": "Will Pearce"
                    }
                ],
                "author_detail": {
                    "name": "Will Pearce"
                },
                "author": "Will Pearce",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19855v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19855v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5; D.4.6; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19851v1",
                "updated": "2025-04-28T14:46:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    46,
                    51,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T14:46:51Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    46,
                    51,
                    0,
                    118,
                    0
                ],
                "title": "Do You Know the Way? Human-in-the-Loop Understanding for Fast\n  Traversability Estimation in Mobile Robotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do You Know the Way? Human-in-the-Loop Understanding for Fast\n  Traversability Estimation in Mobile Robotics"
                },
                "summary": "The increasing use of robots in unstructured environments necessitates the\ndevelopment of effective perception and navigation strategies to enable field\nrobots to successfully perform their tasks. In particular, it is key for such\nrobots to understand where in their environment they can and cannot travel -- a\ntask known as traversability estimation. However, existing geometric approaches\nto traversability estimation may fail to capture nuanced representations of\ntraversability, whereas vision-based approaches typically either involve\nmanually annotating a large number of images or require robot experience. In\naddition, existing methods can struggle to address domain shifts as they\ntypically do not learn during deployment. To this end, we propose a\nhuman-in-the-loop (HiL) method for traversability estimation that prompts a\nhuman for annotations as-needed. Our method uses a foundation model to enable\nrapid learning on new annotations and to provide accurate predictions even when\ntrained on a small number of quickly-provided HiL annotations. We extensively\nvalidate our method in simulation and on real-world data, and demonstrate that\nit can provide state-of-the-art traversability prediction performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of robots in unstructured environments necessitates the\ndevelopment of effective perception and navigation strategies to enable field\nrobots to successfully perform their tasks. In particular, it is key for such\nrobots to understand where in their environment they can and cannot travel -- a\ntask known as traversability estimation. However, existing geometric approaches\nto traversability estimation may fail to capture nuanced representations of\ntraversability, whereas vision-based approaches typically either involve\nmanually annotating a large number of images or require robot experience. In\naddition, existing methods can struggle to address domain shifts as they\ntypically do not learn during deployment. To this end, we propose a\nhuman-in-the-loop (HiL) method for traversability estimation that prompts a\nhuman for annotations as-needed. Our method uses a foundation model to enable\nrapid learning on new annotations and to provide accurate predictions even when\ntrained on a small number of quickly-provided HiL annotations. We extensively\nvalidate our method in simulation and on real-world data, and demonstrate that\nit can provide state-of-the-art traversability prediction performance."
                },
                "authors": [
                    {
                        "name": "Andre Schreiber"
                    },
                    {
                        "name": "Katherine Driggs-Campbell"
                    }
                ],
                "author_detail": {
                    "name": "Katherine Driggs-Campbell"
                },
                "author": "Katherine Driggs-Campbell",
                "arxiv_doi": "10.1109/LRA.2025.3563819",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LRA.2025.3563819",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.19851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by RA-L. Code is available at\n  https://github.com/andreschreiber/CHUNGUS",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19838v1",
                "updated": "2025-04-28T14:39:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    39,
                    25,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T14:39:25Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    39,
                    25,
                    0,
                    118,
                    0
                ],
                "title": "LLM-Powered GUI Agents in Phone Automation: Surveying Progress and\n  Prospects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Powered GUI Agents in Phone Automation: Surveying Progress and\n  Prospects"
                },
                "summary": "With the rapid rise of large language models (LLMs), phone automation has\nundergone transformative changes. This paper systematically reviews LLM-driven\nphone GUI agents, highlighting their evolution from script-based automation to\nintelligent, adaptive systems. We first contextualize key challenges, (i)\nlimited generality, (ii) high maintenance overhead, and (iii) weak intent\ncomprehension, and show how LLMs address these issues through advanced language\nunderstanding, multimodal perception, and robust decision-making. We then\npropose a taxonomy covering fundamental agent frameworks (single-agent,\nmulti-agent, plan-then-act), modeling approaches (prompt engineering,\ntraining-based), and essential datasets and benchmarks. Furthermore, we detail\ntask-specific architectures, supervised fine-tuning, and reinforcement learning\nstrategies that bridge user intent and GUI operations. Finally, we discuss open\nchallenges such as dataset diversity, on-device deployment efficiency,\nuser-centric adaptation, and security concerns, offering forward-looking\ninsights into this rapidly evolving field. By providing a structured overview\nand identifying pressing research gaps, this paper serves as a definitive\nreference for researchers and practitioners seeking to harness LLMs in\ndesigning scalable, user-friendly phone GUI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid rise of large language models (LLMs), phone automation has\nundergone transformative changes. This paper systematically reviews LLM-driven\nphone GUI agents, highlighting their evolution from script-based automation to\nintelligent, adaptive systems. We first contextualize key challenges, (i)\nlimited generality, (ii) high maintenance overhead, and (iii) weak intent\ncomprehension, and show how LLMs address these issues through advanced language\nunderstanding, multimodal perception, and robust decision-making. We then\npropose a taxonomy covering fundamental agent frameworks (single-agent,\nmulti-agent, plan-then-act), modeling approaches (prompt engineering,\ntraining-based), and essential datasets and benchmarks. Furthermore, we detail\ntask-specific architectures, supervised fine-tuning, and reinforcement learning\nstrategies that bridge user intent and GUI operations. Finally, we discuss open\nchallenges such as dataset diversity, on-device deployment efficiency,\nuser-centric adaptation, and security concerns, offering forward-looking\ninsights into this rapidly evolving field. By providing a structured overview\nand identifying pressing research gaps, this paper serves as a definitive\nreference for researchers and practitioners seeking to harness LLMs in\ndesigning scalable, user-friendly phone GUI agents."
                },
                "authors": [
                    {
                        "name": "Guangyi Liu"
                    },
                    {
                        "name": "Pengxiang Zhao"
                    },
                    {
                        "name": "Liang Liu"
                    },
                    {
                        "name": "Yaxuan Guo"
                    },
                    {
                        "name": "Han Xiao"
                    },
                    {
                        "name": "Weifeng Lin"
                    },
                    {
                        "name": "Yuxiang Chai"
                    },
                    {
                        "name": "Yue Han"
                    },
                    {
                        "name": "Shuai Ren"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Xiaoyu Liang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Tianze Wu"
                    },
                    {
                        "name": "Linghao Li"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Guanjing Xiong"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "arxiv_comment": "37 pages, 10 figures, 7 tables, Project Homepage:\n  https://github.com/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15622v2",
                "updated": "2025-04-28T14:28:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    28,
                    14,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-22T06:28:08Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    6,
                    28,
                    8,
                    1,
                    112,
                    0
                ],
                "title": "Exploring the Role of Large Language Models in Cybersecurity: A\n  Systematic Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Role of Large Language Models in Cybersecurity: A\n  Systematic Survey"
                },
                "summary": "With the rapid development of technology and the acceleration of\ndigitalisation, the frequency and complexity of cyber security threats are\nincreasing. Traditional cybersecurity approaches, often based on static rules\nand predefined scenarios, are struggling to adapt to the rapidly evolving\nnature of modern cyberattacks. There is an urgent need for more adaptive and\nintelligent defence strategies. The emergence of Large Language Model (LLM)\nprovides an innovative solution to cope with the increasingly severe cyber\nthreats, and its potential in analysing complex attack patterns, predicting\nthreats and assisting real-time response has attracted a lot of attention in\nthe field of cybersecurity, and exploring how to effectively use LLM to defend\nagainst cyberattacks has become a hot topic in the current research field. This\nsurvey examines the applications of LLM from the perspective of the cyber\nattack lifecycle, focusing on the three phases of defense reconnaissance,\nfoothold establishment, and lateral movement, and it analyzes the potential of\nLLMs in Cyber Threat Intelligence (CTI) tasks. Meanwhile, we investigate how\nLLM-based security solutions are deployed and applied in different network\nscenarios. It also summarizes the internal and external risk issues faced by\nLLM during its application. Finally, this survey also points out the facing\nrisk issues and possible future research directions in this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of technology and the acceleration of\ndigitalisation, the frequency and complexity of cyber security threats are\nincreasing. Traditional cybersecurity approaches, often based on static rules\nand predefined scenarios, are struggling to adapt to the rapidly evolving\nnature of modern cyberattacks. There is an urgent need for more adaptive and\nintelligent defence strategies. The emergence of Large Language Model (LLM)\nprovides an innovative solution to cope with the increasingly severe cyber\nthreats, and its potential in analysing complex attack patterns, predicting\nthreats and assisting real-time response has attracted a lot of attention in\nthe field of cybersecurity, and exploring how to effectively use LLM to defend\nagainst cyberattacks has become a hot topic in the current research field. This\nsurvey examines the applications of LLM from the perspective of the cyber\nattack lifecycle, focusing on the three phases of defense reconnaissance,\nfoothold establishment, and lateral movement, and it analyzes the potential of\nLLMs in Cyber Threat Intelligence (CTI) tasks. Meanwhile, we investigate how\nLLM-based security solutions are deployed and applied in different network\nscenarios. It also summarizes the internal and external risk issues faced by\nLLM during its application. Finally, this survey also points out the facing\nrisk issues and possible future research directions in this domain."
                },
                "authors": [
                    {
                        "name": "Shuang Tian"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Jiqiang Liu"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Xuangou Wu"
                    },
                    {
                        "name": "Xiaoqiang Zhu"
                    },
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Weiting Zhang"
                    },
                    {
                        "name": "Zhenhui Yuan"
                    },
                    {
                        "name": "Shiwen Mao"
                    },
                    {
                        "name": "Dong In Kim"
                    }
                ],
                "author_detail": {
                    "name": "Dong In Kim"
                },
                "author": "Dong In Kim",
                "arxiv_comment": "20 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19811v1",
                "updated": "2025-04-28T14:08:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    8,
                    45,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T14:08:45Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    8,
                    45,
                    0,
                    118,
                    0
                ],
                "title": "Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language\n  Model Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language\n  Model Performance"
                },
                "summary": "Accurately forecasting the performance of Large Language Models (LLMs) before\nextensive fine-tuning or merging can substantially reduce both computational\nexpense and development time. Although prior approaches like scaling laws\naccount for global factors such as parameter size or training tokens, they\noften overlook explicit lineage relationships - i.e., which models are derived\nor merged from which parents. In this work, we propose a novel\nLineage-Regularized Matrix Factorization (LRMF) framework that encodes\nancestral ties among LLMs via a graph Laplacian regularizer. By leveraging\nmulti-hop parent-child connections, LRMF consistently outperforms conventional\nmatrix factorization and collaborative filtering methods in both instance-level\nand benchmark-level performance prediction. Our large-scale study includes\n2,934 publicly available Hugging Face models and 21,000+ instances across 6\nmajor benchmarks, showing that lineage constraints yield up to 7-10 percentage\npoints higher correlation with actual performance compared to baselines.\nMoreover, LRMF effectively addresses the cold-start problem, providing accurate\nestimates for newly derived or merged models even with minimal data. This\nlineage-guided strategy thus offers a resource-efficient way to inform\nhyperparameter tuning, data selection, and model combination in modern LLM\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately forecasting the performance of Large Language Models (LLMs) before\nextensive fine-tuning or merging can substantially reduce both computational\nexpense and development time. Although prior approaches like scaling laws\naccount for global factors such as parameter size or training tokens, they\noften overlook explicit lineage relationships - i.e., which models are derived\nor merged from which parents. In this work, we propose a novel\nLineage-Regularized Matrix Factorization (LRMF) framework that encodes\nancestral ties among LLMs via a graph Laplacian regularizer. By leveraging\nmulti-hop parent-child connections, LRMF consistently outperforms conventional\nmatrix factorization and collaborative filtering methods in both instance-level\nand benchmark-level performance prediction. Our large-scale study includes\n2,934 publicly available Hugging Face models and 21,000+ instances across 6\nmajor benchmarks, showing that lineage constraints yield up to 7-10 percentage\npoints higher correlation with actual performance compared to baselines.\nMoreover, LRMF effectively addresses the cold-start problem, providing accurate\nestimates for newly derived or merged models even with minimal data. This\nlineage-guided strategy thus offers a resource-efficient way to inform\nhyperparameter tuning, data selection, and model combination in modern LLM\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Takuya Tamura"
                    },
                    {
                        "name": "Taro Yano"
                    },
                    {
                        "name": "Masafumi Enomoto"
                    },
                    {
                        "name": "Masafumi Oyamada"
                    }
                ],
                "author_detail": {
                    "name": "Masafumi Oyamada"
                },
                "author": "Masafumi Oyamada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19793v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19793v1",
                "updated": "2025-04-28T13:36:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    13,
                    36,
                    43,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T13:36:43Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    13,
                    36,
                    43,
                    0,
                    118,
                    0
                ],
                "title": "Prompt Injection Attack to Tool Selection in LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Injection Attack to Tool Selection in LLM Agents"
                },
                "summary": "Tool selection is a key component of LLM agents. The process operates through\na two-step mechanism - \\emph{retrieval} and \\emph{selection} - to pick the most\nappropriate tool from a tool library for a given task. In this work, we\nintroduce \\textit{ToolHijacker}, a novel prompt injection attack targeting tool\nselection in no-box scenarios. ToolHijacker injects a malicious tool document\ninto the tool library to manipulate the LLM agent's tool selection process,\ncompelling it to consistently choose the attacker's malicious tool for an\nattacker-chosen target task. Specifically, we formulate the crafting of such\ntool documents as an optimization problem and propose a two-phase optimization\nstrategy to solve it. Our extensive experimental evaluation shows that\nToolHijacker is highly effective, significantly outperforming existing\nmanual-based and automated prompt injection attacks when applied to tool\nselection. Moreover, we explore various defenses, including prevention-based\ndefenses (StruQ and SecAlign) and detection-based defenses (known-answer\ndetection, perplexity detection, and perplexity windowed detection). Our\nexperimental results indicate that these defenses are insufficient,\nhighlighting the urgent need for developing new defense strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool selection is a key component of LLM agents. The process operates through\na two-step mechanism - \\emph{retrieval} and \\emph{selection} - to pick the most\nappropriate tool from a tool library for a given task. In this work, we\nintroduce \\textit{ToolHijacker}, a novel prompt injection attack targeting tool\nselection in no-box scenarios. ToolHijacker injects a malicious tool document\ninto the tool library to manipulate the LLM agent's tool selection process,\ncompelling it to consistently choose the attacker's malicious tool for an\nattacker-chosen target task. Specifically, we formulate the crafting of such\ntool documents as an optimization problem and propose a two-phase optimization\nstrategy to solve it. Our extensive experimental evaluation shows that\nToolHijacker is highly effective, significantly outperforming existing\nmanual-based and automated prompt injection attacks when applied to tool\nselection. Moreover, we explore various defenses, including prevention-based\ndefenses (StruQ and SecAlign) and detection-based defenses (known-answer\ndetection, perplexity detection, and perplexity windowed detection). Our\nexperimental results indicate that these defenses are insufficient,\nhighlighting the urgent need for developing new defense strategies."
                },
                "authors": [
                    {
                        "name": "Jiawen Shi"
                    },
                    {
                        "name": "Zenghui Yuan"
                    },
                    {
                        "name": "Guiyao Tie"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Neil Zhenqiang Gong"
                    },
                    {
                        "name": "Lichao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lichao Sun"
                },
                "author": "Lichao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19793v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19793v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05474v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05474v2",
                "updated": "2025-04-28T13:22:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    13,
                    22,
                    11,
                    0,
                    118,
                    0
                ],
                "published": "2024-11-08T11:00:05Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    11,
                    0,
                    5,
                    4,
                    313,
                    0
                ],
                "title": "Enhancing Robustness in Language-Driven Robotics: A Modular Approach to\n  Failure Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Robustness in Language-Driven Robotics: A Modular Approach to\n  Failure Reduction"
                },
                "summary": "Recent advances in large language models (LLMs) have led to significant\nprogress in robotics, enabling embodied agents to better understand and execute\nopen-ended tasks. However, existing approaches using LLMs face limitations in\ngrounding their outputs within the physical environment and aligning with the\ncapabilities of the robot. This challenge becomes even more pronounced with\nsmaller language models, which are more computationally efficient but less\nrobust in task planning and execution. In this paper, we present a novel\nmodular architecture designed to enhance the robustness of LLM-driven robotics\nby addressing these grounding and alignment issues. We formalize the task\nplanning problem within a goal-conditioned POMDP framework, identify key\nfailure modes in LLM-driven planning, and propose targeted design principles to\nmitigate these issues. Our architecture introduces an ``expected outcomes''\nmodule to prevent mischaracterization of subgoals and a feedback mechanism to\nenable real-time error recovery. Experimental results, both in simulation and\non physical robots, demonstrate that our approach significantly improves task\nsuccess rates for pick-and-place and manipulation tasks compared to both larger\nLLMs and standard baselines. Through hardware experiments, we also demonstrate\nhow our architecture can be run efficiently and locally. This work highlights\nthe potential of smaller, locally-executable LLMs in robotics and provides a\nscalable, efficient solution for robust task execution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have led to significant\nprogress in robotics, enabling embodied agents to better understand and execute\nopen-ended tasks. However, existing approaches using LLMs face limitations in\ngrounding their outputs within the physical environment and aligning with the\ncapabilities of the robot. This challenge becomes even more pronounced with\nsmaller language models, which are more computationally efficient but less\nrobust in task planning and execution. In this paper, we present a novel\nmodular architecture designed to enhance the robustness of LLM-driven robotics\nby addressing these grounding and alignment issues. We formalize the task\nplanning problem within a goal-conditioned POMDP framework, identify key\nfailure modes in LLM-driven planning, and propose targeted design principles to\nmitigate these issues. Our architecture introduces an ``expected outcomes''\nmodule to prevent mischaracterization of subgoals and a feedback mechanism to\nenable real-time error recovery. Experimental results, both in simulation and\non physical robots, demonstrate that our approach significantly improves task\nsuccess rates for pick-and-place and manipulation tasks compared to both larger\nLLMs and standard baselines. Through hardware experiments, we also demonstrate\nhow our architecture can be run efficiently and locally. This work highlights\nthe potential of smaller, locally-executable LLMs in robotics and provides a\nscalable, efficient solution for robust task execution."
                },
                "authors": [
                    {
                        "name": "miland Garrab"
                    },
                    {
                        "name": "Pierre Teixeira"
                    },
                    {
                        "name": "Mahdi Khoramshahi"
                    },
                    {
                        "name": "Stphane Doncieux"
                    }
                ],
                "author_detail": {
                    "name": "Stphane Doncieux"
                },
                "author": "Stphane Doncieux",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05474v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05474v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T40 (Primary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03111v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03111v2",
                "updated": "2025-04-28T13:07:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    13,
                    7,
                    40,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-04T01:41:06Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    1,
                    41,
                    6,
                    4,
                    94,
                    0
                ],
                "title": "Les Dissonances: Cross-Tool Harvesting and Polluting in Multi-Tool\n  Empowered LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Les Dissonances: Cross-Tool Harvesting and Polluting in Multi-Tool\n  Empowered LLM Agents"
                },
                "summary": "Large Language Model (LLM) agents are autonomous systems powered by LLMs,\ncapable of reasoning and planning to solve problems by leveraging a set of\ntools. However, the integration of multi-tool capabilities in LLM agents\nintroduces challenges in securely managing tools, ensuring their compatibility,\nhandling dependency relationships, and protecting control flows within LLM\nagent workflows. In this paper, we present the first systematic security\nanalysis of task control flows in multi-tool-enabled LLM agents. We identify a\nnovel threat, Cross-Tool Harvesting and Polluting (XTHP), which includes\nmultiple attack vectors to first hijack the normal control flows of agent\ntasks, and then collect and pollute confidential or private information within\nLLM agent systems. To understand the impact of this threat, we developed Chord,\na dynamic scanning tool designed to automatically detect real-world agent tools\nsusceptible to XTHP attacks. Our evaluation of 66 real-world tools from the\nrepositories of two major LLM agent development frameworks, LangChain and\nLlamaIndex, revealed a significant security concern: 75\\% are vulnerable to\nXTHP attacks, highlighting the prevalence of this threat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents are autonomous systems powered by LLMs,\ncapable of reasoning and planning to solve problems by leveraging a set of\ntools. However, the integration of multi-tool capabilities in LLM agents\nintroduces challenges in securely managing tools, ensuring their compatibility,\nhandling dependency relationships, and protecting control flows within LLM\nagent workflows. In this paper, we present the first systematic security\nanalysis of task control flows in multi-tool-enabled LLM agents. We identify a\nnovel threat, Cross-Tool Harvesting and Polluting (XTHP), which includes\nmultiple attack vectors to first hijack the normal control flows of agent\ntasks, and then collect and pollute confidential or private information within\nLLM agent systems. To understand the impact of this threat, we developed Chord,\na dynamic scanning tool designed to automatically detect real-world agent tools\nsusceptible to XTHP attacks. Our evaluation of 66 real-world tools from the\nrepositories of two major LLM agent development frameworks, LangChain and\nLlamaIndex, revealed a significant security concern: 75\\% are vulnerable to\nXTHP attacks, highlighting the prevalence of this threat."
                },
                "authors": [
                    {
                        "name": "Zichuan Li"
                    },
                    {
                        "name": "Jian Cui"
                    },
                    {
                        "name": "Xiaojing Liao"
                    },
                    {
                        "name": "Luyi Xing"
                    }
                ],
                "author_detail": {
                    "name": "Luyi Xing"
                },
                "author": "Luyi Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03111v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03111v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19759v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19759v1",
                "updated": "2025-04-28T12:56:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    56,
                    36,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T12:56:36Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    56,
                    36,
                    0,
                    118,
                    0
                ],
                "title": "Moral Reasoning Across Languages: The Critical Role of Low-Resource\n  Languages in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moral Reasoning Across Languages: The Critical Role of Low-Resource\n  Languages in LLMs"
                },
                "summary": "In this paper, we introduce the Multilingual Moral Reasoning Benchmark (MMRB)\nto evaluate the moral reasoning abilities of large language models (LLMs)\nacross five typologically diverse languages and three levels of contextual\ncomplexity: sentence, paragraph, and document. Our results show moral reasoning\nperformance degrades with increasing context complexity, particularly for\nlow-resource languages such as Vietnamese. We further fine-tune the open-source\nLLaMA-3-8B model using curated monolingual data for alignment and poisoning.\nSurprisingly, low-resource languages have a stronger impact on multilingual\nreasoning than high-resource ones, highlighting their critical role in\nmultilingual NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce the Multilingual Moral Reasoning Benchmark (MMRB)\nto evaluate the moral reasoning abilities of large language models (LLMs)\nacross five typologically diverse languages and three levels of contextual\ncomplexity: sentence, paragraph, and document. Our results show moral reasoning\nperformance degrades with increasing context complexity, particularly for\nlow-resource languages such as Vietnamese. We further fine-tune the open-source\nLLaMA-3-8B model using curated monolingual data for alignment and poisoning.\nSurprisingly, low-resource languages have a stronger impact on multilingual\nreasoning than high-resource ones, highlighting their critical role in\nmultilingual NLP."
                },
                "authors": [
                    {
                        "name": "Huichi Zhou"
                    },
                    {
                        "name": "Zehao Xu"
                    },
                    {
                        "name": "Munan Zhao"
                    },
                    {
                        "name": "Kaihong Li"
                    },
                    {
                        "name": "Yiqiang Li"
                    },
                    {
                        "name": "Hongtao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongtao Wang"
                },
                "author": "Hongtao Wang",
                "arxiv_comment": "5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19759v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19759v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04178v2",
                "updated": "2025-04-28T12:53:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    53,
                    23,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-05T13:48:33Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    13,
                    48,
                    33,
                    5,
                    95,
                    0
                ],
                "title": "MSL: Not All Tokens Are What You Need for Tuning LLM as a Recommender",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSL: Not All Tokens Are What You Need for Tuning LLM as a Recommender"
                },
                "summary": "Large language models (LLMs), known for their comprehension capabilities and\nextensive knowledge, have been increasingly applied to recommendation systems\n(RS). Given the fundamental gap between the mechanism of LLMs and the\nrequirement of RS, researchers have focused on fine-tuning LLMs with\nrecommendation-specific data to enhance their performance. Language Modeling\nLoss (LML), originally designed for language generation tasks, is commonly\nadopted. However, we identify two critical limitations of LML: 1) it exhibits\nsignificant divergence from the recommendation objective; 2) it erroneously\ntreats all fictitious item descriptions as negative samples, introducing\nmisleading training signals.\n  To address these limitations, we propose a novel Masked Softmax Loss (MSL)\ntailored for fine-tuning LLMs on recommendation. MSL improves LML by\nidentifying and masking invalid tokens that could lead to fictitious item\ndescriptions during loss computation. This strategy can effectively avoid the\ninterference from erroneous negative signals and ensure well alignment with the\nrecommendation objective supported by theoretical guarantees. During\nimplementation, we identify a potential challenge related to gradient vanishing\nof MSL. To overcome this, we further introduce the temperature coefficient and\npropose an Adaptive Temperature Strategy (ATS) that adaptively adjusts the\ntemperature without requiring extensive hyperparameter tuning. Extensive\nexperiments conducted on four public datasets further validate the\neffectiveness of MSL, achieving an average improvement of 42.24% in NDCG@10.\nThe code is available at https://github.com/WANGBohaO-jpg/MSL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), known for their comprehension capabilities and\nextensive knowledge, have been increasingly applied to recommendation systems\n(RS). Given the fundamental gap between the mechanism of LLMs and the\nrequirement of RS, researchers have focused on fine-tuning LLMs with\nrecommendation-specific data to enhance their performance. Language Modeling\nLoss (LML), originally designed for language generation tasks, is commonly\nadopted. However, we identify two critical limitations of LML: 1) it exhibits\nsignificant divergence from the recommendation objective; 2) it erroneously\ntreats all fictitious item descriptions as negative samples, introducing\nmisleading training signals.\n  To address these limitations, we propose a novel Masked Softmax Loss (MSL)\ntailored for fine-tuning LLMs on recommendation. MSL improves LML by\nidentifying and masking invalid tokens that could lead to fictitious item\ndescriptions during loss computation. This strategy can effectively avoid the\ninterference from erroneous negative signals and ensure well alignment with the\nrecommendation objective supported by theoretical guarantees. During\nimplementation, we identify a potential challenge related to gradient vanishing\nof MSL. To overcome this, we further introduce the temperature coefficient and\npropose an Adaptive Temperature Strategy (ATS) that adaptively adjusts the\ntemperature without requiring extensive hyperparameter tuning. Extensive\nexperiments conducted on four public datasets further validate the\neffectiveness of MSL, achieving an average improvement of 42.24% in NDCG@10.\nThe code is available at https://github.com/WANGBohaO-jpg/MSL."
                },
                "authors": [
                    {
                        "name": "Bohao Wang"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Xingyu Lou"
                    },
                    {
                        "name": "Changwang Zhang"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Yuegang Sun"
                    },
                    {
                        "name": "Yan Feng"
                    },
                    {
                        "name": "Chun Chen"
                    },
                    {
                        "name": "Can Wang"
                    }
                ],
                "author_detail": {
                    "name": "Can Wang"
                },
                "author": "Can Wang",
                "arxiv_comment": "Accepted by SIGIR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19754v1",
                "updated": "2025-04-28T12:52:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    52,
                    5,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T12:52:05Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    52,
                    5,
                    0,
                    118,
                    0
                ],
                "title": "Reconstructing Context: Evaluating Advanced Chunking Strategies for\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing Context: Evaluating Advanced Chunking Strategies for\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) has become a transformative approach for\nenhancing large language models (LLMs) by grounding their outputs in external\nknowledge sources. Yet, a critical question persists: how can vast volumes of\nexternal knowledge be managed effectively within the input constraints of LLMs?\nTraditional methods address this by chunking external documents into smaller,\nfixed-size segments. While this approach alleviates input limitations, it often\nfragments context, resulting in incomplete retrieval and diminished coherence\nin generation. To overcome these shortcomings, two advanced techniques, late\nchunking and contextual retrieval, have been introduced, both aiming to\npreserve global context. Despite their potential, their comparative strengths\nand limitations remain unclear. This study presents a rigorous analysis of late\nchunking and contextual retrieval, evaluating their effectiveness and\nefficiency in optimizing RAG systems. Our results indicate that contextual\nretrieval preserves semantic coherence more effectively but requires greater\ncomputational resources. In contrast, late chunking offers higher efficiency\nbut tends to sacrifice relevance and completeness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has become a transformative approach for\nenhancing large language models (LLMs) by grounding their outputs in external\nknowledge sources. Yet, a critical question persists: how can vast volumes of\nexternal knowledge be managed effectively within the input constraints of LLMs?\nTraditional methods address this by chunking external documents into smaller,\nfixed-size segments. While this approach alleviates input limitations, it often\nfragments context, resulting in incomplete retrieval and diminished coherence\nin generation. To overcome these shortcomings, two advanced techniques, late\nchunking and contextual retrieval, have been introduced, both aiming to\npreserve global context. Despite their potential, their comparative strengths\nand limitations remain unclear. This study presents a rigorous analysis of late\nchunking and contextual retrieval, evaluating their effectiveness and\nefficiency in optimizing RAG systems. Our results indicate that contextual\nretrieval preserves semantic coherence more effectively but requires greater\ncomputational resources. In contrast, late chunking offers higher efficiency\nbut tends to sacrifice relevance and completeness."
                },
                "authors": [
                    {
                        "name": "Carlo Merola"
                    },
                    {
                        "name": "Jaspinder Singh"
                    }
                ],
                "author_detail": {
                    "name": "Jaspinder Singh"
                },
                "author": "Jaspinder Singh",
                "arxiv_comment": "13 pages, 2 figures, Second Workshop on Knowledge-Enhanced\n  Information Retrieval, ECIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19746v1",
                "updated": "2025-04-28T12:47:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    47,
                    23,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T12:47:23Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    47,
                    23,
                    0,
                    118,
                    0
                ],
                "title": "FineQ: Software-Hardware Co-Design for Low-Bit Fine-Grained\n  Mixed-Precision Quantization of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineQ: Software-Hardware Co-Design for Low-Bit Fine-Grained\n  Mixed-Precision Quantization of LLMs"
                },
                "summary": "Large language models (LLMs) have significantly advanced the natural language\nprocessing paradigm but impose substantial demands on memory and computational\nresources. Quantization is one of the most effective ways to reduce memory\nconsumption of LLMs. However, advanced single-precision quantization methods\nexperience significant accuracy degradation when quantizing to ultra-low bits.\nExisting mixed-precision quantization methods are quantized by groups with\ncoarse granularity. Employing high precision for group data leads to\nsubstantial memory overhead, whereas low precision severely impacts model\naccuracy. To address this issue, we propose FineQ, software-hardware co-design\nfor low-bit fine-grained mixed-precision quantization of LLMs. First, FineQ\npartitions the weights into finer-grained clusters and considers the\ndistribution of outliers within these clusters, thus achieving a balance\nbetween model accuracy and memory overhead. Then, we propose an outlier\nprotection mechanism within clusters that uses 3 bits to represent outliers and\nintroduce an encoding scheme for index and data concatenation to enable aligned\nmemory access. Finally, we introduce an accelerator utilizing temporal coding\nthat effectively supports the quantization algorithm while simplifying the\nmultipliers in the systolic array. FineQ achieves higher model accuracy\ncompared to the SOTA mixed-precision quantization algorithm at a close average\nbit-width. Meanwhile, the accelerator achieves up to 1.79x energy efficiency\nand reduces the area of the systolic array by 61.2%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly advanced the natural language\nprocessing paradigm but impose substantial demands on memory and computational\nresources. Quantization is one of the most effective ways to reduce memory\nconsumption of LLMs. However, advanced single-precision quantization methods\nexperience significant accuracy degradation when quantizing to ultra-low bits.\nExisting mixed-precision quantization methods are quantized by groups with\ncoarse granularity. Employing high precision for group data leads to\nsubstantial memory overhead, whereas low precision severely impacts model\naccuracy. To address this issue, we propose FineQ, software-hardware co-design\nfor low-bit fine-grained mixed-precision quantization of LLMs. First, FineQ\npartitions the weights into finer-grained clusters and considers the\ndistribution of outliers within these clusters, thus achieving a balance\nbetween model accuracy and memory overhead. Then, we propose an outlier\nprotection mechanism within clusters that uses 3 bits to represent outliers and\nintroduce an encoding scheme for index and data concatenation to enable aligned\nmemory access. Finally, we introduce an accelerator utilizing temporal coding\nthat effectively supports the quantization algorithm while simplifying the\nmultipliers in the systolic array. FineQ achieves higher model accuracy\ncompared to the SOTA mixed-precision quantization algorithm at a close average\nbit-width. Meanwhile, the accelerator achieves up to 1.79x energy efficiency\nand reduces the area of the systolic array by 61.2%."
                },
                "authors": [
                    {
                        "name": "Xilong Xie"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Limin Xiao"
                    },
                    {
                        "name": "Meng Han"
                    },
                    {
                        "name": "Lin Sun"
                    },
                    {
                        "name": "Shuai Zheng"
                    },
                    {
                        "name": "Xiangrong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangrong Xu"
                },
                "author": "Xiangrong Xu",
                "arxiv_comment": "DATE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19736v1",
                "updated": "2025-04-28T12:33:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    33,
                    19,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T12:33:19Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    33,
                    19,
                    0,
                    118,
                    0
                ],
                "title": "UTTG_ A Universal Teleoperation Approach via Online Trajectory\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UTTG_ A Universal Teleoperation Approach via Online Trajectory\n  Generation"
                },
                "summary": "Teleoperation is crucial for hazardous environment operations and serves as a\nkey tool for collecting expert demonstrations in robot learning. However,\nexisting methods face robotic hardware dependency and control frequency\nmismatches between teleoperation devices and robotic platforms. Our approach\nautomatically extracts kinematic parameters from unified robot description\nformat (URDF) files, and enables pluggable deployment across diverse robots\nthrough uniform interfaces. The proposed interpolation algorithm bridges the\nfrequency gap between low-rate human inputs and high-frequency robotic control\ncommands through online continuous trajectory generation, \\n{while requiring no\naccess to the closed, bottom-level control loop}. To enhance trajectory\nsmoothness, we introduce a minimum-stretch spline that optimizes the motion\nquality. The system further provides precision and rapid modes to accommodate\ndifferent task requirements. Experiments across various robotic platforms\nincluding dual-arm ones demonstrate generality and smooth operation performance\nof our methods. The code is developed in C++ with python interface, and\navailable at https://github.com/IRMV-Manipulation-Group/UTTG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teleoperation is crucial for hazardous environment operations and serves as a\nkey tool for collecting expert demonstrations in robot learning. However,\nexisting methods face robotic hardware dependency and control frequency\nmismatches between teleoperation devices and robotic platforms. Our approach\nautomatically extracts kinematic parameters from unified robot description\nformat (URDF) files, and enables pluggable deployment across diverse robots\nthrough uniform interfaces. The proposed interpolation algorithm bridges the\nfrequency gap between low-rate human inputs and high-frequency robotic control\ncommands through online continuous trajectory generation, \\n{while requiring no\naccess to the closed, bottom-level control loop}. To enhance trajectory\nsmoothness, we introduce a minimum-stretch spline that optimizes the motion\nquality. The system further provides precision and rapid modes to accommodate\ndifferent task requirements. Experiments across various robotic platforms\nincluding dual-arm ones demonstrate generality and smooth operation performance\nof our methods. The code is developed in C++ with python interface, and\navailable at https://github.com/IRMV-Manipulation-Group/UTTG."
                },
                "authors": [
                    {
                        "name": "Shengjian Fang"
                    },
                    {
                        "name": "Yixuan Zhou"
                    },
                    {
                        "name": "Yu Zheng"
                    },
                    {
                        "name": "Pengyu Jiang"
                    },
                    {
                        "name": "Siyuan Liu"
                    },
                    {
                        "name": "Hesheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hesheng Wang"
                },
                "author": "Hesheng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19734v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19734v1",
                "updated": "2025-04-28T12:31:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    31,
                    38,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T12:31:38Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    31,
                    38,
                    0,
                    118,
                    0
                ],
                "title": "LLM-Assisted Automated Deductive Coding of Dialogue Data: Leveraging\n  Dialogue-Specific Characteristics to Enhance Contextual Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Assisted Automated Deductive Coding of Dialogue Data: Leveraging\n  Dialogue-Specific Characteristics to Enhance Contextual Understanding"
                },
                "summary": "Dialogue data has been a key source for understanding learning processes,\noffering critical insights into how students engage in collaborative\ndiscussions and how these interactions shape their knowledge construction. The\nadvent of Large Language Models (LLMs) has introduced promising opportunities\nfor advancing qualitative research, particularly in the automated coding of\ndialogue data. However, the inherent contextual complexity of dialogue presents\nunique challenges for these models, especially in understanding and\ninterpreting complex contextual information. This study addresses these\nchallenges by developing a novel LLM-assisted automated coding approach for\ndialogue data. The novelty of our proposed framework is threefold: 1) We\npredict the code for an utterance based on dialogue-specific characteristics --\ncommunicative acts and communicative events -- using separate prompts following\nthe role prompts and chain-of-thoughts methods; 2) We engaged multiple LLMs\nincluding GPT-4-turbo, GPT-4o, DeepSeek in collaborative code prediction; 3) We\nleveraged the interrelation between events and acts to implement consistency\nchecking using GPT-4o. In particular, our contextual consistency checking\nprovided a substantial accuracy improvement. We also found the accuracy of act\npredictions was consistently higher than that of event predictions. This study\ncontributes a new methodological framework for enhancing the precision of\nautomated coding of dialogue data as well as offers a scalable solution for\naddressing the contextual challenges inherent in dialogue analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue data has been a key source for understanding learning processes,\noffering critical insights into how students engage in collaborative\ndiscussions and how these interactions shape their knowledge construction. The\nadvent of Large Language Models (LLMs) has introduced promising opportunities\nfor advancing qualitative research, particularly in the automated coding of\ndialogue data. However, the inherent contextual complexity of dialogue presents\nunique challenges for these models, especially in understanding and\ninterpreting complex contextual information. This study addresses these\nchallenges by developing a novel LLM-assisted automated coding approach for\ndialogue data. The novelty of our proposed framework is threefold: 1) We\npredict the code for an utterance based on dialogue-specific characteristics --\ncommunicative acts and communicative events -- using separate prompts following\nthe role prompts and chain-of-thoughts methods; 2) We engaged multiple LLMs\nincluding GPT-4-turbo, GPT-4o, DeepSeek in collaborative code prediction; 3) We\nleveraged the interrelation between events and acts to implement consistency\nchecking using GPT-4o. In particular, our contextual consistency checking\nprovided a substantial accuracy improvement. We also found the accuracy of act\npredictions was consistently higher than that of event predictions. This study\ncontributes a new methodological framework for enhancing the precision of\nautomated coding of dialogue data as well as offers a scalable solution for\naddressing the contextual challenges inherent in dialogue analysis."
                },
                "authors": [
                    {
                        "name": "Ying Na"
                    },
                    {
                        "name": "Shihui Feng"
                    }
                ],
                "author_detail": {
                    "name": "Shihui Feng"
                },
                "author": "Shihui Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19734v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19730v1",
                "updated": "2025-04-28T12:28:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    28,
                    55,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T12:28:55Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    28,
                    55,
                    0,
                    118,
                    0
                ],
                "title": "Evaluate-and-Purify: Fortifying Code Language Models Against Adversarial\n  Attacks Using LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluate-and-Purify: Fortifying Code Language Models Against Adversarial\n  Attacks Using LLM-as-a-Judge"
                },
                "summary": "The widespread adoption of code language models in software engineering tasks\nhas exposed vulnerabilities to adversarial attacks, especially the identifier\nsubstitution attacks. Although existing identifier substitution attackers\ndemonstrate high success rates, they often produce adversarial examples with\nunnatural code patterns. In this paper, we systematically assess the quality of\nadversarial examples using LLM-as-a-Judge. Our analysis reveals that over 80%\nof adversarial examples generated by state-of-the-art identifier substitution\nattackers (e.g., ALERT) are actually detectable. Based on this insight, we\npropose EP-Shield, a unified framework for evaluating and purifying identifier\nsubstitution attacks via naturalness-aware reasoning. Specifically, we first\nevaluate the naturalness of code and identify the perturbed adversarial code,\nthen purify it so that the victim model can restore correct prediction.\nExtensive experiments demonstrate the superiority of EP-Shield over adversarial\nfine-tuning (up to 83.36% improvement) and its lightweight design 7B\nparameters) with GPT-4-level performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of code language models in software engineering tasks\nhas exposed vulnerabilities to adversarial attacks, especially the identifier\nsubstitution attacks. Although existing identifier substitution attackers\ndemonstrate high success rates, they often produce adversarial examples with\nunnatural code patterns. In this paper, we systematically assess the quality of\nadversarial examples using LLM-as-a-Judge. Our analysis reveals that over 80%\nof adversarial examples generated by state-of-the-art identifier substitution\nattackers (e.g., ALERT) are actually detectable. Based on this insight, we\npropose EP-Shield, a unified framework for evaluating and purifying identifier\nsubstitution attacks via naturalness-aware reasoning. Specifically, we first\nevaluate the naturalness of code and identify the perturbed adversarial code,\nthen purify it so that the victim model can restore correct prediction.\nExtensive experiments demonstrate the superiority of EP-Shield over adversarial\nfine-tuning (up to 83.36% improvement) and its lightweight design 7B\nparameters) with GPT-4-level performance."
                },
                "authors": [
                    {
                        "name": "Wenhan Mu"
                    },
                    {
                        "name": "Ling Xu"
                    },
                    {
                        "name": "Shuren Pei"
                    },
                    {
                        "name": "Le Mi"
                    },
                    {
                        "name": "Huichi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Huichi Zhou"
                },
                "author": "Huichi Zhou",
                "arxiv_comment": "25 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19728v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19728v1",
                "updated": "2025-04-28T12:28:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    28,
                    39,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T12:28:39Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    28,
                    39,
                    0,
                    118,
                    0
                ],
                "title": "Hector UI: A Flexible Human-Robot User Interface for (Semi-)Autonomous\n  Rescue and Inspection Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hector UI: A Flexible Human-Robot User Interface for (Semi-)Autonomous\n  Rescue and Inspection Robots"
                },
                "summary": "The remote human operator's user interface (UI) is an important link to make\nthe robot an efficient extension of the operator's perception and action. In\nrescue applications, several studies have investigated the design of operator\ninterfaces based on observations during major robotics competitions or field\ndeployments. Based on this research, guidelines for good interface design were\nempirically identified. The investigations on the UIs of teams participating in\ncompetitions are often based on external observations during UI application,\nwhich may miss some relevant requirements for UI flexibility. In this work, we\npresent an open-source and flexibly configurable user interface based on\nestablished guidelines and its exemplary use for wheeled, tracked, and walking\nrobots. We explain the design decisions and cover the insights we have gained\nduring its highly successful applications in multiple robotics competitions and\nevaluations. The presented UI can also be adapted for other robots with little\neffort and is available as open source.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remote human operator's user interface (UI) is an important link to make\nthe robot an efficient extension of the operator's perception and action. In\nrescue applications, several studies have investigated the design of operator\ninterfaces based on observations during major robotics competitions or field\ndeployments. Based on this research, guidelines for good interface design were\nempirically identified. The investigations on the UIs of teams participating in\ncompetitions are often based on external observations during UI application,\nwhich may miss some relevant requirements for UI flexibility. In this work, we\npresent an open-source and flexibly configurable user interface based on\nestablished guidelines and its exemplary use for wheeled, tracked, and walking\nrobots. We explain the design decisions and cover the insights we have gained\nduring its highly successful applications in multiple robotics competitions and\nevaluations. The presented UI can also be adapted for other robots with little\neffort and is available as open source."
                },
                "authors": [
                    {
                        "name": "Stefan Fabian"
                    },
                    {
                        "name": "Oskar von Stryk"
                    }
                ],
                "author_detail": {
                    "name": "Oskar von Stryk"
                },
                "author": "Oskar von Stryk",
                "arxiv_doi": "10.1109/SSRR59696.2023.10499954",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/SSRR59696.2023.10499954",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.19728v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19728v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "2023 IEEE International Symposium on Safety, Security, and Rescue\n  Robotics (SSRR), Naraha, Fukushima, Japan, 2023, pp. 91-98",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03108v2",
                "updated": "2025-04-28T12:27:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    27,
                    25,
                    0,
                    118,
                    0
                ],
                "published": "2025-03-05T02:08:12Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    2,
                    8,
                    12,
                    2,
                    64,
                    0
                ],
                "title": "SoK: Knowledge is All You Need: Accelerating Last Mile Delivery for\n  Automated Provenance-based Intrusion Detection with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: Knowledge is All You Need: Accelerating Last Mile Delivery for\n  Automated Provenance-based Intrusion Detection with LLMs"
                },
                "summary": "Recently, provenance-based intrusion detection systems (PIDSes) have been\nwidely proposed for endpoint threat analysis. However, due to the lack of\nsystematic integration and utilization of knowledge, existing PIDSes still\nrequire significant manual intervention for practical deployment, making full\nautomation challenging. This paper presents a disruptive innovation by\ncategorizing PIDSes according to the types of knowledge they utilize. In\nresponse to the prevalent issue of ``knowledge silos problem'' in existing\nresearch, we introduce a novel knowledge-driven provenance-based intrusion\ndetection framework, powered by large language models (LLMs). We also present\nOmniSec, a best practice system built upon this framework. By integrating\nattack representation knowledge, threat intelligence knowledge, and benign\nbehavior knowledge, OmniSec outperforms the state-of-the-art approaches on\npublic benchmark datasets. OmniSec is available online at\nhttps://anonymous.4open.science/r/PIDS-with-LLM-613B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, provenance-based intrusion detection systems (PIDSes) have been\nwidely proposed for endpoint threat analysis. However, due to the lack of\nsystematic integration and utilization of knowledge, existing PIDSes still\nrequire significant manual intervention for practical deployment, making full\nautomation challenging. This paper presents a disruptive innovation by\ncategorizing PIDSes according to the types of knowledge they utilize. In\nresponse to the prevalent issue of ``knowledge silos problem'' in existing\nresearch, we introduce a novel knowledge-driven provenance-based intrusion\ndetection framework, powered by large language models (LLMs). We also present\nOmniSec, a best practice system built upon this framework. By integrating\nattack representation knowledge, threat intelligence knowledge, and benign\nbehavior knowledge, OmniSec outperforms the state-of-the-art approaches on\npublic benchmark datasets. OmniSec is available online at\nhttps://anonymous.4open.science/r/PIDS-with-LLM-613B."
                },
                "authors": [
                    {
                        "name": "Wenrui Cheng"
                    },
                    {
                        "name": "Tiantian Zhu"
                    },
                    {
                        "name": "Chunlin Xiong"
                    },
                    {
                        "name": "Haofei Sun"
                    },
                    {
                        "name": "Zijun Wang"
                    },
                    {
                        "name": "Shunan Jing"
                    },
                    {
                        "name": "Mingqi Lv"
                    },
                    {
                        "name": "Yan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yan Chen"
                },
                "author": "Yan Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11983v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11983v2",
                "updated": "2025-04-28T12:17:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    17,
                    25,
                    0,
                    118,
                    0
                ],
                "published": "2024-12-16T17:04:40Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    4,
                    40,
                    0,
                    351,
                    0
                ],
                "title": "Leveraging Large Language Models for Effective Label-free Node\n  Classification in Text-Attributed Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Effective Label-free Node\n  Classification in Text-Attributed Graphs"
                },
                "summary": "Graph neural networks (GNNs) have become the preferred models for node\nclassification in graph data due to their robust capabilities in integrating\ngraph structures and attributes. However, these models heavily depend on a\nsubstantial amount of high-quality labeled data for training, which is often\ncostly to obtain. With the rise of large language models (LLMs), a promising\napproach is to utilize their exceptional zero-shot capabilities and extensive\nknowledge for node labeling. Despite encouraging results, this approach either\nrequires numerous queries to LLMs or suffers from reduced performance due to\nnoisy labels generated by LLMs. To address these challenges, we introduce\nLocle, an active self-training framework that does Label-free node\nClassification with LLMs cost-Effectively. Locle iteratively identifies small\nsets of \"critical\" samples using GNNs and extracts informative pseudo-labels\nfor them with both LLMs and GNNs, serving as additional supervision signals to\nenhance model training. Specifically, Locle comprises three key components: (i)\nan effective active node selection strategy for initial annotations; (ii) a\ncareful sample selection scheme to identify \"critical\" nodes based on label\ndisharmonicity and entropy; and (iii) a label refinement module that combines\nLLMs and GNNs with a rewired topology. Extensive experiments on five benchmark\ntext-attributed graph datasets demonstrate that Locle significantly outperforms\nstate-of-the-art methods under the same query budget to LLMs in terms of\nlabel-free node classification. Notably, on the DBLP dataset with 14.3k nodes,\nLocle achieves an 8.08% improvement in accuracy over the state-of-the-art at a\ncost of less than one cent. Our code is available at\nhttps://github.com/HKBU-LAGAS/Locle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) have become the preferred models for node\nclassification in graph data due to their robust capabilities in integrating\ngraph structures and attributes. However, these models heavily depend on a\nsubstantial amount of high-quality labeled data for training, which is often\ncostly to obtain. With the rise of large language models (LLMs), a promising\napproach is to utilize their exceptional zero-shot capabilities and extensive\nknowledge for node labeling. Despite encouraging results, this approach either\nrequires numerous queries to LLMs or suffers from reduced performance due to\nnoisy labels generated by LLMs. To address these challenges, we introduce\nLocle, an active self-training framework that does Label-free node\nClassification with LLMs cost-Effectively. Locle iteratively identifies small\nsets of \"critical\" samples using GNNs and extracts informative pseudo-labels\nfor them with both LLMs and GNNs, serving as additional supervision signals to\nenhance model training. Specifically, Locle comprises three key components: (i)\nan effective active node selection strategy for initial annotations; (ii) a\ncareful sample selection scheme to identify \"critical\" nodes based on label\ndisharmonicity and entropy; and (iii) a label refinement module that combines\nLLMs and GNNs with a rewired topology. Extensive experiments on five benchmark\ntext-attributed graph datasets demonstrate that Locle significantly outperforms\nstate-of-the-art methods under the same query budget to LLMs in terms of\nlabel-free node classification. Notably, on the DBLP dataset with 14.3k nodes,\nLocle achieves an 8.08% improvement in accuracy over the state-of-the-art at a\ncost of less than one cent. Our code is available at\nhttps://github.com/HKBU-LAGAS/Locle."
                },
                "authors": [
                    {
                        "name": "Taiyan Zhang"
                    },
                    {
                        "name": "Renchi Yang"
                    },
                    {
                        "name": "Yurui Lai"
                    },
                    {
                        "name": "Mingyu Yan"
                    },
                    {
                        "name": "Xiaochun Ye"
                    },
                    {
                        "name": "Dongrui Fan"
                    }
                ],
                "author_detail": {
                    "name": "Dongrui Fan"
                },
                "author": "Dongrui Fan",
                "arxiv_comment": "14 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11983v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11983v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19722v1",
                "updated": "2025-04-28T12:15:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    15,
                    42,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T12:15:42Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    15,
                    42,
                    0,
                    118,
                    0
                ],
                "title": "The ATLAS of Traffic Lights: A Reliable Perception Framework for\n  Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ATLAS of Traffic Lights: A Reliable Perception Framework for\n  Autonomous Driving"
                },
                "summary": "Traffic light perception is an essential component of the camera-based\nperception system for autonomous vehicles, enabling accurate detection and\ninterpretation of traffic lights to ensure safe navigation through complex\nurban environments. In this work, we propose a modularized perception framework\nthat integrates state-of-the-art detection models with a novel real-time\nassociation and decision framework, enabling seamless deployment into an\nautonomous driving stack. To address the limitations of existing public\ndatasets, we introduce the ATLAS dataset, which provides comprehensive\nannotations of traffic light states and pictograms across diverse environmental\nconditions and camera setups. This dataset is publicly available at\nhttps://url.fzi.de/ATLAS. We train and evaluate several state-of-the-art\ntraffic light detection architectures on ATLAS, demonstrating significant\nperformance improvements in both accuracy and robustness. Finally, we evaluate\nthe framework in real-world scenarios by deploying it in an autonomous vehicle\nto make decisions at traffic light-controlled intersections, highlighting its\nreliability and effectiveness for real-time operation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic light perception is an essential component of the camera-based\nperception system for autonomous vehicles, enabling accurate detection and\ninterpretation of traffic lights to ensure safe navigation through complex\nurban environments. In this work, we propose a modularized perception framework\nthat integrates state-of-the-art detection models with a novel real-time\nassociation and decision framework, enabling seamless deployment into an\nautonomous driving stack. To address the limitations of existing public\ndatasets, we introduce the ATLAS dataset, which provides comprehensive\nannotations of traffic light states and pictograms across diverse environmental\nconditions and camera setups. This dataset is publicly available at\nhttps://url.fzi.de/ATLAS. We train and evaluate several state-of-the-art\ntraffic light detection architectures on ATLAS, demonstrating significant\nperformance improvements in both accuracy and robustness. Finally, we evaluate\nthe framework in real-world scenarios by deploying it in an autonomous vehicle\nto make decisions at traffic light-controlled intersections, highlighting its\nreliability and effectiveness for real-time operation."
                },
                "authors": [
                    {
                        "name": "Rupert Polley"
                    },
                    {
                        "name": "Nikolai Polley"
                    },
                    {
                        "name": "Dominik Heid"
                    },
                    {
                        "name": "Marc Heinrich"
                    },
                    {
                        "name": "Sven Ochs"
                    },
                    {
                        "name": "J. Marius Zllner"
                    }
                ],
                "author_detail": {
                    "name": "J. Marius Zllner"
                },
                "author": "J. Marius Zllner",
                "arxiv_comment": "Accepted at IEEE Intelligent Vehicles Symposium (IV 2025). Dataset\n  link: https://url.fzi.de/ATLAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19720v1",
                "updated": "2025-04-28T12:14:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    14,
                    2,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T12:14:02Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    14,
                    2,
                    0,
                    118,
                    0
                ],
                "title": "Taming the Titans: A Survey of Efficient LLM Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming the Titans: A Survey of Efficient LLM Inference Serving"
                },
                "summary": "Large Language Models (LLMs) for Generative AI have achieved remarkable\nprogress, evolving into sophisticated and versatile tools widely adopted across\nvarious domains and applications. However, the substantial memory overhead\ncaused by their vast number of parameters, combined with the high computational\ndemands of the attention mechanism, poses significant challenges in achieving\nlow latency and high throughput for LLM inference services. Recent\nadvancements, driven by groundbreaking research, have significantly accelerated\nprogress in this field. This paper provides a comprehensive survey of these\nmethods, covering fundamental instance-level approaches, in-depth cluster-level\nstrategies, emerging scenario directions, and other miscellaneous but important\nareas. At the instance level, we review model placement, request scheduling,\ndecoding length prediction, storage management, and the disaggregation\nparadigm. At the cluster level, we explore GPU cluster deployment,\nmulti-instance load balancing, and cloud service solutions. For emerging\nscenarios, we organize the discussion around specific tasks, modules, and\nauxiliary methods. To ensure a holistic overview, we also highlight several\nniche yet critical areas. Finally, we outline potential research directions to\nfurther advance the field of LLM inference serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) for Generative AI have achieved remarkable\nprogress, evolving into sophisticated and versatile tools widely adopted across\nvarious domains and applications. However, the substantial memory overhead\ncaused by their vast number of parameters, combined with the high computational\ndemands of the attention mechanism, poses significant challenges in achieving\nlow latency and high throughput for LLM inference services. Recent\nadvancements, driven by groundbreaking research, have significantly accelerated\nprogress in this field. This paper provides a comprehensive survey of these\nmethods, covering fundamental instance-level approaches, in-depth cluster-level\nstrategies, emerging scenario directions, and other miscellaneous but important\nareas. At the instance level, we review model placement, request scheduling,\ndecoding length prediction, storage management, and the disaggregation\nparadigm. At the cluster level, we explore GPU cluster deployment,\nmulti-instance load balancing, and cloud service solutions. For emerging\nscenarios, we organize the discussion around specific tasks, modules, and\nauxiliary methods. To ensure a holistic overview, we also highlight several\nniche yet critical areas. Finally, we outline potential research directions to\nfurther advance the field of LLM inference serving."
                },
                "authors": [
                    {
                        "name": "Ranran Zhen"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Yixin Ji"
                    },
                    {
                        "name": "Zhenlin Yang"
                    },
                    {
                        "name": "Tong Liu"
                    },
                    {
                        "name": "Qingrong Xia"
                    },
                    {
                        "name": "Xinyu Duan"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Baoxing Huai"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "work in progress;11 pages of main paper with 7 main figures, overall\n  20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19706v1",
                "updated": "2025-04-28T12:00:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    0,
                    10,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T12:00:10Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    0,
                    10,
                    0,
                    118,
                    0
                ],
                "title": "Open-set Anomaly Segmentation in Complex Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-set Anomaly Segmentation in Complex Scenarios"
                },
                "summary": "Precise segmentation of out-of-distribution (OoD) objects, herein referred to\nas anomalies, is crucial for the reliable deployment of semantic segmentation\nmodels in open-set, safety-critical applications, such as autonomous driving.\nCurrent anomalous segmentation benchmarks predominantly focus on favorable\nweather conditions, resulting in untrustworthy evaluations that overlook the\nrisks posed by diverse meteorological conditions in open-set environments, such\nas low illumination, dense fog, and heavy rain. To bridge this gap, this paper\nintroduces the ComsAmy, a challenging benchmark specifically designed for\nopen-set anomaly segmentation in complex scenarios. ComsAmy encompasses a wide\nspectrum of adverse weather conditions, dynamic driving environments, and\ndiverse anomaly types to comprehensively evaluate the model performance in\nrealistic open-world scenarios. Our extensive evaluation of several\nstate-of-the-art anomalous segmentation models reveals that existing methods\ndemonstrate significant deficiencies in such challenging scenarios,\nhighlighting their serious safety risks for real-world deployment. To solve\nthat, we propose a novel energy-entropy learning (EEL) strategy that integrates\nthe complementary information from energy and entropy to bolster the robustness\nof anomaly segmentation under complex open-world environments. Additionally, a\ndiffusion-based anomalous training data synthesizer is proposed to generate\ndiverse and high-quality anomalous images to enhance the existing copy-paste\ntraining data synthesizer. Extensive experimental results on both public and\nComsAmy benchmarks demonstrate that our proposed diffusion-based synthesizer\nwith energy and entropy learning (DiffEEL) serves as an effective and\ngeneralizable plug-and-play method to enhance existing models, yielding an\naverage improvement of around 4.96% in $\\rm{AUPRC}$ and 9.87% in\n$\\rm{FPR}_{95}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precise segmentation of out-of-distribution (OoD) objects, herein referred to\nas anomalies, is crucial for the reliable deployment of semantic segmentation\nmodels in open-set, safety-critical applications, such as autonomous driving.\nCurrent anomalous segmentation benchmarks predominantly focus on favorable\nweather conditions, resulting in untrustworthy evaluations that overlook the\nrisks posed by diverse meteorological conditions in open-set environments, such\nas low illumination, dense fog, and heavy rain. To bridge this gap, this paper\nintroduces the ComsAmy, a challenging benchmark specifically designed for\nopen-set anomaly segmentation in complex scenarios. ComsAmy encompasses a wide\nspectrum of adverse weather conditions, dynamic driving environments, and\ndiverse anomaly types to comprehensively evaluate the model performance in\nrealistic open-world scenarios. Our extensive evaluation of several\nstate-of-the-art anomalous segmentation models reveals that existing methods\ndemonstrate significant deficiencies in such challenging scenarios,\nhighlighting their serious safety risks for real-world deployment. To solve\nthat, we propose a novel energy-entropy learning (EEL) strategy that integrates\nthe complementary information from energy and entropy to bolster the robustness\nof anomaly segmentation under complex open-world environments. Additionally, a\ndiffusion-based anomalous training data synthesizer is proposed to generate\ndiverse and high-quality anomalous images to enhance the existing copy-paste\ntraining data synthesizer. Extensive experimental results on both public and\nComsAmy benchmarks demonstrate that our proposed diffusion-based synthesizer\nwith energy and entropy learning (DiffEEL) serves as an effective and\ngeneralizable plug-and-play method to enhance existing models, yielding an\naverage improvement of around 4.96% in $\\rm{AUPRC}$ and 9.87% in\n$\\rm{FPR}_{95}$."
                },
                "authors": [
                    {
                        "name": "Song Xia"
                    },
                    {
                        "name": "Yi Yu"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Wenhan Yang"
                    },
                    {
                        "name": "Shifei Liu"
                    },
                    {
                        "name": "Alex C. Kot"
                    },
                    {
                        "name": "Xudong Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xudong Jiang"
                },
                "author": "Xudong Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19702v1",
                "updated": "2025-04-28T11:56:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    11,
                    56,
                    45,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T11:56:45Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    11,
                    56,
                    45,
                    0,
                    118,
                    0
                ],
                "title": "Security of a secret sharing protocol on the Qline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security of a secret sharing protocol on the Qline"
                },
                "summary": "Secret sharing is a fundamental primitive in cryptography, and it can be\nachieved even with perfect security. However, the distribution of shares\nrequires computational assumptions, which can compromise the overall security\nof the protocol. While traditional Quantum Key Distribution (QKD) can maintain\nsecurity, its widespread deployment in general networks would incur prohibitive\ncosts. In this work, we present a quantum protocol for distributing additive\nsecret sharing of 0, which we prove to be composably secure within the Abstract\nCryptography framework. Moreover, our protocol targets the Qline, a recently\nproposed quantum network architecture designed to simplify and reduce the cost\nof quantum communication. Once the shares are distributed, they can be used to\nsecurely perform a wide range of cryptographic tasks, including standard\nadditive secret sharing, anonymous veto, and symmetric key establishment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secret sharing is a fundamental primitive in cryptography, and it can be\nachieved even with perfect security. However, the distribution of shares\nrequires computational assumptions, which can compromise the overall security\nof the protocol. While traditional Quantum Key Distribution (QKD) can maintain\nsecurity, its widespread deployment in general networks would incur prohibitive\ncosts. In this work, we present a quantum protocol for distributing additive\nsecret sharing of 0, which we prove to be composably secure within the Abstract\nCryptography framework. Moreover, our protocol targets the Qline, a recently\nproposed quantum network architecture designed to simplify and reduce the cost\nof quantum communication. Once the shares are distributed, they can be used to\nsecurely perform a wide range of cryptographic tasks, including standard\nadditive secret sharing, anonymous veto, and symmetric key establishment."
                },
                "authors": [
                    {
                        "name": "Alex B. Grilo"
                    },
                    {
                        "name": "Lucas Hanouz"
                    },
                    {
                        "name": "Anne Marin"
                    }
                ],
                "author_detail": {
                    "name": "Anne Marin"
                },
                "author": "Anne Marin",
                "arxiv_comment": "27 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16286v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16286v2",
                "updated": "2025-04-28T11:53:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    11,
                    53,
                    26,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-22T21:48:05Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    21,
                    48,
                    5,
                    1,
                    112,
                    0
                ],
                "title": "The Paradox of Poetic Intent in Back-Translation: Evaluating the Quality\n  of Large Language Models in Chinese Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Paradox of Poetic Intent in Back-Translation: Evaluating the Quality\n  of Large Language Models in Chinese Translation"
                },
                "summary": "The rapid advancement of large language models (LLMs) has reshaped the\nlandscape of machine translation, yet challenges persist in preserving poetic\nintent, cultural heritage, and handling specialized terminology in\nChinese-English translation. This study constructs a diverse corpus\nencompassing Chinese scientific terminology, historical translation paradoxes,\nand literary metaphors. Utilizing a back-translation and Friedman test-based\nevaluation system (BT-Fried), we evaluate BLEU, CHRF, TER, and semantic\nsimilarity metrics across six major LLMs (e.g., GPT-4.5, DeepSeek V3) and three\ntraditional translation tools. Key findings include: (1) Scientific abstracts\noften benefit from back-translation, while traditional tools outperform LLMs in\nlinguistically distinct texts; (2) LLMs struggle with cultural and literary\nretention, exemplifying the \"paradox of poetic intent\"; (3) Some models exhibit\n\"verbatim back-translation\", reflecting emergent memory behavior; (4) A novel\nBLEU variant using Jieba segmentation and n-gram weighting is proposed. The\nstudy contributes to the empirical evaluation of Chinese NLP performance and\nadvances understanding of cultural fidelity in AI-mediated translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has reshaped the\nlandscape of machine translation, yet challenges persist in preserving poetic\nintent, cultural heritage, and handling specialized terminology in\nChinese-English translation. This study constructs a diverse corpus\nencompassing Chinese scientific terminology, historical translation paradoxes,\nand literary metaphors. Utilizing a back-translation and Friedman test-based\nevaluation system (BT-Fried), we evaluate BLEU, CHRF, TER, and semantic\nsimilarity metrics across six major LLMs (e.g., GPT-4.5, DeepSeek V3) and three\ntraditional translation tools. Key findings include: (1) Scientific abstracts\noften benefit from back-translation, while traditional tools outperform LLMs in\nlinguistically distinct texts; (2) LLMs struggle with cultural and literary\nretention, exemplifying the \"paradox of poetic intent\"; (3) Some models exhibit\n\"verbatim back-translation\", reflecting emergent memory behavior; (4) A novel\nBLEU variant using Jieba segmentation and n-gram weighting is proposed. The\nstudy contributes to the empirical evaluation of Chinese NLP performance and\nadvances understanding of cultural fidelity in AI-mediated translation."
                },
                "authors": [
                    {
                        "name": "Li Weigang"
                    },
                    {
                        "name": "Pedro Carvalho Brom"
                    }
                ],
                "author_detail": {
                    "name": "Pedro Carvalho Brom"
                },
                "author": "Pedro Carvalho Brom",
                "arxiv_comment": "24 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16286v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16286v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05999v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05999v2",
                "updated": "2025-04-28T11:42:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    11,
                    42,
                    10,
                    0,
                    118,
                    0
                ],
                "published": "2024-05-09T09:37:22Z",
                "published_parsed": [
                    2024,
                    5,
                    9,
                    9,
                    37,
                    22,
                    3,
                    130,
                    0
                ],
                "title": "LLMPot: Dynamically Configured LLM-based Honeypot for Industrial\n  Protocol and Physical Process Emulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMPot: Dynamically Configured LLM-based Honeypot for Industrial\n  Protocol and Physical Process Emulation"
                },
                "summary": "Industrial Control Systems (ICS) are extensively used in critical\ninfrastructures ensuring efficient, reliable, and continuous operations.\nHowever, their increasing connectivity and addition of advanced features make\nthem vulnerable to cyber threats, potentially leading to severe disruptions in\nessential services. In this context, honeypots play a vital role by acting as\ndecoy targets within ICS networks, or on the Internet, helping to detect, log,\nanalyze, and develop mitigations for ICS-specific cyber threats. Deploying ICS\nhoneypots, however, is challenging due to the necessity of accurately\nreplicating industrial protocols and device characteristics, a crucial\nrequirement for effectively mimicking the unique operational behavior of\ndifferent industrial systems. Moreover, this challenge is compounded by the\nsignificant manual effort required in also mimicking the control logic the PLC\nwould execute, in order to capture attacker traffic aiming to disrupt critical\ninfrastructure operations. In this paper, we propose LLMPot, a novel approach\nfor designing honeypots in ICS networks harnessing the potency of Large\nLanguage Models (LLMs). LLMPot aims to automate and optimize the creation of\nrealistic honeypots with vendor-agnostic configurations, and for any control\nlogic, aiming to eliminate the manual effort and specialized knowledge\ntraditionally required in this domain. We conducted extensive experiments\nfocusing on a wide array of parameters, demonstrating that our LLM-based\napproach can effectively create honeypot devices implementing different\nindustrial protocols and diverse control logic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial Control Systems (ICS) are extensively used in critical\ninfrastructures ensuring efficient, reliable, and continuous operations.\nHowever, their increasing connectivity and addition of advanced features make\nthem vulnerable to cyber threats, potentially leading to severe disruptions in\nessential services. In this context, honeypots play a vital role by acting as\ndecoy targets within ICS networks, or on the Internet, helping to detect, log,\nanalyze, and develop mitigations for ICS-specific cyber threats. Deploying ICS\nhoneypots, however, is challenging due to the necessity of accurately\nreplicating industrial protocols and device characteristics, a crucial\nrequirement for effectively mimicking the unique operational behavior of\ndifferent industrial systems. Moreover, this challenge is compounded by the\nsignificant manual effort required in also mimicking the control logic the PLC\nwould execute, in order to capture attacker traffic aiming to disrupt critical\ninfrastructure operations. In this paper, we propose LLMPot, a novel approach\nfor designing honeypots in ICS networks harnessing the potency of Large\nLanguage Models (LLMs). LLMPot aims to automate and optimize the creation of\nrealistic honeypots with vendor-agnostic configurations, and for any control\nlogic, aiming to eliminate the manual effort and specialized knowledge\ntraditionally required in this domain. We conducted extensive experiments\nfocusing on a wide array of parameters, demonstrating that our LLM-based\napproach can effectively create honeypot devices implementing different\nindustrial protocols and diverse control logic."
                },
                "authors": [
                    {
                        "name": "Christoforos Vasilatos"
                    },
                    {
                        "name": "Dunia J. Mahboobeh"
                    },
                    {
                        "name": "Hithem Lamri"
                    },
                    {
                        "name": "Manaar Alam"
                    },
                    {
                        "name": "Michail Maniatakos"
                    }
                ],
                "author_detail": {
                    "name": "Michail Maniatakos"
                },
                "author": "Michail Maniatakos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05999v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05999v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19678v1",
                "updated": "2025-04-28T11:08:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    11,
                    8,
                    22,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T11:08:22Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    11,
                    8,
                    22,
                    0,
                    118,
                    0
                ],
                "title": "From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review"
                },
                "summary": "Large language models and autonomous AI agents have evolved rapidly,\nresulting in a diverse array of evaluation benchmarks, frameworks, and\ncollaboration protocols. However, the landscape remains fragmented and lacks a\nunified taxonomy or comprehensive survey. Therefore, we present a side-by-side\ncomparison of benchmarks developed between 2019 and 2025 that evaluate these\nmodels and agents across multiple domains. In addition, we propose a taxonomy\nof approximately 60 benchmarks that cover general and academic knowledge\nreasoning, mathematical problem-solving, code generation and software\nengineering, factual grounding and retrieval, domain-specific evaluations,\nmultimodal and embodied tasks, task orchestration, and interactive assessments.\nFurthermore, we review AI-agent frameworks introduced between 2023 and 2025\nthat integrate large language models with modular toolkits to enable autonomous\ndecision-making and multi-step reasoning. Moreover, we present real-world\napplications of autonomous AI agents in materials science, biomedical research,\nacademic ideation, software engineering, synthetic data generation, chemical\nreasoning, mathematical problem-solving, geographic information systems,\nmultimedia, healthcare, and finance. We then survey key agent-to-agent\ncollaboration protocols, namely the Agent Communication Protocol (ACP), the\nModel Context Protocol (MCP), and the Agent-to-Agent Protocol (A2A). Finally,\nwe discuss recommendations for future research, focusing on advanced reasoning\nstrategies, failure modes in multi-agent LLM systems, automated scientific\ndiscovery, dynamic tool integration via reinforcement learning, integrated\nsearch capabilities, and security vulnerabilities in agent protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models and autonomous AI agents have evolved rapidly,\nresulting in a diverse array of evaluation benchmarks, frameworks, and\ncollaboration protocols. However, the landscape remains fragmented and lacks a\nunified taxonomy or comprehensive survey. Therefore, we present a side-by-side\ncomparison of benchmarks developed between 2019 and 2025 that evaluate these\nmodels and agents across multiple domains. In addition, we propose a taxonomy\nof approximately 60 benchmarks that cover general and academic knowledge\nreasoning, mathematical problem-solving, code generation and software\nengineering, factual grounding and retrieval, domain-specific evaluations,\nmultimodal and embodied tasks, task orchestration, and interactive assessments.\nFurthermore, we review AI-agent frameworks introduced between 2023 and 2025\nthat integrate large language models with modular toolkits to enable autonomous\ndecision-making and multi-step reasoning. Moreover, we present real-world\napplications of autonomous AI agents in materials science, biomedical research,\nacademic ideation, software engineering, synthetic data generation, chemical\nreasoning, mathematical problem-solving, geographic information systems,\nmultimedia, healthcare, and finance. We then survey key agent-to-agent\ncollaboration protocols, namely the Agent Communication Protocol (ACP), the\nModel Context Protocol (MCP), and the Agent-to-Agent Protocol (A2A). Finally,\nwe discuss recommendations for future research, focusing on advanced reasoning\nstrategies, failure modes in multi-agent LLM systems, automated scientific\ndiscovery, dynamic tool integration via reinforcement learning, integrated\nsearch capabilities, and security vulnerabilities in agent protocols."
                },
                "authors": [
                    {
                        "name": "Mohamed Amine Ferrag"
                    },
                    {
                        "name": "Norbert Tihanyi"
                    },
                    {
                        "name": "Merouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Merouane Debbah"
                },
                "author": "Merouane Debbah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19675v1",
                "updated": "2025-04-28T11:04:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    11,
                    4,
                    23,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T11:04:23Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    11,
                    4,
                    23,
                    0,
                    118,
                    0
                ],
                "title": "Annif at SemEval-2025 Task 5: Traditional XMTC augmented by LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Annif at SemEval-2025 Task 5: Traditional XMTC augmented by LLMs"
                },
                "summary": "This paper presents the Annif system in SemEval-2025 Task 5 (LLMs4Subjects),\nwhich focussed on subject indexing using large language models (LLMs). The task\nrequired creating subject predictions for bibliographic records from the\nbilingual TIBKAT database using the GND subject vocabulary. Our approach\ncombines traditional natural language processing and machine learning\ntechniques implemented in the Annif toolkit with innovative LLM-based methods\nfor translation and synthetic data generation, and merging predictions from\nmonolingual models. The system ranked first in the all-subjects category and\nsecond in the tib-core-subjects category in the quantitative evaluation, and\nfourth in qualitative evaluations. These findings demonstrate the potential of\ncombining traditional XMTC algorithms with modern LLM techniques to improve the\naccuracy and efficiency of subject indexing in multilingual contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the Annif system in SemEval-2025 Task 5 (LLMs4Subjects),\nwhich focussed on subject indexing using large language models (LLMs). The task\nrequired creating subject predictions for bibliographic records from the\nbilingual TIBKAT database using the GND subject vocabulary. Our approach\ncombines traditional natural language processing and machine learning\ntechniques implemented in the Annif toolkit with innovative LLM-based methods\nfor translation and synthetic data generation, and merging predictions from\nmonolingual models. The system ranked first in the all-subjects category and\nsecond in the tib-core-subjects category in the quantitative evaluation, and\nfourth in qualitative evaluations. These findings demonstrate the potential of\ncombining traditional XMTC algorithms with modern LLM techniques to improve the\naccuracy and efficiency of subject indexing in multilingual contexts."
                },
                "authors": [
                    {
                        "name": "Osma Suominen"
                    },
                    {
                        "name": "Juho Inkinen"
                    },
                    {
                        "name": "Mona Lehtinen"
                    }
                ],
                "author_detail": {
                    "name": "Mona Lehtinen"
                },
                "author": "Mona Lehtinen",
                "arxiv_comment": "6 pages, 4 figures, submitted to SemEval-2025 workshop Task 5:\n  LLMs4Subjects",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19674v1",
                "updated": "2025-04-28T11:01:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    11,
                    1,
                    8,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T11:01:08Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    11,
                    1,
                    8,
                    0,
                    118,
                    0
                ],
                "title": "$\\texttt{SAGE}$: A Generic Framework for LLM Safety Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\texttt{SAGE}$: A Generic Framework for LLM Safety Evaluation"
                },
                "summary": "Safety evaluation of Large Language Models (LLMs) has made progress and\nattracted academic interest, but it remains challenging to keep pace with the\nrapid integration of LLMs across diverse applications. Different applications\nexpose users to various harms, necessitating application-specific safety\nevaluations with tailored harms and policies. Another major gap is the lack of\nfocus on the dynamic and conversational nature of LLM systems. Such potential\noversights can lead to harms that go unnoticed in standard safety benchmarks.\nThis paper identifies the above as key requirements for robust LLM safety\nevaluation and recognizing that current evaluation methodologies do not satisfy\nthese, we introduce the $\\texttt{SAGE}$ (Safety AI Generic Evaluation)\nframework. $\\texttt{SAGE}$ is an automated modular framework designed for\ncustomized and dynamic harm evaluations. It utilizes adversarial user models\nthat are system-aware and have unique personalities, enabling a holistic\nred-teaming evaluation. We demonstrate $\\texttt{SAGE}$'s effectiveness by\nevaluating seven state-of-the-art LLMs across three applications and harm\npolicies. Our experiments with multi-turn conversational evaluations revealed a\nconcerning finding that harm steadily increases with conversation length.\nFurthermore, we observe significant disparities in model behavior when exposed\nto different user personalities and scenarios. Our findings also reveal that\nsome models minimize harmful outputs by employing severe refusal tactics that\ncan hinder their usefulness. These insights highlight the necessity of adaptive\nand context-specific testing to ensure better safety alignment and safer\ndeployment of LLMs in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety evaluation of Large Language Models (LLMs) has made progress and\nattracted academic interest, but it remains challenging to keep pace with the\nrapid integration of LLMs across diverse applications. Different applications\nexpose users to various harms, necessitating application-specific safety\nevaluations with tailored harms and policies. Another major gap is the lack of\nfocus on the dynamic and conversational nature of LLM systems. Such potential\noversights can lead to harms that go unnoticed in standard safety benchmarks.\nThis paper identifies the above as key requirements for robust LLM safety\nevaluation and recognizing that current evaluation methodologies do not satisfy\nthese, we introduce the $\\texttt{SAGE}$ (Safety AI Generic Evaluation)\nframework. $\\texttt{SAGE}$ is an automated modular framework designed for\ncustomized and dynamic harm evaluations. It utilizes adversarial user models\nthat are system-aware and have unique personalities, enabling a holistic\nred-teaming evaluation. We demonstrate $\\texttt{SAGE}$'s effectiveness by\nevaluating seven state-of-the-art LLMs across three applications and harm\npolicies. Our experiments with multi-turn conversational evaluations revealed a\nconcerning finding that harm steadily increases with conversation length.\nFurthermore, we observe significant disparities in model behavior when exposed\nto different user personalities and scenarios. Our findings also reveal that\nsome models minimize harmful outputs by employing severe refusal tactics that\ncan hinder their usefulness. These insights highlight the necessity of adaptive\nand context-specific testing to ensure better safety alignment and safer\ndeployment of LLMs in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Madhur Jindal"
                    },
                    {
                        "name": "Hari Shrawgi"
                    },
                    {
                        "name": "Parag Agrawal"
                    },
                    {
                        "name": "Sandipan Dandapat"
                    }
                ],
                "author_detail": {
                    "name": "Sandipan Dandapat"
                },
                "author": "Sandipan Dandapat",
                "arxiv_comment": "24 pages, 9 main pages excluding references and appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19667v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19667v1",
                "updated": "2025-04-28T10:43:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    10,
                    43,
                    35,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T10:43:35Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    10,
                    43,
                    35,
                    0,
                    118,
                    0
                ],
                "title": "A Tripartite Perspective on GraphRAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Tripartite Perspective on GraphRAG"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious domains, yet they struggle with knowledge-intensive tasks in areas that\ndemand factual accuracy, e.g. industrial automation and healthcare. Key\nlimitations include their tendency to hallucinate, lack of source traceability\n(provenance), and challenges in timely knowledge updates. Combining language\nmodels with knowledge graphs (GraphRAG) offers promising avenues for overcoming\nthese deficits. However, a major challenge lies in creating such a knowledge\ngraph in the first place. Here, we propose a novel approach that combines LLMs\nwith a tripartite knowledge graph representation, which is constructed by\nconnecting complex, domain-specific objects via a curated ontology of\ncorresponding, domain-specific concepts to relevant sections within chunks of\ntext through a concept-anchored pre-analysis of source documents starting from\nan initial lexical graph. As a consequence, our Tripartite-GraphRAG approach\nimplements: i) a concept-specific, information-preserving pre-compression of\ntextual chunks; ii) allows for the formation of a concept-specific relevance\nestimation of embedding similarities grounded in statistics; and iii) avoids\ncommon challenges w.r.t. continuous extendability, such as the need for entity\nresolution and deduplication. By applying a transformation to the knowledge\ngraph, we formulate LLM prompt creation as an unsupervised node classification\nproblem, drawing on ideas from Markov Random Fields. We evaluate our approach\non a healthcare use case, involving multi-faceted analyses of patient anamneses\ngiven a set of medical concepts as well as clinical literature. Experiments\nindicate that it can optimize information density, coverage, and arrangement of\nLLM prompts while reducing their lengths, which may lead to reduced costs and\nmore consistent and reliable LLM outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious domains, yet they struggle with knowledge-intensive tasks in areas that\ndemand factual accuracy, e.g. industrial automation and healthcare. Key\nlimitations include their tendency to hallucinate, lack of source traceability\n(provenance), and challenges in timely knowledge updates. Combining language\nmodels with knowledge graphs (GraphRAG) offers promising avenues for overcoming\nthese deficits. However, a major challenge lies in creating such a knowledge\ngraph in the first place. Here, we propose a novel approach that combines LLMs\nwith a tripartite knowledge graph representation, which is constructed by\nconnecting complex, domain-specific objects via a curated ontology of\ncorresponding, domain-specific concepts to relevant sections within chunks of\ntext through a concept-anchored pre-analysis of source documents starting from\nan initial lexical graph. As a consequence, our Tripartite-GraphRAG approach\nimplements: i) a concept-specific, information-preserving pre-compression of\ntextual chunks; ii) allows for the formation of a concept-specific relevance\nestimation of embedding similarities grounded in statistics; and iii) avoids\ncommon challenges w.r.t. continuous extendability, such as the need for entity\nresolution and deduplication. By applying a transformation to the knowledge\ngraph, we formulate LLM prompt creation as an unsupervised node classification\nproblem, drawing on ideas from Markov Random Fields. We evaluate our approach\non a healthcare use case, involving multi-faceted analyses of patient anamneses\ngiven a set of medical concepts as well as clinical literature. Experiments\nindicate that it can optimize information density, coverage, and arrangement of\nLLM prompts while reducing their lengths, which may lead to reduced costs and\nmore consistent and reliable LLM outputs."
                },
                "authors": [
                    {
                        "name": "Michael Banf"
                    },
                    {
                        "name": "Johannes Kuhn"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Kuhn"
                },
                "author": "Johannes Kuhn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19667v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19667v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19660v1",
                "updated": "2025-04-28T10:20:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    10,
                    20,
                    4,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T10:20:04Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    10,
                    20,
                    4,
                    0,
                    118,
                    0
                ],
                "title": "Decentralization of Generative AI via Mixture of Experts for Wireless\n  Networks: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralization of Generative AI via Mixture of Experts for Wireless\n  Networks: A Comprehensive Survey"
                },
                "summary": "Mixture of Experts (MoE) has emerged as a promising paradigm for scaling\nmodel capacity while preserving computational efficiency, particularly in\nlarge-scale machine learning architectures such as large language models\n(LLMs). Recent advances in MoE have facilitated its adoption in wireless\nnetworks to address the increasing complexity and heterogeneity of modern\ncommunication systems. This paper presents a comprehensive survey of the MoE\nframework in wireless networks, highlighting its potential in optimizing\nresource efficiency, improving scalability, and enhancing adaptability across\ndiverse network tasks. We first introduce the fundamental concepts of MoE,\nincluding various gating mechanisms and the integration with generative AI\n(GenAI) and reinforcement learning (RL). Subsequently, we discuss the extensive\napplications of MoE across critical wireless communication scenarios, such as\nvehicular networks, unmanned aerial vehicles (UAVs), satellite communications,\nheterogeneous networks, integrated sensing and communication (ISAC), and mobile\nedge networks. Furthermore, key applications in channel prediction, physical\nlayer signal processing, radio resource management, network optimization, and\nsecurity are thoroughly examined. Additionally, we present a detailed overview\nof open-source datasets that are widely used in MoE-based models to support\ndiverse machine learning tasks. Finally, this survey identifies crucial future\nresearch directions for MoE, emphasizing the importance of advanced training\ntechniques, resource-aware gating strategies, and deeper integration with\nemerging 6G technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) has emerged as a promising paradigm for scaling\nmodel capacity while preserving computational efficiency, particularly in\nlarge-scale machine learning architectures such as large language models\n(LLMs). Recent advances in MoE have facilitated its adoption in wireless\nnetworks to address the increasing complexity and heterogeneity of modern\ncommunication systems. This paper presents a comprehensive survey of the MoE\nframework in wireless networks, highlighting its potential in optimizing\nresource efficiency, improving scalability, and enhancing adaptability across\ndiverse network tasks. We first introduce the fundamental concepts of MoE,\nincluding various gating mechanisms and the integration with generative AI\n(GenAI) and reinforcement learning (RL). Subsequently, we discuss the extensive\napplications of MoE across critical wireless communication scenarios, such as\nvehicular networks, unmanned aerial vehicles (UAVs), satellite communications,\nheterogeneous networks, integrated sensing and communication (ISAC), and mobile\nedge networks. Furthermore, key applications in channel prediction, physical\nlayer signal processing, radio resource management, network optimization, and\nsecurity are thoroughly examined. Additionally, we present a detailed overview\nof open-source datasets that are widely used in MoE-based models to support\ndiverse machine learning tasks. Finally, this survey identifies crucial future\nresearch directions for MoE, emphasizing the importance of advanced training\ntechniques, resource-aware gating strategies, and deeper integration with\nemerging 6G technologies."
                },
                "authors": [
                    {
                        "name": "Yunting Xu"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Changyuan Zhao"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Jiawen Kang"
                    },
                    {
                        "name": "Zehui Xiong"
                    },
                    {
                        "name": "Bo Qian"
                    },
                    {
                        "name": "Haibo Zhou"
                    },
                    {
                        "name": "Shiwen Mao"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Xuemin Shen"
                    },
                    {
                        "name": "Dong In Kim"
                    }
                ],
                "author_detail": {
                    "name": "Dong In Kim"
                },
                "author": "Dong In Kim",
                "arxiv_comment": "Survey paper, 30 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19646v1",
                "updated": "2025-04-28T10:03:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    10,
                    3,
                    11,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T10:03:11Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    10,
                    3,
                    11,
                    0,
                    118,
                    0
                ],
                "title": "xEdgeFace: Efficient Cross-Spectral Face Recognition for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xEdgeFace: Efficient Cross-Spectral Face Recognition for Edge Devices"
                },
                "summary": "Heterogeneous Face Recognition (HFR) addresses the challenge of matching face\nimages across different sensing modalities, such as thermal to visible or\nnear-infrared to visible, expanding the applicability of face recognition\nsystems in real-world, unconstrained environments. While recent HFR methods\nhave shown promising results, many rely on computation-intensive architectures,\nlimiting their practicality for deployment on resource-constrained edge\ndevices. In this work, we present a lightweight yet effective HFR framework by\nadapting a hybrid CNN-Transformer architecture originally designed for face\nrecognition. Our approach enables efficient end-to-end training with minimal\npaired heterogeneous data while preserving strong performance on standard RGB\nface recognition tasks. This makes it a compelling solution for both\nhomogeneous and heterogeneous scenarios. Extensive experiments across multiple\nchallenging HFR and face recognition benchmarks demonstrate that our method\nconsistently outperforms state-of-the-art approaches while maintaining a low\ncomputational overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Face Recognition (HFR) addresses the challenge of matching face\nimages across different sensing modalities, such as thermal to visible or\nnear-infrared to visible, expanding the applicability of face recognition\nsystems in real-world, unconstrained environments. While recent HFR methods\nhave shown promising results, many rely on computation-intensive architectures,\nlimiting their practicality for deployment on resource-constrained edge\ndevices. In this work, we present a lightweight yet effective HFR framework by\nadapting a hybrid CNN-Transformer architecture originally designed for face\nrecognition. Our approach enables efficient end-to-end training with minimal\npaired heterogeneous data while preserving strong performance on standard RGB\nface recognition tasks. This makes it a compelling solution for both\nhomogeneous and heterogeneous scenarios. Extensive experiments across multiple\nchallenging HFR and face recognition benchmarks demonstrate that our method\nconsistently outperforms state-of-the-art approaches while maintaining a low\ncomputational overhead."
                },
                "authors": [
                    {
                        "name": "Anjith George"
                    },
                    {
                        "name": "Sebastien Marcel"
                    }
                ],
                "author_detail": {
                    "name": "Sebastien Marcel"
                },
                "author": "Sebastien Marcel",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17761v2",
                "updated": "2025-04-28T09:56:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    56,
                    8,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-24T17:25:12Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    17,
                    25,
                    12,
                    3,
                    114,
                    0
                ],
                "title": "Step1X-Edit: A Practical Framework for General Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step1X-Edit: A Practical Framework for General Image Editing"
                },
                "summary": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing."
                },
                "authors": [
                    {
                        "name": "Shiyu Liu"
                    },
                    {
                        "name": "Yucheng Han"
                    },
                    {
                        "name": "Peng Xing"
                    },
                    {
                        "name": "Fukun Yin"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Jiaqi Liao"
                    },
                    {
                        "name": "Yingming Wang"
                    },
                    {
                        "name": "Honghao Fu"
                    },
                    {
                        "name": "Chunrui Han"
                    },
                    {
                        "name": "Guopeng Li"
                    },
                    {
                        "name": "Yuang Peng"
                    },
                    {
                        "name": "Quan Sun"
                    },
                    {
                        "name": "Jingwei Wu"
                    },
                    {
                        "name": "Yan Cai"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Ranchen Ming"
                    },
                    {
                        "name": "Lei Xia"
                    },
                    {
                        "name": "Xianfang Zeng"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Gang Yu"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "arxiv_comment": "code: https://github.com/stepfun-ai/Step1X-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19638v1",
                "updated": "2025-04-28T09:52:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    52,
                    53,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T09:52:53Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    52,
                    53,
                    0,
                    118,
                    0
                ],
                "title": "LODAP: On-Device Incremental Learning Via Lightweight Operations and\n  Data Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LODAP: On-Device Incremental Learning Via Lightweight Operations and\n  Data Pruning"
                },
                "summary": "Incremental learning that learns new classes over time after the model's\ndeployment is becoming increasingly crucial, particularly for industrial edge\nsystems, where it is difficult to communicate with a remote server to conduct\ncomputation-intensive learning. As more classes are expected to learn after\ntheir execution for edge devices. In this paper, we propose LODAP, a new\non-device incremental learning framework for edge systems. The key part of\nLODAP is a new module, namely Efficient Incremental Module (EIM). EIM is\ncomposed of normal convolutions and lightweight operations. During incremental\nlearning, EIM exploits some lightweight operations, called adapters, to\neffectively and efficiently learn features for new classes so that it can\nimprove the accuracy of incremental learning while reducing model complexity as\nwell as training overhead. The efficiency of LODAP is further enhanced by a\ndata pruning strategy that significantly reduces the training data, thereby\nlowering the training overhead. We conducted extensive experiments on the\nCIFAR-100 and Tiny- ImageNet datasets. Experimental results show that LODAP\nimproves the accuracy by up to 4.32\\% over existing methods while reducing\naround 50\\% of model complexity. In addition, evaluations on real edge systems\ndemonstrate its applicability for on-device machine learning. The code is\navailable at https://github.com/duanbiqing/LODAP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incremental learning that learns new classes over time after the model's\ndeployment is becoming increasingly crucial, particularly for industrial edge\nsystems, where it is difficult to communicate with a remote server to conduct\ncomputation-intensive learning. As more classes are expected to learn after\ntheir execution for edge devices. In this paper, we propose LODAP, a new\non-device incremental learning framework for edge systems. The key part of\nLODAP is a new module, namely Efficient Incremental Module (EIM). EIM is\ncomposed of normal convolutions and lightweight operations. During incremental\nlearning, EIM exploits some lightweight operations, called adapters, to\neffectively and efficiently learn features for new classes so that it can\nimprove the accuracy of incremental learning while reducing model complexity as\nwell as training overhead. The efficiency of LODAP is further enhanced by a\ndata pruning strategy that significantly reduces the training data, thereby\nlowering the training overhead. We conducted extensive experiments on the\nCIFAR-100 and Tiny- ImageNet datasets. Experimental results show that LODAP\nimproves the accuracy by up to 4.32\\% over existing methods while reducing\naround 50\\% of model complexity. In addition, evaluations on real edge systems\ndemonstrate its applicability for on-device machine learning. The code is\navailable at https://github.com/duanbiqing/LODAP."
                },
                "authors": [
                    {
                        "name": "Biqing Duan"
                    },
                    {
                        "name": "Qing Wang"
                    },
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Zhenli He"
                    },
                    {
                        "name": "Shengfa Miao"
                    }
                ],
                "author_detail": {
                    "name": "Shengfa Miao"
                },
                "author": "Shengfa Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19636v1",
                "updated": "2025-04-28T09:52:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    52,
                    41,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T09:52:41Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    52,
                    41,
                    0,
                    118,
                    0
                ],
                "title": "Fitness Landscape of Large Language Model-Assisted Automated Algorithm\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fitness Landscape of Large Language Model-Assisted Automated Algorithm\n  Search"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant potential in\nalgorithm design. However, when integrated into search frameworks for iterative\nalgorithm search, the underlying fitness landscape--critical for understanding\nsearch behaviou--remains underexplored. In this paper, we illustrate and\nanalyze the fitness landscape of LLM-assisted Algorithm Search (LAS) using a\ngraph-based approach, where nodes represent algorithms and edges denote\ntransitions between them. We conduct extensive evaluations across six algorithm\ndesign tasks and six commonly used LLMs. Our findings reveal that LAS\nlandscapes are highly multimodal and rugged, particularly in combinatorial\noptimization tasks, with distinct structural variations across tasks and LLMs.\nFor instance, heuristic design tasks exhibit dense clusters of high-performing\nalgorithms, while symbolic regression tasks show sparse, scattered\ndistributions. Additionally, we demonstrate how population size influences\nexploration-exploitation trade-offs and the evolving trajectory of elite\nalgorithms. These insights not only advance our understanding of LAS landscapes\nbut also provide practical guidance for designing more effective LAS methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant potential in\nalgorithm design. However, when integrated into search frameworks for iterative\nalgorithm search, the underlying fitness landscape--critical for understanding\nsearch behaviou--remains underexplored. In this paper, we illustrate and\nanalyze the fitness landscape of LLM-assisted Algorithm Search (LAS) using a\ngraph-based approach, where nodes represent algorithms and edges denote\ntransitions between them. We conduct extensive evaluations across six algorithm\ndesign tasks and six commonly used LLMs. Our findings reveal that LAS\nlandscapes are highly multimodal and rugged, particularly in combinatorial\noptimization tasks, with distinct structural variations across tasks and LLMs.\nFor instance, heuristic design tasks exhibit dense clusters of high-performing\nalgorithms, while symbolic regression tasks show sparse, scattered\ndistributions. Additionally, we demonstrate how population size influences\nexploration-exploitation trade-offs and the evolving trajectory of elite\nalgorithms. These insights not only advance our understanding of LAS landscapes\nbut also provide practical guidance for designing more effective LAS methods."
                },
                "authors": [
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Qingfu Zhang"
                    },
                    {
                        "name": "Xialiang Tong"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Kun Mao"
                    }
                ],
                "author_detail": {
                    "name": "Kun Mao"
                },
                "author": "Kun Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13918v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13918v4",
                "updated": "2025-04-28T09:22:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    22,
                    21,
                    0,
                    118,
                    0
                ],
                "published": "2024-08-25T19:03:46Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    19,
                    3,
                    46,
                    6,
                    238,
                    0
                ],
                "title": "Geo-Llama: Leveraging LLMs for Human Mobility Trajectory Generation with\n  Spatiotemporal Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geo-Llama: Leveraging LLMs for Human Mobility Trajectory Generation with\n  Spatiotemporal Constraints"
                },
                "summary": "Generating realistic human mobility data is essential for various application\ndomains, including transportation, urban planning, and epidemic control, as\nreal data is often inaccessible to researchers due to high costs and privacy\nconcerns. Existing deep generative models learn from real trajectories to\ngenerate synthetic ones. Despite the progress, most of them suffer from\ntraining stability issues and scale poorly with increasing data size. More\nimportantly, they often lack control mechanisms to guide the generated\ntrajectories under constraints such as enforcing specific visits. To address\nthese limitations, we formally define the controlled trajectory generation\nproblem for effectively handling multiple spatiotemporal constraints. We\nintroduce Geo-Llama, a novel LLM finetuning framework that can enforce multiple\nexplicit visit constraints while maintaining contextual coherence of the\ngenerated trajectories. In this approach, pre-trained LLMs are fine-tuned on\ntrajectory data with a visit-wise permutation strategy where each visit\ncorresponds to a specific time and location. This strategy enables the model to\ncapture spatiotemporal patterns regardless of visit orders while maintaining\nflexible and in-context constraint integration through prompts during\ngeneration. Extensive experiments on real-world and synthetic datasets validate\nthe effectiveness of Geo-Llama, demonstrating its versatility and robustness in\nhandling a broad range of constraints to generate more realistic trajectories\ncompared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating realistic human mobility data is essential for various application\ndomains, including transportation, urban planning, and epidemic control, as\nreal data is often inaccessible to researchers due to high costs and privacy\nconcerns. Existing deep generative models learn from real trajectories to\ngenerate synthetic ones. Despite the progress, most of them suffer from\ntraining stability issues and scale poorly with increasing data size. More\nimportantly, they often lack control mechanisms to guide the generated\ntrajectories under constraints such as enforcing specific visits. To address\nthese limitations, we formally define the controlled trajectory generation\nproblem for effectively handling multiple spatiotemporal constraints. We\nintroduce Geo-Llama, a novel LLM finetuning framework that can enforce multiple\nexplicit visit constraints while maintaining contextual coherence of the\ngenerated trajectories. In this approach, pre-trained LLMs are fine-tuned on\ntrajectory data with a visit-wise permutation strategy where each visit\ncorresponds to a specific time and location. This strategy enables the model to\ncapture spatiotemporal patterns regardless of visit orders while maintaining\nflexible and in-context constraint integration through prompts during\ngeneration. Extensive experiments on real-world and synthetic datasets validate\nthe effectiveness of Geo-Llama, demonstrating its versatility and robustness in\nhandling a broad range of constraints to generate more realistic trajectories\ncompared to existing methods."
                },
                "authors": [
                    {
                        "name": "Siyu Li"
                    },
                    {
                        "name": "Toan Tran"
                    },
                    {
                        "name": "Haowen Lin"
                    },
                    {
                        "name": "John Krumm"
                    },
                    {
                        "name": "Cyrus Shahabi"
                    },
                    {
                        "name": "Lingyi Zhao"
                    },
                    {
                        "name": "Khurram Shafique"
                    },
                    {
                        "name": "Li Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiong"
                },
                "author": "Li Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13918v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13918v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19613v1",
                "updated": "2025-04-28T09:20:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    20,
                    5,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T09:20:05Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    20,
                    5,
                    0,
                    118,
                    0
                ],
                "title": "Automatic Configuration Protocols for Optical Quantum Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Configuration Protocols for Optical Quantum Networks"
                },
                "summary": "Before quantum networks can scale up to practical sizes, there are many\ndeployment and configuration tasks that must be automated. Currently, quantum\nnetworking testbeds are largely manually configured: network nodes are\nconstructed out of a combination of free-space and fiber optics before being\nconnected to shared single-photon detectors, time-to-digital converters, and\noptical switches. Information about these connections must be tracked manually;\nmislabeling may result in experimental failure and protracted debugging\nsessions. In this paper, we propose protocols and algorithms to automate two\nsuch manual processes. First, we address the problem of automatically\nidentifying connections between quantum network nodes and time-to-digital\nconverters. Then, we turn to the more complex challenge of identifying the\nnodes attached to a quantum network's optical switches. Implementation of these\nprotocols will help enable the development of other protocols necessary for\nquantum networks, such as network topology discovery, link quality monitoring,\nresource naming, and routing. We intend for this paper to serve as a roadmap\nfor near-term implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Before quantum networks can scale up to practical sizes, there are many\ndeployment and configuration tasks that must be automated. Currently, quantum\nnetworking testbeds are largely manually configured: network nodes are\nconstructed out of a combination of free-space and fiber optics before being\nconnected to shared single-photon detectors, time-to-digital converters, and\noptical switches. Information about these connections must be tracked manually;\nmislabeling may result in experimental failure and protracted debugging\nsessions. In this paper, we propose protocols and algorithms to automate two\nsuch manual processes. First, we address the problem of automatically\nidentifying connections between quantum network nodes and time-to-digital\nconverters. Then, we turn to the more complex challenge of identifying the\nnodes attached to a quantum network's optical switches. Implementation of these\nprotocols will help enable the development of other protocols necessary for\nquantum networks, such as network topology discovery, link quality monitoring,\nresource naming, and routing. We intend for this paper to serve as a roadmap\nfor near-term implementation."
                },
                "authors": [
                    {
                        "name": "Amin Taherkhani"
                    },
                    {
                        "name": "Andrew Todd"
                    },
                    {
                        "name": "Kentaro Teramoto"
                    },
                    {
                        "name": "Shota Nagayama"
                    },
                    {
                        "name": "Rodney Van Meter"
                    }
                ],
                "author_detail": {
                    "name": "Rodney Van Meter"
                },
                "author": "Rodney Van Meter",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19611v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19611v1",
                "updated": "2025-04-28T09:18:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    18,
                    44,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T09:18:44Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    18,
                    44,
                    0,
                    118,
                    0
                ],
                "title": "Scene2Hap: Combining LLMs and Physical Modeling for Automatically\n  Generating Vibrotactile Signals for Full VR Scenes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scene2Hap: Combining LLMs and Physical Modeling for Automatically\n  Generating Vibrotactile Signals for Full VR Scenes"
                },
                "summary": "Haptic feedback contributes to immersive virtual reality (VR) experiences.\nDesigning such feedback at scale, for all objects within a VR scene and their\nrespective arrangements, remains a time-consuming task. We present Scene2Hap,\nan LLM-centered system that automatically designs object-level vibrotactile\nfeedback for entire VR scenes based on the objects' semantic attributes and\nphysical context. Scene2Hap employs a multimodal large language model to\nestimate the semantics and physical context of each object, including its\nmaterial properties and vibration behavior, from the multimodal information\npresent in the VR scene. This semantic and physical context is then used to\ncreate plausible vibrotactile signals by generating or retrieving audio signals\nand converting them to vibrotactile signals. For the more realistic spatial\nrendering of haptics in VR, Scene2Hap estimates the propagation and attenuation\nof vibration signals from their source across objects in the scene, considering\nthe estimated material properties and physical context, such as the distance\nand contact between virtual objects. Results from two user studies confirm that\nScene2Hap successfully estimates the semantics and physical context of VR\nscenes, and the physical modeling of vibration propagation improves usability,\nperceived materiality, and spatial awareness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Haptic feedback contributes to immersive virtual reality (VR) experiences.\nDesigning such feedback at scale, for all objects within a VR scene and their\nrespective arrangements, remains a time-consuming task. We present Scene2Hap,\nan LLM-centered system that automatically designs object-level vibrotactile\nfeedback for entire VR scenes based on the objects' semantic attributes and\nphysical context. Scene2Hap employs a multimodal large language model to\nestimate the semantics and physical context of each object, including its\nmaterial properties and vibration behavior, from the multimodal information\npresent in the VR scene. This semantic and physical context is then used to\ncreate plausible vibrotactile signals by generating or retrieving audio signals\nand converting them to vibrotactile signals. For the more realistic spatial\nrendering of haptics in VR, Scene2Hap estimates the propagation and attenuation\nof vibration signals from their source across objects in the scene, considering\nthe estimated material properties and physical context, such as the distance\nand contact between virtual objects. Results from two user studies confirm that\nScene2Hap successfully estimates the semantics and physical context of VR\nscenes, and the physical modeling of vibration propagation improves usability,\nperceived materiality, and spatial awareness."
                },
                "authors": [
                    {
                        "name": "Arata Jingu"
                    },
                    {
                        "name": "Easa AliAbbasi"
                    },
                    {
                        "name": "Paul Strohmeier"
                    },
                    {
                        "name": "Jrgen Steimle"
                    }
                ],
                "author_detail": {
                    "name": "Jrgen Steimle"
                },
                "author": "Jrgen Steimle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19611v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19606v1",
                "updated": "2025-04-28T09:10:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    10,
                    41,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T09:10:41Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    10,
                    41,
                    0,
                    118,
                    0
                ],
                "title": "Coreference Resolution for Vietnamese Narrative Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coreference Resolution for Vietnamese Narrative Texts"
                },
                "summary": "Coreference resolution is a vital task in natural language processing (NLP)\nthat involves identifying and linking different expressions in a text that\nrefer to the same entity. This task is particularly challenging for Vietnamese,\na low-resource language with limited annotated datasets. To address these\nchallenges, we developed a comprehensive annotated dataset using narrative\ntexts from VnExpress, a widely-read Vietnamese online news platform. We\nestablished detailed guidelines for annotating entities, focusing on ensuring\nconsistency and accuracy. Additionally, we evaluated the performance of large\nlanguage models (LLMs), specifically GPT-3.5-Turbo and GPT-4, on this dataset.\nOur results demonstrate that GPT-4 significantly outperforms GPT-3.5-Turbo in\nterms of both accuracy and response consistency, making it a more reliable tool\nfor coreference resolution in Vietnamese.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coreference resolution is a vital task in natural language processing (NLP)\nthat involves identifying and linking different expressions in a text that\nrefer to the same entity. This task is particularly challenging for Vietnamese,\na low-resource language with limited annotated datasets. To address these\nchallenges, we developed a comprehensive annotated dataset using narrative\ntexts from VnExpress, a widely-read Vietnamese online news platform. We\nestablished detailed guidelines for annotating entities, focusing on ensuring\nconsistency and accuracy. Additionally, we evaluated the performance of large\nlanguage models (LLMs), specifically GPT-3.5-Turbo and GPT-4, on this dataset.\nOur results demonstrate that GPT-4 significantly outperforms GPT-3.5-Turbo in\nterms of both accuracy and response consistency, making it a more reliable tool\nfor coreference resolution in Vietnamese."
                },
                "authors": [
                    {
                        "name": "Hieu-Dai Tran"
                    },
                    {
                        "name": "Duc-Vu Nguyen"
                    },
                    {
                        "name": "Ngan Luu-Thuy Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Ngan Luu-Thuy Nguyen"
                },
                "author": "Ngan Luu-Thuy Nguyen",
                "arxiv_comment": "Accepted at PACLIC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19599v1",
                "updated": "2025-04-28T09:02:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    2,
                    24,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T09:02:24Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    2,
                    24,
                    0,
                    118,
                    0
                ],
                "title": "GVPO: Group Variance Policy Optimization for Large Language Model\n  Post-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GVPO: Group Variance Policy Optimization for Large Language Model\n  Post-Training"
                },
                "summary": "Post-training plays a crucial role in refining and aligning large language\nmodels to meet specific tasks and human preferences. While recent advancements\nin post-training techniques, such as Group Relative Policy Optimization (GRPO),\nleverage increased sampling with relative reward scoring to achieve superior\nperformance, these methods often suffer from training instability that limits\ntheir practical adoption. To address this challenge, we present Group Variance\nPolicy Optimization (GVPO). GVPO incorporates the analytical solution to\nKL-constrained reward maximization directly into its gradient weights, ensuring\nalignment with the optimal policy. The method provides intuitive physical\ninterpretations: its gradient mirrors the mean squared error between the\ncentral distance of implicit rewards and that of actual rewards. GVPO offers\ntwo key advantages: (1) it guarantees a unique optimal solution, exactly the\nKL-constrained reward maximization objective, (2) it supports flexible sampling\ndistributions that avoids on-policy and importance sampling limitations. By\nunifying theoretical guarantees with practical adaptability, GVPO establishes a\nnew paradigm for reliable and versatile LLM post-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training plays a crucial role in refining and aligning large language\nmodels to meet specific tasks and human preferences. While recent advancements\nin post-training techniques, such as Group Relative Policy Optimization (GRPO),\nleverage increased sampling with relative reward scoring to achieve superior\nperformance, these methods often suffer from training instability that limits\ntheir practical adoption. To address this challenge, we present Group Variance\nPolicy Optimization (GVPO). GVPO incorporates the analytical solution to\nKL-constrained reward maximization directly into its gradient weights, ensuring\nalignment with the optimal policy. The method provides intuitive physical\ninterpretations: its gradient mirrors the mean squared error between the\ncentral distance of implicit rewards and that of actual rewards. GVPO offers\ntwo key advantages: (1) it guarantees a unique optimal solution, exactly the\nKL-constrained reward maximization objective, (2) it supports flexible sampling\ndistributions that avoids on-policy and importance sampling limitations. By\nunifying theoretical guarantees with practical adaptability, GVPO establishes a\nnew paradigm for reliable and versatile LLM post-training."
                },
                "authors": [
                    {
                        "name": "Kaichen Zhang"
                    },
                    {
                        "name": "Yuzhong Hong"
                    },
                    {
                        "name": "Junwei Bao"
                    },
                    {
                        "name": "Hongfei Jiang"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Dingqian Hong"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19591v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19591v1",
                "updated": "2025-04-28T08:53:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    53,
                    39,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T08:53:39Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    53,
                    39,
                    0,
                    118,
                    0
                ],
                "title": "Semantic Packet Aggregation for Token Communication via Genetic Beam\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Packet Aggregation for Token Communication via Genetic Beam\n  Search"
                },
                "summary": "Token communication (TC) is poised to play a pivotal role in emerging\nlanguage-driven applications such as AI-generated content (AIGC) and wireless\nlanguage models (LLMs). However, token loss caused by channel noise can\nseverely degrade task performance. To address this, in this article, we focus\non the problem of semantics-aware packetization and develop a novel algorithm,\ntermed semantic packet aggregation with genetic beam search (SemPA-GBeam),\nwhich aims to maximize the average token similarity (ATS) over erasure\nchannels. Inspired from the genetic algorithm (GA) and the beam search\nalgorithm, SemPA-GBeam iteratively optimizes token grouping for packetization\nwithin a fixed number of groups (i.e., fixed beam width in beam search) while\nrandomly swapping a fraction of tokens (i.e., mutation in GA). Experiments on\nthe MS-COCO dataset demonstrate that SemPA-GBeam achieves ATS and LPIPS scores\ncomparable to exhaustive search while reducing complexity by more than 20x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token communication (TC) is poised to play a pivotal role in emerging\nlanguage-driven applications such as AI-generated content (AIGC) and wireless\nlanguage models (LLMs). However, token loss caused by channel noise can\nseverely degrade task performance. To address this, in this article, we focus\non the problem of semantics-aware packetization and develop a novel algorithm,\ntermed semantic packet aggregation with genetic beam search (SemPA-GBeam),\nwhich aims to maximize the average token similarity (ATS) over erasure\nchannels. Inspired from the genetic algorithm (GA) and the beam search\nalgorithm, SemPA-GBeam iteratively optimizes token grouping for packetization\nwithin a fixed number of groups (i.e., fixed beam width in beam search) while\nrandomly swapping a fraction of tokens (i.e., mutation in GA). Experiments on\nthe MS-COCO dataset demonstrate that SemPA-GBeam achieves ATS and LPIPS scores\ncomparable to exhaustive search while reducing complexity by more than 20x."
                },
                "authors": [
                    {
                        "name": "Seunghun Lee"
                    },
                    {
                        "name": "Jihong Park"
                    },
                    {
                        "name": "Jinho Choi"
                    },
                    {
                        "name": "Hyuncheol Park"
                    }
                ],
                "author_detail": {
                    "name": "Hyuncheol Park"
                },
                "author": "Hyuncheol Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19591v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19591v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08800v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08800v2",
                "updated": "2025-04-28T08:45:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    45,
                    38,
                    0,
                    118,
                    0
                ],
                "published": "2024-10-11T13:34:24Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    34,
                    24,
                    4,
                    285,
                    0
                ],
                "title": "Data Processing for the OpenGPT-X Model Family",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Processing for the OpenGPT-X Model Family"
                },
                "summary": "This paper presents a comprehensive overview of the data preparation pipeline\ndeveloped for the OpenGPT-X project, a large-scale initiative aimed at creating\nopen and high-performance multilingual large language models (LLMs). The\nproject goal is to deliver models that cover all major European languages, with\na particular focus on real-world applications within the European Union. We\nexplain all data processing steps, starting with the data selection and\nrequirement definition to the preparation of the final datasets for model\ntraining. We distinguish between curated data and web data, as each of these\ncategories is handled by distinct pipelines, with curated data undergoing\nminimal filtering and web data requiring extensive filtering and deduplication.\nThis distinction guided the development of specialized algorithmic solutions\nfor both pipelines. In addition to describing the processing methodologies, we\nprovide an in-depth analysis of the datasets, increasing transparency and\nalignment with European data regulations. Finally, we share key insights and\nchallenges faced during the project, offering recommendations for future\nendeavors in large-scale multilingual data preparation for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive overview of the data preparation pipeline\ndeveloped for the OpenGPT-X project, a large-scale initiative aimed at creating\nopen and high-performance multilingual large language models (LLMs). The\nproject goal is to deliver models that cover all major European languages, with\na particular focus on real-world applications within the European Union. We\nexplain all data processing steps, starting with the data selection and\nrequirement definition to the preparation of the final datasets for model\ntraining. We distinguish between curated data and web data, as each of these\ncategories is handled by distinct pipelines, with curated data undergoing\nminimal filtering and web data requiring extensive filtering and deduplication.\nThis distinction guided the development of specialized algorithmic solutions\nfor both pipelines. In addition to describing the processing methodologies, we\nprovide an in-depth analysis of the datasets, increasing transparency and\nalignment with European data regulations. Finally, we share key insights and\nchallenges faced during the project, offering recommendations for future\nendeavors in large-scale multilingual data preparation for LLMs."
                },
                "authors": [
                    {
                        "name": "Nicolo' Brandizzi"
                    },
                    {
                        "name": "Hammam Abdelwahab"
                    },
                    {
                        "name": "Anirban Bhowmick"
                    },
                    {
                        "name": "Lennard Helmer"
                    },
                    {
                        "name": "Benny Jrg Stein"
                    },
                    {
                        "name": "Pavel Denisov"
                    },
                    {
                        "name": "Qasid Saleem"
                    },
                    {
                        "name": "Michael Fromm"
                    },
                    {
                        "name": "Mehdi Ali"
                    },
                    {
                        "name": "Richard Rutmann"
                    },
                    {
                        "name": "Farzad Naderi"
                    },
                    {
                        "name": "Mohamad Saif Agy"
                    },
                    {
                        "name": "Alexander Schwirjow"
                    },
                    {
                        "name": "Fabian Kch"
                    },
                    {
                        "name": "Luzian Hahn"
                    },
                    {
                        "name": "Malte Ostendorff"
                    },
                    {
                        "name": "Pedro Ortiz Suarez"
                    },
                    {
                        "name": "Georg Rehm"
                    },
                    {
                        "name": "Dennis Wegener"
                    },
                    {
                        "name": "Nicolas Flores-Herr"
                    },
                    {
                        "name": "Joachim Khler"
                    },
                    {
                        "name": "Johannes Leveling"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Leveling"
                },
                "author": "Johannes Leveling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08800v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.1; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19577v1",
                "updated": "2025-04-28T08:36:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    36,
                    17,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T08:36:17Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    36,
                    17,
                    0,
                    118,
                    0
                ],
                "title": "Smart Placement, Faster Robots -- A Comparison of Algorithms for Robot\n  Base-Pose Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart Placement, Faster Robots -- A Comparison of Algorithms for Robot\n  Base-Pose Optimization"
                },
                "summary": "Robotic automation is a key technology that increases the efficiency and\nflexibility of manufacturing processes. However, one of the challenges in\ndeploying robots in novel environments is finding the optimal base pose for the\nrobot, which affects its reachability and deployment cost. Yet, the existing\nresearch for automatically optimizing the base pose of robots has not been\ncompared. We address this problem by optimizing the base pose of industrial\nrobots with Bayesian optimization, exhaustive search, genetic algorithms, and\nstochastic gradient descent and find that all algorithms can reduce the cycle\ntime for various evaluated tasks in synthetic and real-world environments.\nStochastic gradient descent shows superior performance with regard to success\nrate solving over 90% of our real-world tasks, while genetic algorithms show\nthe lowest final costs. All benchmarks and implemented methods are available as\nbaselines against which novel approaches can be compared.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic automation is a key technology that increases the efficiency and\nflexibility of manufacturing processes. However, one of the challenges in\ndeploying robots in novel environments is finding the optimal base pose for the\nrobot, which affects its reachability and deployment cost. Yet, the existing\nresearch for automatically optimizing the base pose of robots has not been\ncompared. We address this problem by optimizing the base pose of industrial\nrobots with Bayesian optimization, exhaustive search, genetic algorithms, and\nstochastic gradient descent and find that all algorithms can reduce the cycle\ntime for various evaluated tasks in synthetic and real-world environments.\nStochastic gradient descent shows superior performance with regard to success\nrate solving over 90% of our real-world tasks, while genetic algorithms show\nthe lowest final costs. All benchmarks and implemented methods are available as\nbaselines against which novel approaches can be compared."
                },
                "authors": [
                    {
                        "name": "Matthias Mayer"
                    },
                    {
                        "name": "Matthias Althoff"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Althoff"
                },
                "author": "Matthias Althoff",
                "arxiv_comment": "7 pages, 4 Figures, 2 Tables Find visualizations and source code at\n  https://cobra.cps.cit.tum.de/tools/rbo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19565v1",
                "updated": "2025-04-28T08:18:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    18,
                    24,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T08:18:24Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    18,
                    24,
                    0,
                    118,
                    0
                ],
                "title": "m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation\n  Framework for Biomedical Large Language Models Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation\n  Framework for Biomedical Large Language Models Training"
                },
                "summary": "The rapid progress of large language models (LLMs) in biomedical research has\nunderscored the limitations of existing open-source annotated scientific\ncorpora, which are often insufficient in quantity and quality. Addressing the\nchallenge posed by the complex hierarchy of biomedical knowledge, we propose a\nknowledge-driven, multi-agent framework for scientific corpus distillation\ntailored for LLM training in the biomedical domain. Central to our approach is\na collaborative multi-agent architecture, where specialized agents, each guided\nby the Medical Subject Headings (MeSH) hierarchy, work in concert to\nautonomously extract, synthesize, and self-evaluate high-quality textual data\nfrom vast scientific literature. These agents collectively generate and refine\ndomain-specific question-answer pairs, ensuring comprehensive coverage and\nconsistency with biomedical ontologies while minimizing manual involvement.\nExtensive experimental results show that language models trained on our\nmulti-agent distilled datasets achieve notable improvements in biomedical\nquestion-answering tasks, outperforming both strong life sciences LLM baselines\nand advanced proprietary models. Notably, our AI-Ready dataset enables\nLlama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2, despite their larger\nscale. Detailed ablation studies and case analyses further validate the\neffectiveness and synergy of each agent within the framework, highlighting the\npotential of multi-agent collaboration in biomedical LLM training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress of large language models (LLMs) in biomedical research has\nunderscored the limitations of existing open-source annotated scientific\ncorpora, which are often insufficient in quantity and quality. Addressing the\nchallenge posed by the complex hierarchy of biomedical knowledge, we propose a\nknowledge-driven, multi-agent framework for scientific corpus distillation\ntailored for LLM training in the biomedical domain. Central to our approach is\na collaborative multi-agent architecture, where specialized agents, each guided\nby the Medical Subject Headings (MeSH) hierarchy, work in concert to\nautonomously extract, synthesize, and self-evaluate high-quality textual data\nfrom vast scientific literature. These agents collectively generate and refine\ndomain-specific question-answer pairs, ensuring comprehensive coverage and\nconsistency with biomedical ontologies while minimizing manual involvement.\nExtensive experimental results show that language models trained on our\nmulti-agent distilled datasets achieve notable improvements in biomedical\nquestion-answering tasks, outperforming both strong life sciences LLM baselines\nand advanced proprietary models. Notably, our AI-Ready dataset enables\nLlama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2, despite their larger\nscale. Detailed ablation studies and case analyses further validate the\neffectiveness and synergy of each agent within the framework, highlighting the\npotential of multi-agent collaboration in biomedical LLM training."
                },
                "authors": [
                    {
                        "name": "Meng Xiao"
                    },
                    {
                        "name": "Xunxin Cai"
                    },
                    {
                        "name": "Chengrui Wang"
                    },
                    {
                        "name": "Yuanchun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yuanchun Zhou"
                },
                "author": "Yuanchun Zhou",
                "arxiv_comment": "22 pages, Large Language Model, Agentic AI, Dataset Distillation,\n  Multi-agent Collaboration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19564v1",
                "updated": "2025-04-28T08:17:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    17,
                    56,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T08:17:56Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    17,
                    56,
                    0,
                    118,
                    0
                ],
                "title": "Lifecycle Management of Optical Networks with Dynamic-Updating Digital\n  Twin: A Hybrid Data-Driven and Physics-Informed Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lifecycle Management of Optical Networks with Dynamic-Updating Digital\n  Twin: A Hybrid Data-Driven and Physics-Informed Approach"
                },
                "summary": "Digital twin (DT) techniques have been proposed for the autonomous operation\nand lifecycle management of next-generation optical networks. To fully utilize\npotential capacity and accommodate dynamic services, the DT must dynamically\nupdate in sync with deployed optical networks throughout their lifecycle,\nensuring low-margin operation. This paper proposes a dynamic-updating DT for\nthe lifecycle management of optical networks, employing a hybrid approach that\nintegrates data-driven and physics-informed techniques for fiber channel\nmodeling. This integration ensures both rapid calculation speed and high\nphysics consistency in optical performance prediction while enabling the\ndynamic updating of critical physical parameters for DT. The lifecycle\nmanagement of optical networks, covering accurate performance prediction at the\nnetwork deployment and dynamic updating during network operation, is\ndemonstrated through simulation in a large-scale network. Up to 100 times\nspeedup in prediction is observed compared to classical numerical methods. In\naddition, the fiber Raman gain strength, amplifier frequency-dependent gain\nprofile, and connector loss between fiber and amplifier on C and L bands can be\nsimultaneously updated. Moreover, the dynamic-updating DT is verified on a\nfield-trial C+L-band transmission link, achieving a maximum accuracy\nimprovement of 1.4 dB for performance estimation post-device replacement.\nOverall, the dynamic-updating DT holds promise for driving the next-generation\noptical networks towards lifecycle autonomous management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital twin (DT) techniques have been proposed for the autonomous operation\nand lifecycle management of next-generation optical networks. To fully utilize\npotential capacity and accommodate dynamic services, the DT must dynamically\nupdate in sync with deployed optical networks throughout their lifecycle,\nensuring low-margin operation. This paper proposes a dynamic-updating DT for\nthe lifecycle management of optical networks, employing a hybrid approach that\nintegrates data-driven and physics-informed techniques for fiber channel\nmodeling. This integration ensures both rapid calculation speed and high\nphysics consistency in optical performance prediction while enabling the\ndynamic updating of critical physical parameters for DT. The lifecycle\nmanagement of optical networks, covering accurate performance prediction at the\nnetwork deployment and dynamic updating during network operation, is\ndemonstrated through simulation in a large-scale network. Up to 100 times\nspeedup in prediction is observed compared to classical numerical methods. In\naddition, the fiber Raman gain strength, amplifier frequency-dependent gain\nprofile, and connector loss between fiber and amplifier on C and L bands can be\nsimultaneously updated. Moreover, the dynamic-updating DT is verified on a\nfield-trial C+L-band transmission link, achieving a maximum accuracy\nimprovement of 1.4 dB for performance estimation post-device replacement.\nOverall, the dynamic-updating DT holds promise for driving the next-generation\noptical networks towards lifecycle autonomous management."
                },
                "authors": [
                    {
                        "name": "Yuchen Song"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Yao Zhang"
                    },
                    {
                        "name": "Yan Shi"
                    },
                    {
                        "name": "Shikui Shen"
                    },
                    {
                        "name": "Xiongyan Tang"
                    },
                    {
                        "name": "Shanguo Huang"
                    },
                    {
                        "name": "Danshi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Danshi Wang"
                },
                "author": "Danshi Wang",
                "arxiv_doi": "10.1109/JSAC.2025.3543489",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JSAC.2025.3543489",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.19564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This article has been accepted for publication in IEEE Journal on\n  Selected Areas in Communications",
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18200v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18200v2",
                "updated": "2025-04-28T07:59:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    7,
                    59,
                    41,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-25T09:30:43Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    9,
                    30,
                    43,
                    4,
                    115,
                    0
                ],
                "title": "Implementation Analysis of Collaborative Robot Digital Twins in Physics\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implementation Analysis of Collaborative Robot Digital Twins in Physics\n  Engines"
                },
                "summary": "This paper presents a Digital Twin (DT) of a 6G communications system testbed\nthat integrates two robotic manipulators with a high-precision optical infrared\ntracking system in Unreal Engine 5. Practical details of the setup and\nimplementation insights provide valuable guidance for users aiming to replicate\nsuch systems, an endeavor that is crucial to advancing DT applications within\nthe scientific community. Key topics discussed include video streaming,\nintegration within the Robot Operating System 2 (ROS 2), and bidirectional\ncommunication. The insights provided are intended to support the development\nand deployment of DTs in robotics and automation research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a Digital Twin (DT) of a 6G communications system testbed\nthat integrates two robotic manipulators with a high-precision optical infrared\ntracking system in Unreal Engine 5. Practical details of the setup and\nimplementation insights provide valuable guidance for users aiming to replicate\nsuch systems, an endeavor that is crucial to advancing DT applications within\nthe scientific community. Key topics discussed include video streaming,\nintegration within the Robot Operating System 2 (ROS 2), and bidirectional\ncommunication. The insights provided are intended to support the development\nand deployment of DTs in robotics and automation research."
                },
                "authors": [
                    {
                        "name": "Christian Knig"
                    },
                    {
                        "name": "Jan Petershans"
                    },
                    {
                        "name": "Jan Herbst"
                    },
                    {
                        "name": "Matthias Rb"
                    },
                    {
                        "name": "Dennis Krummacker"
                    },
                    {
                        "name": "Eric Mittag"
                    },
                    {
                        "name": "Hans D. Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D. Schotten"
                },
                "author": "Hans D. Schotten",
                "arxiv_comment": "15 Pages, 3 figures, 7th International Congress on Human-Computer\n  Interaction, Optimization and Robotic Applications (ICHORA 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18200v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18200v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10652v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10652v2",
                "updated": "2025-04-28T07:58:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    7,
                    58,
                    34,
                    0,
                    118,
                    0
                ],
                "published": "2024-07-15T12:13:53Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    12,
                    13,
                    53,
                    0,
                    197,
                    0
                ],
                "title": "Cutting Through the Clutter: The Potential of LLMs for Efficient\n  Filtration in Systematic Literature Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cutting Through the Clutter: The Potential of LLMs for Efficient\n  Filtration in Systematic Literature Reviews"
                },
                "summary": "Systematic literature reviews (SLRs) are essential but labor-intensive due to\nhigh publication volumes and inefficient keyword-based filtering. To streamline\nthis process, we evaluate Large Language Models (LLMs) for enhancing efficiency\nand accuracy in corpus filtration while minimizing manual effort. Our\nopen-source tool LLMSurver presents a visual interface to utilize LLMs for\nliterature filtration, evaluate the results, and refine queries in an\ninteractive way. We assess the real-world performance of our approach in\nfiltering over 8.3k articles during a recent survey construction, comparing\nresults with human efforts. The findings show that recent LLM models can reduce\nfiltering time from weeks to minutes. A consensus scheme ensures recall rates\n>98.8%, surpassing typical human error thresholds and improving selection\naccuracy. This work advances literature review methodologies and highlights the\npotential of responsible human-AI collaboration in academic research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic literature reviews (SLRs) are essential but labor-intensive due to\nhigh publication volumes and inefficient keyword-based filtering. To streamline\nthis process, we evaluate Large Language Models (LLMs) for enhancing efficiency\nand accuracy in corpus filtration while minimizing manual effort. Our\nopen-source tool LLMSurver presents a visual interface to utilize LLMs for\nliterature filtration, evaluate the results, and refine queries in an\ninteractive way. We assess the real-world performance of our approach in\nfiltering over 8.3k articles during a recent survey construction, comparing\nresults with human efforts. The findings show that recent LLM models can reduce\nfiltering time from weeks to minutes. A consensus scheme ensures recall rates\n>98.8%, surpassing typical human error thresholds and improving selection\naccuracy. This work advances literature review methodologies and highlights the\npotential of responsible human-AI collaboration in academic research."
                },
                "authors": [
                    {
                        "name": "Lucas Joos"
                    },
                    {
                        "name": "Daniel A. Keim"
                    },
                    {
                        "name": "Maximilian T. Fischer"
                    }
                ],
                "author_detail": {
                    "name": "Maximilian T. Fischer"
                },
                "author": "Maximilian T. Fischer",
                "arxiv_comment": "6 pages, 5 figures, 1 table",
                "arxiv_journal_ref": "16th International EuroVis Workshop on Visual Analytics\n  (EuroVA'25), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10652v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10652v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19550v1",
                "updated": "2025-04-28T07:57:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    7,
                    57,
                    14,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T07:57:14Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    7,
                    57,
                    14,
                    0,
                    118,
                    0
                ],
                "title": "Deployment Optimization for XL-IRS Assisted Multi-User Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deployment Optimization for XL-IRS Assisted Multi-User Communications"
                },
                "summary": "In this paper, we study the deployment optimization for an extremely\nlarge-scale intelligent reflecting surface (XL-IRS) assisted multi-user\ncommunication system, within which the channels between the XL-IRS and the BS\n(or user) are modeled by the near-field spherical wavefronts. To draw some\nvaluable insights, we first consider the single-user case, where an alternating\noptimization (AO) based algorithm is devised to maximize the received\nsignal-to-noise ratio (SNR) at the user. To address the high computational\ncomplexity issue incurred by the AO based algorithm, three approximate received\nSNR expressions are obtained to yield useful insights, corresponding to the\nupper bound, approximate expression, and closed-form. It is demonstrated that\nthe XL-IRS ought to be positioned near the user (rather than the BS) to obtain\na higher beamforming gain. Then, for the multi-user scenario, an efficient\nalgorithm is proposed to obtain a high-quality XL-IRS placement solution by\nusing the AO and successive convex approximation (SCA) techniques. Furthermore,\nthe effective degree of freedom (DoF) of the BS-IRS channel is provided, which\nindicates that the additional effective DoF can be leveraged to improve\nmulti-user spatial multiplexing. Last, numerical results confirm the existence\nof a trade-off between near-field beam-focusing gain and multiplexing gain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study the deployment optimization for an extremely\nlarge-scale intelligent reflecting surface (XL-IRS) assisted multi-user\ncommunication system, within which the channels between the XL-IRS and the BS\n(or user) are modeled by the near-field spherical wavefronts. To draw some\nvaluable insights, we first consider the single-user case, where an alternating\noptimization (AO) based algorithm is devised to maximize the received\nsignal-to-noise ratio (SNR) at the user. To address the high computational\ncomplexity issue incurred by the AO based algorithm, three approximate received\nSNR expressions are obtained to yield useful insights, corresponding to the\nupper bound, approximate expression, and closed-form. It is demonstrated that\nthe XL-IRS ought to be positioned near the user (rather than the BS) to obtain\na higher beamforming gain. Then, for the multi-user scenario, an efficient\nalgorithm is proposed to obtain a high-quality XL-IRS placement solution by\nusing the AO and successive convex approximation (SCA) techniques. Furthermore,\nthe effective degree of freedom (DoF) of the BS-IRS channel is provided, which\nindicates that the additional effective DoF can be leveraged to improve\nmulti-user spatial multiplexing. Last, numerical results confirm the existence\nof a trade-off between near-field beam-focusing gain and multiplexing gain."
                },
                "authors": [
                    {
                        "name": "Chao Zhou"
                    },
                    {
                        "name": "Changsheng You"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Bin Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Bin Lyu"
                },
                "author": "Bin Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08539v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08539v2",
                "updated": "2025-04-28T07:32:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    7,
                    32,
                    42,
                    0,
                    118,
                    0
                ],
                "published": "2025-03-11T15:28:09Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    28,
                    9,
                    1,
                    70,
                    0
                ],
                "title": "Desirable Unfamiliarity: Insights from Eye Movements on Engagement and\n  Readability of Dictation Interfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Desirable Unfamiliarity: Insights from Eye Movements on Engagement and\n  Readability of Dictation Interfaces"
                },
                "summary": "Dictation interfaces support efficient text input, but the transcribed text\ncan be hard to read. To understand how users read and review dictated text, we\nconducted a controlled eye-tracking experiment with 20 participants to compare\nfive dictation interfaces: PLAIN (real-time transcription), AOC (periodic\ncorrections), RAKE (keyword highlights), GP-TSM (grammar-preserving\nhighlights), and SUMMARY (LLM-generated abstraction summary). The study\nanalyzed participants' gaze patterns during their speech composition and\nreviewing processes. The findings show that during composition, participants\nspent only 7--11% of their time actively reading, and they favored real-time\nfeedback and avoided distracting interface changes. During reviewing, although\nSUMMARY introduced unfamiliar words (requiring longer and more frequent\nfixation), they were easier to read (requiring fewer regressions). Participants\npreferred SUMMARY for the polished text that preserved fidelity to original\nmeanings. RAKE guided the reading of self-produced text better than GP-TSM.\nRAKE guides the reading of self-produced text better than GP-TSM. These\nsurprising findings suggest that dictation interfaces could consider showing\nsummaries or key information to support recall instead of raw transcripts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dictation interfaces support efficient text input, but the transcribed text\ncan be hard to read. To understand how users read and review dictated text, we\nconducted a controlled eye-tracking experiment with 20 participants to compare\nfive dictation interfaces: PLAIN (real-time transcription), AOC (periodic\ncorrections), RAKE (keyword highlights), GP-TSM (grammar-preserving\nhighlights), and SUMMARY (LLM-generated abstraction summary). The study\nanalyzed participants' gaze patterns during their speech composition and\nreviewing processes. The findings show that during composition, participants\nspent only 7--11% of their time actively reading, and they favored real-time\nfeedback and avoided distracting interface changes. During reviewing, although\nSUMMARY introduced unfamiliar words (requiring longer and more frequent\nfixation), they were easier to read (requiring fewer regressions). Participants\npreferred SUMMARY for the polished text that preserved fidelity to original\nmeanings. RAKE guided the reading of self-produced text better than GP-TSM.\nRAKE guides the reading of self-produced text better than GP-TSM. These\nsurprising findings suggest that dictation interfaces could consider showing\nsummaries or key information to support recall instead of raw transcripts."
                },
                "authors": [
                    {
                        "name": "Zhaohui Liang"
                    },
                    {
                        "name": "Yonglin Chen"
                    },
                    {
                        "name": "Naser Al Madi"
                    },
                    {
                        "name": "Can Liu"
                    }
                ],
                "author_detail": {
                    "name": "Can Liu"
                },
                "author": "Can Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08539v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08539v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19521v1",
                "updated": "2025-04-28T06:40:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    6,
                    40,
                    1,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T06:40:01Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    6,
                    40,
                    1,
                    0,
                    118,
                    0
                ],
                "title": "Security Steerability is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security Steerability is All You Need"
                },
                "summary": "The adoption of Generative AI (GenAI) in various applications inevitably\ncomes with expanding the attack surface, combining new security threats along\nwith the traditional ones. Consequently, numerous research and industrial\ninitiatives aim to mitigate these security threats in GenAI by developing\nmetrics and designing defenses. However, while most of the GenAI security work\nfocuses on universal threats (e.g. manipulating the LLM to generate forbidden\ncontent), there is significantly less discussion on application-level security\nand how to mitigate it.\n  Thus, in this work we adopt an application-centric approach to GenAI\nsecurity, and show that while LLMs cannot protect against ad-hoc application\nspecific threats, they can provide the framework for applications to protect\nthemselves against such threats. Our first contribution is defining Security\nSteerability - a novel security measure for LLMs, assessing the model's\ncapability to adhere to strict guardrails that are defined in the system prompt\n('Refrain from discussing about politics'). These guardrails, in case\neffective, can stop threats in the presence of malicious users who attempt to\ncircumvent the application and cause harm to its providers.\n  Our second contribution is a methodology to measure the security steerability\nof LLMs, utilizing two newly-developed datasets: VeganRibs assesses the LLM\nbehavior in forcing specific guardrails that are not security per se in the\npresence of malicious user that uses attack boosters (jailbreaks and\nperturbations), and ReverseText takes this approach further and measures the\nLLM ability to force specific treatment of the user input as plain text while\ndo user try to give it additional meanings...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of Generative AI (GenAI) in various applications inevitably\ncomes with expanding the attack surface, combining new security threats along\nwith the traditional ones. Consequently, numerous research and industrial\ninitiatives aim to mitigate these security threats in GenAI by developing\nmetrics and designing defenses. However, while most of the GenAI security work\nfocuses on universal threats (e.g. manipulating the LLM to generate forbidden\ncontent), there is significantly less discussion on application-level security\nand how to mitigate it.\n  Thus, in this work we adopt an application-centric approach to GenAI\nsecurity, and show that while LLMs cannot protect against ad-hoc application\nspecific threats, they can provide the framework for applications to protect\nthemselves against such threats. Our first contribution is defining Security\nSteerability - a novel security measure for LLMs, assessing the model's\ncapability to adhere to strict guardrails that are defined in the system prompt\n('Refrain from discussing about politics'). These guardrails, in case\neffective, can stop threats in the presence of malicious users who attempt to\ncircumvent the application and cause harm to its providers.\n  Our second contribution is a methodology to measure the security steerability\nof LLMs, utilizing two newly-developed datasets: VeganRibs assesses the LLM\nbehavior in forcing specific guardrails that are not security per se in the\npresence of malicious user that uses attack boosters (jailbreaks and\nperturbations), and ReverseText takes this approach further and measures the\nLLM ability to force specific treatment of the user input as plain text while\ndo user try to give it additional meanings..."
                },
                "authors": [
                    {
                        "name": "Itay Hazan"
                    },
                    {
                        "name": "Idan Habler"
                    },
                    {
                        "name": "Ron Bitton"
                    },
                    {
                        "name": "Itsik Mantin"
                    }
                ],
                "author_detail": {
                    "name": "Itsik Mantin"
                },
                "author": "Itsik Mantin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19516v1",
                "updated": "2025-04-28T06:26:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    6,
                    26,
                    21,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T06:26:21Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    6,
                    26,
                    21,
                    0,
                    118,
                    0
                ],
                "title": "Bullet: Boosting GPU Utilization for LLM Serving via Dynamic\n  Spatial-Temporal Orchestration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bullet: Boosting GPU Utilization for LLM Serving via Dynamic\n  Spatial-Temporal Orchestration"
                },
                "summary": "Modern LLM serving systems confront inefficient GPU utilization due to the\nfundamental mismatch between compute-intensive prefill and memory-bound decode\nphases. While current practices attempt to address this by organizing these\nphases into hybrid batches, such solutions create an inefficient tradeoff that\nsacrifices either throughput or latency, leaving substantial GPU resources\nunderutilized. We identify two key root causes: 1) the prefill phase suffers\nfrom suboptimal compute utilization due to wave quantization and attention\nbottlenecks. 2) hybrid batches disproportionately prioritize latency over\nthroughput, resulting in wasted compute and memory bandwidth. To mitigate the\nissues, we present Bullet, a novel spatial-temporal orchestration system that\neliminates these inefficiencies through precise phase coordination. Bullet\nenables concurrent execution of prefill and decode phases, while dynamically\nprovisioning GPU resources using real-time performance modeling. By integrating\nSLO-aware scheduling and adaptive resource allocation, Bullet maximizes\nutilization without compromising latency targets. Experimental evaluations on\nreal-world workloads demonstrate that Bullet delivers 1.26x average throughput\ngains (up to 1.55x) over state-of-the-arts, while consistently meeting latency\nconstraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM serving systems confront inefficient GPU utilization due to the\nfundamental mismatch between compute-intensive prefill and memory-bound decode\nphases. While current practices attempt to address this by organizing these\nphases into hybrid batches, such solutions create an inefficient tradeoff that\nsacrifices either throughput or latency, leaving substantial GPU resources\nunderutilized. We identify two key root causes: 1) the prefill phase suffers\nfrom suboptimal compute utilization due to wave quantization and attention\nbottlenecks. 2) hybrid batches disproportionately prioritize latency over\nthroughput, resulting in wasted compute and memory bandwidth. To mitigate the\nissues, we present Bullet, a novel spatial-temporal orchestration system that\neliminates these inefficiencies through precise phase coordination. Bullet\nenables concurrent execution of prefill and decode phases, while dynamically\nprovisioning GPU resources using real-time performance modeling. By integrating\nSLO-aware scheduling and adaptive resource allocation, Bullet maximizes\nutilization without compromising latency targets. Experimental evaluations on\nreal-world workloads demonstrate that Bullet delivers 1.26x average throughput\ngains (up to 1.55x) over state-of-the-arts, while consistently meeting latency\nconstraints."
                },
                "authors": [
                    {
                        "name": "Zejia Lin"
                    },
                    {
                        "name": "Hongxin Xu"
                    },
                    {
                        "name": "Guanyi Chen"
                    },
                    {
                        "name": "Xianwei Zhang"
                    },
                    {
                        "name": "Yutong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Lu"
                },
                "author": "Yutong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11975v2",
                "updated": "2025-04-28T06:19:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    6,
                    19,
                    32,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-16T11:15:26Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    11,
                    15,
                    26,
                    2,
                    106,
                    0
                ],
                "title": "SemEval-2025 Task 3: Mu-SHROOM, the Multilingual Shared Task on\n  Hallucinations and Related Observable Overgeneration Mistakes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemEval-2025 Task 3: Mu-SHROOM, the Multilingual Shared Task on\n  Hallucinations and Related Observable Overgeneration Mistakes"
                },
                "summary": "We present the Mu-SHROOM shared task which is focused on detecting\nhallucinations and other overgeneration mistakes in the output of\ninstruction-tuned large language models (LLMs). Mu-SHROOM addresses\ngeneral-purpose LLMs in 14 languages, and frames the hallucination detection\nproblem as a span-labeling task. We received 2,618 submissions from 43\nparticipating teams employing diverse methodologies. The large number of\nsubmissions underscores the interest of the community in hallucination\ndetection. We present the results of the participating systems and conduct an\nempirical analysis to identify key factors contributing to strong performance\nin this task. We also emphasize relevant current challenges, notably the\nvarying degree of hallucinations across languages and the high annotator\ndisagreement when labeling hallucination spans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the Mu-SHROOM shared task which is focused on detecting\nhallucinations and other overgeneration mistakes in the output of\ninstruction-tuned large language models (LLMs). Mu-SHROOM addresses\ngeneral-purpose LLMs in 14 languages, and frames the hallucination detection\nproblem as a span-labeling task. We received 2,618 submissions from 43\nparticipating teams employing diverse methodologies. The large number of\nsubmissions underscores the interest of the community in hallucination\ndetection. We present the results of the participating systems and conduct an\nempirical analysis to identify key factors contributing to strong performance\nin this task. We also emphasize relevant current challenges, notably the\nvarying degree of hallucinations across languages and the high annotator\ndisagreement when labeling hallucination spans."
                },
                "authors": [
                    {
                        "name": "Ral Vzquez"
                    },
                    {
                        "name": "Timothee Mickus"
                    },
                    {
                        "name": "Elaine Zosa"
                    },
                    {
                        "name": "Teemu Vahtola"
                    },
                    {
                        "name": "Jrg Tiedemann"
                    },
                    {
                        "name": "Aman Sinha"
                    },
                    {
                        "name": "Vincent Segonne"
                    },
                    {
                        "name": "Fernando Snchez-Vega"
                    },
                    {
                        "name": "Alessandro Raganato"
                    },
                    {
                        "name": "Jindich Libovick"
                    },
                    {
                        "name": "Jussi Karlgren"
                    },
                    {
                        "name": "Shaoxiong Ji"
                    },
                    {
                        "name": "Jindich Helcl"
                    },
                    {
                        "name": "Liane Guillou"
                    },
                    {
                        "name": "Ona de Gibert"
                    },
                    {
                        "name": "Jaione Bengoetxea"
                    },
                    {
                        "name": "Joseph Attieh"
                    },
                    {
                        "name": "Marianna Apidianaki"
                    }
                ],
                "author_detail": {
                    "name": "Marianna Apidianaki"
                },
                "author": "Marianna Apidianaki",
                "arxiv_comment": "Mu-SHROOM is part of SemEval-2025 (Task 3). TBP: Proceedings of the\n  19th International Workshop on Semantic Evaluation (SemEval-2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17596v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17596v3",
                "updated": "2025-04-28T06:12:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    6,
                    12,
                    14,
                    0,
                    118,
                    0
                ],
                "published": "2024-12-23T14:13:44Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    13,
                    44,
                    0,
                    358,
                    0
                ],
                "title": "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea\n  Generation with Minimal Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea\n  Generation with Minimal Context"
                },
                "summary": "While Large Language Models (LLMs) demonstrate remarkable capabilities in\nscientific tasks such as literature analysis and experimental design (e.g.,\naccurately extracting key findings from papers or generating coherent\nexperimental procedures), existing evaluation benchmarks primarily assess\nperformance using rich contextual inputs. We introduce LiveIdeaBench, a\ncomprehensive benchmark evaluating LLMs' scientific idea generation by\nassessing divergent thinking capabilities using single-keyword prompts. Drawing\nfrom Guilford's creativity theory, our benchmark employs a dynamic panel of\nstate-of-the-art LLMs to assess generated ideas across five key dimensions:\noriginality, feasibility, fluency, flexibility, and clarity. Through extensive\nexperimentation with over 40 leading models across 1,180 keywords spanning 22\nscientific domains, we reveal that the scientific idea generation capabilities\nmeasured by our benchmark, are poorly predicted by standard metrics of general\nintelligence. Our results demonstrate that models like QwQ-32B-preview achieve\ncreative performance comparable to top-tier models such as\nclaude-3.7-sonnet:thinking, despite significant gaps in their general\nintelligence scores. These findings highlight the need for specialized\nevaluation benchmarks for scientific idea generation and suggest that enhancing\nthese idea generation capabilities in LLMs may require different training\nstrategies than those used for improving general problem-solving abilities,\npotentially enabling a wider range of AI tools tailored for different stages of\nthe scientific process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) demonstrate remarkable capabilities in\nscientific tasks such as literature analysis and experimental design (e.g.,\naccurately extracting key findings from papers or generating coherent\nexperimental procedures), existing evaluation benchmarks primarily assess\nperformance using rich contextual inputs. We introduce LiveIdeaBench, a\ncomprehensive benchmark evaluating LLMs' scientific idea generation by\nassessing divergent thinking capabilities using single-keyword prompts. Drawing\nfrom Guilford's creativity theory, our benchmark employs a dynamic panel of\nstate-of-the-art LLMs to assess generated ideas across five key dimensions:\noriginality, feasibility, fluency, flexibility, and clarity. Through extensive\nexperimentation with over 40 leading models across 1,180 keywords spanning 22\nscientific domains, we reveal that the scientific idea generation capabilities\nmeasured by our benchmark, are poorly predicted by standard metrics of general\nintelligence. Our results demonstrate that models like QwQ-32B-preview achieve\ncreative performance comparable to top-tier models such as\nclaude-3.7-sonnet:thinking, despite significant gaps in their general\nintelligence scores. These findings highlight the need for specialized\nevaluation benchmarks for scientific idea generation and suggest that enhancing\nthese idea generation capabilities in LLMs may require different training\nstrategies than those used for improving general problem-solving abilities,\npotentially enabling a wider range of AI tools tailored for different stages of\nthe scientific process."
                },
                "authors": [
                    {
                        "name": "Kai Ruan"
                    },
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Jixiang Hong"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Hao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Hao Sun"
                },
                "author": "Hao Sun",
                "arxiv_comment": "Updated manuscript and title",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17596v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17596v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08985v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08985v2",
                "updated": "2025-04-28T06:10:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    6,
                    10,
                    19,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-11T21:30:44Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    21,
                    30,
                    44,
                    4,
                    101,
                    0
                ],
                "title": "Learning from Elders: Making an LLM-powered Chatbot for Retirement\n  Communities more Accessible through User-centered Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Elders: Making an LLM-powered Chatbot for Retirement\n  Communities more Accessible through User-centered Design"
                },
                "summary": "Low technology and eHealth literacy among older adults in retirement\ncommunities hinder engagement with digital tools. To address this, we designed\nan LLM-powered chatbot prototype using a human-centered approach for a local\nretirement community. Through interviews and persona development, we\nprioritized accessibility and dual functionality: simplifying internal\ninformation retrieval and improving technology and eHealth literacy. A pilot\ntrial with residents demonstrated high satisfaction and ease of use, but also\nidentified areas for further improvement. Based on the feedback, we refined the\nchatbot using GPT-3.5 Turbo and Streamlit. The chatbot employs tailored prompt\nengineering to deliver concise responses. Accessible features like adjustable\nfont size, interface theme and personalized follow-up responses were\nimplemented. Future steps include enabling voice-to-text function and\nlongitudinal intervention studies. Together, our results highlight the\npotential of LLM-driven chatbots to empower older adults through accessible,\npersonalized interactions, bridging literacy gaps in retirement communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low technology and eHealth literacy among older adults in retirement\ncommunities hinder engagement with digital tools. To address this, we designed\nan LLM-powered chatbot prototype using a human-centered approach for a local\nretirement community. Through interviews and persona development, we\nprioritized accessibility and dual functionality: simplifying internal\ninformation retrieval and improving technology and eHealth literacy. A pilot\ntrial with residents demonstrated high satisfaction and ease of use, but also\nidentified areas for further improvement. Based on the feedback, we refined the\nchatbot using GPT-3.5 Turbo and Streamlit. The chatbot employs tailored prompt\nengineering to deliver concise responses. Accessible features like adjustable\nfont size, interface theme and personalized follow-up responses were\nimplemented. Future steps include enabling voice-to-text function and\nlongitudinal intervention studies. Together, our results highlight the\npotential of LLM-driven chatbots to empower older adults through accessible,\npersonalized interactions, bridging literacy gaps in retirement communities."
                },
                "authors": [
                    {
                        "name": "Luna Xingyu Li"
                    },
                    {
                        "name": "Ray-yuan Chung"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Wenyu Zeng"
                    },
                    {
                        "name": "Yein Jeon"
                    },
                    {
                        "name": "Oleg Zaslavsky"
                    }
                ],
                "author_detail": {
                    "name": "Oleg Zaslavsky"
                },
                "author": "Oleg Zaslavsky",
                "arxiv_doi": "10.5281/zenodo.15292697",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5281/zenodo.15292697",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.08985v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08985v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted as Research talk for Considering Cultural and Linguistic\n  Diversity in AI Applications workshop at CALD-AI@ASIS&T 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20601v2",
                "updated": "2025-04-28T05:39:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    5,
                    39,
                    17,
                    0,
                    118,
                    0
                ],
                "published": "2025-02-28T00:05:49Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    0,
                    5,
                    49,
                    4,
                    59,
                    0
                ],
                "title": "NutriGen: Personalized Meal Plan Generator Leveraging Large Language\n  Models to Enhance Dietary and Nutritional Adherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NutriGen: Personalized Meal Plan Generator Leveraging Large Language\n  Models to Enhance Dietary and Nutritional Adherence"
                },
                "summary": "Maintaining a balanced diet is essential for overall health, yet many\nindividuals struggle with meal planning due to nutritional complexity, time\nconstraints, and lack of dietary knowledge. Personalized food recommendations\ncan help address these challenges by tailoring meal plans to individual\npreferences, habits, and dietary restrictions. However, existing dietary\nrecommendation systems often lack adaptability, fail to consider real-world\nconstraints such as food ingredient availability, and require extensive user\ninput, making them impractical for sustainable and scalable daily use. To\naddress these limitations, we introduce NutriGen, a framework based on large\nlanguage models (LLM) designed to generate personalized meal plans that align\nwith user-defined dietary preferences and constraints. By building a\npersonalized nutrition database and leveraging prompt engineering, our approach\nenables LLMs to incorporate reliable nutritional references like the USDA\nnutrition database while maintaining flexibility and ease-of-use. We\ndemonstrate that LLMs have strong potential in generating accurate and\nuser-friendly food recommendations, addressing key limitations in existing\ndietary recommendation systems by providing structured, practical, and scalable\nmeal plans. Our evaluation shows that Llama 3.1 8B and GPT-3.5 Turbo achieve\nthe lowest percentage errors of 1.55\\% and 3.68\\%, respectively, producing meal\nplans that closely align with user-defined caloric targets while minimizing\ndeviation and improving precision. Additionally, we compared the performance of\nDeepSeek V3 against several established models to evaluate its potential in\npersonalized nutrition planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maintaining a balanced diet is essential for overall health, yet many\nindividuals struggle with meal planning due to nutritional complexity, time\nconstraints, and lack of dietary knowledge. Personalized food recommendations\ncan help address these challenges by tailoring meal plans to individual\npreferences, habits, and dietary restrictions. However, existing dietary\nrecommendation systems often lack adaptability, fail to consider real-world\nconstraints such as food ingredient availability, and require extensive user\ninput, making them impractical for sustainable and scalable daily use. To\naddress these limitations, we introduce NutriGen, a framework based on large\nlanguage models (LLM) designed to generate personalized meal plans that align\nwith user-defined dietary preferences and constraints. By building a\npersonalized nutrition database and leveraging prompt engineering, our approach\nenables LLMs to incorporate reliable nutritional references like the USDA\nnutrition database while maintaining flexibility and ease-of-use. We\ndemonstrate that LLMs have strong potential in generating accurate and\nuser-friendly food recommendations, addressing key limitations in existing\ndietary recommendation systems by providing structured, practical, and scalable\nmeal plans. Our evaluation shows that Llama 3.1 8B and GPT-3.5 Turbo achieve\nthe lowest percentage errors of 1.55\\% and 3.68\\%, respectively, producing meal\nplans that closely align with user-defined caloric targets while minimizing\ndeviation and improving precision. Additionally, we compared the performance of\nDeepSeek V3 against several established models to evaluate its potential in\npersonalized nutrition planning."
                },
                "authors": [
                    {
                        "name": "Saman Khamesian"
                    },
                    {
                        "name": "Asiful Arefeen"
                    },
                    {
                        "name": "Stephanie M. Carpenter"
                    },
                    {
                        "name": "Hassan Ghasemzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Hassan Ghasemzadeh"
                },
                "author": "Hassan Ghasemzadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.15217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.15217v2",
                "updated": "2025-04-28T05:09:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    5,
                    9,
                    12,
                    0,
                    118,
                    0
                ],
                "published": "2023-09-26T19:23:54Z",
                "published_parsed": [
                    2023,
                    9,
                    26,
                    19,
                    23,
                    54,
                    1,
                    269,
                    0
                ],
                "title": "Ragas: Automated Evaluation of Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ragas: Automated Evaluation of Retrieval Augmented Generation"
                },
                "summary": "We introduce Ragas (Retrieval Augmented Generation Assessment), a framework\nfor reference-free evaluation of Retrieval Augmented Generation (RAG)\npipelines. RAG systems are composed of a retrieval and an LLM based generation\nmodule, and provide LLMs with knowledge from a reference textual database,\nwhich enables them to act as a natural language layer between a user and\ntextual databases, reducing the risk of hallucinations. Evaluating RAG\narchitectures is, however, challenging because there are several dimensions to\nconsider: the ability of the retrieval system to identify relevant and focused\ncontext passages, the ability of the LLM to exploit such passages in a faithful\nway, or the quality of the generation itself. With Ragas, we put forward a\nsuite of metrics which can be used to evaluate these different dimensions\n\\textit{without having to rely on ground truth human annotations}. We posit\nthat such a framework can crucially contribute to faster evaluation cycles of\nRAG architectures, which is especially important given the fast adoption of\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Ragas (Retrieval Augmented Generation Assessment), a framework\nfor reference-free evaluation of Retrieval Augmented Generation (RAG)\npipelines. RAG systems are composed of a retrieval and an LLM based generation\nmodule, and provide LLMs with knowledge from a reference textual database,\nwhich enables them to act as a natural language layer between a user and\ntextual databases, reducing the risk of hallucinations. Evaluating RAG\narchitectures is, however, challenging because there are several dimensions to\nconsider: the ability of the retrieval system to identify relevant and focused\ncontext passages, the ability of the LLM to exploit such passages in a faithful\nway, or the quality of the generation itself. With Ragas, we put forward a\nsuite of metrics which can be used to evaluate these different dimensions\n\\textit{without having to rely on ground truth human annotations}. We posit\nthat such a framework can crucially contribute to faster evaluation cycles of\nRAG architectures, which is especially important given the fast adoption of\nLLMs."
                },
                "authors": [
                    {
                        "name": "Shahul Es"
                    },
                    {
                        "name": "Jithin James"
                    },
                    {
                        "name": "Luis Espinosa-Anke"
                    },
                    {
                        "name": "Steven Schockaert"
                    }
                ],
                "author_detail": {
                    "name": "Steven Schockaert"
                },
                "author": "Steven Schockaert",
                "arxiv_comment": "Reference-free (not tied to having ground truth available) evaluation\n  framework for retrieval agumented generation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.15217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.15217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19487v1",
                "updated": "2025-04-28T05:07:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    5,
                    7,
                    55,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T05:07:55Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    5,
                    7,
                    55,
                    0,
                    118,
                    0
                ],
                "title": "Evolution of Cooperation in LLM-Agent Societies: A Preliminary Study\n  Using Different Punishment Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolution of Cooperation in LLM-Agent Societies: A Preliminary Study\n  Using Different Punishment Strategies"
                },
                "summary": "The evolution of cooperation has been extensively studied using abstract\nmathematical models and simulations. Recent advances in Large Language Models\n(LLM) and the rise of LLM agents have demonstrated their ability to perform\nsocial reasoning, thus providing an opportunity to test the emergence of norms\nin more realistic agent-based simulations with human-like reasoning using\nnatural language. In this research, we investigate whether the cooperation\ndynamics presented in Boyd and Richerson's model persist in a more realistic\nsimulation of the diner's dilemma using LLM agents compared to the abstract\nmathematical nature in the work of Boyd and Richerson. Our findings indicate\nthat agents follow the strategies defined in the Boyd and Richerson model, and\nexplicit punishment mechanisms drive norm emergence, reinforcing cooperative\nbehaviour even when the agent strategy configuration varies. Our results\nsuggest that LLM-based Multi-Agent System simulations, in fact, can replicate\nthe evolution of cooperation predicted by the traditional mathematical models.\nMoreover, our simulations extend beyond the mathematical models by integrating\nnatural language-driven reasoning and a pairwise imitation method for strategy\nadoption, making them a more realistic testbed for cooperative behaviour in\nMASs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of cooperation has been extensively studied using abstract\nmathematical models and simulations. Recent advances in Large Language Models\n(LLM) and the rise of LLM agents have demonstrated their ability to perform\nsocial reasoning, thus providing an opportunity to test the emergence of norms\nin more realistic agent-based simulations with human-like reasoning using\nnatural language. In this research, we investigate whether the cooperation\ndynamics presented in Boyd and Richerson's model persist in a more realistic\nsimulation of the diner's dilemma using LLM agents compared to the abstract\nmathematical nature in the work of Boyd and Richerson. Our findings indicate\nthat agents follow the strategies defined in the Boyd and Richerson model, and\nexplicit punishment mechanisms drive norm emergence, reinforcing cooperative\nbehaviour even when the agent strategy configuration varies. Our results\nsuggest that LLM-based Multi-Agent System simulations, in fact, can replicate\nthe evolution of cooperation predicted by the traditional mathematical models.\nMoreover, our simulations extend beyond the mathematical models by integrating\nnatural language-driven reasoning and a pairwise imitation method for strategy\nadoption, making them a more realistic testbed for cooperative behaviour in\nMASs."
                },
                "authors": [
                    {
                        "name": "Kavindu Warnakulasuriya"
                    },
                    {
                        "name": "Prabhash Dissanayake"
                    },
                    {
                        "name": "Navindu De Silva"
                    },
                    {
                        "name": "Stephen Cranefield"
                    },
                    {
                        "name": "Bastin Tony Roy Savarimuthu"
                    },
                    {
                        "name": "Surangika Ranathunga"
                    },
                    {
                        "name": "Nisansa de Silva"
                    }
                ],
                "author_detail": {
                    "name": "Nisansa de Silva"
                },
                "author": "Nisansa de Silva",
                "arxiv_comment": "19 pages, 10 figures, Accepted for presentation as a full paper at\n  the COINE 2025 workshop at AAMAS 2025\n  (https://coin-workshop.github.io/coine-2025-detroit/accepted_for_presentation.html)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19483v1",
                "updated": "2025-04-28T04:58:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    58,
                    43,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T04:58:43Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    58,
                    43,
                    0,
                    118,
                    0
                ],
                "title": "Improving Reasoning Performance in Large Language Models via\n  Representation Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Reasoning Performance in Large Language Models via\n  Representation Engineering"
                },
                "summary": "Recent advancements in large language models (LLMs) have resulted in\nincreasingly anthropomorphic language concerning the ability of LLMs to reason.\nWhether reasoning in LLMs should be understood to be inherently different is,\nhowever, widely debated. We propose utilizing a representation engineering\napproach wherein model activations are read from the residual stream of an LLM\nwhen processing a reasoning task. The activations are used to derive a control\nvector that is applied to the model as an inference-time intervention,\nmodulating the representational space of the model, to improve performance on\nthe specified task. We publish the code for deriving control vectors and\nanalyzing model representations. The method allows us to improve performance on\nreasoning benchmarks and assess how control vectors influence the final logit\ndistribution of a model via metrics such as KL divergence and entropy. We apply\ncontrol vectors to Mistral-7B-Instruct and a range of Pythia models on an\ninductive, a deductive and mathematical reasoning task. We show that an LLM\ncan, to a certain degree, be controlled to improve its perceived reasoning\nability by modulating activations. The intervention is dependent upon the\nability to reliably extract the model's typical state when correctly solving a\ntask. Our results suggest that reasoning performance can be modulated in the\nsame manner as other information-processing tasks performed by LLMs and\ndemonstrate that we are capable of improving performance on specific tasks via\na simple intervention on the residual stream with no additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have resulted in\nincreasingly anthropomorphic language concerning the ability of LLMs to reason.\nWhether reasoning in LLMs should be understood to be inherently different is,\nhowever, widely debated. We propose utilizing a representation engineering\napproach wherein model activations are read from the residual stream of an LLM\nwhen processing a reasoning task. The activations are used to derive a control\nvector that is applied to the model as an inference-time intervention,\nmodulating the representational space of the model, to improve performance on\nthe specified task. We publish the code for deriving control vectors and\nanalyzing model representations. The method allows us to improve performance on\nreasoning benchmarks and assess how control vectors influence the final logit\ndistribution of a model via metrics such as KL divergence and entropy. We apply\ncontrol vectors to Mistral-7B-Instruct and a range of Pythia models on an\ninductive, a deductive and mathematical reasoning task. We show that an LLM\ncan, to a certain degree, be controlled to improve its perceived reasoning\nability by modulating activations. The intervention is dependent upon the\nability to reliably extract the model's typical state when correctly solving a\ntask. Our results suggest that reasoning performance can be modulated in the\nsame manner as other information-processing tasks performed by LLMs and\ndemonstrate that we are capable of improving performance on specific tasks via\na simple intervention on the residual stream with no additional training."
                },
                "authors": [
                    {
                        "name": "Bertram Hjer"
                    },
                    {
                        "name": "Oliver Jarvis"
                    },
                    {
                        "name": "Stefan Heinrich"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Heinrich"
                },
                "author": "Stefan Heinrich",
                "arxiv_comment": "Has been accepted at \"The Thirteenth International Conference on\n  Learning Representations (ICLR 2025)\" Link to publication:\n  https://openreview.net/forum?id=IssPhpUsKt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19480v1",
                "updated": "2025-04-28T04:41:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    41,
                    15,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T04:41:15Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    41,
                    15,
                    0,
                    118,
                    0
                ],
                "title": "An Automated Reinforcement Learning Reward Design Framework with Large\n  Language Model for Cooperative Platoon Coordination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Automated Reinforcement Learning Reward Design Framework with Large\n  Language Model for Cooperative Platoon Coordination"
                },
                "summary": "Reinforcement Learning (RL) has demonstrated excellent decision-making\npotential in platoon coordination problems. However, due to the variability of\ncoordination goals, the complexity of the decision problem, and the\ntime-consumption of trial-and-error in manual design, finding a well\nperformance reward function to guide RL training to solve complex platoon\ncoordination problems remains challenging. In this paper, we formally define\nthe Platoon Coordination Reward Design Problem (PCRDP), extending the RL-based\ncooperative platoon coordination problem to incorporate automated reward\nfunction generation. To address PCRDP, we propose a Large Language Model\n(LLM)-based Platoon coordination Reward Design (PCRD) framework, which\nsystematically automates reward function discovery through LLM-driven\ninitialization and iterative optimization. In this method, LLM first\ninitializes reward functions based on environment code and task requirements\nwith an Analysis and Initial Reward (AIR) module, and then iteratively\noptimizes them based on training feedback with an evolutionary module. The AIR\nmodule guides LLM to deepen their understanding of code and tasks through a\nchain of thought, effectively mitigating hallucination risks in code\ngeneration. The evolutionary module fine-tunes and reconstructs the reward\nfunction, achieving a balance between exploration diversity and convergence\nstability for training. To validate our approach, we establish six challenging\ncoordination scenarios with varying complexity levels within the Yangtze River\nDelta transportation network simulation. Comparative experimental results\ndemonstrate that RL agents utilizing PCRD-generated reward functions\nconsistently outperform human-engineered reward functions, achieving an average\nof 10\\% higher performance metrics in all scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has demonstrated excellent decision-making\npotential in platoon coordination problems. However, due to the variability of\ncoordination goals, the complexity of the decision problem, and the\ntime-consumption of trial-and-error in manual design, finding a well\nperformance reward function to guide RL training to solve complex platoon\ncoordination problems remains challenging. In this paper, we formally define\nthe Platoon Coordination Reward Design Problem (PCRDP), extending the RL-based\ncooperative platoon coordination problem to incorporate automated reward\nfunction generation. To address PCRDP, we propose a Large Language Model\n(LLM)-based Platoon coordination Reward Design (PCRD) framework, which\nsystematically automates reward function discovery through LLM-driven\ninitialization and iterative optimization. In this method, LLM first\ninitializes reward functions based on environment code and task requirements\nwith an Analysis and Initial Reward (AIR) module, and then iteratively\noptimizes them based on training feedback with an evolutionary module. The AIR\nmodule guides LLM to deepen their understanding of code and tasks through a\nchain of thought, effectively mitigating hallucination risks in code\ngeneration. The evolutionary module fine-tunes and reconstructs the reward\nfunction, achieving a balance between exploration diversity and convergence\nstability for training. To validate our approach, we establish six challenging\ncoordination scenarios with varying complexity levels within the Yangtze River\nDelta transportation network simulation. Comparative experimental results\ndemonstrate that RL agents utilizing PCRD-generated reward functions\nconsistently outperform human-engineered reward functions, achieving an average\nof 10\\% higher performance metrics in all scenarios."
                },
                "authors": [
                    {
                        "name": "Dixiao Wei"
                    },
                    {
                        "name": "Peng Yi"
                    },
                    {
                        "name": "Jinlong Lei"
                    },
                    {
                        "name": "Yiguang Hong"
                    },
                    {
                        "name": "Yuchuan Du"
                    }
                ],
                "author_detail": {
                    "name": "Yuchuan Du"
                },
                "author": "Yuchuan Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19472v1",
                "updated": "2025-04-28T04:24:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    24,
                    1,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T04:24:01Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    24,
                    1,
                    0,
                    118,
                    0
                ],
                "title": "Conflicts in Texts: Data, Implications and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conflicts in Texts: Data, Implications and Challenges"
                },
                "summary": "As NLP models become increasingly integrated into real-world applications, it\nbecomes clear that there is a need to address the fact that models often rely\non and generate conflicting information. Conflicts could reflect the complexity\nof situations, changes that need to be explained and dealt with, difficulties\nin data annotation, and mistakes in generated outputs. In all cases,\ndisregarding the conflicts in data could result in undesired behaviors of\nmodels and undermine NLP models' reliability and trustworthiness. This survey\ncategorizes these conflicts into three key areas: (1) natural texts on the web,\nwhere factual inconsistencies, subjective biases, and multiple perspectives\nintroduce contradictions; (2) human-annotated data, where annotator\ndisagreements, mistakes, and societal biases impact model training; and (3)\nmodel interactions, where hallucinations and knowledge conflicts emerge during\ndeployment. While prior work has addressed some of these conflicts in\nisolation, we unify them under the broader concept of conflicting information,\nanalyze their implications, and discuss mitigation strategies. We highlight key\nchallenges and future directions for developing conflict-aware NLP systems that\ncan reason over and reconcile conflicting information more effectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As NLP models become increasingly integrated into real-world applications, it\nbecomes clear that there is a need to address the fact that models often rely\non and generate conflicting information. Conflicts could reflect the complexity\nof situations, changes that need to be explained and dealt with, difficulties\nin data annotation, and mistakes in generated outputs. In all cases,\ndisregarding the conflicts in data could result in undesired behaviors of\nmodels and undermine NLP models' reliability and trustworthiness. This survey\ncategorizes these conflicts into three key areas: (1) natural texts on the web,\nwhere factual inconsistencies, subjective biases, and multiple perspectives\nintroduce contradictions; (2) human-annotated data, where annotator\ndisagreements, mistakes, and societal biases impact model training; and (3)\nmodel interactions, where hallucinations and knowledge conflicts emerge during\ndeployment. While prior work has addressed some of these conflicts in\nisolation, we unify them under the broader concept of conflicting information,\nanalyze their implications, and discuss mitigation strategies. We highlight key\nchallenges and future directions for developing conflict-aware NLP systems that\ncan reason over and reconcile conflicting information more effectively."
                },
                "authors": [
                    {
                        "name": "Siyi Liu"
                    },
                    {
                        "name": "Dan Roth"
                    }
                ],
                "author_detail": {
                    "name": "Dan Roth"
                },
                "author": "Dan Roth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09013v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09013v2",
                "updated": "2025-04-28T04:20:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    20,
                    13,
                    0,
                    118,
                    0
                ],
                "published": "2024-09-13T17:41:12Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    17,
                    41,
                    12,
                    4,
                    257,
                    0
                ],
                "title": "AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM\n  Agents"
                },
                "summary": "Truthfulness (adherence to factual accuracy) and utility (satisfying human\nneeds and instructions) are both fundamental aspects of Large Language Models,\nyet these goals often conflict (e.g., sell a car with known flaws), which makes\nit challenging to achieve both in real-world deployments. We propose AI-LieDar,\na framework to study how LLM-based agents navigate these scenarios in an\nmulti-turn interactive setting. We design a set of real-world scenarios where\nlanguage agents are instructed to achieve goals that are in conflict with being\ntruthful during a multi-turn conversation with simulated human agents. To\nevaluate the truthfulness at large scale, we develop a truthfulness detector\ninspired by psychological literature to assess the agents' responses. Our\nexperiment demonstrates that all models are truthful less than 50% of the time,\nthough truthfulness and goal achievement (utility) rates vary across models. We\nfurther test the steerability of LLMs towards truthfulness, finding that models\ncan be directed to be truthful or deceptive, and even truth-steered models\nstill lie. These findings reveal the complex nature of truthfulness in LLMs and\nunderscore the importance of further research to ensure the safe and reliable\ndeployment of LLMs and LLM-based agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Truthfulness (adherence to factual accuracy) and utility (satisfying human\nneeds and instructions) are both fundamental aspects of Large Language Models,\nyet these goals often conflict (e.g., sell a car with known flaws), which makes\nit challenging to achieve both in real-world deployments. We propose AI-LieDar,\na framework to study how LLM-based agents navigate these scenarios in an\nmulti-turn interactive setting. We design a set of real-world scenarios where\nlanguage agents are instructed to achieve goals that are in conflict with being\ntruthful during a multi-turn conversation with simulated human agents. To\nevaluate the truthfulness at large scale, we develop a truthfulness detector\ninspired by psychological literature to assess the agents' responses. Our\nexperiment demonstrates that all models are truthful less than 50% of the time,\nthough truthfulness and goal achievement (utility) rates vary across models. We\nfurther test the steerability of LLMs towards truthfulness, finding that models\ncan be directed to be truthful or deceptive, and even truth-steered models\nstill lie. These findings reveal the complex nature of truthfulness in LLMs and\nunderscore the importance of further research to ensure the safe and reliable\ndeployment of LLMs and LLM-based agents."
                },
                "authors": [
                    {
                        "name": "Zhe Su"
                    },
                    {
                        "name": "Xuhui Zhou"
                    },
                    {
                        "name": "Sanketh Rangreji"
                    },
                    {
                        "name": "Anubha Kabra"
                    },
                    {
                        "name": "Julia Mendelsohn"
                    },
                    {
                        "name": "Faeze Brahman"
                    },
                    {
                        "name": "Maarten Sap"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Sap"
                },
                "author": "Maarten Sap",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09013v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09013v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19467v1",
                "updated": "2025-04-28T04:13:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    13,
                    18,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T04:13:18Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    13,
                    18,
                    0,
                    118,
                    0
                ],
                "title": "BRIDGE: Benchmarking Large Language Models for Understanding Real-world\n  Clinical Practice Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BRIDGE: Benchmarking Large Language Models for Understanding Real-world\n  Clinical Practice Text"
                },
                "summary": "Large language models (LLMs) hold great promise for medical applications and\nare evolving rapidly, with new models being released at an accelerated pace.\nHowever, current evaluations of LLMs in clinical contexts remain limited. Most\nexisting benchmarks rely on medical exam-style questions or PubMed-derived\ntext, failing to capture the complexity of real-world electronic health record\n(EHR) data. Others focus narrowly on specific application scenarios, limiting\ntheir generalizability across broader clinical use. To address this gap, we\npresent BRIDGE, a comprehensive multilingual benchmark comprising 87 tasks\nsourced from real-world clinical data sources across nine languages. We\nsystematically evaluated 52 state-of-the-art LLMs (including DeepSeek-R1,\nGPT-4o, Gemini, and Llama 4) under various inference strategies. With a total\nof 13,572 experiments, our results reveal substantial performance variation\nacross model sizes, languages, natural language processing tasks, and clinical\nspecialties. Notably, we demonstrate that open-source LLMs can achieve\nperformance comparable to proprietary models, while medically fine-tuned LLMs\nbased on older architectures often underperform versus updated general-purpose\nmodels. The BRIDGE and its corresponding leaderboard serve as a foundational\nresource and a unique reference for the development and evaluation of new LLMs\nin real-world clinical text understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) hold great promise for medical applications and\nare evolving rapidly, with new models being released at an accelerated pace.\nHowever, current evaluations of LLMs in clinical contexts remain limited. Most\nexisting benchmarks rely on medical exam-style questions or PubMed-derived\ntext, failing to capture the complexity of real-world electronic health record\n(EHR) data. Others focus narrowly on specific application scenarios, limiting\ntheir generalizability across broader clinical use. To address this gap, we\npresent BRIDGE, a comprehensive multilingual benchmark comprising 87 tasks\nsourced from real-world clinical data sources across nine languages. We\nsystematically evaluated 52 state-of-the-art LLMs (including DeepSeek-R1,\nGPT-4o, Gemini, and Llama 4) under various inference strategies. With a total\nof 13,572 experiments, our results reveal substantial performance variation\nacross model sizes, languages, natural language processing tasks, and clinical\nspecialties. Notably, we demonstrate that open-source LLMs can achieve\nperformance comparable to proprietary models, while medically fine-tuned LLMs\nbased on older architectures often underperform versus updated general-purpose\nmodels. The BRIDGE and its corresponding leaderboard serve as a foundational\nresource and a unique reference for the development and evaluation of new LLMs\nin real-world clinical text understanding."
                },
                "authors": [
                    {
                        "name": "Jiageng Wu"
                    },
                    {
                        "name": "Bowen Gu"
                    },
                    {
                        "name": "Ren Zhou"
                    },
                    {
                        "name": "Kevin Xie"
                    },
                    {
                        "name": "Doug Snyder"
                    },
                    {
                        "name": "Yixing Jiang"
                    },
                    {
                        "name": "Valentina Carducci"
                    },
                    {
                        "name": "Richard Wyss"
                    },
                    {
                        "name": "Rishi J Desai"
                    },
                    {
                        "name": "Emily Alsentzer"
                    },
                    {
                        "name": "Leo Anthony Celi"
                    },
                    {
                        "name": "Adam Rodman"
                    },
                    {
                        "name": "Sebastian Schneeweiss"
                    },
                    {
                        "name": "Jonathan H. Chen"
                    },
                    {
                        "name": "Santiago Romero-Brufau"
                    },
                    {
                        "name": "Kueiyu Joshua Lin"
                    },
                    {
                        "name": "Jie Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Yang"
                },
                "author": "Jie Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19459v1",
                "updated": "2025-04-28T03:49:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    3,
                    49,
                    6,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T03:49:06Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    3,
                    49,
                    6,
                    0,
                    118,
                    0
                ],
                "title": "Do Automatic Comment Generation Techniques Fall Short? Exploring the\n  Influence of Method Dependencies on Code Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Automatic Comment Generation Techniques Fall Short? Exploring the\n  Influence of Method Dependencies on Code Understanding"
                },
                "summary": "Method-level comments are critical for improving code comprehension and\nsupporting software maintenance. With advancements in large language models\n(LLMs), automated comment generation has become a major research focus.\nHowever, existing approaches often overlook method dependencies, where one\nmethod relies on or calls others, affecting comment quality and code\nunderstandability. This study investigates the prevalence and impact of\ndependent methods in software projects and introduces a dependency-aware\napproach for method-level comment generation. Analyzing a dataset of 10 popular\nJava GitHub projects, we found that dependent methods account for 69.25% of all\nmethods and exhibit higher engagement and change proneness compared to\nindependent methods. Across 448K dependent and 199K independent methods, we\nobserved that state-of-the-art fine-tuned models (e.g., CodeT5+, CodeBERT)\nstruggle to generate comprehensive comments for dependent methods, a trend also\nreflected in LLM-based approaches like ASAP. To address this, we propose\nHelpCOM, a novel dependency-aware technique that incorporates helper method\ninformation to improve comment clarity, comprehensiveness, and relevance.\nExperiments show that HelpCOM outperforms baseline methods by 5.6% to 50.4%\nacross syntactic (e.g., BLEU), semantic (e.g., SentenceBERT), and LLM-based\nevaluation metrics. A survey of 156 software practitioners further confirms\nthat HelpCOM significantly improves the comprehensibility of code involving\ndependent methods, highlighting its potential to enhance documentation,\nmaintainability, and developer productivity in large-scale systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Method-level comments are critical for improving code comprehension and\nsupporting software maintenance. With advancements in large language models\n(LLMs), automated comment generation has become a major research focus.\nHowever, existing approaches often overlook method dependencies, where one\nmethod relies on or calls others, affecting comment quality and code\nunderstandability. This study investigates the prevalence and impact of\ndependent methods in software projects and introduces a dependency-aware\napproach for method-level comment generation. Analyzing a dataset of 10 popular\nJava GitHub projects, we found that dependent methods account for 69.25% of all\nmethods and exhibit higher engagement and change proneness compared to\nindependent methods. Across 448K dependent and 199K independent methods, we\nobserved that state-of-the-art fine-tuned models (e.g., CodeT5+, CodeBERT)\nstruggle to generate comprehensive comments for dependent methods, a trend also\nreflected in LLM-based approaches like ASAP. To address this, we propose\nHelpCOM, a novel dependency-aware technique that incorporates helper method\ninformation to improve comment clarity, comprehensiveness, and relevance.\nExperiments show that HelpCOM outperforms baseline methods by 5.6% to 50.4%\nacross syntactic (e.g., BLEU), semantic (e.g., SentenceBERT), and LLM-based\nevaluation metrics. A survey of 156 software practitioners further confirms\nthat HelpCOM significantly improves the comprehensibility of code involving\ndependent methods, highlighting its potential to enhance documentation,\nmaintainability, and developer productivity in large-scale systems."
                },
                "authors": [
                    {
                        "name": "Md Mustakim Billah"
                    },
                    {
                        "name": "Md Shamimur Rahman"
                    },
                    {
                        "name": "Banani Roy"
                    }
                ],
                "author_detail": {
                    "name": "Banani Roy"
                },
                "author": "Banani Roy",
                "arxiv_comment": "Just Accepted at EASE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19457v1",
                "updated": "2025-04-28T03:47:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    3,
                    47,
                    5,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T03:47:05Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    3,
                    47,
                    5,
                    0,
                    118,
                    0
                ],
                "title": "Towards Long Context Hallucination Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Long Context Hallucination Detection"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks. However, they are prone to contextual hallucination, generating\ninformation that is either unsubstantiated or contradictory to the given\ncontext. Although many studies have investigated contextual hallucinations in\nLLMs, addressing them in long-context inputs remains an open problem. In this\nwork, we take an initial step toward solving this problem by constructing a\ndataset specifically designed for long-context hallucination detection.\nFurthermore, we propose a novel architecture that enables pre-trained encoder\nmodels, such as BERT, to process long contexts and effectively detect\ncontextual hallucinations through a decomposition and aggregation mechanism.\nOur experimental results show that the proposed architecture significantly\noutperforms previous models of similar size as well as LLM-based models across\nvarious metrics, while providing substantially faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks. However, they are prone to contextual hallucination, generating\ninformation that is either unsubstantiated or contradictory to the given\ncontext. Although many studies have investigated contextual hallucinations in\nLLMs, addressing them in long-context inputs remains an open problem. In this\nwork, we take an initial step toward solving this problem by constructing a\ndataset specifically designed for long-context hallucination detection.\nFurthermore, we propose a novel architecture that enables pre-trained encoder\nmodels, such as BERT, to process long contexts and effectively detect\ncontextual hallucinations through a decomposition and aggregation mechanism.\nOur experimental results show that the proposed architecture significantly\noutperforms previous models of similar size as well as LLM-based models across\nvarious metrics, while providing substantially faster inference."
                },
                "authors": [
                    {
                        "name": "Siyi Liu"
                    },
                    {
                        "name": "Kishaloy Halder"
                    },
                    {
                        "name": "Zheng Qi"
                    },
                    {
                        "name": "Wei Xiao"
                    },
                    {
                        "name": "Nikolaos Pappas"
                    },
                    {
                        "name": "Phu Mon Htut"
                    },
                    {
                        "name": "Neha Anna John"
                    },
                    {
                        "name": "Yassine Benajiba"
                    },
                    {
                        "name": "Dan Roth"
                    }
                ],
                "author_detail": {
                    "name": "Dan Roth"
                },
                "author": "Dan Roth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19456v1",
                "updated": "2025-04-28T03:43:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    3,
                    43,
                    28,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T03:43:28Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    3,
                    43,
                    28,
                    0,
                    118,
                    0
                ],
                "title": "FCGHunter: Towards Evaluating Robustness of Graph-Based Android Malware\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FCGHunter: Towards Evaluating Robustness of Graph-Based Android Malware\n  Detection"
                },
                "summary": "Graph-based detection methods leveraging Function Call Graphs (FCGs) have\nshown promise for Android malware detection (AMD) due to their semantic\ninsights. However, the deployment of malware detectors in dynamic and hostile\nenvironments raises significant concerns about their robustness. While recent\napproaches evaluate the robustness of FCG-based detectors using adversarial\nattacks, their effectiveness is constrained by the vast perturbation space,\nparticularly across diverse models and features.\n  To address these challenges, we introduce FCGHunter, a novel robustness\ntesting framework for FCG-based AMD systems. Specifically, FCGHunter employs\ninnovative techniques to enhance exploration and exploitation within this huge\nsearch space. Initially, it identifies critical areas within the FCG related to\nmalware behaviors to narrow down the perturbation space. We then develop a\ndependency-aware crossover and mutation method to enhance the validity and\ndiversity of perturbations, generating diverse FCGs. Furthermore, FCGHunter\nleverages multi-objective feedback to select perturbed FCGs, significantly\nimproving the search process with interpretation-based feature change feedback.\n  Extensive evaluations across 40 scenarios demonstrate that FCGHunter achieves\nan average attack success rate of 87.9%, significantly outperforming baselines\nby at least 44.7%. Notably, FCGHunter achieves a 100% success rate on robust\nmodels (e.g., AdaBoost with MalScan), where baselines achieve only 11% or are\ninapplicable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based detection methods leveraging Function Call Graphs (FCGs) have\nshown promise for Android malware detection (AMD) due to their semantic\ninsights. However, the deployment of malware detectors in dynamic and hostile\nenvironments raises significant concerns about their robustness. While recent\napproaches evaluate the robustness of FCG-based detectors using adversarial\nattacks, their effectiveness is constrained by the vast perturbation space,\nparticularly across diverse models and features.\n  To address these challenges, we introduce FCGHunter, a novel robustness\ntesting framework for FCG-based AMD systems. Specifically, FCGHunter employs\ninnovative techniques to enhance exploration and exploitation within this huge\nsearch space. Initially, it identifies critical areas within the FCG related to\nmalware behaviors to narrow down the perturbation space. We then develop a\ndependency-aware crossover and mutation method to enhance the validity and\ndiversity of perturbations, generating diverse FCGs. Furthermore, FCGHunter\nleverages multi-objective feedback to select perturbed FCGs, significantly\nimproving the search process with interpretation-based feature change feedback.\n  Extensive evaluations across 40 scenarios demonstrate that FCGHunter achieves\nan average attack success rate of 87.9%, significantly outperforming baselines\nby at least 44.7%. Notably, FCGHunter achieves a 100% success rate on robust\nmodels (e.g., AdaBoost with MalScan), where baselines achieve only 11% or are\ninapplicable."
                },
                "authors": [
                    {
                        "name": "Shiwen Song"
                    },
                    {
                        "name": "Xiaofei Xie"
                    },
                    {
                        "name": "Ruitao Feng"
                    },
                    {
                        "name": "Qi Guo"
                    },
                    {
                        "name": "Sen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Sen Chen"
                },
                "author": "Sen Chen",
                "arxiv_comment": "14 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19451v1",
                "updated": "2025-04-28T03:36:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    3,
                    36,
                    47,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T03:36:47Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    3,
                    36,
                    47,
                    0,
                    118,
                    0
                ],
                "title": "Artificial Intelligence in Number Theory: LLMs for Algorithm Generation\n  and Neural Networks for Conjecture Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence in Number Theory: LLMs for Algorithm Generation\n  and Neural Networks for Conjecture Verification"
                },
                "summary": "This paper presents two applications of Artificial Intelligence to number\ntheory.\n  Part I: We evaluate the state-of-the-art LLM Qwen2.5-Math-7B-Instruct on 30\nalgorithmic and 30 computational number theory problems. The model achieves at\nleast 0.95 accuracy on every problem with an optimal non-spoiling hint. For a\nfixed hint strategy, mean accuracies are 0.88 and 0.89 on algorithmic and\ncomputational tasks respectively. We introduce the Hinted Algorithmic Number\nTheory (HANT) dataset and release code and data at doi:10.5281/zenodo.15293187.\n  Part II: We empirically verify the folklore conjecture that the modulus q of\na Dirichlet character chi is uniquely determined by its initial nontrivial\nzeros in small modulus regimes. Using data from LMFDB, we formulate a\nclassification problem with feature vectors derived from initial zeros and\nlabels given by q. A feed-forward neural network and random forest classifier,\ncombined via a meta-ensemble, achieve perfect test accuracy of 1.0 when\nappropriate zero statistics are included. Based on these results, we propose\ntwo new conjectures: (i) hidden statistical patterns exist in the nontrivial\nzeros of each Dirichlet L-function; (ii) an underlying statistical connection\nlinks zeros of L-functions of characters sharing the same modulus. Code and\ndata are available at doi:10.5281/zenodo.15293203.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents two applications of Artificial Intelligence to number\ntheory.\n  Part I: We evaluate the state-of-the-art LLM Qwen2.5-Math-7B-Instruct on 30\nalgorithmic and 30 computational number theory problems. The model achieves at\nleast 0.95 accuracy on every problem with an optimal non-spoiling hint. For a\nfixed hint strategy, mean accuracies are 0.88 and 0.89 on algorithmic and\ncomputational tasks respectively. We introduce the Hinted Algorithmic Number\nTheory (HANT) dataset and release code and data at doi:10.5281/zenodo.15293187.\n  Part II: We empirically verify the folklore conjecture that the modulus q of\na Dirichlet character chi is uniquely determined by its initial nontrivial\nzeros in small modulus regimes. Using data from LMFDB, we formulate a\nclassification problem with feature vectors derived from initial zeros and\nlabels given by q. A feed-forward neural network and random forest classifier,\ncombined via a meta-ensemble, achieve perfect test accuracy of 1.0 when\nappropriate zero statistics are included. Based on these results, we propose\ntwo new conjectures: (i) hidden statistical patterns exist in the nontrivial\nzeros of each Dirichlet L-function; (ii) an underlying statistical connection\nlinks zeros of L-functions of characters sharing the same modulus. Code and\ndata are available at doi:10.5281/zenodo.15293203."
                },
                "authors": [
                    {
                        "name": "Ali Saraeb"
                    }
                ],
                "author_detail": {
                    "name": "Ali Saraeb"
                },
                "author": "Ali Saraeb",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19449v1",
                "updated": "2025-04-28T03:30:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    3,
                    30,
                    32,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T03:30:32Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    3,
                    30,
                    32,
                    0,
                    118,
                    0
                ],
                "title": "R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference"
                },
                "summary": "Large Language Models (LLMs), while demonstrating remarkable capabilities\nacross various applications, present significant challenges during inference\ndue to their substantial model size, especially when deployed on edge devices.\nActivation sparsity offers a promising solution to reduce computation and\nmemory movement, enabling more efficient inference, particularly for\nsmall-batch on-device applications. However, current approaches face\nlimitations with non-ReLU activation function, which are foundational to most\nadvanced LLMs, or require heavy continual training. Additionally, the\ndifficulty in predicting active channels and limited achievable sparsity ratios\nconstrain the effectiveness of activation sparsity-based methods. In this\npaper, we introduce R-Sparse, a training-free activation sparsity approach\ncapable of achieving high sparsity levels in advanced LLMs. We conducted two\npreliminary investigations into how different components contribute to the\noutput within a single linear layer and found two key observations: (i) the\nnon-sparse components of the input function can be regarded as a few bias\nterms, and (ii) The full computation can be effectively approximated by an\nappropriate combination of input channels and weight singular values. Building\non this, we replace the linear layers in LLMs with a rank-aware sparse\ninference method that leverages the sparsity of input channels and singular\nvalue components, eliminating the need for active channel prediction like the\noutput sparsity based approaches. Experiments on Llama-2/3 and Mistral models\nacross ten diverse tasks demonstrate that R-Sparse achieves comparable\nperformance at 50% model-level sparsity, resulting in a significant 43%\nend-to-end efficient improvements with customized kernels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), while demonstrating remarkable capabilities\nacross various applications, present significant challenges during inference\ndue to their substantial model size, especially when deployed on edge devices.\nActivation sparsity offers a promising solution to reduce computation and\nmemory movement, enabling more efficient inference, particularly for\nsmall-batch on-device applications. However, current approaches face\nlimitations with non-ReLU activation function, which are foundational to most\nadvanced LLMs, or require heavy continual training. Additionally, the\ndifficulty in predicting active channels and limited achievable sparsity ratios\nconstrain the effectiveness of activation sparsity-based methods. In this\npaper, we introduce R-Sparse, a training-free activation sparsity approach\ncapable of achieving high sparsity levels in advanced LLMs. We conducted two\npreliminary investigations into how different components contribute to the\noutput within a single linear layer and found two key observations: (i) the\nnon-sparse components of the input function can be regarded as a few bias\nterms, and (ii) The full computation can be effectively approximated by an\nappropriate combination of input channels and weight singular values. Building\non this, we replace the linear layers in LLMs with a rank-aware sparse\ninference method that leverages the sparsity of input channels and singular\nvalue components, eliminating the need for active channel prediction like the\noutput sparsity based approaches. Experiments on Llama-2/3 and Mistral models\nacross ten diverse tasks demonstrate that R-Sparse achieves comparable\nperformance at 50% model-level sparsity, resulting in a significant 43%\nend-to-end efficient improvements with customized kernels."
                },
                "authors": [
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Zechun Liu"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Harshit Khaitan"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Steven Li"
                    }
                ],
                "author_detail": {
                    "name": "Steven Li"
                },
                "author": "Steven Li",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07611v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07611v3",
                "updated": "2025-04-28T03:28:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    3,
                    28,
                    51,
                    0,
                    118,
                    0
                ],
                "published": "2024-11-12T07:34:56Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    7,
                    34,
                    56,
                    1,
                    317,
                    0
                ],
                "title": "Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease\n  Diagnosis with Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease\n  Diagnosis with Small Language Models"
                },
                "summary": "Interpretation is critical for disease diagnosis, but existing models\nstruggle to balance predictive accuracy with human-understandable rationales.\nWhile large language models (LLMs) offer strong reasoning abilities, their\nclinical use is limited by high computational costs and restricted multimodal\nreasoning ability. Small language models (SLMs) are efficient but lack advanced\nreasoning for integrating multimodal medical data. In addition, both LLMs and\nSLMs lack of domain knowledge for trustworthy reasoning. Therefore, we propose\nClinRaGen, enhancing SLMs by leveraging LLM-derived reasoning ability via\nrationale distillation and domain knowledge injection for trustworthy\nmultimodal rationale generation. Key innovations include a sequential rationale\ndistillation framework that equips SLMs with LLM-comparable mutlimodal\nreasoning abilities, and a knowledge-augmented attention mechanism that jointly\nunifies multimodal representation from time series and textual data in a same\nencoding space, enabling it naturally interpreted by SLMs while incorporating\ndomain knowledge for reliable rationale generation. Experiments on real-world\nmedical datasets show that ClinRaGen achieves state-of-the-art performance in\ndisease diagnosis and rationale generation, demonstrating the effectiveness of\ncombining LLM-driven reasoning with knowledge augmentation for improved\ninterpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretation is critical for disease diagnosis, but existing models\nstruggle to balance predictive accuracy with human-understandable rationales.\nWhile large language models (LLMs) offer strong reasoning abilities, their\nclinical use is limited by high computational costs and restricted multimodal\nreasoning ability. Small language models (SLMs) are efficient but lack advanced\nreasoning for integrating multimodal medical data. In addition, both LLMs and\nSLMs lack of domain knowledge for trustworthy reasoning. Therefore, we propose\nClinRaGen, enhancing SLMs by leveraging LLM-derived reasoning ability via\nrationale distillation and domain knowledge injection for trustworthy\nmultimodal rationale generation. Key innovations include a sequential rationale\ndistillation framework that equips SLMs with LLM-comparable mutlimodal\nreasoning abilities, and a knowledge-augmented attention mechanism that jointly\nunifies multimodal representation from time series and textual data in a same\nencoding space, enabling it naturally interpreted by SLMs while incorporating\ndomain knowledge for reliable rationale generation. Experiments on real-world\nmedical datasets show that ClinRaGen achieves state-of-the-art performance in\ndisease diagnosis and rationale generation, demonstrating the effectiveness of\ncombining LLM-driven reasoning with knowledge augmentation for improved\ninterpretability."
                },
                "authors": [
                    {
                        "name": "Shuai Niu"
                    },
                    {
                        "name": "Jing Ma"
                    },
                    {
                        "name": "Hongzhan Lin"
                    },
                    {
                        "name": "Liang Bai"
                    },
                    {
                        "name": "Zhihua Wang"
                    },
                    {
                        "name": "Yida Xu"
                    },
                    {
                        "name": "Yunya Song"
                    },
                    {
                        "name": "Xian Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xian Yang"
                },
                "author": "Xian Yang",
                "arxiv_comment": "13 pages. 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07611v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07611v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19445v1",
                "updated": "2025-04-28T03:20:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    3,
                    20,
                    55,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T03:20:55Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    3,
                    20,
                    55,
                    0,
                    118,
                    0
                ],
                "title": "Systematic Bias in Large Language Models: Discrepant Response Patterns\n  in Binary vs. Continuous Judgment Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Bias in Large Language Models: Discrepant Response Patterns\n  in Binary vs. Continuous Judgment Tasks"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in tasks such as\npsychological text analysis and decision-making in automated workflows.\nHowever, their reliability remains a concern due to potential biases inherited\nfrom their training process. In this study, we examine how different response\nformat: binary versus continuous, may systematically influence LLMs' judgments.\nIn a value statement judgments task and a text sentiment analysis task, we\nprompted LLMs to simulate human responses and tested both formats across\nseveral models, including both open-source and commercial models. Our findings\nrevealed a consistent negative bias: LLMs were more likely to deliver\n\"negative\" judgments in binary formats compared to continuous ones. Control\nexperiments further revealed that this pattern holds across both tasks. Our\nresults highlight the importance of considering response format when applying\nLLMs to decision tasks, as small changes in task design can introduce\nsystematic biases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in tasks such as\npsychological text analysis and decision-making in automated workflows.\nHowever, their reliability remains a concern due to potential biases inherited\nfrom their training process. In this study, we examine how different response\nformat: binary versus continuous, may systematically influence LLMs' judgments.\nIn a value statement judgments task and a text sentiment analysis task, we\nprompted LLMs to simulate human responses and tested both formats across\nseveral models, including both open-source and commercial models. Our findings\nrevealed a consistent negative bias: LLMs were more likely to deliver\n\"negative\" judgments in binary formats compared to continuous ones. Control\nexperiments further revealed that this pattern holds across both tasks. Our\nresults highlight the importance of considering response format when applying\nLLMs to decision tasks, as small changes in task design can introduce\nsystematic biases."
                },
                "authors": [
                    {
                        "name": "Yi-Long Lu"
                    },
                    {
                        "name": "Chunhui Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19444v1",
                "updated": "2025-04-28T03:16:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    3,
                    16,
                    34,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T03:16:34Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    3,
                    16,
                    34,
                    0,
                    118,
                    0
                ],
                "title": "Large Language Models are Qualified Benchmark Builders: Rebuilding\n  Pre-Training Datasets for Advancing Code Intelligence Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are Qualified Benchmark Builders: Rebuilding\n  Pre-Training Datasets for Advancing Code Intelligence Tasks"
                },
                "summary": "Pre-trained code models rely heavily on high-quality pre-training data,\nparticularly human-written reference comments that bridge code and natural\nlanguage. However, these comments often become outdated as software evolves,\ndegrading model performance. Large language models (LLMs) excel at generating\nhigh-quality code comments. We investigate whether replacing human-written\ncomments with LLM-generated ones improves pre-training datasets. Since standard\nmetrics cannot assess reference comment quality, we propose two novel\nreference-free evaluation tasks: code-comment inconsistency detection and\nsemantic code search. Results show that LLM-generated comments are more\nsemantically consistent with code than human-written ones, as confirmed by\nmanual evaluation. Leveraging this finding, we rebuild the CodeSearchNet\ndataset with LLM-generated comments and re-pre-train CodeT5. Evaluations\ndemonstrate that models trained on LLM-enhanced data outperform those using\noriginal human comments in code summarization, generation, and translation\ntasks. This work validates rebuilding pre-training datasets with LLMs to\nadvance code intelligence, challenging the traditional reliance on human\nreference comments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained code models rely heavily on high-quality pre-training data,\nparticularly human-written reference comments that bridge code and natural\nlanguage. However, these comments often become outdated as software evolves,\ndegrading model performance. Large language models (LLMs) excel at generating\nhigh-quality code comments. We investigate whether replacing human-written\ncomments with LLM-generated ones improves pre-training datasets. Since standard\nmetrics cannot assess reference comment quality, we propose two novel\nreference-free evaluation tasks: code-comment inconsistency detection and\nsemantic code search. Results show that LLM-generated comments are more\nsemantically consistent with code than human-written ones, as confirmed by\nmanual evaluation. Leveraging this finding, we rebuild the CodeSearchNet\ndataset with LLM-generated comments and re-pre-train CodeT5. Evaluations\ndemonstrate that models trained on LLM-enhanced data outperform those using\noriginal human comments in code summarization, generation, and translation\ntasks. This work validates rebuilding pre-training datasets with LLMs to\nadvance code intelligence, challenging the traditional reliance on human\nreference comments."
                },
                "authors": [
                    {
                        "name": "Kang Yang"
                    },
                    {
                        "name": "Xinjun Mao"
                    },
                    {
                        "name": "Shangwen Wang"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Tanghaoran Zhang"
                    },
                    {
                        "name": "Bo Lin"
                    },
                    {
                        "name": "Yihao Qin"
                    },
                    {
                        "name": "Zhang Zhang"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Kamal Al-Sabahi"
                    }
                ],
                "author_detail": {
                    "name": "Kamal Al-Sabahi"
                },
                "author": "Kamal Al-Sabahi",
                "arxiv_comment": "Awarded the ACM SIGSOFT Distinguished Paper Award in ICPC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12311v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12311v4",
                "updated": "2025-04-28T03:06:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    3,
                    6,
                    8,
                    0,
                    118,
                    0
                ],
                "published": "2024-10-16T07:24:28Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    7,
                    24,
                    28,
                    2,
                    290,
                    0
                ],
                "title": "Open Domain Question Answering with Conflicting Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Domain Question Answering with Conflicting Contexts"
                },
                "summary": "Open domain question answering systems frequently rely on information\nretrieved from large collections of text (such as the Web) to answer questions.\nHowever, such collections of text often contain conflicting information, and\nindiscriminately depending on this information may result in untruthful and\ninaccurate answers. To understand the gravity of this problem, we collect a\nhuman-annotated dataset, Question Answering with Conflicting Contexts (QACC),\nand find that as much as 25% of unambiguous, open domain questions can lead to\nconflicting contexts when retrieved using Google Search. We evaluate and\nbenchmark three powerful Large Language Models (LLMs) with our dataset QACC and\ndemonstrate their limitations in effectively addressing questions with\nconflicting information. To explore how humans reason through conflicting\ncontexts, we request our annotators to provide explanations for their\nselections of correct answers. We demonstrate that by finetuning LLMs to\nexplain their answers, we can introduce richer information into their training\nthat guide them through the process of reasoning with conflicting contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open domain question answering systems frequently rely on information\nretrieved from large collections of text (such as the Web) to answer questions.\nHowever, such collections of text often contain conflicting information, and\nindiscriminately depending on this information may result in untruthful and\ninaccurate answers. To understand the gravity of this problem, we collect a\nhuman-annotated dataset, Question Answering with Conflicting Contexts (QACC),\nand find that as much as 25% of unambiguous, open domain questions can lead to\nconflicting contexts when retrieved using Google Search. We evaluate and\nbenchmark three powerful Large Language Models (LLMs) with our dataset QACC and\ndemonstrate their limitations in effectively addressing questions with\nconflicting information. To explore how humans reason through conflicting\ncontexts, we request our annotators to provide explanations for their\nselections of correct answers. We demonstrate that by finetuning LLMs to\nexplain their answers, we can introduce richer information into their training\nthat guide them through the process of reasoning with conflicting contexts."
                },
                "authors": [
                    {
                        "name": "Siyi Liu"
                    },
                    {
                        "name": "Qiang Ning"
                    },
                    {
                        "name": "Kishaloy Halder"
                    },
                    {
                        "name": "Wei Xiao"
                    },
                    {
                        "name": "Zheng Qi"
                    },
                    {
                        "name": "Phu Mon Htut"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Neha Anna John"
                    },
                    {
                        "name": "Bonan Min"
                    },
                    {
                        "name": "Yassine Benajiba"
                    },
                    {
                        "name": "Dan Roth"
                    }
                ],
                "author_detail": {
                    "name": "Dan Roth"
                },
                "author": "Dan Roth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12311v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12311v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19103v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19103v3",
                "updated": "2025-04-28T03:04:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    3,
                    4,
                    46,
                    0,
                    118,
                    0
                ],
                "published": "2024-03-28T02:35:53Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    2,
                    35,
                    53,
                    3,
                    88,
                    0
                ],
                "title": "Automated Black-box Prompt Engineering for Personalized Text-to-Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Black-box Prompt Engineering for Personalized Text-to-Image\n  Generation"
                },
                "summary": "Prompt engineering is an effective but labor-intensive way to control\ntext-to-image (T2I) generative models. Its time-intensive nature and complexity\nhave spurred the development of algorithms for automated prompt generation.\nHowever, these methods often struggle with transferability across T2I models,\nrequire white-box access to the underlying model, or produce non-intuitive\nprompts. In this work, we introduce PRISM, an algorithm that automatically\nproduces human-interpretable and transferable prompts that can effectively\ngenerate desired concepts given only black-box access to T2I models. Inspired\nby large language model (LLM) jailbreaking, PRISM leverages the in-context\nlearning ability of LLMs to iteratively refine the candidate prompt\ndistribution built upon the reference images. Our experiments demonstrate the\nversatility and effectiveness of PRISM in generating accurate prompts for\nobjects, styles, and images across multiple T2I models, including Stable\nDiffusion, DALL-E, and Midjourney.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt engineering is an effective but labor-intensive way to control\ntext-to-image (T2I) generative models. Its time-intensive nature and complexity\nhave spurred the development of algorithms for automated prompt generation.\nHowever, these methods often struggle with transferability across T2I models,\nrequire white-box access to the underlying model, or produce non-intuitive\nprompts. In this work, we introduce PRISM, an algorithm that automatically\nproduces human-interpretable and transferable prompts that can effectively\ngenerate desired concepts given only black-box access to T2I models. Inspired\nby large language model (LLM) jailbreaking, PRISM leverages the in-context\nlearning ability of LLMs to iteratively refine the candidate prompt\ndistribution built upon the reference images. Our experiments demonstrate the\nversatility and effectiveness of PRISM in generating accurate prompts for\nobjects, styles, and images across multiple T2I models, including Stable\nDiffusion, DALL-E, and Midjourney."
                },
                "authors": [
                    {
                        "name": "Yutong He"
                    },
                    {
                        "name": "Alexander Robey"
                    },
                    {
                        "name": "Naoki Murata"
                    },
                    {
                        "name": "Yiding Jiang"
                    },
                    {
                        "name": "Joshua Nathaniel Williams"
                    },
                    {
                        "name": "George J. Pappas"
                    },
                    {
                        "name": "Hamed Hassani"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    },
                    {
                        "name": "Ruslan Salakhutdinov"
                    },
                    {
                        "name": "J. Zico Kolter"
                    }
                ],
                "author_detail": {
                    "name": "J. Zico Kolter"
                },
                "author": "J. Zico Kolter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19103v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19103v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19440v1",
                "updated": "2025-04-28T03:01:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    3,
                    1,
                    51,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T03:01:51Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    3,
                    1,
                    51,
                    0,
                    118,
                    0
                ],
                "title": "JailbreaksOverTime: Detecting Jailbreak Attacks Under Distribution Shift",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JailbreaksOverTime: Detecting Jailbreak Attacks Under Distribution Shift"
                },
                "summary": "Safety and security remain critical concerns in AI deployment. Despite safety\ntraining through reinforcement learning with human feedback (RLHF) [ 32],\nlanguage models remain vulnerable to jailbreak attacks that bypass safety\nguardrails. Universal jailbreaks - prefixes that can circumvent alignment for\nany payload - are particularly concerning. We show empirically that jailbreak\ndetection systems face distribution shift, with detectors trained at one point\nin time performing poorly against newer exploits. To study this problem, we\nrelease JailbreaksOverTime, a comprehensive dataset of timestamped real user\ninteractions containing both benign requests and jailbreak attempts collected\nover 10 months. We propose a two-pronged method for defenders to detect new\njailbreaks and continuously update their detectors. First, we show how to use\ncontinuous learning to detect jailbreaks and adapt rapidly to new emerging\njailbreaks. While detectors trained at a single point in time eventually fail\ndue to drift, we find that universal jailbreaks evolve slowly enough for\nself-training to be effective. Retraining our detection model weekly using its\nown labels - with no new human labels - reduces the false negative rate from 4%\nto 0.3% at a false positive rate of 0.1%. Second, we introduce an unsupervised\nactive monitoring approach to identify novel jailbreaks. Rather than\nclassifying inputs directly, we recognize jailbreaks by their behavior,\nspecifically, their ability to trigger models to respond to known-harmful\nprompts. This approach has a higher false negative rate (4.1%) than supervised\nmethods, but it successfully identified some out-of-distribution attacks that\nwere missed by the continuous learning approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety and security remain critical concerns in AI deployment. Despite safety\ntraining through reinforcement learning with human feedback (RLHF) [ 32],\nlanguage models remain vulnerable to jailbreak attacks that bypass safety\nguardrails. Universal jailbreaks - prefixes that can circumvent alignment for\nany payload - are particularly concerning. We show empirically that jailbreak\ndetection systems face distribution shift, with detectors trained at one point\nin time performing poorly against newer exploits. To study this problem, we\nrelease JailbreaksOverTime, a comprehensive dataset of timestamped real user\ninteractions containing both benign requests and jailbreak attempts collected\nover 10 months. We propose a two-pronged method for defenders to detect new\njailbreaks and continuously update their detectors. First, we show how to use\ncontinuous learning to detect jailbreaks and adapt rapidly to new emerging\njailbreaks. While detectors trained at a single point in time eventually fail\ndue to drift, we find that universal jailbreaks evolve slowly enough for\nself-training to be effective. Retraining our detection model weekly using its\nown labels - with no new human labels - reduces the false negative rate from 4%\nto 0.3% at a false positive rate of 0.1%. Second, we introduce an unsupervised\nactive monitoring approach to identify novel jailbreaks. Rather than\nclassifying inputs directly, we recognize jailbreaks by their behavior,\nspecifically, their ability to trigger models to respond to known-harmful\nprompts. This approach has a higher false negative rate (4.1%) than supervised\nmethods, but it successfully identified some out-of-distribution attacks that\nwere missed by the continuous learning approach."
                },
                "authors": [
                    {
                        "name": "Julien Piet"
                    },
                    {
                        "name": "Xiao Huang"
                    },
                    {
                        "name": "Dennis Jacob"
                    },
                    {
                        "name": "Annabella Chow"
                    },
                    {
                        "name": "Maha Alrashed"
                    },
                    {
                        "name": "Geng Zhao"
                    },
                    {
                        "name": "Zhanhao Hu"
                    },
                    {
                        "name": "Chawin Sitawarin"
                    },
                    {
                        "name": "Basel Alomair"
                    },
                    {
                        "name": "David Wagner"
                    }
                ],
                "author_detail": {
                    "name": "David Wagner"
                },
                "author": "David Wagner",
                "arxiv_comment": "18 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17620v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17620v3",
                "updated": "2025-04-28T02:26:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    2,
                    26,
                    9,
                    0,
                    118,
                    0
                ],
                "published": "2025-03-22T02:32:09Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    2,
                    32,
                    9,
                    5,
                    81,
                    0
                ],
                "title": "A Case Study of Scalable Content Annotation Using Multi-LLM Consensus\n  and Human Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case Study of Scalable Content Annotation Using Multi-LLM Consensus\n  and Human Review"
                },
                "summary": "Content annotation at scale remains challenging, requiring substantial human\nexpertise and effort. This paper presents a case study in code documentation\nanalysis, where we explore the balance between automation efficiency and\nannotation accuracy. We present MCHR (Multi-LLM Consensus with Human Review), a\nnovel semi-automated framework that enhances annotation scalability through the\nsystematic integration of multiple LLMs and targeted human review. Our\nframework introduces a structured consensus-building mechanism among LLMs and\nan adaptive review protocol that strategically engages human expertise. Through\nour case study, we demonstrate that MCHR reduces annotation time by 32% to 100%\ncompared to manual annotation while maintaining high accuracy (85.5% to 98%)\nacross different difficulty levels, from basic binary classification to\nchallenging open-set scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content annotation at scale remains challenging, requiring substantial human\nexpertise and effort. This paper presents a case study in code documentation\nanalysis, where we explore the balance between automation efficiency and\nannotation accuracy. We present MCHR (Multi-LLM Consensus with Human Review), a\nnovel semi-automated framework that enhances annotation scalability through the\nsystematic integration of multiple LLMs and targeted human review. Our\nframework introduces a structured consensus-building mechanism among LLMs and\nan adaptive review protocol that strategically engages human expertise. Through\nour case study, we demonstrate that MCHR reduces annotation time by 32% to 100%\ncompared to manual annotation while maintaining high accuracy (85.5% to 98%)\nacross different difficulty levels, from basic binary classification to\nchallenging open-set scenarios."
                },
                "authors": [
                    {
                        "name": "Mingyue Yuan"
                    },
                    {
                        "name": "Jieshan Chen"
                    },
                    {
                        "name": "Zhenchang Xing"
                    },
                    {
                        "name": "Gelareh Mohammadi"
                    },
                    {
                        "name": "Aaron Quigley"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Quigley"
                },
                "author": "Aaron Quigley",
                "arxiv_comment": "7 pages, GenAICHI: CHI 2025 Workshop on Generative AI and HCI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17620v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17620v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19423v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19423v2",
                "updated": "2025-04-29T07:28:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    28,
                    55,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-28T02:14:08Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    2,
                    14,
                    8,
                    0,
                    118,
                    0
                ],
                "title": "MER 2025: When Affective Computing Meets Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MER 2025: When Affective Computing Meets Large Language Models"
                },
                "summary": "MER2025 is the third year of our MER series of challenges, aiming to bring\ntogether researchers in the affective computing community to explore emerging\ntrends and future directions in the field. Previously, MER2023 focused on\nmulti-label learning, noise robustness, and semi-supervised learning, while\nMER2024 introduced a new track dedicated to open-vocabulary emotion\nrecognition. This year, MER2025 centers on the theme \"When Affective Computing\nMeets Large Language Models (LLMs)\".We aim to shift the paradigm from\ntraditional categorical frameworks reliant on predefined emotion taxonomies to\nLLM-driven generative methods, offering innovative solutions for more accurate\nand reliable emotion understanding. The challenge features four tracks:\nMER-SEMI focuses on fixed categorical emotion recognition enhanced by\nsemi-supervised learning; MER-FG explores fine-grained emotions, expanding\nrecognition from basic to nuanced emotional states; MER-DES incorporates\nmultimodal cues (beyond emotion words) into predictions to enhance model\ninterpretability; MER-PR investigates whether emotion prediction results can\nimprove personality recognition performance. For the first three tracks,\nbaseline code is available at MERTools, and datasets can be accessed via\nHugging Face. For the last track, the dataset and baseline code are available\non GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MER2025 is the third year of our MER series of challenges, aiming to bring\ntogether researchers in the affective computing community to explore emerging\ntrends and future directions in the field. Previously, MER2023 focused on\nmulti-label learning, noise robustness, and semi-supervised learning, while\nMER2024 introduced a new track dedicated to open-vocabulary emotion\nrecognition. This year, MER2025 centers on the theme \"When Affective Computing\nMeets Large Language Models (LLMs)\".We aim to shift the paradigm from\ntraditional categorical frameworks reliant on predefined emotion taxonomies to\nLLM-driven generative methods, offering innovative solutions for more accurate\nand reliable emotion understanding. The challenge features four tracks:\nMER-SEMI focuses on fixed categorical emotion recognition enhanced by\nsemi-supervised learning; MER-FG explores fine-grained emotions, expanding\nrecognition from basic to nuanced emotional states; MER-DES incorporates\nmultimodal cues (beyond emotion words) into predictions to enhance model\ninterpretability; MER-PR investigates whether emotion prediction results can\nimprove personality recognition performance. For the first three tracks,\nbaseline code is available at MERTools, and datasets can be accessed via\nHugging Face. For the last track, the dataset and baseline code are available\non GitHub."
                },
                "authors": [
                    {
                        "name": "Zheng Lian"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Kele Xu"
                    },
                    {
                        "name": "Bin Liu"
                    },
                    {
                        "name": "Xuefei Liu"
                    },
                    {
                        "name": "Yazhou Zhang"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Zebang Cheng"
                    },
                    {
                        "name": "Haolin Zuo"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Xiaojiang Peng"
                    },
                    {
                        "name": "Xie Chen"
                    },
                    {
                        "name": "Ya Li"
                    },
                    {
                        "name": "Erik Cambria"
                    },
                    {
                        "name": "Guoying Zhao"
                    },
                    {
                        "name": "Bjrn W. Schuller"
                    },
                    {
                        "name": "Jianhua Tao"
                    }
                ],
                "author_detail": {
                    "name": "Jianhua Tao"
                },
                "author": "Jianhua Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19423v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19423v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19413v1",
                "updated": "2025-04-28T01:46:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    1,
                    46,
                    35,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T01:46:35Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    1,
                    46,
                    35,
                    0,
                    118,
                    0
                ],
                "title": "Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable prowess in\ngenerating contextually coherent responses, yet their fixed context windows\npose fundamental challenges for maintaining consistency over prolonged\nmulti-session dialogues. We introduce Mem0, a scalable memory-centric\narchitecture that addresses this issue by dynamically extracting,\nconsolidating, and retrieving salient information from ongoing conversations.\nBuilding on this foundation, we further propose an enhanced variant that\nleverages graph-based memory representations to capture complex relational\nstructures among conversational elements. Through comprehensive evaluations on\nLOCOMO benchmark, we systematically compare our approaches against six baseline\ncategories: (i) established memory-augmented systems, (ii) retrieval-augmented\ngeneration (RAG) with varying chunk sizes and k-values, (iii) a full-context\napproach that processes the entire conversation history, (iv) an open-source\nmemory solution, (v) a proprietary model system, and (vi) a dedicated memory\nmanagement platform. Empirical results show that our methods consistently\noutperform all existing memory systems across four question categories:\nsingle-hop, temporal, multi-hop, and open-domain. Notably, Mem0 achieves 26%\nrelative improvements in the LLM-as-a-Judge metric over OpenAI, while Mem0 with\ngraph memory achieves around 2% higher overall score than the base\nconfiguration. Beyond accuracy gains, we also markedly reduce computational\noverhead compared to full-context method. In particular, Mem0 attains a 91%\nlower p95 latency and saves more than 90% token cost, offering a compelling\nbalance between advanced reasoning capabilities and practical deployment\nconstraints. Our findings highlight critical role of structured, persistent\nmemory mechanisms for long-term conversational coherence, paving the way for\nmore reliable and efficient LLM-driven AI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable prowess in\ngenerating contextually coherent responses, yet their fixed context windows\npose fundamental challenges for maintaining consistency over prolonged\nmulti-session dialogues. We introduce Mem0, a scalable memory-centric\narchitecture that addresses this issue by dynamically extracting,\nconsolidating, and retrieving salient information from ongoing conversations.\nBuilding on this foundation, we further propose an enhanced variant that\nleverages graph-based memory representations to capture complex relational\nstructures among conversational elements. Through comprehensive evaluations on\nLOCOMO benchmark, we systematically compare our approaches against six baseline\ncategories: (i) established memory-augmented systems, (ii) retrieval-augmented\ngeneration (RAG) with varying chunk sizes and k-values, (iii) a full-context\napproach that processes the entire conversation history, (iv) an open-source\nmemory solution, (v) a proprietary model system, and (vi) a dedicated memory\nmanagement platform. Empirical results show that our methods consistently\noutperform all existing memory systems across four question categories:\nsingle-hop, temporal, multi-hop, and open-domain. Notably, Mem0 achieves 26%\nrelative improvements in the LLM-as-a-Judge metric over OpenAI, while Mem0 with\ngraph memory achieves around 2% higher overall score than the base\nconfiguration. Beyond accuracy gains, we also markedly reduce computational\noverhead compared to full-context method. In particular, Mem0 attains a 91%\nlower p95 latency and saves more than 90% token cost, offering a compelling\nbalance between advanced reasoning capabilities and practical deployment\nconstraints. Our findings highlight critical role of structured, persistent\nmemory mechanisms for long-term conversational coherence, paving the way for\nmore reliable and efficient LLM-driven AI agents."
                },
                "authors": [
                    {
                        "name": "Prateek Chhikara"
                    },
                    {
                        "name": "Dev Khant"
                    },
                    {
                        "name": "Saket Aryan"
                    },
                    {
                        "name": "Taranjeet Singh"
                    },
                    {
                        "name": "Deshraj Yadav"
                    }
                ],
                "author_detail": {
                    "name": "Deshraj Yadav"
                },
                "author": "Deshraj Yadav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19395v1",
                "updated": "2025-04-28T00:05:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    0,
                    5,
                    29,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T00:05:29Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    0,
                    5,
                    29,
                    0,
                    118,
                    0
                ],
                "title": "ICL CIPHERS: Quantifying \"Learning'' in In-Context Learning via\n  Substitution Ciphers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICL CIPHERS: Quantifying \"Learning'' in In-Context Learning via\n  Substitution Ciphers"
                },
                "summary": "Recent works have suggested that In-Context Learning (ICL) operates in dual\nmodes, i.e. task retrieval (remember learned patterns from pre-training) and\ntask learning (inference-time ``learning'' from demonstrations). However,\ndisentangling these the two modes remains a challenging goal. We introduce ICL\nCIPHERS, a class of task reformulations based on substitution ciphers borrowed\nfrom classic cryptography. In this approach, a subset of tokens in the\nin-context inputs are substituted with other (irrelevant) tokens, rendering\nEnglish sentences less comprehensible to human eye. However, by design, there\nis a latent, fixed pattern to this substitution, making it reversible. This\nbijective (reversible) cipher ensures that the task remains a well-defined task\nin some abstract sense, despite the transformations. It is a curious question\nif LLMs can solve ICL CIPHERS with a BIJECTIVE mapping, which requires\ndeciphering the latent cipher. We show that LLMs are better at solving ICL\nCIPHERS with BIJECTIVE mappings than the NON-BIJECTIVE (irreversible) baseline,\nproviding a novel approach to quantify ``learning'' in ICL. While this gap is\nsmall, it is consistent across the board on four datasets and six models.\nFinally, we examine LLMs' internal representations and identify evidence in\ntheir ability to decode the ciphered inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works have suggested that In-Context Learning (ICL) operates in dual\nmodes, i.e. task retrieval (remember learned patterns from pre-training) and\ntask learning (inference-time ``learning'' from demonstrations). However,\ndisentangling these the two modes remains a challenging goal. We introduce ICL\nCIPHERS, a class of task reformulations based on substitution ciphers borrowed\nfrom classic cryptography. In this approach, a subset of tokens in the\nin-context inputs are substituted with other (irrelevant) tokens, rendering\nEnglish sentences less comprehensible to human eye. However, by design, there\nis a latent, fixed pattern to this substitution, making it reversible. This\nbijective (reversible) cipher ensures that the task remains a well-defined task\nin some abstract sense, despite the transformations. It is a curious question\nif LLMs can solve ICL CIPHERS with a BIJECTIVE mapping, which requires\ndeciphering the latent cipher. We show that LLMs are better at solving ICL\nCIPHERS with BIJECTIVE mappings than the NON-BIJECTIVE (irreversible) baseline,\nproviding a novel approach to quantify ``learning'' in ICL. While this gap is\nsmall, it is consistent across the board on four datasets and six models.\nFinally, we examine LLMs' internal representations and identify evidence in\ntheir ability to decode the ciphered inputs."
                },
                "authors": [
                    {
                        "name": "Zhouxiang Fang"
                    },
                    {
                        "name": "Aayush Mishra"
                    },
                    {
                        "name": "Muhan Gao"
                    },
                    {
                        "name": "Anqi Liu"
                    },
                    {
                        "name": "Daniel Khashabi"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Khashabi"
                },
                "author": "Daniel Khashabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]