[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.11049v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v2",
                "updated": "2024-08-21T17:55:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    55,
                    29,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/."
                },
                "authors": [
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11506v1",
                "updated": "2024-08-21T10:26:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:26:26Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "title": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage"
                },
                "summary": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration."
                },
                "authors": [
                    {
                        "name": "Pedro C Rijo"
                    },
                    {
                        "name": "Francisco J. Galindo-Rosales"
                    }
                ],
                "author_detail": {
                    "name": "Francisco J. Galindo-Rosales"
                },
                "author": "Francisco J. Galindo-Rosales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10685v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10685v2",
                "updated": "2024-08-21T06:10:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    10,
                    2,
                    2,
                    234,
                    0
                ],
                "published": "2024-01-19T13:32:55Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    13,
                    32,
                    55,
                    4,
                    19,
                    0
                ],
                "title": "Towards End-to-End GPS Localization with Neural Pseudorange Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards End-to-End GPS Localization with Neural Pseudorange Correction"
                },
                "summary": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet."
                },
                "authors": [
                    {
                        "name": "Xu Weng"
                    },
                    {
                        "name": "KV Ling"
                    },
                    {
                        "name": "Haochen Liu"
                    },
                    {
                        "name": "Kun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Kun Cao"
                },
                "author": "Kun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10685v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10685v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11325v1",
                "updated": "2024-08-21T04:16:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T04:16:49Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "title": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory"
                },
                "summary": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads."
                },
                "authors": [
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Ehsan Hajyjasini"
                    },
                    {
                        "name": "Seungjin Lee"
                    },
                    {
                        "name": "Zifeng Zhang"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Steven Swanson"
                    }
                ],
                "author_detail": {
                    "name": "Steven Swanson"
                },
                "author": "Steven Swanson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v3",
                "updated": "2024-08-21T02:32:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    2,
                    32,
                    43,
                    2,
                    234,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "Matrix quantization compresses matrix elements into a more compact form to\nreduce storage requirements, with dequantization enabling reconstruction for\nuse. We define the Quantization Error Minimization (QEM) problem as minimizing\nthe difference between the original and quantized matrices while ensuring the\nquantized matrix remains within fixed memory constraints. This technique is\ncrucial in applications like Large Language Model (LLM) weight compression and\nKV cache compression, where large matrix sizes demand efficient storage\nsolutions.\n  As modern LLMs like GPT-4 and BERT continue to grow, effective matrix\ncompression is increasingly important. These models contain billions of\nparameters in matrix form, making efficient weight quantization essential for\nboth storage and computational efficiency. Similarly, KV caches, storing\nintermediate inference results, are matrix-based and benefit significantly from\noptimized compression techniques.\n  To address the QEM problem in the context of LLM weight and KV cache\ncompression, we propose Quantum Entanglement Trees (QET). QET leverages the\nlocal structure of matrix elements by iteratively swapping elements to create a\nlocally ordered matrix, which is then grouped and quantized column by column.\nTo enhance QET, we introduce two optimizations: residual quantization to\nfurther reduce Mean Squared Error (MSE) and masking with batch processing to\naccelerate the algorithm.\n  Our experiments demonstrate that QET can reduce MSE to 12.3% of its original\nvalue at the same compression ratio, outperforming leading baseline methods.\nOur contributions include framing the QEM problem specifically for LLM and KV\ncache compression, developing the QET algorithm, and implementing optimizations\nthat improve accuracy and processing speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix quantization compresses matrix elements into a more compact form to\nreduce storage requirements, with dequantization enabling reconstruction for\nuse. We define the Quantization Error Minimization (QEM) problem as minimizing\nthe difference between the original and quantized matrices while ensuring the\nquantized matrix remains within fixed memory constraints. This technique is\ncrucial in applications like Large Language Model (LLM) weight compression and\nKV cache compression, where large matrix sizes demand efficient storage\nsolutions.\n  As modern LLMs like GPT-4 and BERT continue to grow, effective matrix\ncompression is increasingly important. These models contain billions of\nparameters in matrix form, making efficient weight quantization essential for\nboth storage and computational efficiency. Similarly, KV caches, storing\nintermediate inference results, are matrix-based and benefit significantly from\noptimized compression techniques.\n  To address the QEM problem in the context of LLM weight and KV cache\ncompression, we propose Quantum Entanglement Trees (QET). QET leverages the\nlocal structure of matrix elements by iteratively swapping elements to create a\nlocally ordered matrix, which is then grouped and quantized column by column.\nTo enhance QET, we introduce two optimizations: residual quantization to\nfurther reduce Mean Squared Error (MSE) and masking with batch processing to\naccelerate the algorithm.\n  Our experiments demonstrate that QET can reduce MSE to 12.3% of its original\nvalue at the same compression ratio, outperforming leading baseline methods.\nOur contributions include framing the QEM problem specifically for LLM and KV\ncache compression, developing the QET algorithm, and implementing optimizations\nthat improve accuracy and processing speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10970v1",
                "updated": "2024-08-20T16:02:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T16:02:54Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "title": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "4 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10746v1",
                "updated": "2024-08-20T11:30:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T11:30:12Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "title": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint."
                },
                "authors": [
                    {
                        "name": "Bei Ouyang"
                    },
                    {
                        "name": "Shengyuan Ye"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Tianyi Qian"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "arxiv_comment": "Accepted by The 53rd International Conference on Parallel Processing\n  (ICPP'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09697v2",
                "updated": "2024-08-20T04:46:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    4,
                    46,
                    18,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T04:43:56Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    43,
                    56,
                    0,
                    232,
                    0
                ],
                "title": "Heta: Distributed Training of Heterogeneous Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heta: Distributed Training of Heterogeneous Graph Neural Networks"
                },
                "summary": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhong"
                    },
                    {
                        "name": "Junwei Su"
                    },
                    {
                        "name": "Chuan Wu"
                    },
                    {
                        "name": "Minjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Minjie Wang"
                },
                "author": "Minjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10104v1",
                "updated": "2024-08-19T15:47:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T15:47:17Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "title": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory"
                },
                "summary": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging."
                },
                "authors": [
                    {
                        "name": "Olena Tkach"
                    },
                    {
                        "name": "Gerd Schoenhense"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Schoenhense"
                },
                "author": "Gerd Schoenhense",
                "arxiv_comment": "17 pages, 4 figures, 44 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09848v1",
                "updated": "2024-08-19T09:50:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:50:35Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "title": "Abstract Environment Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract Environment Trimming"
                },
                "summary": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times."
                },
                "authors": [
                    {
                        "name": "Daniel Jurjo-Rivas"
                    },
                    {
                        "name": "Jose F. Morales"
                    },
                    {
                        "name": "Pedro Lpez-Garca"
                    },
                    {
                        "name": "Manuel V. Hermenegildo"
                    }
                ],
                "author_detail": {
                    "name": "Manuel V. Hermenegildo"
                },
                "author": "Manuel V. Hermenegildo",
                "arxiv_comment": "61 pages, 10 figures, 7 tables, submitted to ICLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10284v1",
                "updated": "2024-08-19T03:27:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T03:27:15Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "title": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_doi": "10.1145/3676536.3676741",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676741",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.10284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07092v2",
                "updated": "2024-08-18T17:27:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    17,
                    27,
                    17,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-11T18:40:36Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    40,
                    36,
                    6,
                    224,
                    0
                ],
                "title": "Post-Training Sparse Attention with Double Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Sparse Attention with Double Sparsity"
                },
                "summary": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Lianmin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Lianmin Zheng"
                },
                "author": "Lianmin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09483v1",
                "updated": "2024-08-18T13:54:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-18T13:54:46Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMD: A Cache-assisted GPU Memory Deduplication Architecture"
                },
                "summary": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Dan Feng"
                    },
                    {
                        "name": "Wei Tong"
                    },
                    {
                        "name": "Xueliang Wei"
                    },
                    {
                        "name": "Bing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Bing Wu"
                },
                "author": "Bing Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08795v1",
                "updated": "2024-08-16T15:11:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T15:11:12Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "title": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks"
                },
                "summary": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding"
                },
                "authors": [
                    {
                        "name": "Divya Ojha"
                    },
                    {
                        "name": "Sandhya Dwarkadas"
                    }
                ],
                "author_detail": {
                    "name": "Sandhya Dwarkadas"
                },
                "author": "Sandhya Dwarkadas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v3",
                "updated": "2024-08-16T08:46:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    46,
                    33,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v1",
                "updated": "2024-08-16T06:11:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v2",
                "updated": "2024-08-16T04:12:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    12,
                    25,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v3",
                "updated": "2024-08-15T05:24:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    24,
                    19,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07853v1",
                "updated": "2024-08-14T23:42:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T23:42:46Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "title": "A Case for Enabling Delegation of 5G Core Decisions to the RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case for Enabling Delegation of 5G Core Decisions to the RAN"
                },
                "summary": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation."
                },
                "authors": [
                    {
                        "name": "Lucas Vancina"
                    },
                    {
                        "name": "Geoffrey Xie"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Xie"
                },
                "author": "Geoffrey Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v2",
                "updated": "2024-08-14T09:18:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    18,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07304v1",
                "updated": "2024-08-14T05:42:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T05:42:35Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "title": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption"
                },
                "summary": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS."
                },
                "authors": [
                    {
                        "name": "Jonathan Ly"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Ly"
                },
                "author": "Jonathan Ly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15743v2",
                "updated": "2024-08-13T13:56:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    56,
                    14,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization"
                },
                "summary": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tlli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tlli"
                },
                "author": "Antti Tlli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04043v3",
                "updated": "2024-08-13T13:31:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    31,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-07T18:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    18,
                    51,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "Ownership in low-level intermediate representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ownership in low-level intermediate representation"
                },
                "summary": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving."
                },
                "authors": [
                    {
                        "name": "Siddharth Priya"
                    },
                    {
                        "name": "Arie Gurfinkel"
                    }
                ],
                "author_detail": {
                    "name": "Arie Gurfinkel"
                },
                "author": "Arie Gurfinkel",
                "arxiv_comment": "FMCAD 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06876v1",
                "updated": "2024-08-13T13:14:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T13:14:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "Decision-Focused Learning to Predict Action Costs for Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Focused Learning to Predict Action Costs for Planning"
                },
                "summary": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements."
                },
                "authors": [
                    {
                        "name": "Jayanta Mandi"
                    },
                    {
                        "name": "Marco Foschini"
                    },
                    {
                        "name": "Daniel Holler"
                    },
                    {
                        "name": "Sylvie Thiebaux"
                    },
                    {
                        "name": "Jorg Hoffmann"
                    },
                    {
                        "name": "Tias Guns"
                    }
                ],
                "author_detail": {
                    "name": "Tias Guns"
                },
                "author": "Tias Guns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v3",
                "updated": "2024-08-13T09:55:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    55,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00167v2",
                "updated": "2024-08-13T09:08:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    8,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-31T21:33:56Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    33,
                    56,
                    2,
                    213,
                    0
                ],
                "title": "Finch: Prompt-guided Key-Value Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finch: Prompt-guided Key-Value Cache Compression"
                },
                "summary": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "arxiv_comment": "Accepted for publication at TACL - pre-MIT Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v1",
                "updated": "2024-08-12T08:46:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles"
                },
                "summary": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19895v2",
                "updated": "2024-08-12T07:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    47,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-29T11:17:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    11,
                    17,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor"
                },
                "summary": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Luca Valente"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Massimiliano Giacometti"
                    },
                    {
                        "name": "Abdul Basit Sajjad"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "4 pages, 4 figures, DSD2024 and SEAA2024 Works in Progress Session\n  AUG 2024; Updated the acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05912v1",
                "updated": "2024-08-12T03:53:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T03:53:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Correct Wrong Path",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correct Wrong Path"
                },
                "summary": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP."
                },
                "authors": [
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Sankara Prasad Ramesh"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Svilen Kanev"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "Daniel A. Jimnez"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "5 pages, 7 Figures, Submited to Computer Architecture Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v2",
                "updated": "2024-08-11T16:35:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    16,
                    35,
                    10,
                    6,
                    224,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "Added Section IV - (performance analysis of proposed HPDA\n  construction). The term 'coding delay' is formally defined (page no. 5). 14\n  pages, 10 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19410v2",
                "updated": "2024-08-11T08:07:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    8,
                    7,
                    28,
                    6,
                    224,
                    0
                ],
                "published": "2024-02-29T18:07:58Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    18,
                    7,
                    58,
                    3,
                    60,
                    0
                ],
                "title": "Genie: Smart ROS-based Caching for Connected Autonomous Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genie: Smart ROS-based Caching for Connected Autonomous Robots"
                },
                "summary": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time."
                },
                "authors": [
                    {
                        "name": "Zexin Li"
                    },
                    {
                        "name": "Soroush Bateni"
                    },
                    {
                        "name": "Cong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cong Liu"
                },
                "author": "Cong Liu",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v1",
                "updated": "2024-08-10T22:47:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05614v1",
                "updated": "2024-08-10T19:17:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T19:17:46Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "title": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model"
                },
                "summary": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources."
                },
                "authors": [
                    {
                        "name": "Hanqiu Chen"
                    },
                    {
                        "name": "Yitu Wang"
                    },
                    {
                        "name": "Luis Vitorio Cargnini"
                    },
                    {
                        "name": "Mohammadreza Soltaniyeh"
                    },
                    {
                        "name": "Dongyang Li"
                    },
                    {
                        "name": "Gongjin Sun"
                    },
                    {
                        "name": "Pradeep Subedi"
                    },
                    {
                        "name": "Andrew Chang"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Cong Hao"
                    }
                ],
                "author_detail": {
                    "name": "Cong Hao"
                },
                "author": "Cong Hao",
                "arxiv_comment": "This paper is accepted by DAC2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05171v1",
                "updated": "2024-08-09T16:48:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:48:01Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "title": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch"
                },
                "summary": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin."
                },
                "authors": [
                    {
                        "name": "R. A. Ryan"
                    },
                    {
                        "name": "P. E. Tsai"
                    },
                    {
                        "name": "A. R. Johansen"
                    },
                    {
                        "name": "A. Youmans"
                    },
                    {
                        "name": "D. P. Higginson"
                    },
                    {
                        "name": "J. M. Mitrani"
                    },
                    {
                        "name": "C. S. Adams"
                    },
                    {
                        "name": "D. A. Sutherland"
                    },
                    {
                        "name": "B. Levitt"
                    },
                    {
                        "name": "U. Shumlak"
                    }
                ],
                "author_detail": {
                    "name": "U. Shumlak"
                },
                "author": "U. Shumlak",
                "arxiv_comment": "16 pages, 11 figures, submitted to Journal of Nuclear Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03675v2",
                "updated": "2024-08-08T01:20:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    8,
                    1,
                    20,
                    13,
                    3,
                    221,
                    0
                ],
                "published": "2024-08-07T10:31:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    10,
                    31,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time"
                },
                "summary": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Guoxia Wang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "arxiv_comment": "Accepted by ACL 2024 (main conference, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.10978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.10978v2",
                "updated": "2024-08-07T23:48:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    23,
                    48,
                    59,
                    2,
                    220,
                    0
                ],
                "published": "2022-10-20T02:58:36Z",
                "published_parsed": [
                    2022,
                    10,
                    20,
                    2,
                    58,
                    36,
                    3,
                    293,
                    0
                ],
                "title": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends"
                },
                "summary": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem."
                },
                "authors": [
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Youyang Qu"
                    },
                    {
                        "name": "Yong Xiang"
                    },
                    {
                        "name": "Md Palash Uddin"
                    },
                    {
                        "name": "Dezhong Peng"
                    },
                    {
                        "name": "Longxiang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Longxiang Gao"
                },
                "author": "Longxiang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.10978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.10978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v1",
                "updated": "2024-08-07T22:10:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v2",
                "updated": "2024-08-07T20:43:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    20,
                    43,
                    10,
                    2,
                    220,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration..",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03652v1",
                "updated": "2024-08-07T09:34:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T09:34:55Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "title": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search"
                },
                "summary": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdou"
                    },
                    {
                        "name": "Tasneem Mohsen"
                    }
                ],
                "author_detail": {
                    "name": "Tasneem Mohsen"
                },
                "author": "Tasneem Mohsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v1",
                "updated": "2024-08-06T17:16:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02999v1",
                "updated": "2024-08-06T07:12:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T07:12:09Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "title": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning"
                },
                "summary": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop."
                },
                "authors": [
                    {
                        "name": "Lekai Chen"
                    },
                    {
                        "name": "Ashutosh Trivedi"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    }
                ],
                "author_detail": {
                    "name": "Alvaro Velasquez"
                },
                "author": "Alvaro Velasquez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02911v1",
                "updated": "2024-08-06T02:51:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T02:51:22Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "title": "NVPC: A Transparent NVM Page Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVPC: A Transparent NVM Page Cache"
                },
                "summary": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases."
                },
                "authors": [
                    {
                        "name": "Guoyu Wang"
                    },
                    {
                        "name": "Xilong Che"
                    },
                    {
                        "name": "Haoyang Wei"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Puyi He"
                    },
                    {
                        "name": "Juncheng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Hu"
                },
                "author": "Juncheng Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02409v1",
                "updated": "2024-08-05T12:09:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T12:09:50Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "title": "Electron-beam-induced modification of gold microparticles in an SEM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced modification of gold microparticles in an SEM"
                },
                "summary": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings."
                },
                "authors": [
                    {
                        "name": "Kristina Weinel"
                    },
                    {
                        "name": "Marc Benjamin Hahn"
                    },
                    {
                        "name": "Axel Lubk"
                    },
                    {
                        "name": "Wen Feng"
                    },
                    {
                        "name": "Ignacio Gonzalez Martinez"
                    },
                    {
                        "name": "Bernd Bchner"
                    },
                    {
                        "name": "Leonardo Agudo Jcome"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo Agudo Jcome"
                },
                "author": "Leonardo Agudo Jcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05235v1",
                "updated": "2024-08-05T09:07:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T09:07:06Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "title": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving"
                },
                "summary": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server."
                },
                "authors": [
                    {
                        "name": "Andreas Kosmas Kakolyris"
                    },
                    {
                        "name": "Dimosthenis Masouros"
                    },
                    {
                        "name": "Petros Vavaroutsos"
                    },
                    {
                        "name": "Sotirios Xydis"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11912v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11912v3",
                "updated": "2024-08-04T00:58:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    58,
                    4,
                    6,
                    217,
                    0
                ],
                "published": "2024-04-18T05:25:54Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    5,
                    25,
                    54,
                    3,
                    109,
                    0
                ],
                "title": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding"
                },
                "summary": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11912v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11912v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v1",
                "updated": "2024-08-04T00:38:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Large Language Models"
                },
                "summary": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Working in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01519v1",
                "updated": "2024-08-02T18:25:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-02T18:25:57Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "title": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling"
                },
                "summary": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition."
                },
                "authors": [
                    {
                        "name": "Xiao Jiang"
                    },
                    {
                        "name": "Grace J. Gang"
                    },
                    {
                        "name": "J. Webster Stayman"
                    }
                ],
                "author_detail": {
                    "name": "J. Webster Stayman"
                },
                "author": "J. Webster Stayman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00327v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00327v2",
                "updated": "2024-08-02T07:37:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    7,
                    37,
                    51,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-01T07:00:18Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    0,
                    18,
                    3,
                    214,
                    0
                ],
                "title": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration"
                },
                "summary": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Yuan-Hao Chang"
                    },
                    {
                        "name": "Tei-Wei Kuo"
                    }
                ],
                "author_detail": {
                    "name": "Tei-Wei Kuo"
                },
                "author": "Tei-Wei Kuo",
                "arxiv_comment": "This paper has been accepted for presentation at the The\n  International Conference on Hardware/Software Codesign and System Synthesis\n  (CODES+ISSS) in September, 2024. An extended abstract of this paper was\n  presented in Design, Automation & Test in Europe Conference & Exhibition\n  (DATE), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00327v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00327v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00957v1",
                "updated": "2024-08-01T23:52:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T23:52:43Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "title": "Caching Aided Multi-Tenant Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Aided Multi-Tenant Serverless Computing"
                },
                "summary": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead."
                },
                "authors": [
                    {
                        "name": "Chu Qiao"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Zhenkai Zhang"
                    },
                    {
                        "name": "Yuede Ji"
                    },
                    {
                        "name": "Xing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xing Gao"
                },
                "author": "Xing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00859v2",
                "updated": "2024-08-01T21:21:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    21,
                    21,
                    28,
                    3,
                    214,
                    0
                ],
                "published": "2024-04-01T02:01:28Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    2,
                    1,
                    28,
                    0,
                    92,
                    0
                ],
                "title": "Do language models plan ahead for future tokens?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do language models plan ahead for future tokens?"
                },
                "summary": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale."
                },
                "authors": [
                    {
                        "name": "Wilson Wu"
                    },
                    {
                        "name": "John X. Morris"
                    },
                    {
                        "name": "Lionel Levine"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Levine"
                },
                "author": "Lionel Levine",
                "arxiv_comment": "24 pages, 11 figures. Camera-ready for COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00539v1",
                "updated": "2024-08-01T13:22:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T13:22:01Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs"
                },
                "summary": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance."
                },
                "authors": [
                    {
                        "name": "Mingcong Lu"
                    },
                    {
                        "name": "Jiangcai Zhu"
                    },
                    {
                        "name": "Wang Hao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Shusheng Zhang"
                    },
                    {
                        "name": "Kailai Shao"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Xin Lu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Lu"
                },
                "author": "Xin Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v2",
                "updated": "2024-08-01T13:21:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    21,
                    24,
                    3,
                    214,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Offloading-Efficient MoE Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Offloading-Efficient MoE Model Serving"
                },
                "summary": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15220v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15220v4",
                "updated": "2024-08-01T07:51:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    51,
                    25,
                    3,
                    214,
                    0
                ],
                "published": "2024-02-23T09:29:19Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    9,
                    29,
                    19,
                    4,
                    54,
                    0
                ],
                "title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition"
                },
                "summary": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096."
                },
                "authors": [
                    {
                        "name": "Lu Ye"
                    },
                    {
                        "name": "Ze Tao"
                    },
                    {
                        "name": "Yong Huang"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15220v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15220v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00232v1",
                "updated": "2024-08-01T01:57:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T01:57:09Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "title": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction"
                },
                "summary": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks."
                },
                "authors": [
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Zite Jiang"
                    },
                    {
                        "name": "Haihang You"
                    }
                ],
                "author_detail": {
                    "name": "Haihang You"
                },
                "author": "Haihang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v2",
                "updated": "2024-08-01T00:41:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    0,
                    41,
                    52,
                    3,
                    214,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Towards Variable-Length In-Network Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Variable-Length In-Network Caching"
                },
                "summary": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20485v2",
                "updated": "2024-07-31T02:02:40Z",
                "updated_parsed": [
                    2024,
                    7,
                    31,
                    2,
                    2,
                    40,
                    2,
                    213,
                    0
                ],
                "published": "2024-07-30T01:13:42Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    1,
                    13,
                    42,
                    1,
                    212,
                    0
                ],
                "title": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder"
                },
                "summary": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot."
                },
                "authors": [
                    {
                        "name": "Hyun-rae Jo"
                    },
                    {
                        "name": "Dongkun Shin"
                    }
                ],
                "author_detail": {
                    "name": "Dongkun Shin"
                },
                "author": "Dongkun Shin",
                "arxiv_comment": "11 pages(9 pages + reference 2 pages), 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21201v1",
                "updated": "2024-07-30T21:27:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T21:27:00Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "title": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite"
                },
                "summary": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE"
                },
                "authors": [
                    {
                        "name": "Abdulkarim A. Amirov"
                    },
                    {
                        "name": "Maksim A. Koliushenkov"
                    },
                    {
                        "name": "Abdula A. Mukhuchev"
                    },
                    {
                        "name": "Dibir M. Yusupov"
                    },
                    {
                        "name": "Valeriya V. Govorina"
                    },
                    {
                        "name": "Dmitriy S. Neznakhin"
                    },
                    {
                        "name": "Gennady A. Govor"
                    },
                    {
                        "name": "Akhmed M. Aliev"
                    }
                ],
                "author_detail": {
                    "name": "Akhmed M. Aliev"
                },
                "author": "Akhmed M. Aliev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v1",
                "updated": "2024-07-30T18:19:38Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v1",
                "updated": "2024-07-30T17:59:08Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.06944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.06944v2",
                "updated": "2024-07-30T13:06:36Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    13,
                    6,
                    36,
                    1,
                    212,
                    0
                ],
                "published": "2023-04-14T06:21:57Z",
                "published_parsed": [
                    2023,
                    4,
                    14,
                    6,
                    21,
                    57,
                    4,
                    104,
                    0
                ],
                "title": "SpChar: Characterizing the Sparse Puzzle via Decision Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpChar: Characterizing the Sparse Puzzle via Decision Trees"
                },
                "summary": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied."
                },
                "authors": [
                    {
                        "name": "Francesco Sgherzi"
                    },
                    {
                        "name": "Marco Siracusa"
                    },
                    {
                        "name": "Ivan Fernandez"
                    },
                    {
                        "name": "Adri Armejach"
                    },
                    {
                        "name": "Miquel Moret"
                    }
                ],
                "author_detail": {
                    "name": "Miquel Moret"
                },
                "author": "Miquel Moret",
                "arxiv_comment": "27 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.06944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.06944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20773v1",
                "updated": "2024-07-30T12:16:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T12:16:39Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "title": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications"
                },
                "summary": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability."
                },
                "authors": [
                    {
                        "name": "Andronicus Rajasukumar"
                    },
                    {
                        "name": "Jiya Su"
                    },
                    {
                        "name": "Yuqing"
                    },
                    {
                        "name": "Wang"
                    },
                    {
                        "name": "Tianshuo Su"
                    },
                    {
                        "name": "Marziyeh Nourian"
                    },
                    {
                        "name": "Jose M Monsalve Diaz"
                    },
                    {
                        "name": "Tianchi Zhang"
                    },
                    {
                        "name": "Jianru Ding"
                    },
                    {
                        "name": "Wenyi Wang"
                    },
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Moubarak Jeje"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Yanjing Li"
                    },
                    {
                        "name": "Andrew A. Chien"
                    }
                ],
                "author_detail": {
                    "name": "Andrew A. Chien"
                },
                "arxiv_affiliation": "Ivy",
                "author": "Andrew A. Chien",
                "arxiv_comment": "14 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14928v3",
                "updated": "2024-07-30T08:39:52Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    39,
                    52,
                    1,
                    212,
                    0
                ],
                "published": "2023-09-26T13:35:31Z",
                "published_parsed": [
                    2023,
                    9,
                    26,
                    13,
                    35,
                    31,
                    1,
                    269,
                    0
                ],
                "title": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models"
                },
                "summary": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Eman Ali"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Haris Khan"
                },
                "author": "Muhammad Haris Khan",
                "arxiv_comment": "Accepted at BMVC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.14928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03088v2",
                "updated": "2024-07-30T08:19:53Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    19,
                    53,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-03T22:03:28Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    22,
                    3,
                    28,
                    2,
                    94,
                    0
                ],
                "title": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation"
                },
                "summary": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation."
                },
                "authors": [
                    {
                        "name": "Zexin Fang"
                    },
                    {
                        "name": "Bin Han"
                    },
                    {
                        "name": "Hans D. Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D. Schotten"
                },
                "author": "Hans D. Schotten",
                "arxiv_comment": "Submitted to IEEE GLOBECOM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v3",
                "updated": "2024-07-30T04:01:25Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    4,
                    1,
                    25,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19637v1",
                "updated": "2024-07-29T01:43:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:43:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "STT-RAM-based Hierarchical In-Memory Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STT-RAM-based Hierarchical In-Memory Computing"
                },
                "summary": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kevin Antony Gomez"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/TPDS.2024.3430853",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TPDS.2024.3430853",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: IEEE Transactions on Parallel and Distributed Systems (\n  Volume: 35, Issue: 9, September 2024)",
                "arxiv_journal_ref": "IEEE Transactions on Parallel and Distributed Systems, vol. 35,\n  no. 9, pp. 1615-1629, Sept. 2024",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19627v1",
                "updated": "2024-07-29T01:17:54Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:17:54Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "title": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing"
                },
                "summary": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    },
                    {
                        "name": "Kevin Gomez"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Gomez"
                },
                "author": "Kevin Gomez",
                "arxiv_comment": "Accepted in 35th IEEE International Conference on\n  Application-specific Systems, Architectures and Processors (ASAP 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19612v1",
                "updated": "2024-07-28T23:43:59Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T23:43:59Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "title": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors"
                },
                "summary": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1145/3357526.3357553",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3357526.3357553",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the international symposium on memory systems, pp.\n  439-450. 2019",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19604v1",
                "updated": "2024-07-28T22:34:20Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T22:34:20Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "title": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning"
                },
                "summary": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kyle Kuan"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/IGSC48788.2019.8957182",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IGSC48788.2019.8957182",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: 2019 Tenth International Green and Sustainable\n  Computing Conference (IGSC)",
                "arxiv_journal_ref": "2019 Tenth International Green and Sustainable Computing\n  Conference (IGSC), Alexandria, VA, USA, 2019, pp. 1-7,",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19318v1",
                "updated": "2024-07-27T18:26:32Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T18:26:32Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "title": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review"
                },
                "summary": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture."
                },
                "authors": [
                    {
                        "name": "Anujkumarsinh Donvir"
                    },
                    {
                        "name": "Apeksha Jain"
                    },
                    {
                        "name": "Pradeep Kumar Saraswathi"
                    }
                ],
                "author_detail": {
                    "name": "Pradeep Kumar Saraswathi"
                },
                "author": "Pradeep Kumar Saraswathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13996v2",
                "updated": "2024-07-27T08:52:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    52,
                    39,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-19T03:01:32Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    3,
                    1,
                    32,
                    4,
                    201,
                    0
                ],
                "title": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference"
                },
                "summary": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance."
                },
                "authors": [
                    {
                        "name": "Yongkang Zhang"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Chenxia Han"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Huaicheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaicheng Li"
                },
                "author": "Huaicheng Li",
                "arxiv_comment": "18 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.9; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19205v1",
                "updated": "2024-07-27T08:21:14Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T08:21:14Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "title": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions"
                },
                "summary": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Ashkan Taghipour"
                    },
                    {
                        "name": "Morteza Ghahremani"
                    },
                    {
                        "name": "Mohammed Bennamoun"
                    },
                    {
                        "name": "Aref Miri Rekavandi"
                    },
                    {
                        "name": "Zinuo Li"
                    },
                    {
                        "name": "Hamid Laga"
                    },
                    {
                        "name": "Farid Boussaid"
                    }
                ],
                "author_detail": {
                    "name": "Farid Boussaid"
                },
                "author": "Farid Boussaid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19090v1",
                "updated": "2024-07-26T21:11:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "published": "2024-07-26T21:11:58Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "title": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores"
                },
                "summary": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance."
                },
                "authors": [
                    {
                        "name": "Alireza Heidari"
                    },
                    {
                        "name": "Amirhossein Ahmadi"
                    },
                    {
                        "name": "Zefeng Zhi"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "Cloud Databases",
                "arxiv_journal_ref": "VLDB 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18121v1",
                "updated": "2024-07-25T15:29:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T15:29:05Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "title": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache"
                },
                "summary": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache"
                },
                "authors": [
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Benlin Liu"
                    },
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Yuhao Dong"
                    },
                    {
                        "name": "Guangyi Chen"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02750v2",
                "updated": "2024-07-25T09:16:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    9,
                    16,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-02-05T06:06:47Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    6,
                    6,
                    47,
                    0,
                    36,
                    0
                ],
                "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"
                },
                "summary": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI."
                },
                "authors": [
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongye Jin"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Vladimir Braverman"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "arxiv_doi": "10.13140/RG.2.2.28167.37282",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.13140/RG.2.2.28167.37282",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.02750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ICML2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20272v1",
                "updated": "2024-07-25T07:50:17Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T07:50:17Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "title": "An Efficient Inference Framework for Early-exit Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Inference Framework for Early-exit Large Language Models"
                },
                "summary": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up."
                },
                "authors": [
                    {
                        "name": "Ruijie Miao"
                    },
                    {
                        "name": "Yihan Yan"
                    },
                    {
                        "name": "Xinshuo Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v1",
                "updated": "2024-07-25T00:27:07Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.08711v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.08711v3",
                "updated": "2024-07-24T13:36:03Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    13,
                    36,
                    3,
                    2,
                    206,
                    0
                ],
                "published": "2023-01-20T18:13:38Z",
                "published_parsed": [
                    2023,
                    1,
                    20,
                    18,
                    13,
                    38,
                    4,
                    20,
                    0
                ],
                "title": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers"
                },
                "summary": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers."
                },
                "authors": [
                    {
                        "name": "Qifa Yan"
                    },
                    {
                        "name": "Xiaohu Tang"
                    },
                    {
                        "name": "Zhengchun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zhengchun Zhou"
                },
                "author": "Zhengchun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.08711v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.08711v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15771v2",
                "updated": "2024-07-24T12:56:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    12,
                    56,
                    41,
                    2,
                    206,
                    0
                ],
                "published": "2024-03-13T17:47:39Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    17,
                    47,
                    39,
                    2,
                    73,
                    0
                ],
                "title": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations"
                },
                "summary": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations."
                },
                "authors": [
                    {
                        "name": "Craig Innes"
                    },
                    {
                        "name": "Subramanian Ramamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Subramanian Ramamoorthy"
                },
                "author": "Subramanian Ramamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15569v2",
                "updated": "2024-07-24T08:56:11Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    8,
                    56,
                    11,
                    2,
                    206,
                    0
                ],
                "published": "2024-01-28T05:12:09Z",
                "published_parsed": [
                    2024,
                    1,
                    28,
                    5,
                    12,
                    9,
                    6,
                    28,
                    0
                ],
                "title": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs"
                },
                "summary": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE"
                },
                "authors": [
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Yaoke Wang"
                    },
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Siliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Siliang Tang"
                },
                "author": "Siliang Tang",
                "arxiv_comment": "Accepted by IJCAI2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09636v2",
                "updated": "2024-07-23T17:55:30Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    55,
                    30,
                    1,
                    205,
                    0
                ],
                "published": "2024-03-14T17:59:26Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    17,
                    59,
                    26,
                    3,
                    74,
                    0
                ],
                "title": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference"
                },
                "summary": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget."
                },
                "authors": [
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Adrian acucki"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "David Tarjan"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024) 37396-37412",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16672v1",
                "updated": "2024-07-23T17:42:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T17:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "title": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications"
                },
                "summary": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications."
                },
                "authors": [
                    {
                        "name": "Sahan Liyanaarachchi"
                    },
                    {
                        "name": "Stavros Mitrolaris"
                    },
                    {
                        "name": "Purbesh Mitra"
                    },
                    {
                        "name": "Sennur Ulukus"
                    }
                ],
                "author_detail": {
                    "name": "Sennur Ulukus"
                },
                "author": "Sennur Ulukus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16303v1",
                "updated": "2024-07-23T08:58:06Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:58:06Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "title": "Hidden Web Caches Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden Web Caches Discovery"
                },
                "summary": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers."
                },
                "authors": [
                    {
                        "name": "Matteo Golinelli"
                    },
                    {
                        "name": "Bruno Crispo"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Crispo"
                },
                "author": "Bruno Crispo",
                "arxiv_doi": "10.1145/3678890.3678931",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3678890.3678931",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.16303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The definitive Version of Record was published in The 27th\n  International Symposium on Research in Attacks, Intrusions and Defenses (RAID\n  2024), September 30-October 02, 2024, Padua, Italy,\n  https://doi.org/10.1145/3678890.3678931",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16300v1",
                "updated": "2024-07-23T08:55:10Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:55:10Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "title": "A Programming Model for Disaggregated Memory over CXL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Programming Model for Disaggregated Memory over CXL"
                },
                "summary": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores in a cacheline granularity. Alongside with unleashing unique\nopportunities for a wide range of applications, CXL introduces new challenges\nof data management and crash consistency. Alas, CXL lacks an adequate\nprogramming model, which makes reasoning about the correctness and expected\nbehaviors of algorithms and systems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. Using these transformations, every\nlinearizable algorithm can be easily transformed into its provably correct\nversion in the face of a full-system or sub-system crash. We believe that this\nwork will serve as the stepping stone for systems design and modelling on top\nof CXL, and support the development of future models as software and hardware\nevolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores in a cacheline granularity. Alongside with unleashing unique\nopportunities for a wide range of applications, CXL introduces new challenges\nof data management and crash consistency. Alas, CXL lacks an adequate\nprogramming model, which makes reasoning about the correctness and expected\nbehaviors of algorithms and systems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. Using these transformations, every\nlinearizable algorithm can be easily transformed into its provably correct\nversion in the face of a full-system or sub-system crash. We believe that this\nwork will serve as the stepping stone for systems design and modelling on top\nof CXL, and support the development of future models as software and hardware\nevolve."
                },
                "authors": [
                    {
                        "name": "Gal Assa"
                    },
                    {
                        "name": "Michal Friedman"
                    },
                    {
                        "name": "Ori Lahav"
                    }
                ],
                "author_detail": {
                    "name": "Ori Lahav"
                },
                "author": "Ori Lahav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16286v1",
                "updated": "2024-07-23T08:40:27Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    40,
                    27,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:40:27Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    40,
                    27,
                    1,
                    205,
                    0
                ],
                "title": "A deeper look at depth pruning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A deeper look at depth pruning of LLMs"
                },
                "summary": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique."
                },
                "authors": [
                    {
                        "name": "Shoaib Ahmed Siddiqui"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Greg Heinrich"
                    },
                    {
                        "name": "Thomas Breuel"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15309v1",
                "updated": "2024-07-22T14:37:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    14,
                    37,
                    58,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T14:37:58Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    14,
                    37,
                    58,
                    0,
                    204,
                    0
                ],
                "title": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving"
                },
                "summary": "Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads."
                },
                "authors": [
                    {
                        "name": "Jiale Xu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "arxiv_comment": "16 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15581v1",
                "updated": "2024-07-22T12:17:01Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    12,
                    17,
                    1,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T12:17:01Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    12,
                    17,
                    1,
                    0,
                    204,
                    0
                ],
                "title": "vLSM: Low tail latency and I/O amplification in LSM-based KV stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vLSM: Low tail latency and I/O amplification in LSM-based KV stores"
                },
                "summary": "LSM-based key-value (KV) stores are an important component in modern data\ninfrastructures. However, they suffer from high tail latency, in the order of\nseveral seconds, making them less attractive for user-facing applications. In\nthis paper, we introduce the notion of compaction chains and we analyse how\nthey affect tail latency. Then, we show that modern designs reduce tail\nlatency, by trading I/O amplification or require large amounts of memory. Based\non our analysis, we present vLSM, a new KV store design that improves tail\nlatency significantly without compromising on memory or I/O amplification. vLSM\nreduces (a) compaction chain width by using small SSTs and eliminating the\ntiering compaction required in L0 by modern systems and (b) compaction chain\nlength by using a larger than typical growth factor between L1 and L2 and\nintroducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate\nit using db_bench and YCSB. Our evaluation highlights the underlying trade-off\namong memory requirements, I/O amplification, and tail latency, as well as the\nadvantage of vLSM over current approaches. vLSM improves P99 tail latency by up\nto 4.8x for writes and by up to 12.5x for reads, reduces cumulative write\nstalls by up to 60% while also slightly improves I/O amplification at the same\nmemory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSM-based key-value (KV) stores are an important component in modern data\ninfrastructures. However, they suffer from high tail latency, in the order of\nseveral seconds, making them less attractive for user-facing applications. In\nthis paper, we introduce the notion of compaction chains and we analyse how\nthey affect tail latency. Then, we show that modern designs reduce tail\nlatency, by trading I/O amplification or require large amounts of memory. Based\non our analysis, we present vLSM, a new KV store design that improves tail\nlatency significantly without compromising on memory or I/O amplification. vLSM\nreduces (a) compaction chain width by using small SSTs and eliminating the\ntiering compaction required in L0 by modern systems and (b) compaction chain\nlength by using a larger than typical growth factor between L1 and L2 and\nintroducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate\nit using db_bench and YCSB. Our evaluation highlights the underlying trade-off\namong memory requirements, I/O amplification, and tail latency, as well as the\nadvantage of vLSM over current approaches. vLSM improves P99 tail latency by up\nto 4.8x for writes and by up to 12.5x for reads, reduces cumulative write\nstalls by up to 60% while also slightly improves I/O amplification at the same\nmemory budget."
                },
                "authors": [
                    {
                        "name": "Giorgos Xanthakis"
                    },
                    {
                        "name": "Antonios Katsarakis"
                    },
                    {
                        "name": "Giorgos Saloustros"
                    },
                    {
                        "name": "Angelos Bilas"
                    }
                ],
                "author_detail": {
                    "name": "Angelos Bilas"
                },
                "author": "Angelos Bilas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.11055v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.11055v5",
                "updated": "2024-07-22T10:02:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    10,
                    2,
                    57,
                    0,
                    204,
                    0
                ],
                "published": "2022-12-21T14:59:23Z",
                "published_parsed": [
                    2022,
                    12,
                    21,
                    14,
                    59,
                    23,
                    2,
                    355,
                    0
                ],
                "title": "Coalgebraic Satisfiability Checking for Arithmetic $$-Calculi",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coalgebraic Satisfiability Checking for Arithmetic $$-Calculi"
                },
                "summary": "The coalgebraic $\\mu$-calculus provides a generic semantic framework for\nfixpoint logics over systems whose branching type goes beyond the standard\nrelational setup, e.g. probabilistic, weighted, or game-based. Previous work on\nthe coalgebraic $\\mu$-calculus includes an exponential-time upper bound on\nsatisfiability checking, which however relies on the availability of tableau\nrules for the next-step modalities that are sufficiently well-behaved in a\nformally defined sense; in particular, rule matches need to be representable by\npolynomial-sized codes, and the sequent duals of the rules need to absorb cut.\nWhile such rule sets have been identified for some important cases, they are\nnot known to exist in all cases of interest, in particular ones involving\neither integer weights as in the graded $\\mu$-calculus, or real-valued weights\nin combination with non-linear arithmetic. In the present work, we prove the\nsame upper complexity bound under more general assumptions, specifically\nregarding the complexity of the (much simpler) satisfiability problem for the\nunderlying one-step logic, roughly described as the nesting-free next-step\nfragment of the logic. The bound is realized by a generic global caching\nalgorithm that supports on-the-fly satisfiability checking. Notably, our\napproach directly accommodates unguarded formulae, and thus avoids use of the\nguardedness transformation. Example applications include new exponential-time\nupper bounds for satisfiability checking in an extension of the graded\n$\\mu$-calculus with polynomial inequalities (including positive Presburger\narithmetic), as well as an extension of the (two-valued) probabilistic\n$\\mu$-calculus with polynomial inequalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coalgebraic $\\mu$-calculus provides a generic semantic framework for\nfixpoint logics over systems whose branching type goes beyond the standard\nrelational setup, e.g. probabilistic, weighted, or game-based. Previous work on\nthe coalgebraic $\\mu$-calculus includes an exponential-time upper bound on\nsatisfiability checking, which however relies on the availability of tableau\nrules for the next-step modalities that are sufficiently well-behaved in a\nformally defined sense; in particular, rule matches need to be representable by\npolynomial-sized codes, and the sequent duals of the rules need to absorb cut.\nWhile such rule sets have been identified for some important cases, they are\nnot known to exist in all cases of interest, in particular ones involving\neither integer weights as in the graded $\\mu$-calculus, or real-valued weights\nin combination with non-linear arithmetic. In the present work, we prove the\nsame upper complexity bound under more general assumptions, specifically\nregarding the complexity of the (much simpler) satisfiability problem for the\nunderlying one-step logic, roughly described as the nesting-free next-step\nfragment of the logic. The bound is realized by a generic global caching\nalgorithm that supports on-the-fly satisfiability checking. Notably, our\napproach directly accommodates unguarded formulae, and thus avoids use of the\nguardedness transformation. Example applications include new exponential-time\nupper bounds for satisfiability checking in an extension of the graded\n$\\mu$-calculus with polynomial inequalities (including positive Presburger\narithmetic), as well as an extension of the (two-valued) probabilistic\n$\\mu$-calculus with polynomial inequalities."
                },
                "authors": [
                    {
                        "name": "Daniel Hausmann"
                    },
                    {
                        "name": "Lutz Schrder"
                    }
                ],
                "author_detail": {
                    "name": "Lutz Schrder"
                },
                "author": "Lutz Schrder",
                "arxiv_doi": "10.46298/lmcs-20(3:9)2024",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.46298/lmcs-20(3:9)2024",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2212.11055v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.11055v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Logical Methods in Computer Science, Volume 20, Issue 3 (July 23,\n  2024) lmcs:10532",
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "03B70, 03B44",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15360v1",
                "updated": "2024-07-22T04:07:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    4,
                    7,
                    26,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T04:07:26Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    4,
                    7,
                    26,
                    0,
                    204,
                    0
                ],
                "title": "Dissecting Multiplication in Transformers: Insights into LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting Multiplication in Transformers: Insights into LLMs"
                },
                "summary": "Transformer-based large language models have achieved remarkable performance\nacross various natural language processing tasks. However, they often struggle\nwith seemingly easy tasks like arithmetic despite their vast capabilities. This\nstark disparity raise human's concerns about their safe and ethical use, hinder\ntheir widespread adoption.In this paper, we focus on a typical arithmetic task,\ninteger multiplication, to explore and explain the imperfection of transformers\nin this domain. We provide comprehensive analysis of a vanilla transformer\ntrained to perform n-digit integer multiplication. Our observations indicate\nthat the model decomposes multiplication task into multiple parallel subtasks,\nsequentially optimizing each subtask for each digit to complete the final\nmultiplication. Based on observation and analysis, we infer the reasons of\ntransformers deficiencies in multiplication tasks lies in their difficulty in\ncalculating successive carryovers and caching intermediate results, and\nconfirmed this inference through experiments. Guided by these findings, we\npropose improvements to enhance transformers performance on multiplication\ntasks. These enhancements are validated through rigorous testing and\nmathematical modeling, not only enhance transformer's interpretability, but\nalso improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit\ninteger multiplication with a tiny transformer, outperform LLMs GPT-4. Our\nmethod contributes to the broader fields of model understanding and\ninterpretability, paving the way for analyzing more complex tasks and\nTransformer models. This work underscores the importance of explainable AI,\nhelping to build trust in large language models and promoting their adoption in\ncritical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models have achieved remarkable performance\nacross various natural language processing tasks. However, they often struggle\nwith seemingly easy tasks like arithmetic despite their vast capabilities. This\nstark disparity raise human's concerns about their safe and ethical use, hinder\ntheir widespread adoption.In this paper, we focus on a typical arithmetic task,\ninteger multiplication, to explore and explain the imperfection of transformers\nin this domain. We provide comprehensive analysis of a vanilla transformer\ntrained to perform n-digit integer multiplication. Our observations indicate\nthat the model decomposes multiplication task into multiple parallel subtasks,\nsequentially optimizing each subtask for each digit to complete the final\nmultiplication. Based on observation and analysis, we infer the reasons of\ntransformers deficiencies in multiplication tasks lies in their difficulty in\ncalculating successive carryovers and caching intermediate results, and\nconfirmed this inference through experiments. Guided by these findings, we\npropose improvements to enhance transformers performance on multiplication\ntasks. These enhancements are validated through rigorous testing and\nmathematical modeling, not only enhance transformer's interpretability, but\nalso improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit\ninteger multiplication with a tiny transformer, outperform LLMs GPT-4. Our\nmethod contributes to the broader fields of model understanding and\ninterpretability, paving the way for analyzing more complex tasks and\nTransformer models. This work underscores the importance of explainable AI,\nhelping to build trust in large language models and promoting their adoption in\ncritical applications."
                },
                "authors": [
                    {
                        "name": "Luyu Qiu"
                    },
                    {
                        "name": "Jianing Li"
                    },
                    {
                        "name": "Chi Su"
                    },
                    {
                        "name": "Chen Jason Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15891v1",
                "updated": "2024-07-22T01:12:23Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    1,
                    12,
                    23,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T01:12:23Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    1,
                    12,
                    23,
                    0,
                    204,
                    0
                ],
                "title": "RazorAttention: Efficient KV Cache Compression Through Retrieval Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RazorAttention: Efficient KV Cache Compression Through Retrieval Heads"
                },
                "summary": "The memory and computational demands of Key-Value (KV) cache present\nsignificant challenges for deploying long-context language models. Previous\napproaches attempt to mitigate this issue by selectively dropping tokens, which\nirreversibly erases critical information that might be needed for future\nqueries. In this paper, we propose a novel compression technique for KV cache\nthat preserves all token information. Our investigation reveals that: i) Most\nattention heads primarily focus on the local context; ii) Only a few heads,\ndenoted as retrieval heads, can essentially pay attention to all input tokens.\nThese key observations motivate us to use separate caching strategy for\nattention heads. Therefore, we propose RazorAttention, a training-free KV cache\ncompression algorithm, which maintains a full cache for these crucial retrieval\nheads and discards the remote tokens in non-retrieval heads. Furthermore, we\nintroduce a novel mechanism involving a \"compensation token\" to further recover\nthe information in the dropped tokens. Extensive evaluations across a diverse\nset of large language models (LLMs) demonstrate that RazorAttention achieves a\nreduction in KV cache size by over 70% without noticeable impacts on\nperformance. Additionally, RazorAttention is compatible with FlashAttention,\nrendering it an efficient and plug-and-play solution that enhances LLM\ninference efficiency without overhead or retraining of the original model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The memory and computational demands of Key-Value (KV) cache present\nsignificant challenges for deploying long-context language models. Previous\napproaches attempt to mitigate this issue by selectively dropping tokens, which\nirreversibly erases critical information that might be needed for future\nqueries. In this paper, we propose a novel compression technique for KV cache\nthat preserves all token information. Our investigation reveals that: i) Most\nattention heads primarily focus on the local context; ii) Only a few heads,\ndenoted as retrieval heads, can essentially pay attention to all input tokens.\nThese key observations motivate us to use separate caching strategy for\nattention heads. Therefore, we propose RazorAttention, a training-free KV cache\ncompression algorithm, which maintains a full cache for these crucial retrieval\nheads and discards the remote tokens in non-retrieval heads. Furthermore, we\nintroduce a novel mechanism involving a \"compensation token\" to further recover\nthe information in the dropped tokens. Extensive evaluations across a diverse\nset of large language models (LLMs) demonstrate that RazorAttention achieves a\nreduction in KV cache size by over 70% without noticeable impacts on\nperformance. Additionally, RazorAttention is compatible with FlashAttention,\nrendering it an efficient and plug-and-play solution that enhances LLM\ninference efficiency without overhead or retraining of the original model."
                },
                "authors": [
                    {
                        "name": "Hanlin Tang"
                    },
                    {
                        "name": "Yang Lin"
                    },
                    {
                        "name": "Jing Lin"
                    },
                    {
                        "name": "Qingsen Han"
                    },
                    {
                        "name": "Shikuan Hong"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Gongyi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gongyi Wang"
                },
                "author": "Gongyi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15264v1",
                "updated": "2024-07-21T20:41:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    20,
                    41,
                    39,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-21T20:41:39Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    20,
                    41,
                    39,
                    6,
                    203,
                    0
                ],
                "title": "LSM-GNN: Large-scale Storage-based Multi-GPU GNN Training by Optimizing\n  Data Transfer Scheme",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSM-GNN: Large-scale Storage-based Multi-GPU GNN Training by Optimizing\n  Data Transfer Scheme"
                },
                "summary": "Graph Neural Networks (GNNs) are widely used today in recommendation systems,\nfraud detection, and node/link classification tasks. Real world GNNs continue\nto scale in size and require a large memory footprint for storing graphs and\nembeddings that often exceed the memory capacities of the target GPUs used for\ntraining. To address limited memory capacities, traditional GNN training\napproaches use graph partitioning and sharding techniques to scale up across\nmultiple GPUs within a node and/or scale out across multiple nodes. However,\nthis approach suffers from the high computational costs of graph partitioning\nalgorithms and inefficient communication across GPUs.\n  To address these overheads, we propose Large-scale Storage-based Multi-GPU\nGNN framework (LSM-GNN), a storagebased approach to train GNN models that\nutilizes a novel communication layer enabling GPU software caches to function\nas a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid\neviction policy that intelligently manages cache space by using both static and\ndynamic node information to significantly enhance cache performance.\nFurthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a\nmechanism for prefetching node feature data from a Victim Buffer located in CPU\npinned-memory to further reduce the pressure on the storage devices.\nExperimental results show that despite the lower compute capabilities and\nmemory capacities, LSM-GNN in a single node with two GPUs offers superior\nperformance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x\nspeed up on end-to-end epoch time while running large-scale GNN training",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are widely used today in recommendation systems,\nfraud detection, and node/link classification tasks. Real world GNNs continue\nto scale in size and require a large memory footprint for storing graphs and\nembeddings that often exceed the memory capacities of the target GPUs used for\ntraining. To address limited memory capacities, traditional GNN training\napproaches use graph partitioning and sharding techniques to scale up across\nmultiple GPUs within a node and/or scale out across multiple nodes. However,\nthis approach suffers from the high computational costs of graph partitioning\nalgorithms and inefficient communication across GPUs.\n  To address these overheads, we propose Large-scale Storage-based Multi-GPU\nGNN framework (LSM-GNN), a storagebased approach to train GNN models that\nutilizes a novel communication layer enabling GPU software caches to function\nas a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid\neviction policy that intelligently manages cache space by using both static and\ndynamic node information to significantly enhance cache performance.\nFurthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a\nmechanism for prefetching node feature data from a Victim Buffer located in CPU\npinned-memory to further reduce the pressure on the storage devices.\nExperimental results show that despite the lower compute capabilities and\nmemory capacities, LSM-GNN in a single node with two GPUs offers superior\nperformance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x\nspeed up on end-to-end epoch time while running large-scale GNN training"
                },
                "authors": [
                    {
                        "name": "Jeongmin Brian Park"
                    },
                    {
                        "name": "Kun Wu"
                    },
                    {
                        "name": "Vikram Sharma Mailthody"
                    },
                    {
                        "name": "Zaid Quresh"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Wen-mei Hwu"
                    }
                ],
                "author_detail": {
                    "name": "Wen-mei Hwu"
                },
                "author": "Wen-mei Hwu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15176v1",
                "updated": "2024-07-21T14:23:37Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    23,
                    37,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-21T14:23:37Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    23,
                    37,
                    6,
                    203,
                    0
                ],
                "title": "Farewell to Length Extrapolation, a Training-Free Infinite Context with\n  Finite Attention Scope",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Farewell to Length Extrapolation, a Training-Free Infinite Context with\n  Finite Attention Scope"
                },
                "summary": "The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon."
                },
                "authors": [
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Kai Lv"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.00250v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.00250v3",
                "updated": "2024-07-21T11:47:04Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    11,
                    47,
                    4,
                    6,
                    203,
                    0
                ],
                "published": "2022-12-01T03:35:14Z",
                "published_parsed": [
                    2022,
                    12,
                    1,
                    3,
                    35,
                    14,
                    3,
                    335,
                    0
                ],
                "title": "Split Learning without Local Weight Sharing to Enhance Client-side Data\n  Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split Learning without Local Weight Sharing to Enhance Client-side Data\n  Privacy"
                },
                "summary": "Split learning (SL) aims to protect user data privacy by distributing deep\nmodels between client-server and keeping private data locally. In SL training\nwith multiple clients, the local model weights are shared among the clients for\nlocal model update. This paper first reveals data privacy leakage exacerbated\nfrom local weight sharing among the clients in SL through model inversion\nattacks. Then, to reduce the data privacy leakage issue, we propose and analyze\nprivacy-enhanced SL (P-SL) (or SL without local weight sharing). We further\npropose parallelized P-SL to expedite the training process by duplicating\nmultiple server-side model instances without compromising accuracy. Finally, we\nexplore P-SL with late participating clients and devise a server-side\ncache-based training method to address the forgetting phenomenon in SL when\nlate clients join. Experimental results demonstrate that P-SL helps reduce up\nto 50% of client-side data leakage, which essentially achieves a better\nprivacy-accuracy trade-off than the current trend by using differential privacy\nmechanisms. Moreover, P-SL and its cache-based version achieve comparable\naccuracy to baseline SL under various data distributions, while cost less\ncomputation and communication. Additionally, caching-based training in P-SL\nmitigates the negative effect of forgetting, stabilizes the learning, and\nenables practical and low-complexity training in a dynamic environment with\nlate-arriving clients.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split learning (SL) aims to protect user data privacy by distributing deep\nmodels between client-server and keeping private data locally. In SL training\nwith multiple clients, the local model weights are shared among the clients for\nlocal model update. This paper first reveals data privacy leakage exacerbated\nfrom local weight sharing among the clients in SL through model inversion\nattacks. Then, to reduce the data privacy leakage issue, we propose and analyze\nprivacy-enhanced SL (P-SL) (or SL without local weight sharing). We further\npropose parallelized P-SL to expedite the training process by duplicating\nmultiple server-side model instances without compromising accuracy. Finally, we\nexplore P-SL with late participating clients and devise a server-side\ncache-based training method to address the forgetting phenomenon in SL when\nlate clients join. Experimental results demonstrate that P-SL helps reduce up\nto 50% of client-side data leakage, which essentially achieves a better\nprivacy-accuracy trade-off than the current trend by using differential privacy\nmechanisms. Moreover, P-SL and its cache-based version achieve comparable\naccuracy to baseline SL under various data distributions, while cost less\ncomputation and communication. Additionally, caching-based training in P-SL\nmitigates the negative effect of forgetting, stabilizes the learning, and\nenables practical and low-complexity training in a dynamic environment with\nlate-arriving clients."
                },
                "authors": [
                    {
                        "name": "Ngoc Duy Pham"
                    },
                    {
                        "name": "Tran Khoa Phan"
                    },
                    {
                        "name": "Alsharif Abuadbba"
                    },
                    {
                        "name": "Yansong Gao"
                    },
                    {
                        "name": "Doan Nguyen"
                    },
                    {
                        "name": "Naveen Chilamkurti"
                    }
                ],
                "author_detail": {
                    "name": "Naveen Chilamkurti"
                },
                "author": "Naveen Chilamkurti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.00250v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.00250v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08454v2",
                "updated": "2024-07-21T02:37:11Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    2,
                    37,
                    11,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-11T12:50:42Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    12,
                    50,
                    42,
                    3,
                    193,
                    0
                ],
                "title": "Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on\n  Long-Context Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on\n  Long-Context Tasks"
                },
                "summary": "How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets."
                },
                "authors": [
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Boxiao Jin"
                    },
                    {
                        "name": "Zhongzhi Yu"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.10516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.10516v2",
                "updated": "2024-07-20T22:14:42Z",
                "updated_parsed": [
                    2024,
                    7,
                    20,
                    22,
                    14,
                    42,
                    5,
                    202,
                    0
                ],
                "published": "2023-03-28T03:55:47Z",
                "published_parsed": [
                    2023,
                    3,
                    28,
                    3,
                    55,
                    47,
                    1,
                    87,
                    0
                ],
                "title": "Distributed Neural Representation for Reactive in situ Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Neural Representation for Reactive in situ Visualization"
                },
                "summary": "Implicit neural representations (INRs) have emerged as a powerful tool for\ncompressing large-scale volume data. This opens up new possibilities for in\nsitu visualization. However, the efficient application of INRs to distributed\ndata remains an underexplored area. In this work, we develop a distributed\nvolumetric neural representation and optimize it for in situ visualization. Our\ntechnique eliminates data exchanges between processes, achieving\nstate-of-the-art compression speed, quality and ratios. Our technique also\nenables the implementation of an efficient strategy for caching large-scale\nsimulation data in high temporal frequencies, further facilitating the use of\nreactive in situ visualization in a wider range of scientific problems. We\nintegrate this system with the Ascent infrastructure and evaluate its\nperformance and usability using real-world simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit neural representations (INRs) have emerged as a powerful tool for\ncompressing large-scale volume data. This opens up new possibilities for in\nsitu visualization. However, the efficient application of INRs to distributed\ndata remains an underexplored area. In this work, we develop a distributed\nvolumetric neural representation and optimize it for in situ visualization. Our\ntechnique eliminates data exchanges between processes, achieving\nstate-of-the-art compression speed, quality and ratios. Our technique also\nenables the implementation of an efficient strategy for caching large-scale\nsimulation data in high temporal frequencies, further facilitating the use of\nreactive in situ visualization in a wider range of scientific problems. We\nintegrate this system with the Ascent infrastructure and evaluate its\nperformance and usability using real-world simulations."
                },
                "authors": [
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Joseph A. Insley"
                    },
                    {
                        "name": "Victor A. Mateevitsi"
                    },
                    {
                        "name": "Silvio Rizzi"
                    },
                    {
                        "name": "Michael E. Papka"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.10516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.10516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14801v1",
                "updated": "2024-07-20T08:21:46Z",
                "updated_parsed": [
                    2024,
                    7,
                    20,
                    8,
                    21,
                    46,
                    5,
                    202,
                    0
                ],
                "published": "2024-07-20T08:21:46Z",
                "published_parsed": [
                    2024,
                    7,
                    20,
                    8,
                    21,
                    46,
                    5,
                    202,
                    0
                ],
                "title": "SquareSort: a cache-oblivious sorting algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SquareSort: a cache-oblivious sorting algorithm"
                },
                "summary": "In this paper we consider sorting in the cache-oblivious model of Frigo,\nLeiserson, Prokop, and Ramachandran (1999). We introduce a new simple sorting\nalgorithm in that model which has asymptotically optimal IO complexity\n$O(\\frac{n}{B} \\log_{M/B} n)$, where $n$ is the instance size, $M$ size of the\ncache and $B$ size of a memory block. This is the same as the complexity of the\nbest known cache-oblivious sorting algorithm FunnelSort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we consider sorting in the cache-oblivious model of Frigo,\nLeiserson, Prokop, and Ramachandran (1999). We introduce a new simple sorting\nalgorithm in that model which has asymptotically optimal IO complexity\n$O(\\frac{n}{B} \\log_{M/B} n)$, where $n$ is the instance size, $M$ size of the\ncache and $B$ size of a memory block. This is the same as the complexity of the\nbest known cache-oblivious sorting algorithm FunnelSort."
                },
                "authors": [
                    {
                        "name": "Michal Kouck"
                    },
                    {
                        "name": "Josef Matjka"
                    }
                ],
                "author_detail": {
                    "name": "Josef Matjka"
                },
                "author": "Josef Matjka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07240v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07240v6",
                "updated": "2024-07-19T21:04:14Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    21,
                    4,
                    14,
                    4,
                    201,
                    0
                ],
                "published": "2023-10-11T07:08:20Z",
                "published_parsed": [
                    2023,
                    10,
                    11,
                    7,
                    8,
                    20,
                    2,
                    284,
                    0
                ],
                "title": "CacheGen: KV Cache Compression and Streaming for Fast Large Language\n  Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheGen: KV Cache Compression and Streaming for Fast Large Language\n  Model Serving"
                },
                "summary": "As large language models (LLMs) take on complex tasks, their inputs are\nsupplemented with longer contexts that incorporate domain knowledge. Yet using\nlong contexts is challenging, as nothing can be generated until the whole\ncontext is processed by the LLM. While the context-processing delay can be\nreduced by reusing the KV cache of a context across different inputs, fetching\nthe KV cache, which contains large tensors, over the network can cause high\nextra network delays.\n  CacheGen is a fast context-loading module for LLM systems. First, CacheGen\nuses a custom tensor encoder, leveraging KV cache's distributional properties\nto encode a KV cache into more compact bitstream representations with\nnegligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts\nthe compression level of different parts of a KV cache to cope with changes in\navailable bandwidth, in order to maintain low context-loading delay and high\ngeneration quality. % When available bandwidth drops, CacheGen may raise the\ncompression level for a part of the context or recompute its KV cache on the\nfly. We test CacheGen on popular LLMs and datasets. Compared to the recent\nsystems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5-4.3x\nand the total delay in fetching and processing contexts by 3.2-3.7x with\nnegligible impact on the LLM response quality. Our code is at:\nhttps://github.com/UChi-JCL/CacheGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) take on complex tasks, their inputs are\nsupplemented with longer contexts that incorporate domain knowledge. Yet using\nlong contexts is challenging, as nothing can be generated until the whole\ncontext is processed by the LLM. While the context-processing delay can be\nreduced by reusing the KV cache of a context across different inputs, fetching\nthe KV cache, which contains large tensors, over the network can cause high\nextra network delays.\n  CacheGen is a fast context-loading module for LLM systems. First, CacheGen\nuses a custom tensor encoder, leveraging KV cache's distributional properties\nto encode a KV cache into more compact bitstream representations with\nnegligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts\nthe compression level of different parts of a KV cache to cope with changes in\navailable bandwidth, in order to maintain low context-loading delay and high\ngeneration quality. % When available bandwidth drops, CacheGen may raise the\ncompression level for a part of the context or recompute its KV cache on the\nfly. We test CacheGen on popular LLMs and datasets. Compared to the recent\nsystems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5-4.3x\nand the total delay in fetching and processing contexts by 3.2-3.7x with\nnegligible impact on the LLM response quality. Our code is at:\nhttps://github.com/UChi-JCL/CacheGen."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Ganesh Ananthanarayanan"
                    },
                    {
                        "name": "Michael Maire"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "arxiv_comment": "SIGCOMM'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.07240v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07240v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14346v1",
                "updated": "2024-07-19T14:28:53Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "published": "2024-07-19T14:28:53Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "title": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals"
                },
                "summary": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue."
                },
                "authors": [
                    {
                        "name": "Akash Kumar Mohankumar"
                    },
                    {
                        "name": "Gururaj K"
                    },
                    {
                        "name": "Gagan Madan"
                    },
                    {
                        "name": "Amit Singh"
                    }
                ],
                "author_detail": {
                    "name": "Amit Singh"
                },
                "author": "Amit Singh",
                "arxiv_comment": "8 pages, 8 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04985v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04985v5",
                "updated": "2024-07-19T09:37:19Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    9,
                    37,
                    19,
                    4,
                    201,
                    0
                ],
                "published": "2023-12-08T11:47:35Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    11,
                    47,
                    35,
                    4,
                    342,
                    0
                ],
                "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQ Attention: Bandwidth-Efficient LLM Inference"
                },
                "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Luka Ribar"
                    },
                    {
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "name": "Luke Hudlass-Galley"
                    },
                    {
                        "name": "Charlie Blake"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Douglas Orr"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Orr"
                },
                "author": "Douglas Orr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04985v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04985v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14057v1",
                "updated": "2024-07-19T06:34:45Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    6,
                    34,
                    45,
                    4,
                    201,
                    0
                ],
                "published": "2024-07-19T06:34:45Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    6,
                    34,
                    45,
                    4,
                    201,
                    0
                ],
                "title": "LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference"
                },
                "summary": "The inference of transformer-based large language models consists of two\nsequential stages: 1) a prefilling stage to compute the KV cache of prompts and\ngenerate the first token, and 2) a decoding stage to generate subsequent\ntokens. For long prompts, the KV cache must be computed for all tokens during\nthe prefilling stage, which can significantly increase the time needed to\ngenerate the first token. Consequently, the prefilling stage may become a\nbottleneck in the generation process. An open question remains whether all\nprompt tokens are essential for generating the first token. To answer this, we\nintroduce a novel method, LazyLLM, that selectively computes the KV for tokens\nimportant for the next token prediction in both the prefilling and decoding\nstages. Contrary to static pruning approaches that prune the prompt at once,\nLazyLLM allows language models to dynamically select different subsets of\ntokens from the context in different generation steps, even though they might\nbe pruned in previous steps. Extensive experiments on standard datasets across\nvarious tasks demonstrate that LazyLLM is a generic method that can be\nseamlessly integrated with existing language models to significantly accelerate\nthe generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34x while maintaining accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference of transformer-based large language models consists of two\nsequential stages: 1) a prefilling stage to compute the KV cache of prompts and\ngenerate the first token, and 2) a decoding stage to generate subsequent\ntokens. For long prompts, the KV cache must be computed for all tokens during\nthe prefilling stage, which can significantly increase the time needed to\ngenerate the first token. Consequently, the prefilling stage may become a\nbottleneck in the generation process. An open question remains whether all\nprompt tokens are essential for generating the first token. To answer this, we\nintroduce a novel method, LazyLLM, that selectively computes the KV for tokens\nimportant for the next token prediction in both the prefilling and decoding\nstages. Contrary to static pruning approaches that prune the prompt at once,\nLazyLLM allows language models to dynamically select different subsets of\ntokens from the context in different generation steps, even though they might\nbe pruned in previous steps. Extensive experiments on standard datasets across\nvarious tasks demonstrate that LazyLLM is a generic method that can be\nseamlessly integrated with existing language models to significantly accelerate\nthe generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34x while maintaining accuracy."
                },
                "authors": [
                    {
                        "name": "Qichen Fu"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "Thomas Merth"
                    },
                    {
                        "name": "Sachin Mehta"
                    },
                    {
                        "name": "Mohammad Rastegari"
                    },
                    {
                        "name": "Mahyar Najibi"
                    }
                ],
                "author_detail": {
                    "name": "Mahyar Najibi"
                },
                "author": "Mahyar Najibi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v1",
                "updated": "2024-07-18T18:47:52Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Data-driven Forecasting of Deep Learning Performance on GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven Forecasting of Deep Learning Performance on GPUs"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.11813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11813v1",
                "updated": "2024-08-21T17:58:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    58,
                    2,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T17:58:02Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    58,
                    2,
                    2,
                    234,
                    0
                ],
                "title": "SEA: Supervised Embedding Alignment for Token-Level Visual-Textual\n  Integration in MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEA: Supervised Embedding Alignment for Token-Level Visual-Textual\n  Integration in MLLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have recently demonstrated\nremarkable perceptual and reasoning abilities, typically comprising a Vision\nEncoder, an Adapter, and a Large Language Model (LLM). The adapter serves as\nthe critical bridge between the visual and language components. However,\ntraining adapters with image-level supervision often results in significant\nmisalignment, undermining the LLMs' capabilities and limiting the potential of\nMultimodal LLMs. To address this, we introduce Supervised Embedding Alignment\n(SEA), a token-level alignment method that leverages vision-language\npre-trained models, such as CLIP, to align visual tokens with the LLM's\nembedding space through contrastive learning. This approach ensures a more\ncoherent integration of visual and language representations, enhancing the\nperformance and interpretability of multimodal LLMs while preserving their\ninherent capabilities. Extensive experiments show that SEA effectively improves\nMLLMs, particularly for smaller models, without adding extra data or inference\ncomputation. SEA also lays the groundwork for developing more general and\nadaptable solutions to enhance multimodal systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have recently demonstrated\nremarkable perceptual and reasoning abilities, typically comprising a Vision\nEncoder, an Adapter, and a Large Language Model (LLM). The adapter serves as\nthe critical bridge between the visual and language components. However,\ntraining adapters with image-level supervision often results in significant\nmisalignment, undermining the LLMs' capabilities and limiting the potential of\nMultimodal LLMs. To address this, we introduce Supervised Embedding Alignment\n(SEA), a token-level alignment method that leverages vision-language\npre-trained models, such as CLIP, to align visual tokens with the LLM's\nembedding space through contrastive learning. This approach ensures a more\ncoherent integration of visual and language representations, enhancing the\nperformance and interpretability of multimodal LLMs while preserving their\ninherent capabilities. Extensive experiments show that SEA effectively improves\nMLLMs, particularly for smaller models, without adding extra data or inference\ncomputation. SEA also lays the groundwork for developing more general and\nadaptable solutions to enhance multimodal systems."
                },
                "authors": [
                    {
                        "name": "Yuanyang Yin"
                    },
                    {
                        "name": "Yaqi Zhao"
                    },
                    {
                        "name": "Yajie Zhang"
                    },
                    {
                        "name": "Ke Lin"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Xin Tao"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Baoqun Yin"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11811v1",
                "updated": "2024-08-21T17:57:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    57,
                    6,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T17:57:06Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    57,
                    6,
                    2,
                    234,
                    0
                ],
                "title": "EmbodiedSAM: Online Segment Any 3D Thing in Real Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmbodiedSAM: Online Segment Any 3D Thing in Real Time"
                },
                "summary": "Embodied tasks require the agent to fully understand 3D scenes simultaneously\nwith its exploration, so an online, real-time, fine-grained and\nhighly-generalized 3D perception model is desperately needed. Since\nhigh-quality 3D data is limited, directly training such a model in 3D is almost\ninfeasible. Meanwhile, vision foundation models (VFM) has revolutionized the\nfield of 2D computer vision with superior performance, which makes the use of\nVFM to assist embodied 3D perception a promising direction. However, most\nexisting VFM-assisted 3D perception methods are either offline or too slow that\ncannot be applied in practical embodied tasks. In this paper, we aim to\nleverage Segment Anything Model (SAM) for real-time 3D instance segmentation in\nan online setting. This is a challenging problem since future frames are not\navailable in the input streaming RGB-D video, and an instance may be observed\nin several frames so object matching between frames is required. To address\nthese challenges, we first propose a geometric-aware query lifting module to\nrepresent the 2D masks generated by SAM by 3D-aware queries, which is then\niteratively refined by a dual-level query decoder. In this way, the 2D masks\nare transferred to fine-grained shapes on 3D point clouds. Benefit from the\nquery representation for 3D masks, we can compute the similarity matrix between\nthe 3D masks from different views by efficient matrix operation, which enables\nreal-time inference. Experiments on ScanNet, ScanNet200, SceneNN and 3RScan\nshow our method achieves leading performance even compared with offline\nmethods. Our method also demonstrates great generalization ability in several\nzero-shot dataset transferring experiments and show great potential in\nopen-vocabulary and data-efficient setting. Code and demo are available at\nhttps://xuxw98.github.io/ESAM/, with only one RTX 3090 GPU required for\ntraining and evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied tasks require the agent to fully understand 3D scenes simultaneously\nwith its exploration, so an online, real-time, fine-grained and\nhighly-generalized 3D perception model is desperately needed. Since\nhigh-quality 3D data is limited, directly training such a model in 3D is almost\ninfeasible. Meanwhile, vision foundation models (VFM) has revolutionized the\nfield of 2D computer vision with superior performance, which makes the use of\nVFM to assist embodied 3D perception a promising direction. However, most\nexisting VFM-assisted 3D perception methods are either offline or too slow that\ncannot be applied in practical embodied tasks. In this paper, we aim to\nleverage Segment Anything Model (SAM) for real-time 3D instance segmentation in\nan online setting. This is a challenging problem since future frames are not\navailable in the input streaming RGB-D video, and an instance may be observed\nin several frames so object matching between frames is required. To address\nthese challenges, we first propose a geometric-aware query lifting module to\nrepresent the 2D masks generated by SAM by 3D-aware queries, which is then\niteratively refined by a dual-level query decoder. In this way, the 2D masks\nare transferred to fine-grained shapes on 3D point clouds. Benefit from the\nquery representation for 3D masks, we can compute the similarity matrix between\nthe 3D masks from different views by efficient matrix operation, which enables\nreal-time inference. Experiments on ScanNet, ScanNet200, SceneNN and 3RScan\nshow our method achieves leading performance even compared with offline\nmethods. Our method also demonstrates great generalization ability in several\nzero-shot dataset transferring experiments and show great potential in\nopen-vocabulary and data-efficient setting. Code and demo are available at\nhttps://xuxw98.github.io/ESAM/, with only one RTX 3090 GPU required for\ntraining and evaluation."
                },
                "authors": [
                    {
                        "name": "Xiuwei Xu"
                    },
                    {
                        "name": "Huangxing Chen"
                    },
                    {
                        "name": "Linqing Zhao"
                    },
                    {
                        "name": "Ziwei Wang"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Project page: https://xuxw98.github.io/ESAM/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v2",
                "updated": "2024-08-21T17:55:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    55,
                    29,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/."
                },
                "authors": [
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.00631v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.00631v2",
                "updated": "2024-08-21T17:47:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    47,
                    53,
                    2,
                    234,
                    0
                ],
                "published": "2024-01-01T01:54:53Z",
                "published_parsed": [
                    2024,
                    1,
                    1,
                    1,
                    54,
                    53,
                    0,
                    1,
                    0
                ],
                "title": "Edge AI as a Service with Coordinated Deep Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge AI as a Service with Coordinated Deep Neural Networks"
                },
                "summary": "As artificial intelligence (AI) applications continue to expand in\nnext-generation networks, there is a growing need for deep neural network (DNN)\nmodels. Although DNN models deployed at the edge are promising for providing AI\nas a service with low latency, their cooperation is yet to be explored. In this\npaper, we consider that DNN service providers share their computing resources\nas well as their models' parameters and allow other DNNs to offload their\ncomputations without mirroring. We propose a novel algorithm called coordinated\nDNNs on edge (\\textbf{CoDE}) that facilitates coordination among DNN services\nby establishing new inference paths. CoDE aims to find the optimal path, which\nis the path with the highest possible reward, by creating multi-task DNNs from\nindividual models. The reward reflects the inference throughput and model\naccuracy. With CoDE, DNN models can make new paths for inference by using their\nown or other models' parameters. We then evaluate the performance of CoDE\nthrough numerical experiments. The results demonstrate a $40\\%$ increase in the\ninference throughput while degrading the average accuracy by only $2.3\\%$.\nExperiments show that CoDE enhances the inference throughput and, achieves\nhigher precision compared to a state-of-the-art existing method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As artificial intelligence (AI) applications continue to expand in\nnext-generation networks, there is a growing need for deep neural network (DNN)\nmodels. Although DNN models deployed at the edge are promising for providing AI\nas a service with low latency, their cooperation is yet to be explored. In this\npaper, we consider that DNN service providers share their computing resources\nas well as their models' parameters and allow other DNNs to offload their\ncomputations without mirroring. We propose a novel algorithm called coordinated\nDNNs on edge (\\textbf{CoDE}) that facilitates coordination among DNN services\nby establishing new inference paths. CoDE aims to find the optimal path, which\nis the path with the highest possible reward, by creating multi-task DNNs from\nindividual models. The reward reflects the inference throughput and model\naccuracy. With CoDE, DNN models can make new paths for inference by using their\nown or other models' parameters. We then evaluate the performance of CoDE\nthrough numerical experiments. The results demonstrate a $40\\%$ increase in the\ninference throughput while degrading the average accuracy by only $2.3\\%$.\nExperiments show that CoDE enhances the inference throughput and, achieves\nhigher precision compared to a state-of-the-art existing method."
                },
                "authors": [
                    {
                        "name": "Alireza Maleki"
                    },
                    {
                        "name": "Hamed Shah-Mansouri"
                    },
                    {
                        "name": "Babak H. Khalaj"
                    }
                ],
                "author_detail": {
                    "name": "Babak H. Khalaj"
                },
                "author": "Babak H. Khalaj",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.00631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.00631v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10188v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10188v3",
                "updated": "2024-08-21T17:47:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    47,
                    33,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-19T17:48:08Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    17,
                    48,
                    8,
                    0,
                    232,
                    0
                ],
                "title": "LongVILA: Scaling Long-Context Visual Language Models for Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongVILA: Scaling Long-Context Visual Language Models for Long Videos"
                },
                "summary": "Long-context capability is critical for multi-modal foundation models,\nespecially for long video understanding. We introduce LongVILA, a full-stack\nsolution for long-context visual-language models by co-designing the algorithm\nand system. For model training, we upgrade existing VLMs to support long video\nunderstanding by incorporating two additional stages, i.e., long context\nextension and long supervised fine-tuning. However, training on long video is\ncomputationally and memory intensive. We introduce the long-context Multi-Modal\nSequence Parallelism (MM-SP) system that efficiently parallelizes long video\ntraining and inference, enabling 2M context length training on 256 GPUs without\nany gradient checkpointing. LongVILA efficiently extends the number of video\nframes of VILA from 8 to 1024, improving the long video captioning score from\n2.00 to 3.26 (out of 5), achieving 99.5% accuracy in 1400-frame (274k context\nlength) video needle-in-a-haystack. LongVILA-8B demonstrates consistent\naccuracy improvements on long videos in the VideoMME benchmark as the number of\nframes increases. Besides, MM-SP is 2.1x - 5.7x faster than ring sequence\nparallelism and 1.1x - 1.4x faster than Megatron with context parallelism +\ntensor parallelism. Moreover, it seamlessly integrates with Hugging Face\nTransformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context capability is critical for multi-modal foundation models,\nespecially for long video understanding. We introduce LongVILA, a full-stack\nsolution for long-context visual-language models by co-designing the algorithm\nand system. For model training, we upgrade existing VLMs to support long video\nunderstanding by incorporating two additional stages, i.e., long context\nextension and long supervised fine-tuning. However, training on long video is\ncomputationally and memory intensive. We introduce the long-context Multi-Modal\nSequence Parallelism (MM-SP) system that efficiently parallelizes long video\ntraining and inference, enabling 2M context length training on 256 GPUs without\nany gradient checkpointing. LongVILA efficiently extends the number of video\nframes of VILA from 8 to 1024, improving the long video captioning score from\n2.00 to 3.26 (out of 5), achieving 99.5% accuracy in 1400-frame (274k context\nlength) video needle-in-a-haystack. LongVILA-8B demonstrates consistent\naccuracy improvements on long videos in the VideoMME benchmark as the number of\nframes increases. Besides, MM-SP is 2.1x - 5.7x faster than ring sequence\nparallelism and 1.1x - 1.4x faster than Megatron with context parallelism +\ntensor parallelism. Moreover, it seamlessly integrates with Hugging Face\nTransformers."
                },
                "authors": [
                    {
                        "name": "Fuzhao Xue"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Dacheng Li"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Yunhao Fang"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Ethan He"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Linxi Fan"
                    },
                    {
                        "name": "Yuke Zhu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Code and models are available at\n  https://github.com/NVlabs/VILA/blob/main/LongVILA.md",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10188v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10188v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11801v1",
                "updated": "2024-08-21T17:43:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    43,
                    15,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T17:43:15Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    43,
                    15,
                    2,
                    234,
                    0
                ],
                "title": "Story3D-Agent: Exploring 3D Storytelling Visualization with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Story3D-Agent: Exploring 3D Storytelling Visualization with Large\n  Language Models"
                },
                "summary": "Traditional visual storytelling is complex, requiring specialized knowledge\nand substantial resources, yet often constrained by human creativity and\ncreation precision. While Large Language Models (LLMs) enhance visual\nstorytelling, current approaches often limit themselves to 2D visuals or\noversimplify stories through motion synthesis and behavioral simulation,\nfailing to create comprehensive, multi-dimensional narratives. To this end, we\npresent Story3D-Agent, a pioneering approach that leverages the capabilities of\nLLMs to transform provided narratives into 3D-rendered visualizations. By\nintegrating procedural modeling, our approach enables precise control over\nmulti-character actions and motions, as well as diverse decorative elements,\nensuring the long-range and dynamic 3D representation. Furthermore, our method\nsupports narrative extension through logical reasoning, ensuring that generated\ncontent remains consistent with existing conditions. We have thoroughly\nevaluated our Story3D-Agent to validate its effectiveness, offering a basic\nframework to advance 3D story representation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional visual storytelling is complex, requiring specialized knowledge\nand substantial resources, yet often constrained by human creativity and\ncreation precision. While Large Language Models (LLMs) enhance visual\nstorytelling, current approaches often limit themselves to 2D visuals or\noversimplify stories through motion synthesis and behavioral simulation,\nfailing to create comprehensive, multi-dimensional narratives. To this end, we\npresent Story3D-Agent, a pioneering approach that leverages the capabilities of\nLLMs to transform provided narratives into 3D-rendered visualizations. By\nintegrating procedural modeling, our approach enables precise control over\nmulti-character actions and motions, as well as diverse decorative elements,\nensuring the long-range and dynamic 3D representation. Furthermore, our method\nsupports narrative extension through logical reasoning, ensuring that generated\ncontent remains consistent with existing conditions. We have thoroughly\nevaluated our Story3D-Agent to validate its effectiveness, offering a basic\nframework to advance 3D story representation."
                },
                "authors": [
                    {
                        "name": "Yuzhou Huang"
                    },
                    {
                        "name": "Yiran Qin"
                    },
                    {
                        "name": "Shunlin Lu"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Ying Shan"
                    },
                    {
                        "name": "Ruimao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruimao Zhang"
                },
                "author": "Ruimao Zhang",
                "arxiv_comment": "Project page: https://yuzhou914.github.io/Story3D-Agent/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11800v1",
                "updated": "2024-08-21T17:43:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    43,
                    11,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T17:43:11Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    43,
                    11,
                    2,
                    234,
                    0
                ],
                "title": "PermitQA: A Benchmark for Retrieval Augmented Generation in Wind Siting\n  and Permitting domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PermitQA: A Benchmark for Retrieval Augmented Generation in Wind Siting\n  and Permitting domain"
                },
                "summary": "In the rapidly evolving landscape of Natural Language Processing (NLP) and\ntext generation, the emergence of Retrieval Augmented Generation (RAG) presents\na promising avenue for improving the quality and reliability of generated text\nby leveraging information retrieved from user specified database. Benchmarking\nis essential to evaluate and compare the performance of the different RAG\nconfigurations in terms of retriever and generator, providing insights into\ntheir effectiveness, scalability, and suitability for the specific domain and\napplications. In this paper, we present a comprehensive framework to generate a\ndomain relevant RAG benchmark. Our framework is based on automatic\nquestion-answer generation with Human (domain experts)-AI Large Language Model\n(LLM) teaming. As a case study, we demonstrate the framework by introducing\nPermitQA, a first-of-its-kind benchmark on the wind siting and permitting\ndomain which comprises of multiple scientific documents/reports related to\nenvironmental impact of wind energy projects. Our framework systematically\nevaluates RAG performance using diverse metrics and multiple question types\nwith varying complexity level. We also demonstrate the performance of different\nmodels on our benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving landscape of Natural Language Processing (NLP) and\ntext generation, the emergence of Retrieval Augmented Generation (RAG) presents\na promising avenue for improving the quality and reliability of generated text\nby leveraging information retrieved from user specified database. Benchmarking\nis essential to evaluate and compare the performance of the different RAG\nconfigurations in terms of retriever and generator, providing insights into\ntheir effectiveness, scalability, and suitability for the specific domain and\napplications. In this paper, we present a comprehensive framework to generate a\ndomain relevant RAG benchmark. Our framework is based on automatic\nquestion-answer generation with Human (domain experts)-AI Large Language Model\n(LLM) teaming. As a case study, we demonstrate the framework by introducing\nPermitQA, a first-of-its-kind benchmark on the wind siting and permitting\ndomain which comprises of multiple scientific documents/reports related to\nenvironmental impact of wind energy projects. Our framework systematically\nevaluates RAG performance using diverse metrics and multiple question types\nwith varying complexity level. We also demonstrate the performance of different\nmodels on our benchmark."
                },
                "authors": [
                    {
                        "name": "Rounak Meyur"
                    },
                    {
                        "name": "Hung Phan"
                    },
                    {
                        "name": "Sridevi Wagle"
                    },
                    {
                        "name": "Jan Strube"
                    },
                    {
                        "name": "Mahantesh Halappanavar"
                    },
                    {
                        "name": "Sameera Horawalavithana"
                    },
                    {
                        "name": "Anurag Acharya"
                    },
                    {
                        "name": "Sai Munikoti"
                    }
                ],
                "author_detail": {
                    "name": "Sai Munikoti"
                },
                "author": "Sai Munikoti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11799v1",
                "updated": "2024-08-21T17:42:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    42,
                    17,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T17:42:17Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    42,
                    17,
                    2,
                    234,
                    0
                ],
                "title": "Practical token pruning for foundation models in few-shot conversational\n  virtual assistant systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical token pruning for foundation models in few-shot conversational\n  virtual assistant systems"
                },
                "summary": "In an enterprise Virtual Assistant (VA) system, intent classification is the\ncrucial component that determines how a user input is handled based on what the\nuser wants. The VA system is expected to be a cost-efficient SaaS service with\nlow training and inference time while achieving high accuracy even with a small\nnumber of training samples. We pretrain a transformer-based sentence embedding\nmodel with a contrastive learning objective and leverage the embedding of the\nmodel as features when training intent classification models. Our approach\nachieves the state-of-the-art results for few-shot scenarios and performs\nbetter than other commercial solutions on popular intent classification\nbenchmarks. However, generating features via a transformer-based model\nincreases the inference time, especially for longer user inputs, due to the\nquadratic runtime of the transformer's attention mechanism. On top of model\ndistillation, we introduce a practical multi-task adaptation approach that\nconfigures dynamic token pruning without the need for task-specific training\nfor intent classification. We demonstrate that this approach improves the\ninference speed of popular sentence transformer models without affecting model\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an enterprise Virtual Assistant (VA) system, intent classification is the\ncrucial component that determines how a user input is handled based on what the\nuser wants. The VA system is expected to be a cost-efficient SaaS service with\nlow training and inference time while achieving high accuracy even with a small\nnumber of training samples. We pretrain a transformer-based sentence embedding\nmodel with a contrastive learning objective and leverage the embedding of the\nmodel as features when training intent classification models. Our approach\nachieves the state-of-the-art results for few-shot scenarios and performs\nbetter than other commercial solutions on popular intent classification\nbenchmarks. However, generating features via a transformer-based model\nincreases the inference time, especially for longer user inputs, due to the\nquadratic runtime of the transformer's attention mechanism. On top of model\ndistillation, we introduce a practical multi-task adaptation approach that\nconfigures dynamic token pruning without the need for task-specific training\nfor intent classification. We demonstrate that this approach improves the\ninference speed of popular sentence transformer models without affecting model\nperformance."
                },
                "authors": [
                    {
                        "name": "Haode Qi"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Jian Ni"
                    },
                    {
                        "name": "Pratyush Singh"
                    },
                    {
                        "name": "Reza Fazeli"
                    },
                    {
                        "name": "Gengyu Wang"
                    },
                    {
                        "name": "Zhongzheng Shu"
                    },
                    {
                        "name": "Eric Wayne"
                    },
                    {
                        "name": "Juergen Bross"
                    }
                ],
                "author_detail": {
                    "name": "Juergen Bross"
                },
                "author": "Juergen Bross",
                "arxiv_comment": "6 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11796v1",
                "updated": "2024-08-21T17:38:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    38,
                    48,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T17:38:48Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    38,
                    48,
                    2,
                    234,
                    0
                ],
                "title": "LLM Pruning and Distillation in Practice: The Minitron Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Pruning and Distillation in Practice: The Minitron Approach"
                },
                "summary": "We present a comprehensive report on compressing the Llama 3.1 8B and Mistral\nNeMo 12B models to 4B and 8B parameters, respectively, using pruning and\ndistillation. We explore two distinct pruning strategies: (1) depth pruning and\n(2) joint hidden/attention/MLP (width) pruning, and evaluate the results on\ncommon benchmarks from the LM Evaluation Harness. The models are then aligned\nwith NeMo Aligner and tested in instruct-tuned versions. This approach produces\na compelling 4B model from Llama 3.1 8B and a state-of-the-art\nMistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo\n12B. We found that with no access to the original data, it is beneficial to\nslightly fine-tune teacher models on the distillation dataset. We open-source\nour base model weights on Hugging Face with a permissive license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive report on compressing the Llama 3.1 8B and Mistral\nNeMo 12B models to 4B and 8B parameters, respectively, using pruning and\ndistillation. We explore two distinct pruning strategies: (1) depth pruning and\n(2) joint hidden/attention/MLP (width) pruning, and evaluate the results on\ncommon benchmarks from the LM Evaluation Harness. The models are then aligned\nwith NeMo Aligner and tested in instruct-tuned versions. This approach produces\na compelling 4B model from Llama 3.1 8B and a state-of-the-art\nMistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo\n12B. We found that with no access to the original data, it is beneficial to\nslightly fine-tune teacher models on the distillation dataset. We open-source\nour base model weights on Hugging Face with a permissive license."
                },
                "authors": [
                    {
                        "name": "Sharath Turuvekere Sreenivas"
                    },
                    {
                        "name": "Saurav Muralidharan"
                    },
                    {
                        "name": "Raviraj Joshi"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "Mostofa Patwary"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11795v1",
                "updated": "2024-08-21T17:36:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    36,
                    37,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T17:36:37Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    36,
                    37,
                    2,
                    234,
                    0
                ],
                "title": "EE-MLLM: A Data-Efficient and Compute-Efficient Multimodal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EE-MLLM: A Data-Efficient and Compute-Efficient Multimodal Large\n  Language Model"
                },
                "summary": "In the realm of multimodal research, numerous studies leverage substantial\nimage-text pairs to conduct modal alignment learning, transforming Large\nLanguage Models (LLMs) into Multimodal LLMs and excelling in a variety of\nvisual-language tasks. The prevailing methodologies primarily fall into two\ncategories: self-attention-based and cross-attention-based methods. While\nself-attention-based methods offer superior data efficiency due to their simple\nMLP architecture, they often suffer from lower computational efficiency due to\nconcatenating visual and textual tokens as input for LLM. Conversely,\ncross-attention-based methods, although less data-efficient due to additional\nlearnable parameters, exhibit higher computational efficiency by avoiding long\nsequence input for LLM. To address these trade-offs, we introduce the\nData-Efficient and Compute-Efficient Multimodal Large Language Model (EE-MLLM).\nWithout introducing additional modules or learnable parameters, EE-MLLM\nachieves both data and compute efficiency. Specifically, we modify the original\nself-attention mechanism in MLLM to a composite attention mechanism. This\nmechanism has two key characteristics: 1) Eliminating the computational\noverhead of self-attention within visual tokens to achieve compute efficiency,\nand 2) Reusing the weights on each layer of LLM to facilitate effective\nmodality alignment between vision and language for data efficiency.\nExperimental results demonstrate the effectiveness of EE-MLLM across a range of\nbenchmarks, including general-purpose datasets like MMBench and SeedBench, as\nwell as fine-grained tasks such as TextVQA and DocVQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of multimodal research, numerous studies leverage substantial\nimage-text pairs to conduct modal alignment learning, transforming Large\nLanguage Models (LLMs) into Multimodal LLMs and excelling in a variety of\nvisual-language tasks. The prevailing methodologies primarily fall into two\ncategories: self-attention-based and cross-attention-based methods. While\nself-attention-based methods offer superior data efficiency due to their simple\nMLP architecture, they often suffer from lower computational efficiency due to\nconcatenating visual and textual tokens as input for LLM. Conversely,\ncross-attention-based methods, although less data-efficient due to additional\nlearnable parameters, exhibit higher computational efficiency by avoiding long\nsequence input for LLM. To address these trade-offs, we introduce the\nData-Efficient and Compute-Efficient Multimodal Large Language Model (EE-MLLM).\nWithout introducing additional modules or learnable parameters, EE-MLLM\nachieves both data and compute efficiency. Specifically, we modify the original\nself-attention mechanism in MLLM to a composite attention mechanism. This\nmechanism has two key characteristics: 1) Eliminating the computational\noverhead of self-attention within visual tokens to achieve compute efficiency,\nand 2) Reusing the weights on each layer of LLM to facilitate effective\nmodality alignment between vision and language for data efficiency.\nExperimental results demonstrate the effectiveness of EE-MLLM across a range of\nbenchmarks, including general-purpose datasets like MMBench and SeedBench, as\nwell as fine-grained tasks such as TextVQA and DocVQA."
                },
                "authors": [
                    {
                        "name": "Feipeng Ma"
                    },
                    {
                        "name": "Yizhou Zhou"
                    },
                    {
                        "name": "Hebei Li"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Siying Wu"
                    },
                    {
                        "name": "Fengyun Rao"
                    },
                    {
                        "name": "Yueyi Zhang"
                    },
                    {
                        "name": "Xiaoyan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyan Sun"
                },
                "author": "Xiaoyan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05590v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05590v2",
                "updated": "2024-08-21T17:34:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    34,
                    7,
                    2,
                    234,
                    0
                ],
                "published": "2024-06-08T22:21:42Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    22,
                    21,
                    42,
                    5,
                    160,
                    0
                ],
                "title": "NYU CTF Dataset: A Scalable Open-Source Benchmark Dataset for Evaluating\n  LLMs in Offensive Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NYU CTF Dataset: A Scalable Open-Source Benchmark Dataset for Evaluating\n  LLMs in Offensive Security"
                },
                "summary": "Large Language Models (LLMs) are being deployed across various domains today.\nHowever, their capacity to solve Capture the Flag (CTF) challenges in\ncybersecurity has not been thoroughly evaluated. To address this, we develop a\nnovel method to assess LLMs in solving CTF challenges by creating a scalable,\nopen-source benchmark database specifically designed for these applications.\nThis database includes metadata for LLM testing and adaptive learning,\ncompiling a diverse range of CTF challenges from popular competitions.\nUtilizing the advanced function calling capabilities of LLMs, we build a fully\nautomated system with an enhanced workflow and support for external tool calls.\nOur benchmark dataset and automated framework allow us to evaluate the\nperformance of five LLMs, encompassing both black-box and open-source models.\nThis work lays the foundation for future research into improving the efficiency\nof LLMs in interactive cybersecurity tasks and automated task planning. By\nproviding a specialized dataset, our project offers an ideal platform for\ndeveloping, testing, and refining LLM-based approaches to vulnerability\ndetection and resolution. Evaluating LLMs on these challenges and comparing\nwith human performance yields insights into their potential for AI-driven\ncybersecurity solutions to perform real-world threat management. We make our\ndataset open source to public https://github.com/NYU-LLM-CTF/LLM_CTF_Database\nalong with our playground automated framework\nhttps://github.com/NYU-LLM-CTF/llm_ctf_automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are being deployed across various domains today.\nHowever, their capacity to solve Capture the Flag (CTF) challenges in\ncybersecurity has not been thoroughly evaluated. To address this, we develop a\nnovel method to assess LLMs in solving CTF challenges by creating a scalable,\nopen-source benchmark database specifically designed for these applications.\nThis database includes metadata for LLM testing and adaptive learning,\ncompiling a diverse range of CTF challenges from popular competitions.\nUtilizing the advanced function calling capabilities of LLMs, we build a fully\nautomated system with an enhanced workflow and support for external tool calls.\nOur benchmark dataset and automated framework allow us to evaluate the\nperformance of five LLMs, encompassing both black-box and open-source models.\nThis work lays the foundation for future research into improving the efficiency\nof LLMs in interactive cybersecurity tasks and automated task planning. By\nproviding a specialized dataset, our project offers an ideal platform for\ndeveloping, testing, and refining LLM-based approaches to vulnerability\ndetection and resolution. Evaluating LLMs on these challenges and comparing\nwith human performance yields insights into their potential for AI-driven\ncybersecurity solutions to perform real-world threat management. We make our\ndataset open source to public https://github.com/NYU-LLM-CTF/LLM_CTF_Database\nalong with our playground automated framework\nhttps://github.com/NYU-LLM-CTF/llm_ctf_automation."
                },
                "authors": [
                    {
                        "name": "Minghao Shao"
                    },
                    {
                        "name": "Sofija Jancheska"
                    },
                    {
                        "name": "Meet Udeshi"
                    },
                    {
                        "name": "Brendan Dolan-Gavitt"
                    },
                    {
                        "name": "Haoran Xi"
                    },
                    {
                        "name": "Kimberly Milner"
                    },
                    {
                        "name": "Boyuan Chen"
                    },
                    {
                        "name": "Max Yin"
                    },
                    {
                        "name": "Siddharth Garg"
                    },
                    {
                        "name": "Prashanth Krishnamurthy"
                    },
                    {
                        "name": "Farshad Khorrami"
                    },
                    {
                        "name": "Ramesh Karri"
                    },
                    {
                        "name": "Muhammad Shafique"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Shafique"
                },
                "author": "Muhammad Shafique",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05590v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05590v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.00333v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.00333v4",
                "updated": "2024-08-21T17:27:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    27,
                    47,
                    2,
                    234,
                    0
                ],
                "published": "2023-03-01T08:53:36Z",
                "published_parsed": [
                    2023,
                    3,
                    1,
                    8,
                    53,
                    36,
                    2,
                    60,
                    0
                ],
                "title": "Competence-Based Analysis of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Competence-Based Analysis of Language Models"
                },
                "summary": "Despite the recent successes of large, pretrained neural language models\n(LLMs), comparatively little is known about the representations of linguistic\nstructure they learn during pretraining, which can lead to unexpected behaviors\nin response to prompt variation or distribution shift. To better understand\nthese models and behaviors, we introduce a general model analysis framework to\nstudy LLMs with respect to their representation and use of human-interpretable\nlinguistic properties. Our framework, CALM (Competence-based Analysis of\nLanguage Models), is designed to investigate LLM competence in the context of\nspecific tasks by intervening on models' internal representations of different\nlinguistic properties using causal probing, and measuring models' alignment\nunder these interventions with a given ground-truth causal model of the task.\nWe also develop a new approach for performing causal probing interventions\nusing gradient-based adversarial attacks, which can target a broader range of\nproperties and representations than prior techniques. Finally, we carry out a\ncase study of CALM using these interventions to analyze and compare LLM\ncompetence across a variety of lexical inference tasks, showing that CALM can\nbe used to explain and predict behaviors across these tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent successes of large, pretrained neural language models\n(LLMs), comparatively little is known about the representations of linguistic\nstructure they learn during pretraining, which can lead to unexpected behaviors\nin response to prompt variation or distribution shift. To better understand\nthese models and behaviors, we introduce a general model analysis framework to\nstudy LLMs with respect to their representation and use of human-interpretable\nlinguistic properties. Our framework, CALM (Competence-based Analysis of\nLanguage Models), is designed to investigate LLM competence in the context of\nspecific tasks by intervening on models' internal representations of different\nlinguistic properties using causal probing, and measuring models' alignment\nunder these interventions with a given ground-truth causal model of the task.\nWe also develop a new approach for performing causal probing interventions\nusing gradient-based adversarial attacks, which can target a broader range of\nproperties and representations than prior techniques. Finally, we carry out a\ncase study of CALM using these interventions to analyze and compare LLM\ncompetence across a variety of lexical inference tasks, showing that CALM can\nbe used to explain and predict behaviors across these tasks."
                },
                "authors": [
                    {
                        "name": "Adam Davies"
                    },
                    {
                        "name": "Jize Jiang"
                    },
                    {
                        "name": "ChengXiang Zhai"
                    }
                ],
                "author_detail": {
                    "name": "ChengXiang Zhai"
                },
                "author": "ChengXiang Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.00333v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.00333v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11793v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11793v1",
                "updated": "2024-08-21T17:25:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    25,
                    45,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T17:25:45Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    25,
                    45,
                    2,
                    234,
                    0
                ],
                "title": "Leveraging Chemistry Foundation Models to Facilitate Structure Focused\n  Retrieval Augmented Generation in Multi-Agent Workflows for Catalyst and\n  Materials Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Chemistry Foundation Models to Facilitate Structure Focused\n  Retrieval Augmented Generation in Multi-Agent Workflows for Catalyst and\n  Materials Design"
                },
                "summary": "Molecular property prediction and generative design via deep learning models\nhas been the subject of intense research given its potential to accelerate\ndevelopment of new, high-performance materials. More recently, these workflows\nhave been significantly augmented with the advent of large language models\n(LLMs) and systems of LLM-driven agents capable of utilizing pre-trained models\nto make predictions in the context of more complex research tasks. While\neffective, there is still room for substantial improvement within the agentic\nsystems on the retrieval of salient information for material design tasks.\nMoreover, alternative uses of predictive deep learning models, such as\nleveraging their latent representations to facilitate cross-modal retrieval\naugmented generation within agentic systems to enable task-specific materials\ndesign, has remained unexplored. Herein, we demonstrate that large, pre-trained\nchemistry foundation models can serve as a basis for enabling semantic\nchemistry information retrieval for both small-molecules, complex polymeric\nmaterials, and reactions. Additionally, we show the use of chemistry foundation\nmodels in conjunction with image models such as OpenCLIP facilitate\nunprecedented queries and information retrieval across multiple\ncharacterization data domains. Finally, we demonstrate the integration of these\nsystems within multi-agent systems to facilitate structure and\ntopological-based natural language queries and information retrieval for\ncomplex research tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular property prediction and generative design via deep learning models\nhas been the subject of intense research given its potential to accelerate\ndevelopment of new, high-performance materials. More recently, these workflows\nhave been significantly augmented with the advent of large language models\n(LLMs) and systems of LLM-driven agents capable of utilizing pre-trained models\nto make predictions in the context of more complex research tasks. While\neffective, there is still room for substantial improvement within the agentic\nsystems on the retrieval of salient information for material design tasks.\nMoreover, alternative uses of predictive deep learning models, such as\nleveraging their latent representations to facilitate cross-modal retrieval\naugmented generation within agentic systems to enable task-specific materials\ndesign, has remained unexplored. Herein, we demonstrate that large, pre-trained\nchemistry foundation models can serve as a basis for enabling semantic\nchemistry information retrieval for both small-molecules, complex polymeric\nmaterials, and reactions. Additionally, we show the use of chemistry foundation\nmodels in conjunction with image models such as OpenCLIP facilitate\nunprecedented queries and information retrieval across multiple\ncharacterization data domains. Finally, we demonstrate the integration of these\nsystems within multi-agent systems to facilitate structure and\ntopological-based natural language queries and information retrieval for\ncomplex research tasks."
                },
                "authors": [
                    {
                        "name": "Nathaniel H. Park"
                    },
                    {
                        "name": "Tiffany J. Callahan"
                    },
                    {
                        "name": "James L. Hedrick"
                    },
                    {
                        "name": "Tim Erdmann"
                    },
                    {
                        "name": "Sara Capponi"
                    }
                ],
                "author_detail": {
                    "name": "Sara Capponi"
                },
                "author": "Sara Capponi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11793v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11793v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11791v1",
                "updated": "2024-08-21T17:24:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    24,
                    15,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T17:24:15Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    24,
                    15,
                    2,
                    234,
                    0
                ],
                "title": "Critique-out-Loud Reward Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critique-out-Loud Reward Models"
                },
                "summary": "Traditionally, reward models used for reinforcement learning from human\nfeedback (RLHF) are trained to directly predict preference scores without\nleveraging the generation capabilities of the underlying large language model\n(LLM). This limits the capabilities of reward models as they must reason\nimplicitly about the quality of a response, i.e., preference modeling must be\nperformed in a single forward pass through the model. To enable reward models\nto reason explicitly about the quality of a response, we introduce\nCritique-out-Loud (CLoud) reward models. CLoud reward models operate by first\ngenerating a natural language critique of the assistant's response that is then\nused to predict a scalar reward for the quality of the response. We demonstrate\nthe success of CLoud reward models for both Llama-3-8B and 70B base models:\ncompared to classic reward models CLoud reward models improve pairwise\npreference classification accuracy on RewardBench by 4.65 and 5.84 percentage\npoints for the 8B and 70B base models respectively. Furthermore, CLoud reward\nmodels lead to a Pareto improvement for win rate on ArenaHard when used as the\nscoring model for Best-of-N. Finally, we explore how to exploit the dynamic\ninference compute capabilities of CLoud reward models by performing\nself-consistency decoding for reward prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditionally, reward models used for reinforcement learning from human\nfeedback (RLHF) are trained to directly predict preference scores without\nleveraging the generation capabilities of the underlying large language model\n(LLM). This limits the capabilities of reward models as they must reason\nimplicitly about the quality of a response, i.e., preference modeling must be\nperformed in a single forward pass through the model. To enable reward models\nto reason explicitly about the quality of a response, we introduce\nCritique-out-Loud (CLoud) reward models. CLoud reward models operate by first\ngenerating a natural language critique of the assistant's response that is then\nused to predict a scalar reward for the quality of the response. We demonstrate\nthe success of CLoud reward models for both Llama-3-8B and 70B base models:\ncompared to classic reward models CLoud reward models improve pairwise\npreference classification accuracy on RewardBench by 4.65 and 5.84 percentage\npoints for the 8B and 70B base models respectively. Furthermore, CLoud reward\nmodels lead to a Pareto improvement for win rate on ArenaHard when used as the\nscoring model for Best-of-N. Finally, we explore how to exploit the dynamic\ninference compute capabilities of CLoud reward models by performing\nself-consistency decoding for reward prediction."
                },
                "authors": [
                    {
                        "name": "Zachary Ankner"
                    },
                    {
                        "name": "Mansheej Paul"
                    },
                    {
                        "name": "Brandon Cui"
                    },
                    {
                        "name": "Jonathan D. Chang"
                    },
                    {
                        "name": "Prithviraj Ammanabrolu"
                    }
                ],
                "author_detail": {
                    "name": "Prithviraj Ammanabrolu"
                },
                "author": "Prithviraj Ammanabrolu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11457v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11457v2",
                "updated": "2024-08-21T17:23:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    23,
                    3,
                    2,
                    234,
                    0
                ],
                "published": "2024-04-17T15:05:03Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    15,
                    5,
                    3,
                    2,
                    108,
                    0
                ],
                "title": "Bias and Unfairness in Information Retrieval Systems: New Challenges in\n  the LLM Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias and Unfairness in Information Retrieval Systems: New Challenges in\n  the LLM Era"
                },
                "summary": "With the rapid advancements of large language models (LLMs), information\nretrieval (IR) systems, such as search engines and recommender systems, have\nundergone a significant paradigm shift. This evolution, while heralding new\nopportunities, introduces emerging challenges, particularly in terms of biases\nand unfairness, which may threaten the information ecosystem. In this paper, we\npresent a comprehensive survey of existing works on emerging and pressing bias\nand unfairness issues in IR systems when the integration of LLMs. We first\nunify bias and unfairness issues as distribution mismatch problems, providing a\ngroundwork for categorizing various mitigation strategies through distribution\nalignment. Subsequently, we systematically delve into the specific bias and\nunfairness issues arising from three critical stages of LLMs integration into\nIR systems: data collection, model development, and result evaluation. In doing\nso, we meticulously review and analyze recent literature, focusing on the\ndefinitions, characteristics, and corresponding mitigation strategies\nassociated with these issues. Finally, we identify and highlight some open\nproblems and challenges for future work, aiming to inspire researchers and\nstakeholders in the IR field and beyond to better understand and mitigate bias\nand unfairness issues of IR in this LLM era. We also consistently maintain a\nGitHub repository for the relevant papers and resources in this rising\ndirection at https://github.com/KID-22/LLM-IR-Bias-Fairness-Survey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancements of large language models (LLMs), information\nretrieval (IR) systems, such as search engines and recommender systems, have\nundergone a significant paradigm shift. This evolution, while heralding new\nopportunities, introduces emerging challenges, particularly in terms of biases\nand unfairness, which may threaten the information ecosystem. In this paper, we\npresent a comprehensive survey of existing works on emerging and pressing bias\nand unfairness issues in IR systems when the integration of LLMs. We first\nunify bias and unfairness issues as distribution mismatch problems, providing a\ngroundwork for categorizing various mitigation strategies through distribution\nalignment. Subsequently, we systematically delve into the specific bias and\nunfairness issues arising from three critical stages of LLMs integration into\nIR systems: data collection, model development, and result evaluation. In doing\nso, we meticulously review and analyze recent literature, focusing on the\ndefinitions, characteristics, and corresponding mitigation strategies\nassociated with these issues. Finally, we identify and highlight some open\nproblems and challenges for future work, aiming to inspire researchers and\nstakeholders in the IR field and beyond to better understand and mitigate bias\nand unfairness issues of IR in this LLM era. We also consistently maintain a\nGitHub repository for the relevant papers and resources in this rising\ndirection at https://github.com/KID-22/LLM-IR-Bias-Fairness-Survey."
                },
                "authors": [
                    {
                        "name": "Sunhao Dai"
                    },
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Shicheng Xu"
                    },
                    {
                        "name": "Liang Pang"
                    },
                    {
                        "name": "Zhenhua Dong"
                    },
                    {
                        "name": "Jun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xu"
                },
                "author": "Jun Xu",
                "arxiv_doi": "10.1145/3637528.3671458",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3637528.3671458",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.11457v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11457v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "KDD 2024 Tutorial&Survey; Tutorial Website:\n  https://llm-ir-bias-fairness.github.io/",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11788v1",
                "updated": "2024-08-21T17:21:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    21,
                    13,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T17:21:13Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    21,
                    13,
                    2,
                    234,
                    0
                ],
                "title": "DreamFactory: Pioneering Multi-Scene Long Video Generation with a\n  Multi-Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DreamFactory: Pioneering Multi-Scene Long Video Generation with a\n  Multi-Agent Framework"
                },
                "summary": "Current video generation models excel at creating short, realistic clips, but\nstruggle with longer, multi-scene videos. We introduce \\texttt{DreamFactory},\nan LLM-based framework that tackles this challenge. \\texttt{DreamFactory}\nleverages multi-agent collaboration principles and a Key Frames Iteration\nDesign Method to ensure consistency and style across long videos. It utilizes\nChain of Thought (COT) to address uncertainties inherent in large language\nmodels. \\texttt{DreamFactory} generates long, stylistically coherent, and\ncomplex videos. Evaluating these long-form videos presents a challenge. We\npropose novel metrics such as Cross-Scene Face Distance Score and Cross-Scene\nStyle Consistency Score. To further research in this area, we contribute the\nMulti-Scene Videos Dataset containing over 150 human-rated videos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video generation models excel at creating short, realistic clips, but\nstruggle with longer, multi-scene videos. We introduce \\texttt{DreamFactory},\nan LLM-based framework that tackles this challenge. \\texttt{DreamFactory}\nleverages multi-agent collaboration principles and a Key Frames Iteration\nDesign Method to ensure consistency and style across long videos. It utilizes\nChain of Thought (COT) to address uncertainties inherent in large language\nmodels. \\texttt{DreamFactory} generates long, stylistically coherent, and\ncomplex videos. Evaluating these long-form videos presents a challenge. We\npropose novel metrics such as Cross-Scene Face Distance Score and Cross-Scene\nStyle Consistency Score. To further research in this area, we contribute the\nMulti-Scene Videos Dataset containing over 150 human-rated videos."
                },
                "authors": [
                    {
                        "name": "Zhifei Xie"
                    },
                    {
                        "name": "Daniel Tang"
                    },
                    {
                        "name": "Dingwei Tan"
                    },
                    {
                        "name": "Jacques Klein"
                    },
                    {
                        "name": "Tegawend F. Bissyand"
                    },
                    {
                        "name": "Saad Ezzini"
                    }
                ],
                "author_detail": {
                    "name": "Saad Ezzini"
                },
                "author": "Saad Ezzini",
                "arxiv_comment": "13 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "TsingHua University",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11779v1",
                "updated": "2024-08-21T17:09:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    9,
                    0,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T17:09:00Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    9,
                    0,
                    2,
                    234,
                    0
                ],
                "title": "Personality Alignment of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personality Alignment of Large Language Models"
                },
                "summary": "Current methods for aligning large language models (LLMs) typically aim to\nreflect general human values and behaviors, but they often fail to capture the\nunique characteristics and preferences of individual users. To address this\ngap, we introduce the concept of Personality Alignment. This approach tailors\nLLMs' responses and decisions to match the specific preferences of individual\nusers or closely related groups. Inspired by psychometrics, we created the\nPersonality Alignment with Personality Inventories (PAPI) dataset, which\nincludes data from 300,000 real subjects, each providing behavioral preferences\nbased on the Big Five Personality Factors. This dataset allows us to\nquantitatively evaluate the extent to which LLMs can align with each subject's\nbehavioral patterns. Recognizing the challenges of personality alignments: such\nas limited personal data, diverse preferences, and scalability requirements: we\ndeveloped an activation intervention optimization method. This method enhances\nLLMs' ability to efficiently align with individual behavioral preferences using\nminimal data and computational resources. Remarkably, our method, PAS, achieves\nsuperior performance while requiring only 1/5 of the optimization time compared\nto DPO, offering practical value for personality alignment. Our work paves the\nway for future AI systems to make decisions and reason in truly personality\nways, enhancing the relevance and meaning of AI interactions for each user and\nadvancing human-centered artificial intelligence.The code has released in\n\\url{https://github.com/zhu-minjun/PAlign}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current methods for aligning large language models (LLMs) typically aim to\nreflect general human values and behaviors, but they often fail to capture the\nunique characteristics and preferences of individual users. To address this\ngap, we introduce the concept of Personality Alignment. This approach tailors\nLLMs' responses and decisions to match the specific preferences of individual\nusers or closely related groups. Inspired by psychometrics, we created the\nPersonality Alignment with Personality Inventories (PAPI) dataset, which\nincludes data from 300,000 real subjects, each providing behavioral preferences\nbased on the Big Five Personality Factors. This dataset allows us to\nquantitatively evaluate the extent to which LLMs can align with each subject's\nbehavioral patterns. Recognizing the challenges of personality alignments: such\nas limited personal data, diverse preferences, and scalability requirements: we\ndeveloped an activation intervention optimization method. This method enhances\nLLMs' ability to efficiently align with individual behavioral preferences using\nminimal data and computational resources. Remarkably, our method, PAS, achieves\nsuperior performance while requiring only 1/5 of the optimization time compared\nto DPO, offering practical value for personality alignment. Our work paves the\nway for future AI systems to make decisions and reason in truly personality\nways, enhancing the relevance and meaning of AI interactions for each user and\nadvancing human-centered artificial intelligence.The code has released in\n\\url{https://github.com/zhu-minjun/PAlign}."
                },
                "authors": [
                    {
                        "name": "Minjun Zhu"
                    },
                    {
                        "name": "Linyi Yang"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11778v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11778v1",
                "updated": "2024-08-21T17:08:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    8,
                    5,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T17:08:05Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    8,
                    5,
                    2,
                    234,
                    0
                ],
                "title": "Sum of Squares Circuits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sum of Squares Circuits"
                },
                "summary": "Designing expressive generative models that support exact and efficient\ninference is a core question in probabilistic ML. Probabilistic circuits (PCs)\noffer a framework where this tractability-vs-expressiveness trade-off can be\nanalyzed theoretically. Recently, squared PCs encoding subtractive mixtures via\nnegative parameters have emerged as tractable models that can be exponentially\nmore expressive than monotonic PCs, i.e., PCs with positive parameters only. In\nthis paper, we provide a more precise theoretical characterization of the\nexpressiveness relationships among these models. First, we prove that squared\nPCs can be less expressive than monotonic ones. Second, we formalize a novel\nclass of PCs -- sum of squares PCs -- that can be exponentially more expressive\nthan both squared and monotonic PCs. Around sum of squares PCs, we build an\nexpressiveness hierarchy that allows us to precisely unify and separate\ndifferent tractable model classes such as Born Machines and PSD models, and\nother recently introduced tractable probabilistic models by using complex\nparameters. Finally, we empirically show the effectiveness of sum of squares\ncircuits in performing distribution estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing expressive generative models that support exact and efficient\ninference is a core question in probabilistic ML. Probabilistic circuits (PCs)\noffer a framework where this tractability-vs-expressiveness trade-off can be\nanalyzed theoretically. Recently, squared PCs encoding subtractive mixtures via\nnegative parameters have emerged as tractable models that can be exponentially\nmore expressive than monotonic PCs, i.e., PCs with positive parameters only. In\nthis paper, we provide a more precise theoretical characterization of the\nexpressiveness relationships among these models. First, we prove that squared\nPCs can be less expressive than monotonic ones. Second, we formalize a novel\nclass of PCs -- sum of squares PCs -- that can be exponentially more expressive\nthan both squared and monotonic PCs. Around sum of squares PCs, we build an\nexpressiveness hierarchy that allows us to precisely unify and separate\ndifferent tractable model classes such as Born Machines and PSD models, and\nother recently introduced tractable probabilistic models by using complex\nparameters. Finally, we empirically show the effectiveness of sum of squares\ncircuits in performing distribution estimation."
                },
                "authors": [
                    {
                        "name": "Lorenzo Loconte"
                    },
                    {
                        "name": "Stefan Mengel"
                    },
                    {
                        "name": "Antonio Vergari"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Vergari"
                },
                "author": "Antonio Vergari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11778v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11778v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09544v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09544v2",
                "updated": "2024-08-21T17:04:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    4,
                    8,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-18T17:01:42Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    17,
                    1,
                    42,
                    6,
                    231,
                    0
                ],
                "title": "No Such Thing as a General Learner: Language models and their dual\n  optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Such Thing as a General Learner: Language models and their dual\n  optimization"
                },
                "summary": "What role can the otherwise successful Large Language Models (LLMs) play in\nthe understanding of human cognition, and in particular in terms of informing\nlanguage acquisition debates? To contribute to this question, we first argue\nthat neither humans nor LLMs are general learners, in a variety of senses. We\nmake a novel case for how in particular LLMs follow a dual-optimization\nprocess: they are optimized during their training (which is typically compared\nto language acquisition), and modern LLMs have also been selected, through a\nprocess akin to natural selection in a species. From this perspective, we argue\nthat the performance of LLMs, whether similar or dissimilar to that of humans,\ndoes not weigh easily on important debates about the importance of human\ncognitive biases for language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What role can the otherwise successful Large Language Models (LLMs) play in\nthe understanding of human cognition, and in particular in terms of informing\nlanguage acquisition debates? To contribute to this question, we first argue\nthat neither humans nor LLMs are general learners, in a variety of senses. We\nmake a novel case for how in particular LLMs follow a dual-optimization\nprocess: they are optimized during their training (which is typically compared\nto language acquisition), and modern LLMs have also been selected, through a\nprocess akin to natural selection in a species. From this perspective, we argue\nthat the performance of LLMs, whether similar or dissimilar to that of humans,\ndoes not weigh easily on important debates about the importance of human\ncognitive biases for language."
                },
                "authors": [
                    {
                        "name": "Emmanuel Chemla"
                    },
                    {
                        "name": "Ryan M. Nefdt"
                    }
                ],
                "author_detail": {
                    "name": "Ryan M. Nefdt"
                },
                "author": "Ryan M. Nefdt",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09544v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11775v1",
                "updated": "2024-08-21T17:00:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    0,
                    5,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T17:00:05Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    0,
                    5,
                    2,
                    234,
                    0
                ],
                "title": "Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context\n  Support: For 3GPP Standards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context\n  Support: For 3GPP Standards"
                },
                "summary": "Recent studies show that large language models (LLMs) struggle with technical\nstandards in telecommunications. We propose a fine-tuned retrieval-augmented\ngeneration (RAG) system based on the Phi-2 small language model (SLM) to serve\nas an oracle for communication networks. Our developed system leverages\nforward-looking semantic chunking to adaptively determine parsing breakpoints\nbased on embedding similarity, enabling effective processing of diverse\ndocument formats. To handle the challenge of multiple similar contexts in\ntechnical standards, we employ a re-ranking algorithm to prioritize the most\nrelevant retrieved chunks. Recognizing the limitations of Phi-2's small context\nwindow, we implement a recent technique, namely SelfExtend, to expand the\ncontext window during inference, which not only boosts the performance but also\ncan accommodate a wider range of user queries and design requirements from\ncustomers to specialized technicians. For fine-tuning, we utilize the low-rank\nadaptation (LoRA) technique to enhance computational efficiency during training\nand enable effective fine-tuning on small datasets. Our comprehensive\nexperiments demonstrate substantial improvements over existing\nquestion-answering approaches in the telecom domain, achieving performance that\nexceeds larger language models such as GPT-4 (which is about 880 times larger\nin size). This work presents a novel approach to leveraging SLMs for\ncommunication networks, offering a balance of efficiency and performance. This\nwork can serve as a foundation towards agentic language models for networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies show that large language models (LLMs) struggle with technical\nstandards in telecommunications. We propose a fine-tuned retrieval-augmented\ngeneration (RAG) system based on the Phi-2 small language model (SLM) to serve\nas an oracle for communication networks. Our developed system leverages\nforward-looking semantic chunking to adaptively determine parsing breakpoints\nbased on embedding similarity, enabling effective processing of diverse\ndocument formats. To handle the challenge of multiple similar contexts in\ntechnical standards, we employ a re-ranking algorithm to prioritize the most\nrelevant retrieved chunks. Recognizing the limitations of Phi-2's small context\nwindow, we implement a recent technique, namely SelfExtend, to expand the\ncontext window during inference, which not only boosts the performance but also\ncan accommodate a wider range of user queries and design requirements from\ncustomers to specialized technicians. For fine-tuning, we utilize the low-rank\nadaptation (LoRA) technique to enhance computational efficiency during training\nand enable effective fine-tuning on small datasets. Our comprehensive\nexperiments demonstrate substantial improvements over existing\nquestion-answering approaches in the telecom domain, achieving performance that\nexceeds larger language models such as GPT-4 (which is about 880 times larger\nin size). This work presents a novel approach to leveraging SLMs for\ncommunication networks, offering a balance of efficiency and performance. This\nwork can serve as a foundation towards agentic language models for networks."
                },
                "authors": [
                    {
                        "name": "Omar Erak"
                    },
                    {
                        "name": "Nouf Alabbasi"
                    },
                    {
                        "name": "Omar Alhussein"
                    },
                    {
                        "name": "Ismail Lotfi"
                    },
                    {
                        "name": "Amr Hussein"
                    },
                    {
                        "name": "Sami Muhaidat"
                    },
                    {
                        "name": "Merouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Merouane Debbah"
                },
                "author": "Merouane Debbah",
                "arxiv_comment": "submitted to Proc. IEEE Globecom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.15673v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.15673v2",
                "updated": "2024-08-21T16:34:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    16,
                    34,
                    19,
                    2,
                    234,
                    0
                ],
                "published": "2023-11-27T10:02:12Z",
                "published_parsed": [
                    2023,
                    11,
                    27,
                    10,
                    2,
                    12,
                    0,
                    331,
                    0
                ],
                "title": "Accelerating Hopfield Network Dynamics: Beyond Synchronous Updates and\n  Forward Euler",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Hopfield Network Dynamics: Beyond Synchronous Updates and\n  Forward Euler"
                },
                "summary": "The Hopfield network serves as a fundamental energy-based model in machine\nlearning, capturing memory retrieval dynamics through an ordinary differential\nequation (ODE). The model's output, the equilibrium point of the ODE, is\ntraditionally computed via synchronous updates using the forward Euler method.\nThis paper aims to overcome some of the disadvantages of this approach. We\npropose a conceptual shift, viewing Hopfield networks as instances of Deep\nEquilibrium Models (DEQs). The DEQ framework not only allows for the use of\nspecialized solvers, but also leads to new insights on an empirical inference\ntechnique that we will refer to as 'even-odd splitting'. Our theoretical\nanalysis of the method uncovers a parallelizable asynchronous update scheme,\nwhich should converge roughly twice as fast as the conventional synchronous\nupdates. Empirical evaluations validate these findings, showcasing the\nadvantages of both the DEQ framework and even-odd splitting in digitally\nsimulating energy minimization in Hopfield networks. The code is available at\nhttps://github.com/cgoemaere/hopdeq",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hopfield network serves as a fundamental energy-based model in machine\nlearning, capturing memory retrieval dynamics through an ordinary differential\nequation (ODE). The model's output, the equilibrium point of the ODE, is\ntraditionally computed via synchronous updates using the forward Euler method.\nThis paper aims to overcome some of the disadvantages of this approach. We\npropose a conceptual shift, viewing Hopfield networks as instances of Deep\nEquilibrium Models (DEQs). The DEQ framework not only allows for the use of\nspecialized solvers, but also leads to new insights on an empirical inference\ntechnique that we will refer to as 'even-odd splitting'. Our theoretical\nanalysis of the method uncovers a parallelizable asynchronous update scheme,\nwhich should converge roughly twice as fast as the conventional synchronous\nupdates. Empirical evaluations validate these findings, showcasing the\nadvantages of both the DEQ framework and even-odd splitting in digitally\nsimulating energy minimization in Hopfield networks. The code is available at\nhttps://github.com/cgoemaere/hopdeq"
                },
                "authors": [
                    {
                        "name": "Cdric Goemaere"
                    },
                    {
                        "name": "Johannes Deleu"
                    },
                    {
                        "name": "Thomas Demeester"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Demeester"
                },
                "author": "Thomas Demeester",
                "arxiv_comment": "Accepted at the ML-DE Workshop at ECAI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.15673v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.15673v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11749v1",
                "updated": "2024-08-21T16:16:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    16,
                    16,
                    34,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T16:16:34Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    16,
                    16,
                    34,
                    2,
                    234,
                    0
                ],
                "title": "Against All Odds: Overcoming Typology, Script, and Language Confusion in\n  Multilingual Embedding Inversion Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Against All Odds: Overcoming Typology, Script, and Language Confusion in\n  Multilingual Embedding Inversion Attacks"
                },
                "summary": "Large Language Models (LLMs) are susceptible to malicious influence by cyber\nattackers through intrusions such as adversarial, backdoor, and embedding\ninversion attacks. In response, the burgeoning field of LLM Security aims to\nstudy and defend against such threats. Thus far, the majority of works in this\narea have focused on monolingual English models, however, emerging research\nsuggests that multilingual LLMs may be more vulnerable to various attacks than\ntheir monolingual counterparts. While previous work has investigated embedding\ninversion over a small subset of European languages, it is challenging to\nextrapolate these findings to languages from different linguistic families and\nwith differing scripts. To this end, we explore the security of multilingual\nLLMs in the context of embedding inversion attacks and investigate\ncross-lingual and cross-script inversion across 20 languages, spanning over 8\nlanguage families and 12 scripts. Our findings indicate that languages written\nin Arabic script and Cyrillic script are particularly vulnerable to embedding\ninversion, as are languages within the Indo-Aryan language family. We further\nobserve that inversion models tend to suffer from language confusion, sometimes\ngreatly reducing the efficacy of an attack. Accordingly, we systematically\nexplore this bottleneck for inversion models, uncovering predictable patterns\nwhich could be leveraged by attackers. Ultimately, this study aims to further\nthe field's understanding of the outstanding security vulnerabilities facing\nmultilingual LLMs and raise awareness for the languages most at risk of\nnegative impact from these attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are susceptible to malicious influence by cyber\nattackers through intrusions such as adversarial, backdoor, and embedding\ninversion attacks. In response, the burgeoning field of LLM Security aims to\nstudy and defend against such threats. Thus far, the majority of works in this\narea have focused on monolingual English models, however, emerging research\nsuggests that multilingual LLMs may be more vulnerable to various attacks than\ntheir monolingual counterparts. While previous work has investigated embedding\ninversion over a small subset of European languages, it is challenging to\nextrapolate these findings to languages from different linguistic families and\nwith differing scripts. To this end, we explore the security of multilingual\nLLMs in the context of embedding inversion attacks and investigate\ncross-lingual and cross-script inversion across 20 languages, spanning over 8\nlanguage families and 12 scripts. Our findings indicate that languages written\nin Arabic script and Cyrillic script are particularly vulnerable to embedding\ninversion, as are languages within the Indo-Aryan language family. We further\nobserve that inversion models tend to suffer from language confusion, sometimes\ngreatly reducing the efficacy of an attack. Accordingly, we systematically\nexplore this bottleneck for inversion models, uncovering predictable patterns\nwhich could be leveraged by attackers. Ultimately, this study aims to further\nthe field's understanding of the outstanding security vulnerabilities facing\nmultilingual LLMs and raise awareness for the languages most at risk of\nnegative impact from these attacks."
                },
                "authors": [
                    {
                        "name": "Yiyi Chen"
                    },
                    {
                        "name": "Russa Biswas"
                    },
                    {
                        "name": "Heather Lent"
                    },
                    {
                        "name": "Johannes Bjerva"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Bjerva"
                },
                "author": "Johannes Bjerva",
                "arxiv_comment": "11 pages, 4 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11746v1",
                "updated": "2024-08-21T16:13:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    16,
                    13,
                    16,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T16:13:16Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    16,
                    13,
                    16,
                    2,
                    234,
                    0
                ],
                "title": "Mixed Sparsity Training: Achieving 4$\\times$ FLOP Reduction for\n  Transformer Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed Sparsity Training: Achieving 4$\\times$ FLOP Reduction for\n  Transformer Pretraining"
                },
                "summary": "Large language models (LLMs) have made significant strides in complex tasks,\nyet their widespread adoption is impeded by substantial computational demands.\nWith hundreds of billion parameters, transformer-based LLMs necessitate months\nof pretraining across a high-end GPU cluster. However, this paper reveals a\ncompelling finding: transformers exhibit considerable redundancy in pretraining\ncomputations, which motivates our proposed solution, Mixed Sparsity Training\n(MST), an efficient pretraining method that can reduce about $75\\%$ of Floating\nPoint Operations (FLOPs) while maintaining performance. MST integrates dynamic\nsparse training (DST) with Sparsity Variation (SV) and Hybrid Sparse Attention\n(HSA) during pretraining, involving three distinct phases: warm-up,\nultra-sparsification, and restoration. The warm-up phase transforms the dense\nmodel into a sparse one, and the restoration phase reinstates connections.\nThroughout these phases, the model is trained with a dynamically evolving\nsparse topology and an HSA mechanism to maintain performance and minimize\ntraining FLOPs concurrently. Our experiment on GPT-2 showcases a FLOP reduction\nof $4\\times$ without compromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant strides in complex tasks,\nyet their widespread adoption is impeded by substantial computational demands.\nWith hundreds of billion parameters, transformer-based LLMs necessitate months\nof pretraining across a high-end GPU cluster. However, this paper reveals a\ncompelling finding: transformers exhibit considerable redundancy in pretraining\ncomputations, which motivates our proposed solution, Mixed Sparsity Training\n(MST), an efficient pretraining method that can reduce about $75\\%$ of Floating\nPoint Operations (FLOPs) while maintaining performance. MST integrates dynamic\nsparse training (DST) with Sparsity Variation (SV) and Hybrid Sparse Attention\n(HSA) during pretraining, involving three distinct phases: warm-up,\nultra-sparsification, and restoration. The warm-up phase transforms the dense\nmodel into a sparse one, and the restoration phase reinstates connections.\nThroughout these phases, the model is trained with a dynamically evolving\nsparse topology and an HSA mechanism to maintain performance and minimize\ntraining FLOPs concurrently. Our experiment on GPT-2 showcases a FLOP reduction\nof $4\\times$ without compromising performance."
                },
                "authors": [
                    {
                        "name": "Pihe Hu"
                    },
                    {
                        "name": "Shaolong Li"
                    },
                    {
                        "name": "Longbo Huang"
                    }
                ],
                "author_detail": {
                    "name": "Longbo Huang"
                },
                "author": "Longbo Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11745v1",
                "updated": "2024-08-21T16:11:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    16,
                    11,
                    59,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T16:11:59Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    16,
                    11,
                    59,
                    2,
                    234,
                    0
                ],
                "title": "FocusLLM: Scaling LLM's Context by Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FocusLLM: Scaling LLM's Context by Parallel Decoding"
                },
                "summary": "Empowering LLMs with the ability to utilize useful information from a long\ncontext is crucial for many downstream applications. However, achieving long\ncontext lengths with the conventional transformer architecture requires\nsubstantial training and inference resources. In this paper, we present\nFocusLLM, a framework designed to extend the context length of any decoder-only\nLLM, enabling the model to focus on relevant information from very long\nsequences. FocusLLM processes long text inputs by dividing them into chunks\nbased on the model's original context length to alleviate the issue of\nattention distraction. Then, it appends the local context to each chunk as a\nprompt to extract essential information from each chunk based on a novel\nparallel decoding mechanism, and ultimately integrates the extracted\ninformation into the local context. FocusLLM stands out for great training\nefficiency and versatility: trained with an 8K input length with much less\ntraining cost than previous methods, FocusLLM exhibits superior performance\nacross downstream long-context tasks and maintains strong language modeling\nability when handling extensive long texts, even up to 400K tokens. Our code is\navailable at https://github.com/leezythu/FocusLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering LLMs with the ability to utilize useful information from a long\ncontext is crucial for many downstream applications. However, achieving long\ncontext lengths with the conventional transformer architecture requires\nsubstantial training and inference resources. In this paper, we present\nFocusLLM, a framework designed to extend the context length of any decoder-only\nLLM, enabling the model to focus on relevant information from very long\nsequences. FocusLLM processes long text inputs by dividing them into chunks\nbased on the model's original context length to alleviate the issue of\nattention distraction. Then, it appends the local context to each chunk as a\nprompt to extract essential information from each chunk based on a novel\nparallel decoding mechanism, and ultimately integrates the extracted\ninformation into the local context. FocusLLM stands out for great training\nefficiency and versatility: trained with an 8K input length with much less\ntraining cost than previous methods, FocusLLM exhibits superior performance\nacross downstream long-context tasks and maintains strong language modeling\nability when handling extensive long texts, even up to 400K tokens. Our code is\navailable at https://github.com/leezythu/FocusLLM."
                },
                "authors": [
                    {
                        "name": "Zhenyu Li"
                    },
                    {
                        "name": "Yike Zhang"
                    },
                    {
                        "name": "Tengyu Pan"
                    },
                    {
                        "name": "Yutao Sun"
                    },
                    {
                        "name": "Zhichao Duan"
                    },
                    {
                        "name": "Junjie Fang"
                    },
                    {
                        "name": "Rong Han"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Jianyong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyong Wang"
                },
                "author": "Jianyong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11743v1",
                "updated": "2024-08-21T16:10:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    16,
                    10,
                    41,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T16:10:41Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    16,
                    10,
                    41,
                    2,
                    234,
                    0
                ],
                "title": "MARLIN: Mixed-Precision Auto-Regressive Parallel Inference on Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARLIN: Mixed-Precision Auto-Regressive Parallel Inference on Large\n  Language Models"
                },
                "summary": "As inference on Large Language Models (LLMs) emerges as an important workload\nin machine learning applications, weight quantization has become a standard\ntechnique for efficient GPU deployment. Quantization not only reduces model\nsize, but has also been shown to yield substantial speedups for single-user\ninference, due to reduced memory movement, with low accuracy impact. Yet, it\nremains open whether speedups are achievable also in \\emph{batched} settings\nwith multiple parallel clients, which are highly relevant for practical\nserving. It is unclear whether GPU kernels can be designed to remain\npractically memory-bound, while supporting the substantially increased compute\nrequirements of batched workloads.\n  This paper resolves this question positively by describing the design of\nMixed-precision Auto-Regressive LINear kernels, called MARLIN. Concretely,\ngiven a model whose weights are compressed via quantization to, e.g., 4 bits\nper element, MARLIN shows that batchsizes up to 16-32 can be supported with\nclose to maximum ($4\\times$) quantization speedup, and larger batchsizes up to\n64-128 with gradually decreasing, but still significant, acceleration. MARLIN\naccomplishes this via a combination of techniques, such as asynchronous memory\naccess, complex task scheduling and pipelining, and bespoke quantization\nsupport. Our experiments show that MARLIN's near-optimal performance on\nindividual LLM layers across different scenarios can also lead to end-to-end\nLLM inference speedups (of up to $2.8\\times$) when integrated with the popular\nvLLM serving engine. Finally, MARLIN is extensible to further compression\ntechniques, like NVIDIA 2:4 sparsity, leading to additional speedups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As inference on Large Language Models (LLMs) emerges as an important workload\nin machine learning applications, weight quantization has become a standard\ntechnique for efficient GPU deployment. Quantization not only reduces model\nsize, but has also been shown to yield substantial speedups for single-user\ninference, due to reduced memory movement, with low accuracy impact. Yet, it\nremains open whether speedups are achievable also in \\emph{batched} settings\nwith multiple parallel clients, which are highly relevant for practical\nserving. It is unclear whether GPU kernels can be designed to remain\npractically memory-bound, while supporting the substantially increased compute\nrequirements of batched workloads.\n  This paper resolves this question positively by describing the design of\nMixed-precision Auto-Regressive LINear kernels, called MARLIN. Concretely,\ngiven a model whose weights are compressed via quantization to, e.g., 4 bits\nper element, MARLIN shows that batchsizes up to 16-32 can be supported with\nclose to maximum ($4\\times$) quantization speedup, and larger batchsizes up to\n64-128 with gradually decreasing, but still significant, acceleration. MARLIN\naccomplishes this via a combination of techniques, such as asynchronous memory\naccess, complex task scheduling and pipelining, and bespoke quantization\nsupport. Our experiments show that MARLIN's near-optimal performance on\nindividual LLM layers across different scenarios can also lead to end-to-end\nLLM inference speedups (of up to $2.8\\times$) when integrated with the popular\nvLLM serving engine. Finally, MARLIN is extensible to further compression\ntechniques, like NVIDIA 2:4 sparsity, leading to additional speedups."
                },
                "authors": [
                    {
                        "name": "Elias Frantar"
                    },
                    {
                        "name": "Roberto L. Castro"
                    },
                    {
                        "name": "Jiale Chen"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.09104v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.09104v2",
                "updated": "2024-08-21T16:01:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    16,
                    1,
                    6,
                    2,
                    234,
                    0
                ],
                "published": "2023-08-17T17:14:18Z",
                "published_parsed": [
                    2023,
                    8,
                    17,
                    17,
                    14,
                    18,
                    3,
                    229,
                    0
                ],
                "title": "Spike-and-slab shrinkage priors for structurally sparse Bayesian neural\n  networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spike-and-slab shrinkage priors for structurally sparse Bayesian neural\n  networks"
                },
                "summary": "Network complexity and computational efficiency have become increasingly\nsignificant aspects of deep learning. Sparse deep learning addresses these\nchallenges by recovering a sparse representation of the underlying target\nfunction by reducing heavily over-parameterized deep neural networks.\nSpecifically, deep neural architectures compressed via structured sparsity\n(e.g. node sparsity) provide low latency inference, higher data throughput, and\nreduced energy consumption. In this paper, we explore two well-established\nshrinkage techniques, Lasso and Horseshoe, for model compression in Bayesian\nneural networks. To this end, we propose structurally sparse Bayesian neural\nnetworks which systematically prune excessive nodes with (i) Spike-and-Slab\nGroup Lasso (SS-GL), and (ii) Spike-and-Slab Group Horseshoe (SS-GHS) priors,\nand develop computationally tractable variational inference including\ncontinuous relaxation of Bernoulli variables. We establish the contraction\nrates of the variational posterior of our proposed models as a function of the\nnetwork topology, layer-wise node cardinalities, and bounds on the network\nweights. We empirically demonstrate the competitive performance of our models\ncompared to the baseline models in prediction accuracy, model compression, and\ninference latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network complexity and computational efficiency have become increasingly\nsignificant aspects of deep learning. Sparse deep learning addresses these\nchallenges by recovering a sparse representation of the underlying target\nfunction by reducing heavily over-parameterized deep neural networks.\nSpecifically, deep neural architectures compressed via structured sparsity\n(e.g. node sparsity) provide low latency inference, higher data throughput, and\nreduced energy consumption. In this paper, we explore two well-established\nshrinkage techniques, Lasso and Horseshoe, for model compression in Bayesian\nneural networks. To this end, we propose structurally sparse Bayesian neural\nnetworks which systematically prune excessive nodes with (i) Spike-and-Slab\nGroup Lasso (SS-GL), and (ii) Spike-and-Slab Group Horseshoe (SS-GHS) priors,\nand develop computationally tractable variational inference including\ncontinuous relaxation of Bernoulli variables. We establish the contraction\nrates of the variational posterior of our proposed models as a function of the\nnetwork topology, layer-wise node cardinalities, and bounds on the network\nweights. We empirically demonstrate the competitive performance of our models\ncompared to the baseline models in prediction accuracy, model compression, and\ninference latency."
                },
                "authors": [
                    {
                        "name": "Sanket Jantre"
                    },
                    {
                        "name": "Shrijita Bhattacharya"
                    },
                    {
                        "name": "Tapabrata Maiti"
                    }
                ],
                "author_detail": {
                    "name": "Tapabrata Maiti"
                },
                "author": "Tapabrata Maiti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.09104v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.09104v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11735v1",
                "updated": "2024-08-21T15:59:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    59,
                    33,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T15:59:33Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    59,
                    33,
                    2,
                    234,
                    0
                ],
                "title": "Clinical Insights: A Comprehensive Review of Language Models in Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical Insights: A Comprehensive Review of Language Models in Medicine"
                },
                "summary": "This paper provides a detailed examination of the advancements and\napplications of large language models in the healthcare sector, with a\nparticular emphasis on clinical applications. The study traces the evolution of\nLLMs from their foundational technologies to the latest developments in\ndomain-specific models and multimodal integration. It explores the technical\nprogression from encoder-based models requiring fine-tuning to sophisticated\napproaches that integrate textual, visual, and auditory data, thereby\nfacilitating comprehensive AI solutions in healthcare. The paper discusses both\nthe opportunities these technologies present for enhancing clinical efficiency\nand the challenges they pose in terms of ethics, data privacy, and\nimplementation. Additionally, it critically evaluates the deployment strategies\nof LLMs, emphasizing the necessity of open-source models to ensure data privacy\nand adaptability within healthcare environments. Future research directions are\nproposed, focusing on empirical studies to evaluate the real-world efficacy of\nLLMs in healthcare and the development of open datasets for further research.\nThis review aims to provide a comprehensive resource for both newcomers and\nmultidisciplinary researchers interested in the intersection of AI and\nhealthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides a detailed examination of the advancements and\napplications of large language models in the healthcare sector, with a\nparticular emphasis on clinical applications. The study traces the evolution of\nLLMs from their foundational technologies to the latest developments in\ndomain-specific models and multimodal integration. It explores the technical\nprogression from encoder-based models requiring fine-tuning to sophisticated\napproaches that integrate textual, visual, and auditory data, thereby\nfacilitating comprehensive AI solutions in healthcare. The paper discusses both\nthe opportunities these technologies present for enhancing clinical efficiency\nand the challenges they pose in terms of ethics, data privacy, and\nimplementation. Additionally, it critically evaluates the deployment strategies\nof LLMs, emphasizing the necessity of open-source models to ensure data privacy\nand adaptability within healthcare environments. Future research directions are\nproposed, focusing on empirical studies to evaluate the real-world efficacy of\nLLMs in healthcare and the development of open datasets for further research.\nThis review aims to provide a comprehensive resource for both newcomers and\nmultidisciplinary researchers interested in the intersection of AI and\nhealthcare."
                },
                "authors": [
                    {
                        "name": "Nikita Neveditsin"
                    },
                    {
                        "name": "Pawan Lingras"
                    },
                    {
                        "name": "Vijay Mago"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Mago"
                },
                "author": "Vijay Mago",
                "arxiv_comment": "Submitted to PLOS Digital Health",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11729v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11729v2",
                "updated": "2024-08-22T02:38:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    2,
                    38,
                    56,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-21T15:54:17Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    54,
                    17,
                    2,
                    234,
                    0
                ],
                "title": "LLM4VV: Exploring LLM-as-a-Judge for Validation and Verification\n  Testsuites",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4VV: Exploring LLM-as-a-Judge for Validation and Verification\n  Testsuites"
                },
                "summary": "Large Language Models (LLM) are evolving and have significantly\nrevolutionized the landscape of software development. If used well, they can\nsignificantly accelerate the software development cycle. At the same time, the\ncommunity is very cautious of the models being trained on biased or sensitive\ndata, which can lead to biased outputs along with the inadvertent release of\nconfidential information. Additionally, the carbon footprints and the\nun-explainability of these black box models continue to raise questions about\nthe usability of LLMs.\n  With the abundance of opportunities LLMs have to offer, this paper explores\nthe idea of judging tests used to evaluate compiler implementations of\ndirective-based programming models as well as probe into the black box of LLMs.\nBased on our results, utilizing an agent-based prompting approach and setting\nup a validation pipeline structure drastically increased the quality of\nDeepSeek Coder, the LLM chosen for the evaluation purposes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) are evolving and have significantly\nrevolutionized the landscape of software development. If used well, they can\nsignificantly accelerate the software development cycle. At the same time, the\ncommunity is very cautious of the models being trained on biased or sensitive\ndata, which can lead to biased outputs along with the inadvertent release of\nconfidential information. Additionally, the carbon footprints and the\nun-explainability of these black box models continue to raise questions about\nthe usability of LLMs.\n  With the abundance of opportunities LLMs have to offer, this paper explores\nthe idea of judging tests used to evaluate compiler implementations of\ndirective-based programming models as well as probe into the black box of LLMs.\nBased on our results, utilizing an agent-based prompting approach and setting\nup a validation pipeline structure drastically increased the quality of\nDeepSeek Coder, the LLM chosen for the evaluation purposes."
                },
                "authors": [
                    {
                        "name": "Zachariah Sollenberger"
                    },
                    {
                        "name": "Jay Patel"
                    },
                    {
                        "name": "Christian Munley"
                    },
                    {
                        "name": "Aaron Jarmusch"
                    },
                    {
                        "name": "Sunita Chandrasekaran"
                    }
                ],
                "author_detail": {
                    "name": "Sunita Chandrasekaran"
                },
                "author": "Sunita Chandrasekaran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11729v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11729v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11727v1",
                "updated": "2024-08-21T15:54:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    54,
                    4,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T15:54:04Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    54,
                    4,
                    2,
                    234,
                    0
                ],
                "title": "Efficient Detection of Toxic Prompts in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Detection of Toxic Prompts in Large Language Models"
                },
                "summary": "Large language models (LLMs) like ChatGPT and Gemini have significantly\nadvanced natural language processing, enabling various applications such as\nchatbots and automated content generation. However, these models can be\nexploited by malicious individuals who craft toxic prompts to elicit harmful or\nunethical responses. These individuals often employ jailbreaking techniques to\nbypass safety mechanisms, highlighting the need for robust toxic prompt\ndetection methods. Existing detection techniques, both blackbox and whitebox,\nface challenges related to the diversity of toxic prompts, scalability, and\ncomputational efficiency. In response, we propose ToxicDetector, a lightweight\ngreybox method designed to efficiently detect toxic prompts in LLMs.\nToxicDetector leverages LLMs to create toxic concept prompts, uses embedding\nvectors to form feature vectors, and employs a Multi-Layer Perceptron (MLP)\nclassifier for prompt classification. Our evaluation on various versions of the\nLLama models, Gemma-2, and multiple datasets demonstrates that ToxicDetector\nachieves a high accuracy of 96.39\\% and a low false positive rate of 2.00\\%,\noutperforming state-of-the-art methods. Additionally, ToxicDetector's\nprocessing time of 0.0780 seconds per prompt makes it highly suitable for\nreal-time applications. ToxicDetector achieves high accuracy, efficiency, and\nscalability, making it a practical method for toxic prompt detection in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) like ChatGPT and Gemini have significantly\nadvanced natural language processing, enabling various applications such as\nchatbots and automated content generation. However, these models can be\nexploited by malicious individuals who craft toxic prompts to elicit harmful or\nunethical responses. These individuals often employ jailbreaking techniques to\nbypass safety mechanisms, highlighting the need for robust toxic prompt\ndetection methods. Existing detection techniques, both blackbox and whitebox,\nface challenges related to the diversity of toxic prompts, scalability, and\ncomputational efficiency. In response, we propose ToxicDetector, a lightweight\ngreybox method designed to efficiently detect toxic prompts in LLMs.\nToxicDetector leverages LLMs to create toxic concept prompts, uses embedding\nvectors to form feature vectors, and employs a Multi-Layer Perceptron (MLP)\nclassifier for prompt classification. Our evaluation on various versions of the\nLLama models, Gemma-2, and multiple datasets demonstrates that ToxicDetector\nachieves a high accuracy of 96.39\\% and a low false positive rate of 2.00\\%,\noutperforming state-of-the-art methods. Additionally, ToxicDetector's\nprocessing time of 0.0780 seconds per prompt makes it highly suitable for\nreal-time applications. ToxicDetector achieves high accuracy, efficiency, and\nscalability, making it a practical method for toxic prompt detection in LLMs."
                },
                "authors": [
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Junzhe Yu"
                    },
                    {
                        "name": "Huijia Sun"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Gelei Deng"
                    },
                    {
                        "name": "Yuqi Chen"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "Accepted by the 39th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.06644v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.06644v3",
                "updated": "2024-08-21T15:53:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    53,
                    2,
                    2,
                    234,
                    0
                ],
                "published": "2023-10-10T14:07:37Z",
                "published_parsed": [
                    2023,
                    10,
                    10,
                    14,
                    7,
                    37,
                    1,
                    283,
                    0
                ],
                "title": "HYVE: Hybrid Vertex Encoder for Neural Distance Fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HYVE: Hybrid Vertex Encoder for Neural Distance Fields"
                },
                "summary": "Neural shape representation generally refers to representing 3D geometry\nusing neural networks, e.g., computing a signed distance or occupancy value at\na specific spatial position. In this paper we present a neural-network\narchitecture suitable for accurate encoding of 3D shapes in a single forward\npass. Our architecture is based on a multi-scale hybrid system incorporating\ngraph-based and voxel-based components, as well as a continuously\ndifferentiable decoder. The hybrid system includes a novel way of voxelizing\npoint-based features in neural networks, which we show can be used in\ncombination with oriented point-clouds to obtain smoother and more detailed\nreconstructions. Furthermore, our network is trained to solve the eikonal\nequation and only requires knowledge of the zero-level set for training and\ninference. This means that in contrast to most previous shape encoder\narchitectures, our network is able to output valid signed distance fields\nwithout explicit prior knowledge of non-zero distance values or shape\noccupancy. It also requires only a single forward-pass, instead of the\nlatent-code optimization used in auto-decoder methods. We further propose a\nmodification to the loss function in case that surface normals are not well\ndefined, e.g., in the context of non-watertight surfaces and non-manifold\ngeometry, resulting in an unsigned distance field. Overall, our system can help\nto reduce the computational overhead of training and evaluating neural distance\nfields, as well as enabling the application to difficult geometry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural shape representation generally refers to representing 3D geometry\nusing neural networks, e.g., computing a signed distance or occupancy value at\na specific spatial position. In this paper we present a neural-network\narchitecture suitable for accurate encoding of 3D shapes in a single forward\npass. Our architecture is based on a multi-scale hybrid system incorporating\ngraph-based and voxel-based components, as well as a continuously\ndifferentiable decoder. The hybrid system includes a novel way of voxelizing\npoint-based features in neural networks, which we show can be used in\ncombination with oriented point-clouds to obtain smoother and more detailed\nreconstructions. Furthermore, our network is trained to solve the eikonal\nequation and only requires knowledge of the zero-level set for training and\ninference. This means that in contrast to most previous shape encoder\narchitectures, our network is able to output valid signed distance fields\nwithout explicit prior knowledge of non-zero distance values or shape\noccupancy. It also requires only a single forward-pass, instead of the\nlatent-code optimization used in auto-decoder methods. We further propose a\nmodification to the loss function in case that surface normals are not well\ndefined, e.g., in the context of non-watertight surfaces and non-manifold\ngeometry, resulting in an unsigned distance field. Overall, our system can help\nto reduce the computational overhead of training and evaluating neural distance\nfields, as well as enabling the application to difficult geometry."
                },
                "authors": [
                    {
                        "name": "Stefan Rhys Jeske"
                    },
                    {
                        "name": "Jonathan Klein"
                    },
                    {
                        "name": "Dominik L. Michels"
                    },
                    {
                        "name": "Jan Bender"
                    }
                ],
                "author_detail": {
                    "name": "Jan Bender"
                },
                "author": "Jan Bender",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.06644v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.06644v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11725v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11725v2",
                "updated": "2024-08-22T04:04:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    4,
                    4,
                    21,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-21T15:52:37Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    52,
                    37,
                    2,
                    234,
                    0
                ],
                "title": "A Multiple Random Scan Strategy for Latent Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multiple Random Scan Strategy for Latent Space Models"
                },
                "summary": "Latent Space (LS) network models project the nodes of a network on a\n$d$-dimensional latent space to achieve dimensionality reduction of the network\nwhile preserving its relevant features. Inference is often carried out within a\nMarkov Chain Monte Carlo (MCMC) framework. Nonetheless, it is well-known that\nthe computational time for this set of models increases quadratically with the\nnumber of nodes. In this work, we build on the Random-Scan (RS) approach to\npropose an MCMC strategy that alleviates the computational burden for LS models\nwhile maintaining the benefits of a general-purpose technique. We call this\nnovel strategy Multiple RS (MRS). This strategy is effective in reducing the\ncomputational cost by a factor without severe consequences on the MCMC draws.\nMoreover, we introduce a novel adaptation strategy that consists of a\nprobabilistic update of the set of latent coordinates of each node. Our\nAdaptive MRS adapts the acceptance rate of the Metropolis step to adjust the\nprobability of updating the latent coordinates. We show via simulation that the\nAdaptive MRS approach performs better than MRS in terms of mixing. Finally, we\napply our algorithm to a multi-layer temporal LS model and show how our\nadaptive strategy may be beneficial to empirical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Space (LS) network models project the nodes of a network on a\n$d$-dimensional latent space to achieve dimensionality reduction of the network\nwhile preserving its relevant features. Inference is often carried out within a\nMarkov Chain Monte Carlo (MCMC) framework. Nonetheless, it is well-known that\nthe computational time for this set of models increases quadratically with the\nnumber of nodes. In this work, we build on the Random-Scan (RS) approach to\npropose an MCMC strategy that alleviates the computational burden for LS models\nwhile maintaining the benefits of a general-purpose technique. We call this\nnovel strategy Multiple RS (MRS). This strategy is effective in reducing the\ncomputational cost by a factor without severe consequences on the MCMC draws.\nMoreover, we introduce a novel adaptation strategy that consists of a\nprobabilistic update of the set of latent coordinates of each node. Our\nAdaptive MRS adapts the acceptance rate of the Metropolis step to adjust the\nprobability of updating the latent coordinates. We show via simulation that the\nAdaptive MRS approach performs better than MRS in terms of mixing. Finally, we\napply our algorithm to a multi-layer temporal LS model and show how our\nadaptive strategy may be beneficial to empirical applications."
                },
                "authors": [
                    {
                        "name": "Roberto Casarin"
                    },
                    {
                        "name": "Antonio Peruzzi"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Peruzzi"
                },
                "author": "Antonio Peruzzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11725v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11725v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11721v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11721v1",
                "updated": "2024-08-21T15:51:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    51,
                    46,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T15:51:46Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    51,
                    46,
                    2,
                    234,
                    0
                ],
                "title": "Iterative Object Count Optimization for Text-to-image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative Object Count Optimization for Text-to-image Diffusion Models"
                },
                "summary": "We address a persistent challenge in text-to-image models: accurately\ngenerating a specified number of objects. Current models, which learn from\nimage-text pairs, inherently struggle with counting, as training data cannot\ndepict every possible number of objects for any given object. To solve this, we\npropose optimizing the generated image based on a counting loss derived from a\ncounting model that aggregates an object\\'s potential. Employing an\nout-of-the-box counting model is challenging for two reasons: first, the model\nrequires a scaling hyperparameter for the potential aggregation that varies\ndepending on the viewpoint of the objects, and second, classifier guidance\ntechniques require modified models that operate on noisy intermediate diffusion\nsteps. To address these challenges, we propose an iterated online training mode\nthat improves the accuracy of inferred images while altering the text\nconditioning embedding and dynamically adjusting hyperparameters. Our method\noffers three key advantages: (i) it can consider non-derivable counting\ntechniques based on detection models, (ii) it is a zero-shot plug-and-play\nsolution facilitating rapid changes to the counting techniques and image\ngeneration methods, and (iii) the optimized counting token can be reused to\ngenerate accurate images without additional optimization. We evaluate the\ngeneration of various objects and show significant improvements in accuracy.\nThe project page is available at https://ozzafar.github.io/count_token.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address a persistent challenge in text-to-image models: accurately\ngenerating a specified number of objects. Current models, which learn from\nimage-text pairs, inherently struggle with counting, as training data cannot\ndepict every possible number of objects for any given object. To solve this, we\npropose optimizing the generated image based on a counting loss derived from a\ncounting model that aggregates an object\\'s potential. Employing an\nout-of-the-box counting model is challenging for two reasons: first, the model\nrequires a scaling hyperparameter for the potential aggregation that varies\ndepending on the viewpoint of the objects, and second, classifier guidance\ntechniques require modified models that operate on noisy intermediate diffusion\nsteps. To address these challenges, we propose an iterated online training mode\nthat improves the accuracy of inferred images while altering the text\nconditioning embedding and dynamically adjusting hyperparameters. Our method\noffers three key advantages: (i) it can consider non-derivable counting\ntechniques based on detection models, (ii) it is a zero-shot plug-and-play\nsolution facilitating rapid changes to the counting techniques and image\ngeneration methods, and (iii) the optimized counting token can be reused to\ngenerate accurate images without additional optimization. We evaluate the\ngeneration of various objects and show significant improvements in accuracy.\nThe project page is available at https://ozzafar.github.io/count_token."
                },
                "authors": [
                    {
                        "name": "Oz Zafar"
                    },
                    {
                        "name": "Lior Wolf"
                    },
                    {
                        "name": "Idan Schwartz"
                    }
                ],
                "author_detail": {
                    "name": "Idan Schwartz"
                },
                "author": "Idan Schwartz",
                "arxiv_comment": "Pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11721v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11721v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10923v2",
                "updated": "2024-08-21T15:51:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    51,
                    33,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-20T15:05:02Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    5,
                    2,
                    1,
                    233,
                    0
                ],
                "title": "LBC: Language-Based-Classifier for Out-Of-Variable Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LBC: Language-Based-Classifier for Out-Of-Variable Generalization"
                },
                "summary": "Large Language Models (LLMs) have great success in natural language\nprocessing tasks such as response generation. However, their use in tabular\ndata has been limited due to their inferior performance compared to traditional\nmachine learning models (TMLs) such as XGBoost. We find that the pre-trained\nknowledge of LLMs enables them to interpret new variables that appear in a test\nwithout additional training, a capability central to the concept of\nOut-of-Variable (OOV). From the findings, we propose a\nLanguage-Based-Classifier (LBC), a classifier that maximizes the benefits of\nLLMs to outperform TMLs on OOV tasks. LBC employs three key methodological\nstrategies: 1) Categorical changes to adjust data to better fit the model's\nunderstanding, 2) Advanced order and indicator to enhance data representation\nto the model, and 3) Using verbalizer to map logit scores to classes during\ninference to generate model predictions. These strategies, combined with the\npre-trained knowledge of LBC, emphasize the model's ability to effectively\nhandle OOV tasks. We empirically and theoretically validate the superiority of\nLBC. LBC is the first study to apply an LLM-based model to OOV tasks. The\nsource code is at https://github.com/sksmssh/LBCforOOVGen",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have great success in natural language\nprocessing tasks such as response generation. However, their use in tabular\ndata has been limited due to their inferior performance compared to traditional\nmachine learning models (TMLs) such as XGBoost. We find that the pre-trained\nknowledge of LLMs enables them to interpret new variables that appear in a test\nwithout additional training, a capability central to the concept of\nOut-of-Variable (OOV). From the findings, we propose a\nLanguage-Based-Classifier (LBC), a classifier that maximizes the benefits of\nLLMs to outperform TMLs on OOV tasks. LBC employs three key methodological\nstrategies: 1) Categorical changes to adjust data to better fit the model's\nunderstanding, 2) Advanced order and indicator to enhance data representation\nto the model, and 3) Using verbalizer to map logit scores to classes during\ninference to generate model predictions. These strategies, combined with the\npre-trained knowledge of LBC, emphasize the model's ability to effectively\nhandle OOV tasks. We empirically and theoretically validate the superiority of\nLBC. LBC is the first study to apply an LLM-based model to OOV tasks. The\nsource code is at https://github.com/sksmssh/LBCforOOVGen"
                },
                "authors": [
                    {
                        "name": "Kangjun Noh"
                    },
                    {
                        "name": "Baekryun Seong"
                    },
                    {
                        "name": "Hoyoon Byun"
                    },
                    {
                        "name": "Youngjun Choi"
                    },
                    {
                        "name": "Sungjin Song"
                    },
                    {
                        "name": "Kyungwoo Song"
                    }
                ],
                "author_detail": {
                    "name": "Kyungwoo Song"
                },
                "author": "Kyungwoo Song",
                "arxiv_comment": "16 pages, 7 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11713v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11713v1",
                "updated": "2024-08-21T15:37:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    37,
                    18,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T15:37:18Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    37,
                    18,
                    2,
                    234,
                    0
                ],
                "title": "Properties of intermediate- to high-mass stars in the young cluster M17\n  -- Characterizing the (pre-)zero-age main sequence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Properties of intermediate- to high-mass stars in the young cluster M17\n  -- Characterizing the (pre-)zero-age main sequence"
                },
                "summary": "The outcome of the formation of massive stars is an important anchor point in\ntheir evolution. It provides insight into the physics of the assembly process,\nand sets the conditions for stellar evolution. We characterize a population of\n18 highly reddened O4.5 to B9 stars in the very young massive star-forming\nregion M17. Their properties allow us to identify the empirical location of the\nZAMS, and rotation and mass-loss rate of stars there. We performed quantitative\nspectroscopic modeling of VLT/X-shooter spectra using NLTE atmosphere code\nFastwind and fitting approach Kiwi-GA. The observed SEDs were used to determine\nthe line-of-sight extinction. From a comparison of their positions in the HRD\nwith MIST evolutionary tracks, we inferred the stellar masses and ages. We find\nan age of $0.4_{-0.2}^{+0.6}$ Myr for our sample, however we also identify a\nstrong relation between the age and the mass of the stars. The extinction\ntowards the sources ranges from $A_V = 3.6$ to 10.6. Stars more massive than 10\nM$_{\\odot}$ have reached the ZAMS. Their projected ZAMS spin rate distribution\nextends to 0.3 of the critical velocity; their mass-loss rates agree with those\nof other main-sequence OB stars. Stars with a mass in the range $3 <\nM/$M$_{\\odot} < 7$ are still on the pre-main sequence (PMS). Evolving their $v\n\\sin i$ to the ZAMS yields values up to $\\sim 0.6 v_{\\rm crit}$. For PMS stars\nwithout disks, we find tentative mass-loss rates up to\n$10^{-8.5}\\,$M$_{\\odot}$\\,yr$^{-1}$. We constrain the empirical location of the\nZAMS for massive ($10 < M/$M$_{\\odot} < 50$) stars. The ZAMS rotation rates for\nintermediate-mass stars are twice as high as for massive stars, suggesting that\nthe angular momentum gain processes differ between the two groups. The relation\nbetween the age and mass of the stars suggests a lag in the formation of more\nmassive stars relative to lower mass stars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The outcome of the formation of massive stars is an important anchor point in\ntheir evolution. It provides insight into the physics of the assembly process,\nand sets the conditions for stellar evolution. We characterize a population of\n18 highly reddened O4.5 to B9 stars in the very young massive star-forming\nregion M17. Their properties allow us to identify the empirical location of the\nZAMS, and rotation and mass-loss rate of stars there. We performed quantitative\nspectroscopic modeling of VLT/X-shooter spectra using NLTE atmosphere code\nFastwind and fitting approach Kiwi-GA. The observed SEDs were used to determine\nthe line-of-sight extinction. From a comparison of their positions in the HRD\nwith MIST evolutionary tracks, we inferred the stellar masses and ages. We find\nan age of $0.4_{-0.2}^{+0.6}$ Myr for our sample, however we also identify a\nstrong relation between the age and the mass of the stars. The extinction\ntowards the sources ranges from $A_V = 3.6$ to 10.6. Stars more massive than 10\nM$_{\\odot}$ have reached the ZAMS. Their projected ZAMS spin rate distribution\nextends to 0.3 of the critical velocity; their mass-loss rates agree with those\nof other main-sequence OB stars. Stars with a mass in the range $3 <\nM/$M$_{\\odot} < 7$ are still on the pre-main sequence (PMS). Evolving their $v\n\\sin i$ to the ZAMS yields values up to $\\sim 0.6 v_{\\rm crit}$. For PMS stars\nwithout disks, we find tentative mass-loss rates up to\n$10^{-8.5}\\,$M$_{\\odot}$\\,yr$^{-1}$. We constrain the empirical location of the\nZAMS for massive ($10 < M/$M$_{\\odot} < 50$) stars. The ZAMS rotation rates for\nintermediate-mass stars are twice as high as for massive stars, suggesting that\nthe angular momentum gain processes differ between the two groups. The relation\nbetween the age and mass of the stars suggests a lag in the formation of more\nmassive stars relative to lower mass stars."
                },
                "authors": [
                    {
                        "name": "Frank Backs"
                    },
                    {
                        "name": "S. A. Brands"
                    },
                    {
                        "name": "M. C. Ramrez-Tannus"
                    },
                    {
                        "name": "A. R. Derkink"
                    },
                    {
                        "name": "A. de Koter"
                    },
                    {
                        "name": "J. Poorta"
                    },
                    {
                        "name": "J. Puls"
                    },
                    {
                        "name": "Lex Kaper"
                    }
                ],
                "author_detail": {
                    "name": "Lex Kaper"
                },
                "author": "Lex Kaper",
                "arxiv_comment": "37 pages, 31 figures, accepted for publication in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11713v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11707v1",
                "updated": "2024-08-21T15:31:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    31,
                    37,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T15:31:37Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    31,
                    37,
                    2,
                    234,
                    0
                ],
                "title": "biorecap: an R package for summarizing bioRxiv preprints with a local\n  LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "biorecap: an R package for summarizing bioRxiv preprints with a local\n  LLM"
                },
                "summary": "The establishment of bioRxiv facilitated the rapid adoption of preprints in\nthe life sciences, accelerating the dissemination of new research findings.\nHowever, the sheer volume of preprints published daily can be overwhelming,\nmaking it challenging for researchers to stay updated on the latest\ndevelopments. Here, I introduce biorecap, an R package that retrieves and\nsummarizes bioRxiv preprints using a large language model (LLM) running locally\non nearly any commodity laptop. biorecap leverages the ollamar package to\ninterface with the Ollama server and API endpoints, allowing users to prompt\nany local LLM available through Ollama. The package follows tidyverse\nconventions, enabling users to pipe the output of one function as input to\nanother. Additionally, biorecap provides a single wrapper function that\ngenerates a timestamped CSV file and HTML report containing short summaries of\nrecent preprints published in user-configurable subject areas. By combining the\nstrengths of LLMs with the flexibility and security of local execution,\nbiorecap represents an advancement in the tools available for managing the\ninformation overload in modern scientific research. The biorecap R package is\navailable on GitHub at https://github.com/stephenturner/biorecap under an\nopen-source (MIT) license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The establishment of bioRxiv facilitated the rapid adoption of preprints in\nthe life sciences, accelerating the dissemination of new research findings.\nHowever, the sheer volume of preprints published daily can be overwhelming,\nmaking it challenging for researchers to stay updated on the latest\ndevelopments. Here, I introduce biorecap, an R package that retrieves and\nsummarizes bioRxiv preprints using a large language model (LLM) running locally\non nearly any commodity laptop. biorecap leverages the ollamar package to\ninterface with the Ollama server and API endpoints, allowing users to prompt\nany local LLM available through Ollama. The package follows tidyverse\nconventions, enabling users to pipe the output of one function as input to\nanother. Additionally, biorecap provides a single wrapper function that\ngenerates a timestamped CSV file and HTML report containing short summaries of\nrecent preprints published in user-configurable subject areas. By combining the\nstrengths of LLMs with the flexibility and security of local execution,\nbiorecap represents an advancement in the tools available for managing the\ninformation overload in modern scientific research. The biorecap R package is\navailable on GitHub at https://github.com/stephenturner/biorecap under an\nopen-source (MIT) license."
                },
                "authors": [
                    {
                        "name": "Stephen D. Turner"
                    }
                ],
                "author_detail": {
                    "name": "Stephen D. Turner"
                },
                "author": "Stephen D. Turner",
                "arxiv_comment": "5 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.OT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11706v1",
                "updated": "2024-08-21T15:30:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    30,
                    35,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T15:30:35Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    30,
                    35,
                    2,
                    234,
                    0
                ],
                "title": "FRAP: Faithful and Realistic Text-to-Image Generation with Adaptive\n  Prompt Weighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FRAP: Faithful and Realistic Text-to-Image Generation with Adaptive\n  Prompt Weighting"
                },
                "summary": "Text-to-image (T2I) diffusion models have demonstrated impressive\ncapabilities in generating high-quality images given a text prompt. However,\nensuring the prompt-image alignment remains a considerable challenge, i.e.,\ngenerating images that faithfully align with the prompt's semantics. Recent\nworks attempt to improve the faithfulness by optimizing the latent code, which\npotentially could cause the latent code to go out-of-distribution and thus\nproduce unrealistic images. In this paper, we propose FRAP, a simple, yet\neffective approach based on adaptively adjusting the per-token prompt weights\nto improve prompt-image alignment and authenticity of the generated images. We\ndesign an online algorithm to adaptively update each token's weight\ncoefficient, which is achieved by minimizing a unified objective function that\nencourages object presence and the binding of object-modifier pairs. Through\nextensive evaluations, we show FRAP generates images with significantly higher\nprompt-image alignment to prompts from complex datasets, while having a lower\naverage latency compared to recent latent code optimization methods, e.g., 4\nseconds faster than D&B on the COCO-Subject dataset. Furthermore, through\nvisual comparisons and evaluation on the CLIP-IQA-Real metric, we show that\nFRAP not only improves prompt-image alignment but also generates more authentic\nimages with realistic appearances. We also explore combining FRAP with prompt\nrewriting LLM to recover their degraded prompt-image alignment, where we\nobserve improvements in both prompt-image alignment and image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) diffusion models have demonstrated impressive\ncapabilities in generating high-quality images given a text prompt. However,\nensuring the prompt-image alignment remains a considerable challenge, i.e.,\ngenerating images that faithfully align with the prompt's semantics. Recent\nworks attempt to improve the faithfulness by optimizing the latent code, which\npotentially could cause the latent code to go out-of-distribution and thus\nproduce unrealistic images. In this paper, we propose FRAP, a simple, yet\neffective approach based on adaptively adjusting the per-token prompt weights\nto improve prompt-image alignment and authenticity of the generated images. We\ndesign an online algorithm to adaptively update each token's weight\ncoefficient, which is achieved by minimizing a unified objective function that\nencourages object presence and the binding of object-modifier pairs. Through\nextensive evaluations, we show FRAP generates images with significantly higher\nprompt-image alignment to prompts from complex datasets, while having a lower\naverage latency compared to recent latent code optimization methods, e.g., 4\nseconds faster than D&B on the COCO-Subject dataset. Furthermore, through\nvisual comparisons and evaluation on the CLIP-IQA-Real metric, we show that\nFRAP not only improves prompt-image alignment but also generates more authentic\nimages with realistic appearances. We also explore combining FRAP with prompt\nrewriting LLM to recover their degraded prompt-image alignment, where we\nobserve improvements in both prompt-image alignment and image quality."
                },
                "authors": [
                    {
                        "name": "Liyao Jiang"
                    },
                    {
                        "name": "Negar Hassanpour"
                    },
                    {
                        "name": "Mohammad Salameh"
                    },
                    {
                        "name": "Mohan Sai Singamsetti"
                    },
                    {
                        "name": "Fengyu Sun"
                    },
                    {
                        "name": "Wei Lu"
                    },
                    {
                        "name": "Di Niu"
                    }
                ],
                "author_detail": {
                    "name": "Di Niu"
                },
                "author": "Di Niu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03862v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03862v2",
                "updated": "2024-08-21T15:23:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    23,
                    28,
                    2,
                    234,
                    0
                ],
                "published": "2024-04-05T02:27:09Z",
                "published_parsed": [
                    2024,
                    4,
                    5,
                    2,
                    27,
                    9,
                    4,
                    96,
                    0
                ],
                "title": "Verifiable by Design: Aligning Language Models to Quote from\n  Pre-Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verifiable by Design: Aligning Language Models to Quote from\n  Pre-Training Data"
                },
                "summary": "To trust the fluent generations of large language models (LLMs), humans must\nbe able to verify their correctness against trusted, external sources. Recent\nefforts, such as providing citations via retrieved documents or post-hoc\nprovenance, enhance verifiability but still provide no guarantees on their\ncorrectness. To address these limitations, we tackle the verifiability goal\nwith a different philosophy: trivializing the verification process by\ndeveloping models that quote verbatim statements from trusted sources in\npre-training data. We propose Quote-Tuning, and demonstrate it is feasible to\nalign LLMs to provide quoted statements from data memorized during\npre-training. The core of Quote-Tuning is a fast membership inference function\n(Marone and Van Durme, 2023) that efficiently verifies text against a trusted\ncorpus. We leverage this tool to design a reward function to quantify quotes in\nmodel responses, which is then used to create a dataset for preference\nlearning. Experimental results show that Quote-Tuning significantly increases\nverbatim quotes from high-quality pre-training documents by 55% to 130%\nrelative to un-tuned models while maintaining response quality. Quote-Tuning\nalso generalizes quoting to out-of-domain data, is applicable in different\ntasks, and provides additional benefits to truthfulness. Our method not only\nserves as a hassle-free method to increase quoting but also opens up avenues\nfor improving LLM trustworthiness through better verifiability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To trust the fluent generations of large language models (LLMs), humans must\nbe able to verify their correctness against trusted, external sources. Recent\nefforts, such as providing citations via retrieved documents or post-hoc\nprovenance, enhance verifiability but still provide no guarantees on their\ncorrectness. To address these limitations, we tackle the verifiability goal\nwith a different philosophy: trivializing the verification process by\ndeveloping models that quote verbatim statements from trusted sources in\npre-training data. We propose Quote-Tuning, and demonstrate it is feasible to\nalign LLMs to provide quoted statements from data memorized during\npre-training. The core of Quote-Tuning is a fast membership inference function\n(Marone and Van Durme, 2023) that efficiently verifies text against a trusted\ncorpus. We leverage this tool to design a reward function to quantify quotes in\nmodel responses, which is then used to create a dataset for preference\nlearning. Experimental results show that Quote-Tuning significantly increases\nverbatim quotes from high-quality pre-training documents by 55% to 130%\nrelative to un-tuned models while maintaining response quality. Quote-Tuning\nalso generalizes quoting to out-of-domain data, is applicable in different\ntasks, and provides additional benefits to truthfulness. Our method not only\nserves as a hassle-free method to increase quoting but also opens up avenues\nfor improving LLM trustworthiness through better verifiability."
                },
                "authors": [
                    {
                        "name": "Jingyu Zhang"
                    },
                    {
                        "name": "Marc Marone"
                    },
                    {
                        "name": "Tianjian Li"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    },
                    {
                        "name": "Daniel Khashabi"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Khashabi"
                },
                "author": "Daniel Khashabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03862v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03862v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10264v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10264v3",
                "updated": "2024-08-21T15:12:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    12,
                    37,
                    2,
                    234,
                    0
                ],
                "published": "2024-07-14T16:12:57Z",
                "published_parsed": [
                    2024,
                    7,
                    14,
                    16,
                    12,
                    57,
                    6,
                    196,
                    0
                ],
                "title": "What Makes and Breaks Safety Fine-tuning? A Mechanistic Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Makes and Breaks Safety Fine-tuning? A Mechanistic Study"
                },
                "summary": "Safety fine-tuning helps align Large Language Models (LLMs) with human\npreferences for their safe deployment. To better understand the underlying\nfactors that make models safe via safety fine-tuning, we design a synthetic\ndata generation framework that captures salient aspects of an unsafe input by\nmodeling the interaction between the task the model is asked to perform (e.g.,\n\"design\") versus the specific concepts the task is asked to be performed upon\n(e.g., a \"cycle\" vs. a \"bomb\"). Using this, we investigate three well-known\nsafety fine-tuning methods -- supervised safety fine-tuning, direct preference\noptimization, and unlearning -- and provide significant evidence demonstrating\nthat these methods minimally transform MLP weights to specifically align unsafe\ninputs into its weights' null space. This yields a clustering of inputs based\non whether the model deems them safe or not. Correspondingly, when an\nadversarial input (e.g., a jailbreak) is provided, its activations are closer\nto safer samples, leading to the model processing such an input as if it were\nsafe. We validate our findings, wherever possible, on real-world models --\nspecifically, Llama-2 7B and Llama-3 8B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety fine-tuning helps align Large Language Models (LLMs) with human\npreferences for their safe deployment. To better understand the underlying\nfactors that make models safe via safety fine-tuning, we design a synthetic\ndata generation framework that captures salient aspects of an unsafe input by\nmodeling the interaction between the task the model is asked to perform (e.g.,\n\"design\") versus the specific concepts the task is asked to be performed upon\n(e.g., a \"cycle\" vs. a \"bomb\"). Using this, we investigate three well-known\nsafety fine-tuning methods -- supervised safety fine-tuning, direct preference\noptimization, and unlearning -- and provide significant evidence demonstrating\nthat these methods minimally transform MLP weights to specifically align unsafe\ninputs into its weights' null space. This yields a clustering of inputs based\non whether the model deems them safe or not. Correspondingly, when an\nadversarial input (e.g., a jailbreak) is provided, its activations are closer\nto safer samples, leading to the model processing such an input as if it were\nsafe. We validate our findings, wherever possible, on real-world models --\nspecifically, Llama-2 7B and Llama-3 8B."
                },
                "authors": [
                    {
                        "name": "Samyak Jain"
                    },
                    {
                        "name": "Ekdeep Singh Lubana"
                    },
                    {
                        "name": "Kemal Oksuz"
                    },
                    {
                        "name": "Tom Joy"
                    },
                    {
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "name": "Amartya Sanyal"
                    },
                    {
                        "name": "Puneet K. Dokania"
                    }
                ],
                "author_detail": {
                    "name": "Puneet K. Dokania"
                },
                "author": "Puneet K. Dokania",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10264v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10264v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11681v1",
                "updated": "2024-08-21T15:01:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    1,
                    51,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T15:01:51Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    1,
                    51,
                    2,
                    234,
                    0
                ],
                "title": "Variational autoencoder inverse mapper for extraction of Compton form\n  factors: Benchmarks and conditional learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational autoencoder inverse mapper for extraction of Compton form\n  factors: Benchmarks and conditional learning"
                },
                "summary": "Deeply virtual exclusive scattering processes (DVES) serve as precise probes\nof nucleon quark and gluon distributions in coordinate space. These\ndistributions are derived from generalized parton distributions (GPDs) via\nFourier transform relative to proton momentum transfer. QCD factorization\ntheorems enable DVES to be parameterized by Compton form factors (CFFs), which\nare convolutions of GPDs with perturbatively calculable kernels. Accurate\nextraction of CFFs from DVCS, benefiting from interference with the\nBethe-Heitler (BH) process and a simpler final state structure, is essential\nfor inferring GPDs. This paper focuses on extracting CFFs from DVCS data using\na variational autoencoder inverse mapper (VAIM) and its constrained variant\n(C-VAIM). VAIM is shown to be consistent with Markov Chain Monte Carlo (MCMC)\nmethods in extracting multiple CFF solutions for given kinematics, while C-VAIM\neffectively captures correlations among CFFs across different kinematic values,\nproviding more constrained solutions. This study represents a crucial first\nstep towards a comprehensive analysis pipeline towards the extraction of GPDs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deeply virtual exclusive scattering processes (DVES) serve as precise probes\nof nucleon quark and gluon distributions in coordinate space. These\ndistributions are derived from generalized parton distributions (GPDs) via\nFourier transform relative to proton momentum transfer. QCD factorization\ntheorems enable DVES to be parameterized by Compton form factors (CFFs), which\nare convolutions of GPDs with perturbatively calculable kernels. Accurate\nextraction of CFFs from DVCS, benefiting from interference with the\nBethe-Heitler (BH) process and a simpler final state structure, is essential\nfor inferring GPDs. This paper focuses on extracting CFFs from DVCS data using\na variational autoencoder inverse mapper (VAIM) and its constrained variant\n(C-VAIM). VAIM is shown to be consistent with Markov Chain Monte Carlo (MCMC)\nmethods in extracting multiple CFF solutions for given kinematics, while C-VAIM\neffectively captures correlations among CFFs across different kinematic values,\nproviding more constrained solutions. This study represents a crucial first\nstep towards a comprehensive analysis pipeline towards the extraction of GPDs."
                },
                "authors": [
                    {
                        "name": "Fayaz Hossen"
                    },
                    {
                        "name": "Douglas Adams"
                    },
                    {
                        "name": "Joshua Bautista"
                    },
                    {
                        "name": "Yaohang Li"
                    },
                    {
                        "name": "Gia-Wei Chern"
                    },
                    {
                        "name": "Simonetta Liuti"
                    },
                    {
                        "name": "Marie Boer"
                    },
                    {
                        "name": "Marija Cuic"
                    },
                    {
                        "name": "Gari R. Goldstein"
                    },
                    {
                        "name": "Michael Engelhardt"
                    },
                    {
                        "name": "Huey-Wen Li"
                    }
                ],
                "author_detail": {
                    "name": "Huey-Wen Li"
                },
                "author": "Huey-Wen Li",
                "arxiv_comment": "12 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17758v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17758v2",
                "updated": "2024-08-21T14:56:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    14,
                    56,
                    0,
                    2,
                    234,
                    0
                ],
                "published": "2024-06-25T17:42:25Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    17,
                    42,
                    25,
                    1,
                    177,
                    0
                ],
                "title": "MotionBooth: Motion-Aware Customized Text-to-Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MotionBooth: Motion-Aware Customized Text-to-Video Generation"
                },
                "summary": "In this work, we present MotionBooth, an innovative framework designed for\nanimating customized subjects with precise control over both object and camera\nmovements. By leveraging a few images of a specific object, we efficiently\nfine-tune a text-to-video model to capture the object's shape and attributes\naccurately. Our approach presents subject region loss and video preservation\nloss to enhance the subject's learning performance, along with a subject token\ncross-attention loss to integrate the customized subject with motion control\nsignals. Additionally, we propose training-free techniques for managing subject\nand camera motions during inference. In particular, we utilize cross-attention\nmap manipulation to govern subject motion and introduce a novel latent shift\nmodule for camera movement control as well. MotionBooth excels in preserving\nthe appearance of subjects while simultaneously controlling the motions in\ngenerated videos. Extensive quantitative and qualitative evaluations\ndemonstrate the superiority and effectiveness of our method. Our project page\nis at https://jianzongwu.github.io/projects/motionbooth",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present MotionBooth, an innovative framework designed for\nanimating customized subjects with precise control over both object and camera\nmovements. By leveraging a few images of a specific object, we efficiently\nfine-tune a text-to-video model to capture the object's shape and attributes\naccurately. Our approach presents subject region loss and video preservation\nloss to enhance the subject's learning performance, along with a subject token\ncross-attention loss to integrate the customized subject with motion control\nsignals. Additionally, we propose training-free techniques for managing subject\nand camera motions during inference. In particular, we utilize cross-attention\nmap manipulation to govern subject motion and introduce a novel latent shift\nmodule for camera movement control as well. MotionBooth excels in preserving\nthe appearance of subjects while simultaneously controlling the motions in\ngenerated videos. Extensive quantitative and qualitative evaluations\ndemonstrate the superiority and effectiveness of our method. Our project page\nis at https://jianzongwu.github.io/projects/motionbooth"
                },
                "authors": [
                    {
                        "name": "Jianzong Wu"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Yanhong Zeng"
                    },
                    {
                        "name": "Jiangning Zhang"
                    },
                    {
                        "name": "Qianyu Zhou"
                    },
                    {
                        "name": "Yining Li"
                    },
                    {
                        "name": "Yunhai Tong"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "Project page at https://jianzongwu.github.io/projects/motionbooth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17758v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17758v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07595v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07595v4",
                "updated": "2024-08-21T14:51:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    14,
                    51,
                    6,
                    2,
                    234,
                    0
                ],
                "published": "2024-06-11T13:42:57Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    13,
                    42,
                    57,
                    1,
                    163,
                    0
                ],
                "title": "VulDetectBench: Evaluating the Deep Capability of Vulnerability\n  Detection with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VulDetectBench: Evaluating the Deep Capability of Vulnerability\n  Detection with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have training corpora containing large amounts\nof program code, greatly improving the model's code comprehension and\ngeneration capabilities. However, sound comprehensive research on detecting\nprogram vulnerabilities, a more specific task related to code, and evaluating\nthe performance of LLMs in this more specialized scenario is still lacking. To\naddress common challenges in vulnerability analysis, our study introduces a new\nbenchmark, VulDetectBench, specifically designed to assess the vulnerability\ndetection capabilities of LLMs. The benchmark comprehensively evaluates LLM's\nability to identify, classify, and locate vulnerabilities through five tasks of\nincreasing difficulty. We evaluate the performance of 17 models (both open- and\nclosed-source) and find that while existing models can achieve over 80%\naccuracy on tasks related to vulnerability identification and classification,\nthey still fall short on specific, more detailed vulnerability analysis tasks,\nwith less than 30% accuracy, making it difficult to provide valuable auxiliary\ninformation for professional vulnerability mining. Our benchmark effectively\nevaluates the capabilities of various LLMs at different levels in the specific\ntask of vulnerability detection, providing a foundation for future research and\nimprovements in this critical area of code security. VulDetectBench is publicly\navailable at https://github.com/Sweetaroo/VulDetectBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have training corpora containing large amounts\nof program code, greatly improving the model's code comprehension and\ngeneration capabilities. However, sound comprehensive research on detecting\nprogram vulnerabilities, a more specific task related to code, and evaluating\nthe performance of LLMs in this more specialized scenario is still lacking. To\naddress common challenges in vulnerability analysis, our study introduces a new\nbenchmark, VulDetectBench, specifically designed to assess the vulnerability\ndetection capabilities of LLMs. The benchmark comprehensively evaluates LLM's\nability to identify, classify, and locate vulnerabilities through five tasks of\nincreasing difficulty. We evaluate the performance of 17 models (both open- and\nclosed-source) and find that while existing models can achieve over 80%\naccuracy on tasks related to vulnerability identification and classification,\nthey still fall short on specific, more detailed vulnerability analysis tasks,\nwith less than 30% accuracy, making it difficult to provide valuable auxiliary\ninformation for professional vulnerability mining. Our benchmark effectively\nevaluates the capabilities of various LLMs at different levels in the specific\ntask of vulnerability detection, providing a foundation for future research and\nimprovements in this critical area of code security. VulDetectBench is publicly\navailable at https://github.com/Sweetaroo/VulDetectBench."
                },
                "authors": [
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Lang Gao"
                    },
                    {
                        "name": "Mingxin Yang"
                    },
                    {
                        "name": "Yu Xie"
                    },
                    {
                        "name": "Ping Chen"
                    },
                    {
                        "name": "Xiaojin Zhang"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07595v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07595v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02528v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02528v3",
                "updated": "2024-08-21T14:39:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    14,
                    39,
                    24,
                    2,
                    234,
                    0
                ],
                "published": "2024-05-04T00:32:20Z",
                "published_parsed": [
                    2024,
                    5,
                    4,
                    0,
                    32,
                    20,
                    5,
                    125,
                    0
                ],
                "title": "GigSense: An LLM-Infused Tool for Workers Collective Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GigSense: An LLM-Infused Tool for Workers Collective Intelligence"
                },
                "summary": "Collective intelligence among gig workers yields considerable advantages,\nincluding improved information exchange, deeper social bonds, and stronger\nadvocacy for better labor conditions. Especially as it enables workers to\ncollaboratively pinpoint shared challenges and devise optimal strategies for\naddressing these issues. However, enabling collective intelligence remains\nchallenging, as existing tools often overestimate gig workers' available time\nand uniformity in analytical reasoning. To overcome this, we introduce\nGigSense, a tool that leverages large language models alongside theories of\ncollective intelligence and sensemaking. GigSense enables gig workers to\nrapidly understand and address shared challenges effectively, irrespective of\ntheir diverse backgrounds. Our user study showed that GigSense users\noutperformed those using a control interface in problem identification and\ngenerated solutions more quickly and of higher quality, with better usability\nexperiences reported. GigSense not only empowers gig workers but also opens up\nnew possibilities for supporting workers more broadly, demonstrating the\npotential of large language model interfaces to enhance collective intelligence\nefforts in the evolving workplace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collective intelligence among gig workers yields considerable advantages,\nincluding improved information exchange, deeper social bonds, and stronger\nadvocacy for better labor conditions. Especially as it enables workers to\ncollaboratively pinpoint shared challenges and devise optimal strategies for\naddressing these issues. However, enabling collective intelligence remains\nchallenging, as existing tools often overestimate gig workers' available time\nand uniformity in analytical reasoning. To overcome this, we introduce\nGigSense, a tool that leverages large language models alongside theories of\ncollective intelligence and sensemaking. GigSense enables gig workers to\nrapidly understand and address shared challenges effectively, irrespective of\ntheir diverse backgrounds. Our user study showed that GigSense users\noutperformed those using a control interface in problem identification and\ngenerated solutions more quickly and of higher quality, with better usability\nexperiences reported. GigSense not only empowers gig workers but also opens up\nnew possibilities for supporting workers more broadly, demonstrating the\npotential of large language model interfaces to enhance collective intelligence\nefforts in the evolving workplace."
                },
                "authors": [
                    {
                        "name": "Kashif Imteyaz"
                    },
                    {
                        "name": "Claudia Flores-Saviaga"
                    },
                    {
                        "name": "Saiph Savage"
                    }
                ],
                "author_detail": {
                    "name": "Saiph Savage"
                },
                "author": "Saiph Savage",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02528v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02528v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10468v2",
                "updated": "2024-08-21T14:35:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    14,
                    35,
                    48,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-20T00:40:49Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    0,
                    40,
                    49,
                    1,
                    233,
                    0
                ],
                "title": "Tracing Privacy Leakage of Language Models to Training Data via Adjusted\n  Influence Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracing Privacy Leakage of Language Models to Training Data via Adjusted\n  Influence Functions"
                },
                "summary": "The responses generated by Large Language Models (LLMs) can include sensitive\ninformation from individuals and organizations, leading to potential privacy\nleakage. This work implements Influence Functions (IFs) to trace privacy\nleakage back to the training data, thereby mitigating privacy concerns of\nLanguage Models (LMs). However, we notice that current IFs struggle to\naccurately estimate the influence of tokens with large gradient norms,\npotentially overestimating their influence. When tracing the most influential\nsamples, this leads to frequently tracing back to samples with large gradient\nnorm tokens, overshadowing the actual most influential samples even if their\ninfluences are well estimated. To address this issue, we propose Heuristically\nAdjusted IF (HAIF), which reduces the weight of tokens with large gradient\nnorms, thereby significantly improving the accuracy of tracing the most\ninfluential samples. To establish easily obtained groundtruth for tracing\nprivacy leakage, we construct two datasets, PII-E and PII-CR, representing two\ndistinct scenarios: one with identical text in the model outputs and\npre-training data, and the other where models leverage their reasoning\nabilities to generate text divergent from pre-training data. HAIF significantly\nimproves tracing accuracy, enhancing it by 20.96\\% to 73.71\\% on the PII-E\ndataset and 3.21\\% to 45.93\\% on the PII-CR dataset, compared to the best SOTA\nIFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs\non real-world pretraining data CLUECorpus2020, demonstrating strong robustness\nregardless prompt and response lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The responses generated by Large Language Models (LLMs) can include sensitive\ninformation from individuals and organizations, leading to potential privacy\nleakage. This work implements Influence Functions (IFs) to trace privacy\nleakage back to the training data, thereby mitigating privacy concerns of\nLanguage Models (LMs). However, we notice that current IFs struggle to\naccurately estimate the influence of tokens with large gradient norms,\npotentially overestimating their influence. When tracing the most influential\nsamples, this leads to frequently tracing back to samples with large gradient\nnorm tokens, overshadowing the actual most influential samples even if their\ninfluences are well estimated. To address this issue, we propose Heuristically\nAdjusted IF (HAIF), which reduces the weight of tokens with large gradient\nnorms, thereby significantly improving the accuracy of tracing the most\ninfluential samples. To establish easily obtained groundtruth for tracing\nprivacy leakage, we construct two datasets, PII-E and PII-CR, representing two\ndistinct scenarios: one with identical text in the model outputs and\npre-training data, and the other where models leverage their reasoning\nabilities to generate text divergent from pre-training data. HAIF significantly\nimproves tracing accuracy, enhancing it by 20.96\\% to 73.71\\% on the PII-E\ndataset and 3.21\\% to 45.93\\% on the PII-CR dataset, compared to the best SOTA\nIFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs\non real-world pretraining data CLUECorpus2020, demonstrating strong robustness\nregardless prompt and response lengths."
                },
                "authors": [
                    {
                        "name": "Jinxin Liu"
                    },
                    {
                        "name": "Zao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zao Yang"
                },
                "author": "Zao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11650v1",
                "updated": "2024-08-21T14:24:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    14,
                    24,
                    4,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T14:24:04Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    14,
                    24,
                    4,
                    2,
                    234,
                    0
                ],
                "title": "CIPHER: Cybersecurity Intelligent Penetration-testing Helper for Ethical\n  Researcher",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CIPHER: Cybersecurity Intelligent Penetration-testing Helper for Ethical\n  Researcher"
                },
                "summary": "Penetration testing, a critical component of cybersecurity, typically\nrequires extensive time and effort to find vulnerabilities. Beginners in this\nfield often benefit from collaborative approaches with the community or\nexperts. To address this, we develop CIPHER (Cybersecurity Intelligent\nPenetration-testing Helper for Ethical Researchers), a large language model\nspecifically trained to assist in penetration testing tasks. We trained CIPHER\nusing over 300 high-quality write-ups of vulnerable machines, hacking\ntechniques, and documentation of open-source penetration testing tools.\nAdditionally, we introduced the Findings, Action, Reasoning, and Results (FARR)\nFlow augmentation, a novel method to augment penetration testing write-ups to\nestablish a fully automated pentesting simulation benchmark tailored for large\nlanguage models. This approach fills a significant gap in traditional\ncybersecurity Q\\&A benchmarks and provides a realistic and rigorous standard\nfor evaluating AI's technical knowledge, reasoning capabilities, and practical\nutility in dynamic penetration testing scenarios. In our assessments, CIPHER\nachieved the best overall performance in providing accurate suggestion\nresponses compared to other open-source penetration testing models of similar\nsize and even larger state-of-the-art models like Llama 3 70B and Qwen1.5 72B\nChat, particularly on insane difficulty machine setups. This demonstrates that\nthe current capabilities of general LLMs are insufficient for effectively\nguiding users through the penetration testing process. We also discuss the\npotential for improvement through scaling and the development of better\nbenchmarks using FARR Flow augmentation results. Our benchmark will be released\npublicly at https://github.com/ibndias/CIPHER.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Penetration testing, a critical component of cybersecurity, typically\nrequires extensive time and effort to find vulnerabilities. Beginners in this\nfield often benefit from collaborative approaches with the community or\nexperts. To address this, we develop CIPHER (Cybersecurity Intelligent\nPenetration-testing Helper for Ethical Researchers), a large language model\nspecifically trained to assist in penetration testing tasks. We trained CIPHER\nusing over 300 high-quality write-ups of vulnerable machines, hacking\ntechniques, and documentation of open-source penetration testing tools.\nAdditionally, we introduced the Findings, Action, Reasoning, and Results (FARR)\nFlow augmentation, a novel method to augment penetration testing write-ups to\nestablish a fully automated pentesting simulation benchmark tailored for large\nlanguage models. This approach fills a significant gap in traditional\ncybersecurity Q\\&A benchmarks and provides a realistic and rigorous standard\nfor evaluating AI's technical knowledge, reasoning capabilities, and practical\nutility in dynamic penetration testing scenarios. In our assessments, CIPHER\nachieved the best overall performance in providing accurate suggestion\nresponses compared to other open-source penetration testing models of similar\nsize and even larger state-of-the-art models like Llama 3 70B and Qwen1.5 72B\nChat, particularly on insane difficulty machine setups. This demonstrates that\nthe current capabilities of general LLMs are insufficient for effectively\nguiding users through the penetration testing process. We also discuss the\npotential for improvement through scaling and the development of better\nbenchmarks using FARR Flow augmentation results. Our benchmark will be released\npublicly at https://github.com/ibndias/CIPHER."
                },
                "authors": [
                    {
                        "name": "Derry Pratama"
                    },
                    {
                        "name": "Naufal Suryanto"
                    },
                    {
                        "name": "Andro Aprila Adiputra"
                    },
                    {
                        "name": "Thi-Thu-Huong Le"
                    },
                    {
                        "name": "Ahmada Yusril Kadiptya"
                    },
                    {
                        "name": "Muhammad Iqbal"
                    },
                    {
                        "name": "Howon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Howon Kim"
                },
                "author": "Howon Kim",
                "arxiv_comment": "28 pages, github available",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.02984v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.02984v2",
                "updated": "2024-08-21T13:55:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    13,
                    55,
                    37,
                    2,
                    234,
                    0
                ],
                "published": "2024-01-01T17:35:52Z",
                "published_parsed": [
                    2024,
                    1,
                    1,
                    17,
                    35,
                    52,
                    0,
                    1,
                    0
                ],
                "title": "Large Language Models in Mental Health Care: a Scoping Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models in Mental Health Care: a Scoping Review"
                },
                "summary": "The integration of large language models (LLMs) in mental health care is an\nemerging field. There is a need to systematically review the application\noutcomes and delineate the advantages and limitations in clinical settings.\nThis review aims to provide a comprehensive overview of the use of LLMs in\nmental health care, assessing their efficacy, challenges, and potential for\nfuture applications. A systematic search was conducted across multiple\ndatabases including PubMed, Web of Science, Google Scholar, arXiv, medRxiv, and\nPsyArXiv in November 2023. All forms of original research, peer-reviewed or\nnot, published or disseminated between October 1, 2019, and December 2, 2023,\nare included without language restrictions if they used LLMs developed after T5\nand directly addressed research questions in mental health care settings. From\nan initial pool of 313 articles, 34 met the inclusion criteria based on their\nrelevance to LLM application in mental health care and the robustness of\nreported outcomes. Diverse applications of LLMs in mental health care are\nidentified, including diagnosis, therapy, patient engagement enhancement, etc.\nKey challenges include data availability and reliability, nuanced handling of\nmental states, and effective evaluation methods. Despite successes in accuracy\nand accessibility improvement, gaps in clinical applicability and ethical\nconsiderations were evident, pointing to the need for robust data, standardized\nevaluations, and interdisciplinary collaboration. LLMs hold substantial promise\nfor enhancing mental health care. For their full potential to be realized,\nemphasis must be placed on developing robust datasets, development and\nevaluation frameworks, ethical guidelines, and interdisciplinary collaborations\nto address current limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language models (LLMs) in mental health care is an\nemerging field. There is a need to systematically review the application\noutcomes and delineate the advantages and limitations in clinical settings.\nThis review aims to provide a comprehensive overview of the use of LLMs in\nmental health care, assessing their efficacy, challenges, and potential for\nfuture applications. A systematic search was conducted across multiple\ndatabases including PubMed, Web of Science, Google Scholar, arXiv, medRxiv, and\nPsyArXiv in November 2023. All forms of original research, peer-reviewed or\nnot, published or disseminated between October 1, 2019, and December 2, 2023,\nare included without language restrictions if they used LLMs developed after T5\nand directly addressed research questions in mental health care settings. From\nan initial pool of 313 articles, 34 met the inclusion criteria based on their\nrelevance to LLM application in mental health care and the robustness of\nreported outcomes. Diverse applications of LLMs in mental health care are\nidentified, including diagnosis, therapy, patient engagement enhancement, etc.\nKey challenges include data availability and reliability, nuanced handling of\nmental states, and effective evaluation methods. Despite successes in accuracy\nand accessibility improvement, gaps in clinical applicability and ethical\nconsiderations were evident, pointing to the need for robust data, standardized\nevaluations, and interdisciplinary collaboration. LLMs hold substantial promise\nfor enhancing mental health care. For their full potential to be realized,\nemphasis must be placed on developing robust datasets, development and\nevaluation frameworks, ethical guidelines, and interdisciplinary collaborations\nto address current limitations."
                },
                "authors": [
                    {
                        "name": "Yining Hua"
                    },
                    {
                        "name": "Fenglin Liu"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Zehan Li"
                    },
                    {
                        "name": "Hongbin Na"
                    },
                    {
                        "name": "Yi-han Sheu"
                    },
                    {
                        "name": "Peilin Zhou"
                    },
                    {
                        "name": "Lauren V. Moran"
                    },
                    {
                        "name": "Sophia Ananiadou"
                    },
                    {
                        "name": "Andrew Beam"
                    },
                    {
                        "name": "John Torous"
                    }
                ],
                "author_detail": {
                    "name": "John Torous"
                },
                "author": "John Torous",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.02984v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.02984v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11623v1",
                "updated": "2024-08-21T13:48:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    13,
                    48,
                    0,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T13:48:00Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    13,
                    48,
                    0,
                    2,
                    234,
                    0
                ],
                "title": "End-to-End Cost-Effective Incentive Recommendation under Budget\n  Constraint with Uplift Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Cost-Effective Incentive Recommendation under Budget\n  Constraint with Uplift Modeling"
                },
                "summary": "In modern online platforms, incentives are essential factors that enhance\nuser engagement and increase platform revenue. Over recent years, uplift\nmodeling has been introduced as a strategic approach to assign incentives to\nindividual customers. Especially in many real-world applications, online\nplatforms can only incentivize customers with specific budget constraints. This\nproblem can be reformulated as the multi-choice knapsack problem. This\noptimization aims to select the optimal incentive for each customer to maximize\nthe return on investment. Recent works in this field frequently tackle the\nbudget allocation problem using a two-stage approach. However, this solution is\nconfronted with the following challenges: (1) The causal inference methods\noften ignore the domain knowledge in online marketing, where the expected\nresponse curve of a customer should be monotonic and smooth as the incentive\nincreases. (2) An optimality gap between the two stages results in inferior\nsub-optimal allocation performance due to the loss of the incentive\nrecommendation information for the uplift prediction under the limited budget\nconstraint. To address these challenges, we propose a novel End-to-End\nCost-Effective Incentive Recommendation (E3IR) model under budget constraints.\nSpecifically, our methods consist of two modules, i.e., the uplift prediction\nmodule and the differentiable allocation module. In the uplift prediction\nmodule, we construct prediction heads to capture the incremental improvement\nbetween adjacent treatments with the marketing domain constraints (i.e.,\nmonotonic and smooth). We incorporate integer linear programming (ILP) as a\ndifferentiable layer input in the allocation module. Furthermore, we conduct\nextensive experiments on public and real product datasets, demonstrating that\nour E3IR improves allocation performance compared to existing two-stage\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern online platforms, incentives are essential factors that enhance\nuser engagement and increase platform revenue. Over recent years, uplift\nmodeling has been introduced as a strategic approach to assign incentives to\nindividual customers. Especially in many real-world applications, online\nplatforms can only incentivize customers with specific budget constraints. This\nproblem can be reformulated as the multi-choice knapsack problem. This\noptimization aims to select the optimal incentive for each customer to maximize\nthe return on investment. Recent works in this field frequently tackle the\nbudget allocation problem using a two-stage approach. However, this solution is\nconfronted with the following challenges: (1) The causal inference methods\noften ignore the domain knowledge in online marketing, where the expected\nresponse curve of a customer should be monotonic and smooth as the incentive\nincreases. (2) An optimality gap between the two stages results in inferior\nsub-optimal allocation performance due to the loss of the incentive\nrecommendation information for the uplift prediction under the limited budget\nconstraint. To address these challenges, we propose a novel End-to-End\nCost-Effective Incentive Recommendation (E3IR) model under budget constraints.\nSpecifically, our methods consist of two modules, i.e., the uplift prediction\nmodule and the differentiable allocation module. In the uplift prediction\nmodule, we construct prediction heads to capture the incremental improvement\nbetween adjacent treatments with the marketing domain constraints (i.e.,\nmonotonic and smooth). We incorporate integer linear programming (ILP) as a\ndifferentiable layer input in the allocation module. Furthermore, we conduct\nextensive experiments on public and real product datasets, demonstrating that\nour E3IR improves allocation performance compared to existing two-stage\napproaches."
                },
                "authors": [
                    {
                        "name": "Zexu Sun"
                    },
                    {
                        "name": "Hao Yang an Dugang Liu"
                    },
                    {
                        "name": "Yunpeng Weng"
                    },
                    {
                        "name": "Xing Tang"
                    },
                    {
                        "name": "Xiuqiang He"
                    }
                ],
                "author_detail": {
                    "name": "Xiuqiang He"
                },
                "author": "Xiuqiang He",
                "arxiv_comment": "Accepted by RecSys 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11614v1",
                "updated": "2024-08-21T13:42:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    13,
                    42,
                    5,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T13:42:05Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    13,
                    42,
                    5,
                    2,
                    234,
                    0
                ],
                "title": "How the QCD trace anomaly behaves at the core of twin stars?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How the QCD trace anomaly behaves at the core of twin stars?"
                },
                "summary": "We investigate the behavior of the dense and cold (normalized) QCD trace\nanomaly, $\\Delta$, in the interior of twin neutron stars (obtained from several\nsets of equations of state in agreement with modern compact-star and\nmultimessenger data) satisfying static and dynamic stability conditions. We\nscan this parameter space in order to look for effects caused by the presence\nof a strong first-order phase transition connecting hadron and quark phases by\nmeans of a Maxwell construction. We found robustly that $\\Delta$ suffers an\nabrupt decrease around the transition point, even reaching large negative\nvalues ($\\Delta\\simeq-0.35$), in marked contrast to current studies pointing\nout a smooth behavior with $\\Delta\\gtrsim 0$ at all densities. Besides, we\ncharacterize the behavior of conformal factor, $d_{c}$, in twin stars for which\nwe perform comparisons with theoretical constraints, e.g. from Bayesian studies\nadjusted to agree with pQCD. All this allows us to infer and hypothesize\nmodifications in the strong QCD coupling in dense nuclear matter with a strong\nthermodynamic discontinuity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the behavior of the dense and cold (normalized) QCD trace\nanomaly, $\\Delta$, in the interior of twin neutron stars (obtained from several\nsets of equations of state in agreement with modern compact-star and\nmultimessenger data) satisfying static and dynamic stability conditions. We\nscan this parameter space in order to look for effects caused by the presence\nof a strong first-order phase transition connecting hadron and quark phases by\nmeans of a Maxwell construction. We found robustly that $\\Delta$ suffers an\nabrupt decrease around the transition point, even reaching large negative\nvalues ($\\Delta\\simeq-0.35$), in marked contrast to current studies pointing\nout a smooth behavior with $\\Delta\\gtrsim 0$ at all densities. Besides, we\ncharacterize the behavior of conformal factor, $d_{c}$, in twin stars for which\nwe perform comparisons with theoretical constraints, e.g. from Bayesian studies\nadjusted to agree with pQCD. All this allows us to infer and hypothesize\nmodifications in the strong QCD coupling in dense nuclear matter with a strong\nthermodynamic discontinuity."
                },
                "authors": [
                    {
                        "name": "Jos C. Jimnez"
                    },
                    {
                        "name": "Lucas Lazzari"
                    },
                    {
                        "name": "Victor P. Gonalves"
                    }
                ],
                "author_detail": {
                    "name": "Victor P. Gonalves"
                },
                "author": "Victor P. Gonalves",
                "arxiv_comment": "18 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-lat",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.09835v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.09835v5",
                "updated": "2024-08-21T13:36:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    13,
                    36,
                    30,
                    2,
                    234,
                    0
                ],
                "published": "2023-11-16T12:03:21Z",
                "published_parsed": [
                    2023,
                    11,
                    16,
                    12,
                    3,
                    21,
                    3,
                    320,
                    0
                ],
                "title": "ML-Bench: Evaluating Large Language Models and Agents for Machine\n  Learning Tasks on Repository-Level Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-Bench: Evaluating Large Language Models and Agents for Machine\n  Learning Tasks on Repository-Level Code"
                },
                "summary": "Despite Large Language Models (LLMs) like GPT-4 achieving impressive results\nin function-level code generation, they struggle with repository-scale code\nunderstanding (e.g., coming up with the right arguments for calling routines),\nrequiring a deeper comprehension of complex file interactions. Also, recently,\npeople have developed LLM agents that attempt to interact with repository code\n(e.g., compiling and evaluating its execution), prompting the need to evaluate\ntheir performance. These gaps have motivated our development of ML-Bench, a\nbenchmark rooted in real-world programming applications that leverage existing\ncode repositories to perform tasks. Addressing the need for LLMs to interpret\nlong code contexts and translate instructions into precise, executable scripts,\nML-Bench encompasses annotated 9,641 examples across 18 GitHub repositories,\nchallenging LLMs to accommodate user-specified arguments and documentation\nintricacies effectively. To evaluate both LLMs and AI agents, two setups are\nemployed: ML-LLM-Bench for assessing LLMs' text-to-code conversion within a\npredefined deployment environment, and ML-Agent-Bench for testing autonomous\nagents in an end-to-end task execution within a Linux sandbox environment. Our\nfindings indicate that while GPT-4o leads with a Pass@5 rate surpassing 50%,\nthere remains significant scope for improvement, highlighted by issues such as\nhallucinated outputs and difficulties with bash script generation. Notably, in\nthe more demanding ML-Agent-Bench, GPT-4o achieves a 76.47% success rate,\nreflecting the efficacy of iterative action and feedback in complex task\nresolution. Our code, dataset, and models are available at\nhttps://github.com/gersteinlab/ML-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite Large Language Models (LLMs) like GPT-4 achieving impressive results\nin function-level code generation, they struggle with repository-scale code\nunderstanding (e.g., coming up with the right arguments for calling routines),\nrequiring a deeper comprehension of complex file interactions. Also, recently,\npeople have developed LLM agents that attempt to interact with repository code\n(e.g., compiling and evaluating its execution), prompting the need to evaluate\ntheir performance. These gaps have motivated our development of ML-Bench, a\nbenchmark rooted in real-world programming applications that leverage existing\ncode repositories to perform tasks. Addressing the need for LLMs to interpret\nlong code contexts and translate instructions into precise, executable scripts,\nML-Bench encompasses annotated 9,641 examples across 18 GitHub repositories,\nchallenging LLMs to accommodate user-specified arguments and documentation\nintricacies effectively. To evaluate both LLMs and AI agents, two setups are\nemployed: ML-LLM-Bench for assessing LLMs' text-to-code conversion within a\npredefined deployment environment, and ML-Agent-Bench for testing autonomous\nagents in an end-to-end task execution within a Linux sandbox environment. Our\nfindings indicate that while GPT-4o leads with a Pass@5 rate surpassing 50%,\nthere remains significant scope for improvement, highlighted by issues such as\nhallucinated outputs and difficulties with bash script generation. Notably, in\nthe more demanding ML-Agent-Bench, GPT-4o achieves a 76.47% success rate,\nreflecting the efficacy of iterative action and feedback in complex task\nresolution. Our code, dataset, and models are available at\nhttps://github.com/gersteinlab/ML-bench."
                },
                "authors": [
                    {
                        "name": "Xiangru Tang"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Yanjun Shao"
                    },
                    {
                        "name": "Junjie Lu"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Zexuan Deng"
                    },
                    {
                        "name": "Helan Hu"
                    },
                    {
                        "name": "Kaikai An"
                    },
                    {
                        "name": "Ruijun Huang"
                    },
                    {
                        "name": "Shuzheng Si"
                    },
                    {
                        "name": "Sheng Chen"
                    },
                    {
                        "name": "Haozhe Zhao"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Zhiwei Jiang"
                    },
                    {
                        "name": "Baobao Chang"
                    },
                    {
                        "name": "Yin Fang"
                    },
                    {
                        "name": "Yujia Qin"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Yilun Zhao"
                    },
                    {
                        "name": "Arman Cohan"
                    },
                    {
                        "name": "Mark Gerstein"
                    }
                ],
                "author_detail": {
                    "name": "Mark Gerstein"
                },
                "author": "Mark Gerstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.09835v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.09835v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11609v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11609v1",
                "updated": "2024-08-21T13:34:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    13,
                    34,
                    29,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T13:34:29Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    13,
                    34,
                    29,
                    2,
                    234,
                    0
                ],
                "title": "Xinyu: An Efficient LLM-based System for Commentary Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Xinyu: An Efficient LLM-based System for Commentary Generation"
                },
                "summary": "Commentary provides readers with a deep understanding of events by presenting\ndiverse arguments and evidence. However, creating commentary is a\ntime-consuming task, even for skilled commentators. Large language models\n(LLMs) have simplified the process of natural language generation, but their\ndirect application in commentary creation still faces challenges due to unique\ntask requirements. These requirements can be categorized into two levels: 1)\nfundamental requirements, which include creating well-structured and logically\nconsistent narratives, and 2) advanced requirements, which involve generating\nquality arguments and providing convincing evidence. In this paper, we\nintroduce Xinyu, an efficient LLM-based system designed to assist commentators\nin generating Chinese commentaries. To meet the fundamental requirements, we\ndeconstruct the generation process into sequential steps, proposing targeted\nstrategies and supervised fine-tuning (SFT) for each step. To address the\nadvanced requirements, we present an argument ranking model for arguments and\nestablish a comprehensive evidence database that includes up-to-date events and\nclassic books, thereby strengthening the substantiation of the evidence with\nretrieval augmented generation (RAG) technology. To evaluate the generated\ncommentaries more fairly, corresponding to the two-level requirements, we\nintroduce a comprehensive evaluation metric that considers five distinct\nperspectives in commentary generation. Our experiments confirm the\neffectiveness of our proposed system. We also observe a significant increase in\nthe efficiency of commentators in real-world scenarios, with the average time\nspent on creating a commentary dropping from 4 hours to 20 minutes.\nImportantly, such an increase in efficiency does not compromise the quality of\nthe commentaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commentary provides readers with a deep understanding of events by presenting\ndiverse arguments and evidence. However, creating commentary is a\ntime-consuming task, even for skilled commentators. Large language models\n(LLMs) have simplified the process of natural language generation, but their\ndirect application in commentary creation still faces challenges due to unique\ntask requirements. These requirements can be categorized into two levels: 1)\nfundamental requirements, which include creating well-structured and logically\nconsistent narratives, and 2) advanced requirements, which involve generating\nquality arguments and providing convincing evidence. In this paper, we\nintroduce Xinyu, an efficient LLM-based system designed to assist commentators\nin generating Chinese commentaries. To meet the fundamental requirements, we\ndeconstruct the generation process into sequential steps, proposing targeted\nstrategies and supervised fine-tuning (SFT) for each step. To address the\nadvanced requirements, we present an argument ranking model for arguments and\nestablish a comprehensive evidence database that includes up-to-date events and\nclassic books, thereby strengthening the substantiation of the evidence with\nretrieval augmented generation (RAG) technology. To evaluate the generated\ncommentaries more fairly, corresponding to the two-level requirements, we\nintroduce a comprehensive evaluation metric that considers five distinct\nperspectives in commentary generation. Our experiments confirm the\neffectiveness of our proposed system. We also observe a significant increase in\nthe efficiency of commentators in real-world scenarios, with the average time\nspent on creating a commentary dropping from 4 hours to 20 minutes.\nImportantly, such an increase in efficiency does not compromise the quality of\nthe commentaries."
                },
                "authors": [
                    {
                        "name": "Yiquan Wu"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Chenyang Xi"
                    },
                    {
                        "name": "Yu Yu"
                    },
                    {
                        "name": "Pengyu Wang"
                    },
                    {
                        "name": "Yifei Liu"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Haiying Deng"
                    },
                    {
                        "name": "Zhiyu Li"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Zhonghao Wang"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Yi Luo"
                    },
                    {
                        "name": "Mingchuan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mingchuan Yang"
                },
                "author": "Mingchuan Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11609v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11609v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14795v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14795v4",
                "updated": "2024-08-21T13:32:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    13,
                    32,
                    18,
                    2,
                    234,
                    0
                ],
                "published": "2024-04-23T07:19:20Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    7,
                    19,
                    20,
                    1,
                    114,
                    0
                ],
                "title": "Watch Out for Your Guidance on Generation! Exploring Conditional\n  Backdoor Attacks against Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watch Out for Your Guidance on Generation! Exploring Conditional\n  Backdoor Attacks against Large Language Models"
                },
                "summary": "Mainstream backdoor attacks on large language models (LLMs) typically set a\nfixed trigger in the input instance and specific responses for triggered\nqueries. However, the fixed trigger setting (e.g., unusual words) may be easily\ndetected by human detection, limiting the effectiveness and practicality in\nreal-world scenarios. To enhance the stealthiness of backdoor activation, we\npresent a new poisoning paradigm against LLMs triggered by specifying\ngeneration conditions, which are commonly adopted strategies by users during\nmodel inference. The poisoned model performs normally for output under\nnormal/other generation conditions, while becomes harmful for output under\ntarget generation conditions. To achieve this objective, we introduce BrieFool,\nan efficient attack framework. It leverages the characteristics of generation\nconditions by efficient instruction sampling and poisoning data generation,\nthereby influencing the behavior of LLMs under target conditions. Our attack\ncan be generally divided into two types with different targets: Safety\nunalignment attack and Ability degradation attack. Our extensive experiments\ndemonstrate that BrieFool is effective across safety domains and ability\ndomains, achieving higher success rates than baseline methods, with 94.3 % on\nGPT-3.5-turbo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainstream backdoor attacks on large language models (LLMs) typically set a\nfixed trigger in the input instance and specific responses for triggered\nqueries. However, the fixed trigger setting (e.g., unusual words) may be easily\ndetected by human detection, limiting the effectiveness and practicality in\nreal-world scenarios. To enhance the stealthiness of backdoor activation, we\npresent a new poisoning paradigm against LLMs triggered by specifying\ngeneration conditions, which are commonly adopted strategies by users during\nmodel inference. The poisoned model performs normally for output under\nnormal/other generation conditions, while becomes harmful for output under\ntarget generation conditions. To achieve this objective, we introduce BrieFool,\nan efficient attack framework. It leverages the characteristics of generation\nconditions by efficient instruction sampling and poisoning data generation,\nthereby influencing the behavior of LLMs under target conditions. Our attack\ncan be generally divided into two types with different targets: Safety\nunalignment attack and Ability degradation attack. Our extensive experiments\ndemonstrate that BrieFool is effective across safety domains and ability\ndomains, achieving higher success rates than baseline methods, with 94.3 % on\nGPT-3.5-turbo"
                },
                "authors": [
                    {
                        "name": "Jiaming He"
                    },
                    {
                        "name": "Wenbo Jiang"
                    },
                    {
                        "name": "Guanyu Hou"
                    },
                    {
                        "name": "Wenshu Fan"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Hongwei Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongwei Li"
                },
                "author": "Hongwei Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14795v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14795v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11605v1",
                "updated": "2024-08-21T13:27:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    13,
                    27,
                    17,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T13:27:17Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    13,
                    27,
                    17,
                    2,
                    234,
                    0
                ],
                "title": "Optimizing QoS in HD Map Updates: Cross-Layer Multi-Agent with\n  Hierarchical and Independent Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing QoS in HD Map Updates: Cross-Layer Multi-Agent with\n  Hierarchical and Independent Learning"
                },
                "summary": "The data collected by autonomous vehicle (AV) sensors such as LiDAR and\ncameras is crucial for creating high-definition (HD) maps to provide higher\naccuracy and enable a higher level of automation. Nevertheless, offloading this\nlarge volume of raw data to edge servers leads to increased latency due to\nnetwork congestion in highly dense environments such as Vehicular Adhoc\nnetworks (VANET). To address this challenge, researchers have focused on the\ndynamic allocation of minimum contention window (CWmin) value. While this\napproach could be sufficient for fairness, it might not be adequate for\nprioritizing different services, as it also involves other parameters such as\nmaximum contention window (CWmax) and infer-frame space number (IFSn). In\nresponse to this, we extend the scope of previous solutions to include the\ncontrol of not only CWmin but also the adjustment of two other parameters in\nthe standard IEEE802.11: CWmax and IFSn, alongside waiting transmission time.\nTo achieve this, we introduced a methodology involving a cross-layer solution\nbetween the application and MAC layers. Additionally, we utilised multi-agent\ntechniques, emphasising a hierarchical structure and independent learning (IL)\nto improve latency to efficiently handle map updates while interacting with\nmultiple services. This approach demonstrated an improvement in latency against\nthe standard IEEE802.11p EDCA by $31\\%$, $49\\%$, $87.3\\%$, and $64\\%$ for\nVoice, Video, HD Map, and Best-effort, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The data collected by autonomous vehicle (AV) sensors such as LiDAR and\ncameras is crucial for creating high-definition (HD) maps to provide higher\naccuracy and enable a higher level of automation. Nevertheless, offloading this\nlarge volume of raw data to edge servers leads to increased latency due to\nnetwork congestion in highly dense environments such as Vehicular Adhoc\nnetworks (VANET). To address this challenge, researchers have focused on the\ndynamic allocation of minimum contention window (CWmin) value. While this\napproach could be sufficient for fairness, it might not be adequate for\nprioritizing different services, as it also involves other parameters such as\nmaximum contention window (CWmax) and infer-frame space number (IFSn). In\nresponse to this, we extend the scope of previous solutions to include the\ncontrol of not only CWmin but also the adjustment of two other parameters in\nthe standard IEEE802.11: CWmax and IFSn, alongside waiting transmission time.\nTo achieve this, we introduced a methodology involving a cross-layer solution\nbetween the application and MAC layers. Additionally, we utilised multi-agent\ntechniques, emphasising a hierarchical structure and independent learning (IL)\nto improve latency to efficiently handle map updates while interacting with\nmultiple services. This approach demonstrated an improvement in latency against\nthe standard IEEE802.11p EDCA by $31\\%$, $49\\%$, $87.3\\%$, and $64\\%$ for\nVoice, Video, HD Map, and Best-effort, respectively."
                },
                "authors": [
                    {
                        "name": "Jeffrey Redondo"
                    },
                    {
                        "name": "Nauman Aslam"
                    },
                    {
                        "name": "Juan Zhang"
                    },
                    {
                        "name": "Zhenhui Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenhui Yuan"
                },
                "author": "Zhenhui Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11599v1",
                "updated": "2024-08-21T13:11:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    13,
                    11,
                    3,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T13:11:03Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    13,
                    11,
                    3,
                    2,
                    234,
                    0
                ],
                "title": "Cause-Aware Empathetic Response Generation via Chain-of-Thought\n  Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cause-Aware Empathetic Response Generation via Chain-of-Thought\n  Fine-Tuning"
                },
                "summary": "Empathetic response generation endows agents with the capability to\ncomprehend dialogue contexts and react to expressed emotions. Previous works\npredominantly focus on leveraging the speaker's emotional labels, but ignore\nthe importance of emotion cause reasoning in empathetic response generation,\nwhich hinders the model's capacity for further affective understanding and\ncognitive inference. In this paper, we propose a cause-aware empathetic\ngeneration approach by integrating emotions and causes through a well-designed\nChain-of-Thought (CoT) prompt on Large Language Models (LLMs). Our approach can\ngreatly promote LLMs' performance of empathy by instruction tuning and\nenhancing the role awareness of an empathetic listener in the prompt.\nAdditionally, we propose to incorporate cause-oriented external knowledge from\nCOMET into the prompt, which improves the diversity of generation and\nalleviates conflicts between internal and external knowledge at the same time.\nExperimental results on the benchmark dataset demonstrate that our approach on\nLLaMA-7b achieves state-of-the-art performance in both automatic and human\nevaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empathetic response generation endows agents with the capability to\ncomprehend dialogue contexts and react to expressed emotions. Previous works\npredominantly focus on leveraging the speaker's emotional labels, but ignore\nthe importance of emotion cause reasoning in empathetic response generation,\nwhich hinders the model's capacity for further affective understanding and\ncognitive inference. In this paper, we propose a cause-aware empathetic\ngeneration approach by integrating emotions and causes through a well-designed\nChain-of-Thought (CoT) prompt on Large Language Models (LLMs). Our approach can\ngreatly promote LLMs' performance of empathy by instruction tuning and\nenhancing the role awareness of an empathetic listener in the prompt.\nAdditionally, we propose to incorporate cause-oriented external knowledge from\nCOMET into the prompt, which improves the diversity of generation and\nalleviates conflicts between internal and external knowledge at the same time.\nExperimental results on the benchmark dataset demonstrate that our approach on\nLLaMA-7b achieves state-of-the-art performance in both automatic and human\nevaluations."
                },
                "authors": [
                    {
                        "name": "Xinhao Chen"
                    },
                    {
                        "name": "Chong Yang"
                    },
                    {
                        "name": "Man Lan"
                    },
                    {
                        "name": "Li Cai"
                    },
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Tu Hu"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Aimin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Aimin Zhou"
                },
                "author": "Aimin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11587v1",
                "updated": "2024-08-21T12:50:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    12,
                    50,
                    23,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T12:50:23Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    12,
                    50,
                    23,
                    2,
                    234,
                    0
                ],
                "title": "Large Language Models are Good Attackers: Efficient and Stealthy Textual\n  Backdoor Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are Good Attackers: Efficient and Stealthy Textual\n  Backdoor Attacks"
                },
                "summary": "With the burgeoning advancements in the field of natural language processing\n(NLP), the demand for training data has increased significantly. To save costs,\nit has become common for users and businesses to outsource the labor-intensive\ntask of data collection to third-party entities. Unfortunately, recent research\nhas unveiled the inherent risk associated with this practice, particularly in\nexposing NLP systems to potential backdoor attacks. Specifically, these attacks\nenable malicious control over the behavior of a trained model by poisoning a\nsmall portion of the training data. Unlike backdoor attacks in computer vision,\ntextual backdoor attacks impose stringent requirements for attack stealthiness.\nHowever, existing attack methods meet significant trade-off between\neffectiveness and stealthiness, largely due to the high information entropy\ninherent in textual data. In this paper, we introduce the Efficient and\nStealthy Textual backdoor attack method, EST-Bad, leveraging Large Language\nModels (LLMs). Our EST-Bad encompasses three core strategies: optimizing the\ninherent flaw of models as the trigger, stealthily injecting triggers with\nLLMs, and meticulously selecting the most impactful samples for backdoor\ninjection. Through the integration of these techniques, EST-Bad demonstrates an\nefficient achievement of competitive attack performance while maintaining\nsuperior stealthiness compared to prior methods across various text classifier\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the burgeoning advancements in the field of natural language processing\n(NLP), the demand for training data has increased significantly. To save costs,\nit has become common for users and businesses to outsource the labor-intensive\ntask of data collection to third-party entities. Unfortunately, recent research\nhas unveiled the inherent risk associated with this practice, particularly in\nexposing NLP systems to potential backdoor attacks. Specifically, these attacks\nenable malicious control over the behavior of a trained model by poisoning a\nsmall portion of the training data. Unlike backdoor attacks in computer vision,\ntextual backdoor attacks impose stringent requirements for attack stealthiness.\nHowever, existing attack methods meet significant trade-off between\neffectiveness and stealthiness, largely due to the high information entropy\ninherent in textual data. In this paper, we introduce the Efficient and\nStealthy Textual backdoor attack method, EST-Bad, leveraging Large Language\nModels (LLMs). Our EST-Bad encompasses three core strategies: optimizing the\ninherent flaw of models as the trigger, stealthily injecting triggers with\nLLMs, and meticulously selecting the most impactful samples for backdoor\ninjection. Through the integration of these techniques, EST-Bad demonstrates an\nefficient achievement of competitive attack performance while maintaining\nsuperior stealthiness compared to prior methods across various text classifier\ndatasets."
                },
                "authors": [
                    {
                        "name": "Ziqiang Li"
                    },
                    {
                        "name": "Yueqi Zeng"
                    },
                    {
                        "name": "Pengfei Xia"
                    },
                    {
                        "name": "Lei Liu"
                    },
                    {
                        "name": "Zhangjie Fu"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07666v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07666v3",
                "updated": "2024-08-21T12:47:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    12,
                    47,
                    31,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-14T16:58:48Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    16,
                    58,
                    48,
                    2,
                    227,
                    0
                ],
                "title": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories,\n  Applications and Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories,\n  Applications and Opportunities"
                },
                "summary": "Model merging is an efficient empowerment technique in the machine learning\ncommunity that does not require the collection of raw training data and does\nnot require expensive computation. As model merging becomes increasingly\nprevalent across various fields, it is crucial to understand the available\nmodel merging techniques comprehensively. However, there is a significant gap\nin the literature regarding a systematic and thorough review of these\ntechniques. This survey provides a comprehensive overview of model merging\nmethods and theories, their applications in various domains and settings, and\nfuture research directions. Specifically, we first propose a new taxonomic\napproach that exhaustively discusses existing model merging methods. Secondly,\nwe discuss the application of model merging techniques in large language\nmodels, multimodal large language models, and 10+ machine learning subfields,\nincluding continual learning, multi-task learning, few-shot learning, etc.\nFinally, we highlight the remaining challenges of model merging and discuss\nfuture research directions. A comprehensive list of papers about model merging\nis available at\n\\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging is an efficient empowerment technique in the machine learning\ncommunity that does not require the collection of raw training data and does\nnot require expensive computation. As model merging becomes increasingly\nprevalent across various fields, it is crucial to understand the available\nmodel merging techniques comprehensively. However, there is a significant gap\nin the literature regarding a systematic and thorough review of these\ntechniques. This survey provides a comprehensive overview of model merging\nmethods and theories, their applications in various domains and settings, and\nfuture research directions. Specifically, we first propose a new taxonomic\napproach that exhaustively discusses existing model merging methods. Secondly,\nwe discuss the application of model merging techniques in large language\nmodels, multimodal large language models, and 10+ machine learning subfields,\nincluding continual learning, multi-task learning, few-shot learning, etc.\nFinally, we highlight the remaining challenges of model merging and discuss\nfuture research directions. A comprehensive list of papers about model merging\nis available at\n\\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}."
                },
                "authors": [
                    {
                        "name": "Enneng Yang"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Guibing Guo"
                    },
                    {
                        "name": "Xingwei Wang"
                    },
                    {
                        "name": "Xiaochun Cao"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07666v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07666v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12775v2",
                "updated": "2024-08-21T12:26:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    12,
                    26,
                    44,
                    2,
                    234,
                    0
                ],
                "published": "2024-04-19T10:30:02Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    10,
                    30,
                    2,
                    4,
                    110,
                    0
                ],
                "title": "The open-source sunbather code: modeling escaping planetary atmospheres\n  and their transit spectra",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The open-source sunbather code: modeling escaping planetary atmospheres\n  and their transit spectra"
                },
                "summary": "Atmospheric escape is thought to significantly influence the evolution of\nexoplanets, especially for sub-Jupiter planets on short orbital periods.\nTheoretical models predict that hydrodynamic escape could erode the atmospheres\nof such gaseous planets, leaving only a rocky core. Deriving atmospheric\nmass-loss rates from observations is necessary to check these predictions. One\nof the ways to obtain mass-loss rate estimates is to fit transit spectra of the\n10830 {\\AA} helium or UV metal lines with Parker wind models. We aim to provide\nthe community with a tool that enables performing this type of analysis, and\npresent sunbather, an open-source Python code to model escaping exoplanet\natmospheres and their transit spectra. sunbather incorporates the Parker wind\ncode p-winds and the photoionization code Cloudy, with the ability to calculate\nany currently known spectral tracer at an arbitrary atmospheric composition.\nWith sunbather, we investigate how the atmospheric structure of a generic hot\nNeptune planet depends on the metallicity. We find that the mass-loss rate\ndrops by roughly one order of magnitude as we increase the metallicity from\nsolar to 50 times solar. Line cooling by metal species is important already for\na solar composition, and more so at higher metallicity. We then demonstrate how\nsunbather can be used to interpret observations of spectral lines that form in\nthe upper atmosphere. We fit the observed helium spectrum of the mini-Neptune\nTOI-2134 b and show how even for helium data, the inferred mass-loss rate\ndepends on the metallicity by up to a factor of three.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atmospheric escape is thought to significantly influence the evolution of\nexoplanets, especially for sub-Jupiter planets on short orbital periods.\nTheoretical models predict that hydrodynamic escape could erode the atmospheres\nof such gaseous planets, leaving only a rocky core. Deriving atmospheric\nmass-loss rates from observations is necessary to check these predictions. One\nof the ways to obtain mass-loss rate estimates is to fit transit spectra of the\n10830 {\\AA} helium or UV metal lines with Parker wind models. We aim to provide\nthe community with a tool that enables performing this type of analysis, and\npresent sunbather, an open-source Python code to model escaping exoplanet\natmospheres and their transit spectra. sunbather incorporates the Parker wind\ncode p-winds and the photoionization code Cloudy, with the ability to calculate\nany currently known spectral tracer at an arbitrary atmospheric composition.\nWith sunbather, we investigate how the atmospheric structure of a generic hot\nNeptune planet depends on the metallicity. We find that the mass-loss rate\ndrops by roughly one order of magnitude as we increase the metallicity from\nsolar to 50 times solar. Line cooling by metal species is important already for\na solar composition, and more so at higher metallicity. We then demonstrate how\nsunbather can be used to interpret observations of spectral lines that form in\nthe upper atmosphere. We fit the observed helium spectrum of the mini-Neptune\nTOI-2134 b and show how even for helium data, the inferred mass-loss rate\ndepends on the metallicity by up to a factor of three."
                },
                "authors": [
                    {
                        "name": "Dion Linssen"
                    },
                    {
                        "name": "Jim Shih"
                    },
                    {
                        "name": "Morgan MacLeod"
                    },
                    {
                        "name": "Antonija Oklopi"
                    }
                ],
                "author_detail": {
                    "name": "Antonija Oklopi"
                },
                "author": "Antonija Oklopi",
                "arxiv_doi": "10.1051/0004-6361/202450240",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202450240",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.12775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to A&A",
                "arxiv_journal_ref": "A&A 688, A43 (2024)",
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18764v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18764v2",
                "updated": "2024-08-21T12:23:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    12,
                    23,
                    21,
                    2,
                    234,
                    0
                ],
                "published": "2024-07-26T14:22:30Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    14,
                    22,
                    30,
                    4,
                    208,
                    0
                ],
                "title": "TAGIFY: LLM-powered Tagging Interface for Improved Data Findability on\n  OGD portals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAGIFY: LLM-powered Tagging Interface for Improved Data Findability on\n  OGD portals"
                },
                "summary": "Efforts directed towards promoting Open Government Data (OGD) have gained\nsignificant traction across various governmental tiers since the mid-2000s. As\nmore datasets are published on OGD portals, finding specific data becomes\nharder, leading to information overload. Complete and accurate documentation of\ndatasets, including association of proper tags with datasets is key to\nimproving dataset findability and accessibility. Analysis conducted on the\nEstonian Open Data Portal, revealed that 11% datasets have no associated tags,\nwhile 26% had only one tag assigned to them, which underscores challenges in\ndata findability and accessibility within the portal, which, according to the\nrecent Open Data Maturity Report, is considered trend-setter. The aim of this\nstudy is to propose an automated solution to tagging datasets to improve data\nfindability on OGD portals. This paper presents Tagify - a prototype of tagging\ninterface that employs large language models (LLM) such as GPT-3.5-turbo and\nGPT-4 to automate dataset tagging, generating tags for datasets in English and\nEstonian, thereby augmenting metadata preparation by data publishers and\nimproving data findability on OGD portals by data users. The developed solution\nwas evaluated by users and their feedback was collected to define an agenda for\nfuture prototype improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efforts directed towards promoting Open Government Data (OGD) have gained\nsignificant traction across various governmental tiers since the mid-2000s. As\nmore datasets are published on OGD portals, finding specific data becomes\nharder, leading to information overload. Complete and accurate documentation of\ndatasets, including association of proper tags with datasets is key to\nimproving dataset findability and accessibility. Analysis conducted on the\nEstonian Open Data Portal, revealed that 11% datasets have no associated tags,\nwhile 26% had only one tag assigned to them, which underscores challenges in\ndata findability and accessibility within the portal, which, according to the\nrecent Open Data Maturity Report, is considered trend-setter. The aim of this\nstudy is to propose an automated solution to tagging datasets to improve data\nfindability on OGD portals. This paper presents Tagify - a prototype of tagging\ninterface that employs large language models (LLM) such as GPT-3.5-turbo and\nGPT-4 to automate dataset tagging, generating tags for datasets in English and\nEstonian, thereby augmenting metadata preparation by data publishers and\nimproving data findability on OGD portals by data users. The developed solution\nwas evaluated by users and their feedback was collected to define an agenda for\nfuture prototype improvements."
                },
                "authors": [
                    {
                        "name": "Kevin Kliimask"
                    },
                    {
                        "name": "Anastasija Nikiforova"
                    }
                ],
                "author_detail": {
                    "name": "Anastasija Nikiforova"
                },
                "author": "Anastasija Nikiforova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18764v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18764v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19345v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19345v2",
                "updated": "2024-08-21T12:22:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    12,
                    22,
                    51,
                    2,
                    234,
                    0
                ],
                "published": "2024-07-27T21:56:23Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    21,
                    56,
                    23,
                    5,
                    209,
                    0
                ],
                "title": "Inference-Time Selective Debiasing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Selective Debiasing"
                },
                "summary": "We propose selective debiasing -- an inference-time safety mechanism that\naims to increase the overall quality of models in terms of prediction\nperformance and fairness in the situation when re-training a model is\nprohibitive. The method is inspired by selective prediction, where some\npredictions that are considered low quality are discarded at inference time. In\nour approach, we identify the potentially biased model predictions and, instead\nof discarding them, we debias them using LEACE -- a post-processing debiasing\nmethod. To select problematic predictions, we propose a bias quantification\napproach based on KL divergence, which achieves better results than standard UQ\nmethods. Experiments with text classification datasets demonstrate that\nselective debiasing helps to close the performance gap between post-processing\nmethods and at-training and pre-processing debiasing techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose selective debiasing -- an inference-time safety mechanism that\naims to increase the overall quality of models in terms of prediction\nperformance and fairness in the situation when re-training a model is\nprohibitive. The method is inspired by selective prediction, where some\npredictions that are considered low quality are discarded at inference time. In\nour approach, we identify the potentially biased model predictions and, instead\nof discarding them, we debias them using LEACE -- a post-processing debiasing\nmethod. To select problematic predictions, we propose a bias quantification\napproach based on KL divergence, which achieves better results than standard UQ\nmethods. Experiments with text classification datasets demonstrate that\nselective debiasing helps to close the performance gap between post-processing\nmethods and at-training and pre-processing debiasing techniques."
                },
                "authors": [
                    {
                        "name": "Gleb Kuzmin"
                    },
                    {
                        "name": "Neemesh Yadav"
                    },
                    {
                        "name": "Ivan Smirnov"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Artem Shelmanov"
                    }
                ],
                "author_detail": {
                    "name": "Artem Shelmanov"
                },
                "author": "Artem Shelmanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19345v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19345v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11557v1",
                "updated": "2024-08-21T12:09:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    12,
                    9,
                    37,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T12:09:37Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    12,
                    9,
                    37,
                    2,
                    234,
                    0
                ],
                "title": "A Quick, trustworthy spectral detection Q&A system based on the SDAAP\n  Dataset and large language model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Quick, trustworthy spectral detection Q&A system based on the SDAAP\n  Dataset and large language model"
                },
                "summary": "Large Language Model (LLM) has demonstrated significant success in a range of\nnatural language processing (NLP) tasks within general domain. The emergence of\nLLM has introduced innovative methodologies across diverse fields, including\nthe natural sciences. Researchers aim to implement automated, concurrent\nprocess driven by LLM to supplant conventional manual, repetitive and\nlabor-intensive work. In the domain of spectral analysis and detection, it is\nimperative for researchers to autonomously acquire pertinent knowledge across\nvarious research objects, which encompasses the spectroscopic techniques and\nthe chemometric methods that are employed in experiments and analysis.\nParadoxically, despite the recognition of spectroscopic detection as an\neffective analytical method, the fundamental process of knowledge retrieval\nremains both time-intensive and repetitive. In response to this challenge, we\nfirst introduced the Spectral Detection and Analysis Based Paper(SDAAP)\ndataset, which is the first open-source textual knowledge dataset for spectral\nanalysis and detection and contains annotated literature data as well as\ncorresponding knowledge instruction data. Subsequently, we also designed an\nautomated Q\\&A framework based on the SDAAP dataset, which can retrieve\nrelevant knowledge and generate high-quality responses by extracting entities\nin the input as retrieval parameters. It is worth noting that: within this\nframework, LLM is only used as a tool to provide generalizability, while RAG\ntechnique is used to accurately capture the source of the knowledge.This\napproach not only improves the quality of the generated responses, but also\nensures the traceability of the knowledge. Experimental results show that our\nframework generates responses with more reliable expertise compared to the\nbaseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) has demonstrated significant success in a range of\nnatural language processing (NLP) tasks within general domain. The emergence of\nLLM has introduced innovative methodologies across diverse fields, including\nthe natural sciences. Researchers aim to implement automated, concurrent\nprocess driven by LLM to supplant conventional manual, repetitive and\nlabor-intensive work. In the domain of spectral analysis and detection, it is\nimperative for researchers to autonomously acquire pertinent knowledge across\nvarious research objects, which encompasses the spectroscopic techniques and\nthe chemometric methods that are employed in experiments and analysis.\nParadoxically, despite the recognition of spectroscopic detection as an\neffective analytical method, the fundamental process of knowledge retrieval\nremains both time-intensive and repetitive. In response to this challenge, we\nfirst introduced the Spectral Detection and Analysis Based Paper(SDAAP)\ndataset, which is the first open-source textual knowledge dataset for spectral\nanalysis and detection and contains annotated literature data as well as\ncorresponding knowledge instruction data. Subsequently, we also designed an\nautomated Q\\&A framework based on the SDAAP dataset, which can retrieve\nrelevant knowledge and generate high-quality responses by extracting entities\nin the input as retrieval parameters. It is worth noting that: within this\nframework, LLM is only used as a tool to provide generalizability, while RAG\ntechnique is used to accurately capture the source of the knowledge.This\napproach not only improves the quality of the generated responses, but also\nensures the traceability of the knowledge. Experimental results show that our\nframework generates responses with more reliable expertise compared to the\nbaseline."
                },
                "authors": [
                    {
                        "name": "Jiheng Liang"
                    },
                    {
                        "name": "Ziru Yu"
                    },
                    {
                        "name": "Zujie Xie"
                    },
                    {
                        "name": "Xiangyang Yu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Yu"
                },
                "author": "Xiangyang Yu",
                "arxiv_comment": "16 pages,10 figures,3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11551v1",
                "updated": "2024-08-21T11:57:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    11,
                    57,
                    42,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T11:57:42Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    11,
                    57,
                    42,
                    2,
                    234,
                    0
                ],
                "title": "High Performance Unstructured SpMM Computation Using Tensor Cores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Performance Unstructured SpMM Computation Using Tensor Cores"
                },
                "summary": "High-performance sparse matrix-matrix (SpMM) multiplication is paramount for\nscience and industry, as the ever-increasing sizes of data prohibit using dense\ndata structures. Yet, existing hardware, such as Tensor Cores (TC), is\nill-suited for SpMM, as it imposes strict constraints on data structures that\ncannot be met by unstructured sparsity found in many applications. To address\nthis, we introduce (S)parse (Ma)trix Matrix (T)ensor Core-accelerated (SMaT): a\nnovel SpMM library that utilizes TCs for unstructured sparse matrices. Our\nblock-sparse library leverages the low-level CUDA MMA\n(matrix-matrix-accumulate) API, maximizing the performance offered by modern\nGPUs. Algorithmic optimizations such as sparse matrix permutation further\nimprove performance by minimizing the number of non-zero blocks. The evaluation\non NVIDIA A100 shows that SMaT outperforms SotA libraries (DASP, cuSPARSE, and\nMagicube) by up to 125x (on average 2.6x). SMaT can be used to accelerate many\nworkloads in scientific computing, large-model training, inference, and others.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-performance sparse matrix-matrix (SpMM) multiplication is paramount for\nscience and industry, as the ever-increasing sizes of data prohibit using dense\ndata structures. Yet, existing hardware, such as Tensor Cores (TC), is\nill-suited for SpMM, as it imposes strict constraints on data structures that\ncannot be met by unstructured sparsity found in many applications. To address\nthis, we introduce (S)parse (Ma)trix Matrix (T)ensor Core-accelerated (SMaT): a\nnovel SpMM library that utilizes TCs for unstructured sparse matrices. Our\nblock-sparse library leverages the low-level CUDA MMA\n(matrix-matrix-accumulate) API, maximizing the performance offered by modern\nGPUs. Algorithmic optimizations such as sparse matrix permutation further\nimprove performance by minimizing the number of non-zero blocks. The evaluation\non NVIDIA A100 shows that SMaT outperforms SotA libraries (DASP, cuSPARSE, and\nMagicube) by up to 125x (on average 2.6x). SMaT can be used to accelerate many\nworkloads in scientific computing, large-model training, inference, and others."
                },
                "authors": [
                    {
                        "name": "Patrik Okanovic"
                    },
                    {
                        "name": "Grzegorz Kwasniewski"
                    },
                    {
                        "name": "Paolo Sylos Labini"
                    },
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Flavio Vella"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "arxiv_comment": "Accepted by 2024 International Conference on High Performance\n  Computing, Networking, Storage and Analysis, 2023 (SC'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12975v2",
                "updated": "2024-08-21T11:57:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    11,
                    57,
                    5,
                    2,
                    234,
                    0
                ],
                "published": "2024-06-18T18:00:03Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    18,
                    0,
                    3,
                    1,
                    170,
                    0
                ],
                "title": "SHIELD: Evaluation and Defense Strategies for Copyright Compliance in\n  LLM Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SHIELD: Evaluation and Defense Strategies for Copyright Compliance in\n  LLM Text Generation"
                },
                "summary": "Large Language Models (LLMs) have transformed machine learning but raised\nsignificant legal concerns due to their potential to produce text that\ninfringes on copyrights, resulting in several high-profile lawsuits. The legal\nlandscape is struggling to keep pace with these rapid advancements, with\nongoing debates about whether generated text might plagiarize copyrighted\nmaterials. Current LLMs may infringe on copyrights or overly restrict\nnon-copyrighted texts, leading to these challenges: (i) the need for a\ncomprehensive evaluation benchmark to assess copyright compliance from multiple\naspects; (ii) evaluating robustness against safeguard bypassing attacks; and\n(iii) developing effective defense targeted against the generation of\ncopyrighted text. To tackle these challenges, we introduce a curated dataset to\nevaluate methods, test attack strategies, and propose lightweight, real-time\ndefense to prevent the generation of copyrighted text, ensuring the safe and\nlawful use of LLMs. Our experiments demonstrate that current LLMs frequently\noutput copyrighted text, and that jailbreaking attacks can significantly\nincrease the volume of copyrighted output. Our proposed defense mechanism\nsignificantly reduces the volume of copyrighted text generated by LLMs by\neffectively refusing malicious requests. Code is publicly available at\nhttps://github.com/xz-liu/SHIELD",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed machine learning but raised\nsignificant legal concerns due to their potential to produce text that\ninfringes on copyrights, resulting in several high-profile lawsuits. The legal\nlandscape is struggling to keep pace with these rapid advancements, with\nongoing debates about whether generated text might plagiarize copyrighted\nmaterials. Current LLMs may infringe on copyrights or overly restrict\nnon-copyrighted texts, leading to these challenges: (i) the need for a\ncomprehensive evaluation benchmark to assess copyright compliance from multiple\naspects; (ii) evaluating robustness against safeguard bypassing attacks; and\n(iii) developing effective defense targeted against the generation of\ncopyrighted text. To tackle these challenges, we introduce a curated dataset to\nevaluate methods, test attack strategies, and propose lightweight, real-time\ndefense to prevent the generation of copyrighted text, ensuring the safe and\nlawful use of LLMs. Our experiments demonstrate that current LLMs frequently\noutput copyrighted text, and that jailbreaking attacks can significantly\nincrease the volume of copyrighted output. Our proposed defense mechanism\nsignificantly reduces the volume of copyrighted text generated by LLMs by\neffectively refusing malicious requests. Code is publicly available at\nhttps://github.com/xz-liu/SHIELD"
                },
                "authors": [
                    {
                        "name": "Xiaoze Liu"
                    },
                    {
                        "name": "Ting Sun"
                    },
                    {
                        "name": "Tianyang Xu"
                    },
                    {
                        "name": "Feijie Wu"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Xiaoqian Wang"
                    },
                    {
                        "name": "Jing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Gao"
                },
                "author": "Jing Gao",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01647v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01647v2",
                "updated": "2024-08-21T11:55:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    11,
                    55,
                    22,
                    2,
                    234,
                    0
                ],
                "published": "2024-06-03T12:58:29Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    12,
                    58,
                    29,
                    0,
                    155,
                    0
                ],
                "title": "An Analysis under a Unified Fomulation of Learning Algorithms with\n  Output Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Analysis under a Unified Fomulation of Learning Algorithms with\n  Output Constraints"
                },
                "summary": "Neural networks (NN) perform well in diverse tasks, but sometimes produce\nnonsensical results to humans. Most NN models \"solely\" learn from (input,\noutput) pairs, occasionally conflicting with human knowledge. Many studies\nindicate injecting human knowledge by reducing output constraints during\ntraining can improve model performance and reduce constraint violations. While\nthere have been several attempts to compare different existing algorithms under\nthe same programming framework, nonetheless, there has been no previous work\nthat categorizes learning algorithms with output constraints in a unified\nmanner. Our contributions are as follows: (1) We categorize the previous\nstudies based on three axes: type of constraint loss used (e.g. probabilistic\nsoft logic, REINFORCE), exploration strategy of constraint-violating examples,\nand integration mechanism of learning signals from main task and constraint.\n(2) We propose new algorithms to integrate the information of main task and\nconstraint injection, inspired by continual-learning algorithms. (3)\nFurthermore, we propose the $H\\beta$-score as a metric for considering the main\ntask metric and constraint violation simultaneously. To provide a thorough\nanalysis, we examine all the algorithms on three NLP tasks: natural language\ninference (NLI), synthetic transduction examples (STE), and semantic role\nlabeling (SRL). We explore and reveal the key factors of various algorithms\nassociated with achieving high $H\\beta$-scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks (NN) perform well in diverse tasks, but sometimes produce\nnonsensical results to humans. Most NN models \"solely\" learn from (input,\noutput) pairs, occasionally conflicting with human knowledge. Many studies\nindicate injecting human knowledge by reducing output constraints during\ntraining can improve model performance and reduce constraint violations. While\nthere have been several attempts to compare different existing algorithms under\nthe same programming framework, nonetheless, there has been no previous work\nthat categorizes learning algorithms with output constraints in a unified\nmanner. Our contributions are as follows: (1) We categorize the previous\nstudies based on three axes: type of constraint loss used (e.g. probabilistic\nsoft logic, REINFORCE), exploration strategy of constraint-violating examples,\nand integration mechanism of learning signals from main task and constraint.\n(2) We propose new algorithms to integrate the information of main task and\nconstraint injection, inspired by continual-learning algorithms. (3)\nFurthermore, we propose the $H\\beta$-score as a metric for considering the main\ntask metric and constraint violation simultaneously. To provide a thorough\nanalysis, we examine all the algorithms on three NLP tasks: natural language\ninference (NLI), synthetic transduction examples (STE), and semantic role\nlabeling (SRL). We explore and reveal the key factors of various algorithms\nassociated with achieving high $H\\beta$-scores."
                },
                "authors": [
                    {
                        "name": "Mooho Song"
                    },
                    {
                        "name": "Jay-Yoon Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jay-Yoon Lee"
                },
                "author": "Jay-Yoon Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01647v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01647v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11546v1",
                "updated": "2024-08-21T11:54:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    11,
                    54,
                    22,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T11:54:22Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    11,
                    54,
                    22,
                    2,
                    234,
                    0
                ],
                "title": "Memorization In In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memorization In In-Context Learning"
                },
                "summary": "In-context learning (ICL) has proven to be an effective strategy for\nimproving the performance of large language models (LLMs) with no additional\ntraining. However, the exact mechanism behind these performance improvements\nremains unclear. This study is the first to show how ICL surfaces memorized\ntraining data and to explore the correlation between this memorization and\nperformance across various ICL regimes: zero-shot, few-shot, and many-shot. Our\nmost notable findings include: (1) ICL significantly surfaces memorization\ncompared to zero-shot learning in most cases; (2) demonstrations, without their\nlabels, are the most effective element in surfacing memorization; (3) ICL\nimproves performance when the surfaced memorization in few-shot regimes reaches\na high level (about 40%); and (4) there is a very strong correlation between\nperformance and memorization in ICL when it outperforms zero-shot learning.\nOverall, our study uncovers a hidden phenomenon -- memorization -- at the core\nof ICL, raising an important question: to what extent do LLMs truly generalize\nfrom demonstrations in ICL, and how much of their success is due to\nmemorization?",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) has proven to be an effective strategy for\nimproving the performance of large language models (LLMs) with no additional\ntraining. However, the exact mechanism behind these performance improvements\nremains unclear. This study is the first to show how ICL surfaces memorized\ntraining data and to explore the correlation between this memorization and\nperformance across various ICL regimes: zero-shot, few-shot, and many-shot. Our\nmost notable findings include: (1) ICL significantly surfaces memorization\ncompared to zero-shot learning in most cases; (2) demonstrations, without their\nlabels, are the most effective element in surfacing memorization; (3) ICL\nimproves performance when the surfaced memorization in few-shot regimes reaches\na high level (about 40%); and (4) there is a very strong correlation between\nperformance and memorization in ICL when it outperforms zero-shot learning.\nOverall, our study uncovers a hidden phenomenon -- memorization -- at the core\nof ICL, raising an important question: to what extent do LLMs truly generalize\nfrom demonstrations in ICL, and how much of their success is due to\nmemorization?"
                },
                "authors": [
                    {
                        "name": "Shahriar Golchin"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    },
                    {
                        "name": "Steven Bethard"
                    },
                    {
                        "name": "Eduardo Blanco"
                    },
                    {
                        "name": "Ellen Riloff"
                    }
                ],
                "author_detail": {
                    "name": "Ellen Riloff"
                },
                "author": "Ellen Riloff",
                "arxiv_comment": "v1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11539v1",
                "updated": "2024-08-21T11:38:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    11,
                    38,
                    32,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T11:38:32Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    11,
                    38,
                    32,
                    2,
                    234,
                    0
                ],
                "title": "Research on the Application of Large Language Models in Automatic\n  Question Generation: A Case Study of ChatGLM in the Context of High School\n  Information Technology Curriculum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on the Application of Large Language Models in Automatic\n  Question Generation: A Case Study of ChatGLM in the Context of High School\n  Information Technology Curriculum"
                },
                "summary": "This study investigates the application effectiveness of the Large Language\nModel (LLMs) ChatGLM in the automated generation of high school information\ntechnology exam questions. Through meticulously designed prompt engineering\nstrategies, the model is guided to generate diverse questions, which are then\ncomprehensively evaluated by domain experts. The evaluation dimensions include\nthe Hitting(the degree of alignment with teaching content), Fitting (the degree\nof embodiment of core competencies), Clarity (the explicitness of question\ndescriptions), and Willing to use (the teacher's willingness to use the\nquestion in teaching). The results indicate that ChatGLM outperforms\nhuman-generated questions in terms of clarity and teachers' willingness to use,\nalthough there is no significant difference in hit rate and fit. This finding\nsuggests that ChatGLM has the potential to enhance the efficiency of question\ngeneration and alleviate the burden on teachers, providing a new perspective\nfor the future development of educational assessment systems. Future research\ncould explore further optimizations to the ChatGLM model to maintain high fit\nand hit rates while improving the clarity of questions and teachers'\nwillingness to use them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the application effectiveness of the Large Language\nModel (LLMs) ChatGLM in the automated generation of high school information\ntechnology exam questions. Through meticulously designed prompt engineering\nstrategies, the model is guided to generate diverse questions, which are then\ncomprehensively evaluated by domain experts. The evaluation dimensions include\nthe Hitting(the degree of alignment with teaching content), Fitting (the degree\nof embodiment of core competencies), Clarity (the explicitness of question\ndescriptions), and Willing to use (the teacher's willingness to use the\nquestion in teaching). The results indicate that ChatGLM outperforms\nhuman-generated questions in terms of clarity and teachers' willingness to use,\nalthough there is no significant difference in hit rate and fit. This finding\nsuggests that ChatGLM has the potential to enhance the efficiency of question\ngeneration and alleviate the burden on teachers, providing a new perspective\nfor the future development of educational assessment systems. Future research\ncould explore further optimizations to the ChatGLM model to maintain high fit\nand hit rates while improving the clarity of questions and teachers'\nwillingness to use them."
                },
                "authors": [
                    {
                        "name": "Yanxin Chen"
                    },
                    {
                        "name": "Ling He"
                    }
                ],
                "author_detail": {
                    "name": "Ling He"
                },
                "author": "Ling He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09205v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09205v2",
                "updated": "2024-08-21T11:34:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    11,
                    34,
                    56,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-17T13:54:34Z",
                "published_parsed": [
                    2024,
                    8,
                    17,
                    13,
                    54,
                    34,
                    5,
                    230,
                    0
                ],
                "title": "Architectural Foundations for the Large Language Model Infrastructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architectural Foundations for the Large Language Model Infrastructures"
                },
                "summary": "The development of a large language model (LLM) infrastructure is a pivotal\nundertaking in artificial intelligence. This paper explores the intricate\nlandscape of LLM infrastructure, software, and data management. By analyzing\nthese core components, we emphasize the pivotal considerations and safeguards\ncrucial for successful LLM development. This work presents a concise synthesis\nof the challenges and strategies inherent in constructing a robust and\neffective LLM infrastructure, offering valuable insights for researchers and\npractitioners alike.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of a large language model (LLM) infrastructure is a pivotal\nundertaking in artificial intelligence. This paper explores the intricate\nlandscape of LLM infrastructure, software, and data management. By analyzing\nthese core components, we emphasize the pivotal considerations and safeguards\ncrucial for successful LLM development. This work presents a concise synthesis\nof the challenges and strategies inherent in constructing a robust and\neffective LLM infrastructure, offering valuable insights for researchers and\npractitioners alike."
                },
                "authors": [
                    {
                        "name": "Hongyin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Hongyin Zhu"
                },
                "author": "Hongyin Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09205v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09205v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05688v2",
                "updated": "2024-08-21T11:10:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    11,
                    10,
                    36,
                    2,
                    234,
                    0
                ],
                "published": "2024-05-09T11:38:23Z",
                "published_parsed": [
                    2024,
                    5,
                    9,
                    11,
                    38,
                    23,
                    3,
                    130,
                    0
                ],
                "title": "Evaluating Dialect Robustness of Language Models via Conversation\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Dialect Robustness of Language Models via Conversation\n  Understanding"
                },
                "summary": "With an evergrowing number of LLMs reporting superlative performance for\nEnglish, their ability to perform equitably for different dialects of English\n($\\textit{i.e.}$, dialect robustness) needs to be ascertained. Specifically, we\nuse English language (US English or Indian English) conversations between\nhumans who play the word-guessing game of 'taboo'. We formulate two evaluative\ntasks: target word prediction (TWP) ($\\textit{i.e.}$, predict the masked target\nword in a conversation) and target word selection (TWS) ($\\textit{i.e.}$,\nselect the most likely masked target word in a conversation, from among a set\nof candidate words). Extending MD3, an existing dialectic dataset of\ntaboo-playing conversations, we introduce M-MD3, a target-word-masked version\nof MD3 with the en-US and en-IN subsets. We create two subsets: en-MV (where\nen-US is transformed to include dialectal information) and en-TR (where\ndialectal information is removed from en-IN). We evaluate one open-source\n(Llama3) and two closed-source (GPT-4/3.5) LLMs. LLMs perform significantly\nbetter for US English than Indian English for both TWP and TWS tasks, for all\nsettings, exhibiting marginalisation against the Indian dialect of English.\nWhile GPT-based models perform the best, the comparatively smaller models work\nmore equitably after fine-tuning. Our error analysis shows that the LLMs can\nunderstand the dialect better after fine-tuning using dialectal data. Our\nevaluation methodology exhibits a novel way to examine attributes of language\nmodels using pre-existing dialogue datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With an evergrowing number of LLMs reporting superlative performance for\nEnglish, their ability to perform equitably for different dialects of English\n($\\textit{i.e.}$, dialect robustness) needs to be ascertained. Specifically, we\nuse English language (US English or Indian English) conversations between\nhumans who play the word-guessing game of 'taboo'. We formulate two evaluative\ntasks: target word prediction (TWP) ($\\textit{i.e.}$, predict the masked target\nword in a conversation) and target word selection (TWS) ($\\textit{i.e.}$,\nselect the most likely masked target word in a conversation, from among a set\nof candidate words). Extending MD3, an existing dialectic dataset of\ntaboo-playing conversations, we introduce M-MD3, a target-word-masked version\nof MD3 with the en-US and en-IN subsets. We create two subsets: en-MV (where\nen-US is transformed to include dialectal information) and en-TR (where\ndialectal information is removed from en-IN). We evaluate one open-source\n(Llama3) and two closed-source (GPT-4/3.5) LLMs. LLMs perform significantly\nbetter for US English than Indian English for both TWP and TWS tasks, for all\nsettings, exhibiting marginalisation against the Indian dialect of English.\nWhile GPT-based models perform the best, the comparatively smaller models work\nmore equitably after fine-tuning. Our error analysis shows that the LLMs can\nunderstand the dialect better after fine-tuning using dialectal data. Our\nevaluation methodology exhibits a novel way to examine attributes of language\nmodels using pre-existing dialogue datasets."
                },
                "authors": [
                    {
                        "name": "Dipankar Srirag"
                    },
                    {
                        "name": "Nihar Ranjan Sahoo"
                    },
                    {
                        "name": "Aditya Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Joshi"
                },
                "author": "Aditya Joshi",
                "arxiv_comment": "12 pages, 3 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11523v1",
                "updated": "2024-08-21T10:56:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    56,
                    26,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:56:26Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    56,
                    26,
                    2,
                    234,
                    0
                ],
                "title": "LARR: Large Language Model Aided Real-time Scene Recommendation with\n  Semantic Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LARR: Large Language Model Aided Real-time Scene Recommendation with\n  Semantic Understanding"
                },
                "summary": "Click-Through Rate (CTR) prediction is crucial for Recommendation System(RS),\naiming to provide personalized recommendation services for users in many\naspects such as food delivery, e-commerce and so on. However, traditional RS\nrelies on collaborative signals, which lacks semantic understanding to\nreal-time scenes. We also noticed that a major challenge in utilizing Large\nLanguage Models (LLMs) for practical recommendation purposes is their\nefficiency in dealing with long text input. To break through the problems\nabove, we propose Large Language Model Aided Real-time Scene\nRecommendation(LARR), adopt LLMs for semantic understanding, utilizing\nreal-time scene information in RS without requiring LLM to process the entire\nreal-time scene text directly, thereby enhancing the efficiency of LLM-based\nCTR modeling. Specifically, recommendation domain-specific knowledge is\ninjected into LLM and then RS employs an aggregation encoder to build real-time\nscene information from separate LLM's outputs. Firstly, a LLM is continual\npretrained on corpus built from recommendation data with the aid of special\ntokens. Subsequently, the LLM is fine-tuned via contrastive learning on three\nkinds of sample construction strategies. Through this step, LLM is transformed\ninto a text embedding model. Finally, LLM's separate outputs for different\nscene features are aggregated by an encoder, aligning to collaborative signals\nin RS, enhancing the performance of recommendation model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Click-Through Rate (CTR) prediction is crucial for Recommendation System(RS),\naiming to provide personalized recommendation services for users in many\naspects such as food delivery, e-commerce and so on. However, traditional RS\nrelies on collaborative signals, which lacks semantic understanding to\nreal-time scenes. We also noticed that a major challenge in utilizing Large\nLanguage Models (LLMs) for practical recommendation purposes is their\nefficiency in dealing with long text input. To break through the problems\nabove, we propose Large Language Model Aided Real-time Scene\nRecommendation(LARR), adopt LLMs for semantic understanding, utilizing\nreal-time scene information in RS without requiring LLM to process the entire\nreal-time scene text directly, thereby enhancing the efficiency of LLM-based\nCTR modeling. Specifically, recommendation domain-specific knowledge is\ninjected into LLM and then RS employs an aggregation encoder to build real-time\nscene information from separate LLM's outputs. Firstly, a LLM is continual\npretrained on corpus built from recommendation data with the aid of special\ntokens. Subsequently, the LLM is fine-tuned via contrastive learning on three\nkinds of sample construction strategies. Through this step, LLM is transformed\ninto a text embedding model. Finally, LLM's separate outputs for different\nscene features are aggregated by an encoder, aligning to collaborative signals\nin RS, enhancing the performance of recommendation model."
                },
                "authors": [
                    {
                        "name": "Zhizhong Wan"
                    },
                    {
                        "name": "Bin Yin"
                    },
                    {
                        "name": "Junjie Xie"
                    },
                    {
                        "name": "Fei Jiang"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "arxiv_doi": "10.1145/3640457.3688135",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3640457.3688135",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.11523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11512v1",
                "updated": "2024-08-21T10:44:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    44,
                    10,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:44:10Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    44,
                    10,
                    2,
                    234,
                    0
                ],
                "title": "IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine\n  Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine\n  Translation"
                },
                "summary": "This paper introduces two multilingual systems, IKUN and IKUN-C, developed\nfor the general machine translation task in WMT24. IKUN and IKUN-C represent an\nopen system and a constrained system, respectively, built on Llama-3-8b and\nMistral-7B-v0.3. Both systems are designed to handle all 11 language directions\nusing a single model. According to automatic evaluation metrics, IKUN-C\nachieved 6 first-place and 3 second-place finishes among all constrained\nsystems, while IKUN secured 1 first-place and 2 second-place finishes across\nboth open and constrained systems. These encouraging results suggest that large\nlanguage models (LLMs) are nearing the level of proficiency required for\neffective multilingual machine translation. The systems are based on a\ntwo-stage approach: first, continuous pre-training on monolingual data in 10\nlanguages, followed by fine-tuning on high-quality parallel data for 11\nlanguage directions. The primary difference between IKUN and IKUN-C lies in\ntheir monolingual pre-training strategy. IKUN-C is pre-trained using\nconstrained monolingual data, whereas IKUN leverages monolingual data from the\nOSCAR dataset. In the second phase, both systems are fine-tuned on parallel\ndata sourced from NTREX, Flores, and WMT16-23 for all 11 language pairs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces two multilingual systems, IKUN and IKUN-C, developed\nfor the general machine translation task in WMT24. IKUN and IKUN-C represent an\nopen system and a constrained system, respectively, built on Llama-3-8b and\nMistral-7B-v0.3. Both systems are designed to handle all 11 language directions\nusing a single model. According to automatic evaluation metrics, IKUN-C\nachieved 6 first-place and 3 second-place finishes among all constrained\nsystems, while IKUN secured 1 first-place and 2 second-place finishes across\nboth open and constrained systems. These encouraging results suggest that large\nlanguage models (LLMs) are nearing the level of proficiency required for\neffective multilingual machine translation. The systems are based on a\ntwo-stage approach: first, continuous pre-training on monolingual data in 10\nlanguages, followed by fine-tuning on high-quality parallel data for 11\nlanguage directions. The primary difference between IKUN and IKUN-C lies in\ntheir monolingual pre-training strategy. IKUN-C is pre-trained using\nconstrained monolingual data, whereas IKUN leverages monolingual data from the\nOSCAR dataset. In the second phase, both systems are fine-tuned on parallel\ndata sourced from NTREX, Flores, and WMT16-23 for all 11 language pairs."
                },
                "authors": [
                    {
                        "name": "Baohao Liao"
                    },
                    {
                        "name": "Christian Herold"
                    },
                    {
                        "name": "Shahram Khadivi"
                    },
                    {
                        "name": "Christof Monz"
                    }
                ],
                "author_detail": {
                    "name": "Christof Monz"
                },
                "author": "Christof Monz",
                "arxiv_comment": "5 pages, 1 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.09237v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.09237v3",
                "updated": "2024-08-21T10:36:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    36,
                    44,
                    2,
                    234,
                    0
                ],
                "published": "2023-06-15T16:19:15Z",
                "published_parsed": [
                    2023,
                    6,
                    15,
                    16,
                    19,
                    15,
                    3,
                    166,
                    0
                ],
                "title": "One Law, Many Languages: Benchmarking Multilingual Legal Reasoning for\n  Judicial Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Law, Many Languages: Benchmarking Multilingual Legal Reasoning for\n  Judicial Support"
                },
                "summary": "Recent strides in Large Language Models (LLMs) have saturated many Natural\nLanguage Processing (NLP) benchmarks, emphasizing the need for more challenging\nones to properly assess LLM capabilities. However, domain-specific and\nmultilingual benchmarks are rare because they require in-depth expertise to\ndevelop. Still, most public models are trained predominantly on English\ncorpora, while other languages remain understudied, particularly for practical\ndomain-specific NLP tasks. In this work, we introduce a novel NLP benchmark for\nthe legal domain that challenges LLMs in five key dimensions: processing\n\\emph{long documents} (up to 50K tokens), using \\emph{domain-specific\nknowledge} (embodied in legal texts), \\emph{multilingual} understanding\n(covering five languages), \\emph{multitasking} (comprising legal\ndocument-to-document Information Retrieval, Court View Generation, Leading\nDecision Summarization, Citation Extraction, and eight challenging Text\nClassification tasks) and \\emph{reasoning} (comprising especially Court View\nGeneration, but also the Text Classification tasks). Our benchmark contains\ndiverse datasets from the Swiss legal system, allowing for a comprehensive\nstudy of the underlying non-English, inherently multilingual legal system.\nDespite the large size of our datasets (some with hundreds of thousands of\nexamples), existing publicly available multilingual models struggle with most\ntasks, even after extensive in-domain pre-training and fine-tuning. We publish\nall resources (benchmark suite, pre-trained models, code) under permissive open\nCC BY-SA licenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent strides in Large Language Models (LLMs) have saturated many Natural\nLanguage Processing (NLP) benchmarks, emphasizing the need for more challenging\nones to properly assess LLM capabilities. However, domain-specific and\nmultilingual benchmarks are rare because they require in-depth expertise to\ndevelop. Still, most public models are trained predominantly on English\ncorpora, while other languages remain understudied, particularly for practical\ndomain-specific NLP tasks. In this work, we introduce a novel NLP benchmark for\nthe legal domain that challenges LLMs in five key dimensions: processing\n\\emph{long documents} (up to 50K tokens), using \\emph{domain-specific\nknowledge} (embodied in legal texts), \\emph{multilingual} understanding\n(covering five languages), \\emph{multitasking} (comprising legal\ndocument-to-document Information Retrieval, Court View Generation, Leading\nDecision Summarization, Citation Extraction, and eight challenging Text\nClassification tasks) and \\emph{reasoning} (comprising especially Court View\nGeneration, but also the Text Classification tasks). Our benchmark contains\ndiverse datasets from the Swiss legal system, allowing for a comprehensive\nstudy of the underlying non-English, inherently multilingual legal system.\nDespite the large size of our datasets (some with hundreds of thousands of\nexamples), existing publicly available multilingual models struggle with most\ntasks, even after extensive in-domain pre-training and fine-tuning. We publish\nall resources (benchmark suite, pre-trained models, code) under permissive open\nCC BY-SA licenses."
                },
                "authors": [
                    {
                        "name": "Ronja Stern"
                    },
                    {
                        "name": "Vishvaksenan Rasiah"
                    },
                    {
                        "name": "Veton Matoshi"
                    },
                    {
                        "name": "Srinanda Brgger Bose"
                    },
                    {
                        "name": "Matthias Strmer"
                    },
                    {
                        "name": "Ilias Chalkidis"
                    },
                    {
                        "name": "Daniel E. Ho"
                    },
                    {
                        "name": "Joel Niklaus"
                    }
                ],
                "author_detail": {
                    "name": "Joel Niklaus"
                },
                "author": "Joel Niklaus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.09237v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.09237v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.03717v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.03717v2",
                "updated": "2024-08-21T10:25:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    25,
                    0,
                    2,
                    234,
                    0
                ],
                "published": "2024-01-08T08:00:04Z",
                "published_parsed": [
                    2024,
                    1,
                    8,
                    8,
                    0,
                    4,
                    0,
                    8,
                    0
                ],
                "title": "Universal Time-Series Representation Learning: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Time-Series Representation Learning: A Survey"
                },
                "summary": "Time-series data exists in every corner of real-world systems and services,\nranging from satellites in the sky to wearable devices on human bodies.\nLearning representations by extracting and inferring valuable information from\nthese time series is crucial for understanding the complex dynamics of\nparticular phenomena and enabling informed decisions. With the learned\nrepresentations, we can perform numerous downstream analyses more effectively.\nAmong several approaches, deep learning has demonstrated remarkable performance\nin extracting hidden patterns and features from time-series data without manual\nfeature engineering. This survey first presents a novel taxonomy based on three\nfundamental elements in designing state-of-the-art universal representation\nlearning methods for time series. According to the proposed taxonomy, we\ncomprehensively review existing studies and discuss their intuitions and\ninsights into how these methods enhance the quality of learned representations.\nFinally, as a guideline for future studies, we summarize commonly used\nexperimental setups and datasets and discuss several promising research\ndirections. An up-to-date corresponding resource is available at\nhttps://github.com/itouchz/awesome-deep-time-series-representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-series data exists in every corner of real-world systems and services,\nranging from satellites in the sky to wearable devices on human bodies.\nLearning representations by extracting and inferring valuable information from\nthese time series is crucial for understanding the complex dynamics of\nparticular phenomena and enabling informed decisions. With the learned\nrepresentations, we can perform numerous downstream analyses more effectively.\nAmong several approaches, deep learning has demonstrated remarkable performance\nin extracting hidden patterns and features from time-series data without manual\nfeature engineering. This survey first presents a novel taxonomy based on three\nfundamental elements in designing state-of-the-art universal representation\nlearning methods for time series. According to the proposed taxonomy, we\ncomprehensively review existing studies and discuss their intuitions and\ninsights into how these methods enhance the quality of learned representations.\nFinally, as a guideline for future studies, we summarize commonly used\nexperimental setups and datasets and discuss several promising research\ndirections. An up-to-date corresponding resource is available at\nhttps://github.com/itouchz/awesome-deep-time-series-representations."
                },
                "authors": [
                    {
                        "name": "Patara Trirat"
                    },
                    {
                        "name": "Yooju Shin"
                    },
                    {
                        "name": "Junhyeok Kang"
                    },
                    {
                        "name": "Youngeun Nam"
                    },
                    {
                        "name": "Jihye Na"
                    },
                    {
                        "name": "Minyoung Bae"
                    },
                    {
                        "name": "Joeun Kim"
                    },
                    {
                        "name": "Byunghyun Kim"
                    },
                    {
                        "name": "Jae-Gil Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Gil Lee"
                },
                "author": "Jae-Gil Lee",
                "arxiv_comment": "43 pages, 7 figures, reference updates",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.03717v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.03717v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11497v1",
                "updated": "2024-08-21T10:12:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    12,
                    56,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:12:56Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    12,
                    56,
                    2,
                    234,
                    0
                ],
                "title": "Climate Change in Austria: Precipitation and Dry Spells over 50 years",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Climate Change in Austria: Precipitation and Dry Spells over 50 years"
                },
                "summary": "We propose a spatio-temporal generalised additive model (GAM) to study if\nprecipitation patterns have changed between two 10-year time periods in the\nlast 50 years in Austria. In particular, we model three scenarios: monthly mean\nand monthly maximum precipitation as well as the maximum length of a dry spell\nper month with a gamma, blended generalised extreme value and negative binomial\ndistribution, respectively, over the periods 1973-1982 and 2013-2022. In order\nto model the spatial dependencies in the data more realistically, we intend to\ntake the mountainous landscape of Austria into account. Therefore, we have\nchosen a non-stationary version of the Mat\\'ern covariance function, which\naccounts for elevation differences, as a spatial argument of the latent field\nin the GAM. The temporal part of the latent field is captured by an AR(1)\nprocess. We use the stochastic partial differential equation approach in\ncombination with integrated nested Laplace approximation to perform inference\ncomputationally efficient. The model outputs are visualised and support\nexisting climate change studies in the Alpine region obtained with, for\nexample, projections from regional climate models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a spatio-temporal generalised additive model (GAM) to study if\nprecipitation patterns have changed between two 10-year time periods in the\nlast 50 years in Austria. In particular, we model three scenarios: monthly mean\nand monthly maximum precipitation as well as the maximum length of a dry spell\nper month with a gamma, blended generalised extreme value and negative binomial\ndistribution, respectively, over the periods 1973-1982 and 2013-2022. In order\nto model the spatial dependencies in the data more realistically, we intend to\ntake the mountainous landscape of Austria into account. Therefore, we have\nchosen a non-stationary version of the Mat\\'ern covariance function, which\naccounts for elevation differences, as a spatial argument of the latent field\nin the GAM. The temporal part of the latent field is captured by an AR(1)\nprocess. We use the stochastic partial differential equation approach in\ncombination with integrated nested Laplace approximation to perform inference\ncomputationally efficient. The model outputs are visualised and support\nexisting climate change studies in the Alpine region obtained with, for\nexample, projections from regional climate models."
                },
                "authors": [
                    {
                        "name": "Corinna Perchtold"
                    },
                    {
                        "name": "Evelyn Buckwar"
                    }
                ],
                "author_detail": {
                    "name": "Evelyn Buckwar"
                },
                "author": "Evelyn Buckwar",
                "arxiv_comment": "24 pages, 8 tables and 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11494v1",
                "updated": "2024-08-21T10:10:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    10,
                    8,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:10:08Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    10,
                    8,
                    2,
                    234,
                    0
                ],
                "title": "Mutagenesis screen to map the functionals of parameters of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutagenesis screen to map the functionals of parameters of Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have significantly advanced artificial\nintelligence, excelling in numerous tasks. Although the functionality of a\nmodel is inherently tied to its parameters, a systematic method for exploring\nthe connections between the parameters and the functionality are lacking.\nModels sharing similar structure and parameter counts exhibit significant\nperformance disparities across various tasks, prompting investigations into the\nvarying patterns that govern their performance. We adopted a mutagenesis screen\napproach inspired by the methods used in biological studies, to investigate\nLlama2-7b and Zephyr. This technique involved mutating elements within the\nmodels' matrices to their maximum or minimum values to examine the relationship\nbetween model parameters and their functionalities. Our research uncovered\nmultiple levels of fine structures within both models. Many matrices showed a\nmixture of maximum and minimum mutations following mutagenesis, but others were\npredominantly sensitive to one type. Notably, mutations that produced\nphenotypes, especially those with severe outcomes, tended to cluster along\naxes. Additionally, the location of maximum and minimum mutations often\ndisplayed a complementary pattern on matrix in both models, with the Gate\nmatrix showing a unique two-dimensional asymmetry after rearrangement. In\nZephyr, certain mutations consistently resulted in poetic or conversational\nrather than descriptive outputs. These \"writer\" mutations grouped according to\nthe high-frequency initial word of the output, with a marked tendency to share\nthe row coordinate even when they are in different matrices. Our findings\naffirm that the mutagenesis screen is an effective tool for deciphering the\ncomplexities of large language models and identifying unexpected ways to expand\ntheir potential, providing deeper insights into the foundational aspects of AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly advanced artificial\nintelligence, excelling in numerous tasks. Although the functionality of a\nmodel is inherently tied to its parameters, a systematic method for exploring\nthe connections between the parameters and the functionality are lacking.\nModels sharing similar structure and parameter counts exhibit significant\nperformance disparities across various tasks, prompting investigations into the\nvarying patterns that govern their performance. We adopted a mutagenesis screen\napproach inspired by the methods used in biological studies, to investigate\nLlama2-7b and Zephyr. This technique involved mutating elements within the\nmodels' matrices to their maximum or minimum values to examine the relationship\nbetween model parameters and their functionalities. Our research uncovered\nmultiple levels of fine structures within both models. Many matrices showed a\nmixture of maximum and minimum mutations following mutagenesis, but others were\npredominantly sensitive to one type. Notably, mutations that produced\nphenotypes, especially those with severe outcomes, tended to cluster along\naxes. Additionally, the location of maximum and minimum mutations often\ndisplayed a complementary pattern on matrix in both models, with the Gate\nmatrix showing a unique two-dimensional asymmetry after rearrangement. In\nZephyr, certain mutations consistently resulted in poetic or conversational\nrather than descriptive outputs. These \"writer\" mutations grouped according to\nthe high-frequency initial word of the output, with a marked tendency to share\nthe row coordinate even when they are in different matrices. Our findings\naffirm that the mutagenesis screen is an effective tool for deciphering the\ncomplexities of large language models and identifying unexpected ways to expand\ntheir potential, providing deeper insights into the foundational aspects of AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Yue Hu"
                    },
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Patrick X. Zhao"
                    },
                    {
                        "name": "Javed Khan"
                    },
                    {
                        "name": "Chengming Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengming Xu"
                },
                "author": "Chengming Xu",
                "arxiv_comment": "10 pages, 6 figures, supplementary material available online",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11491v1",
                "updated": "2024-08-21T10:01:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    1,
                    34,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:01:34Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    1,
                    34,
                    2,
                    234,
                    0
                ],
                "title": "Nothing in Excess: Mitigating the Exaggerated Safety for LLMs via\n  Safety-Conscious Activation Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nothing in Excess: Mitigating the Exaggerated Safety for LLMs via\n  Safety-Conscious Activation Steering"
                },
                "summary": "Safety alignment is indispensable for Large language models (LLMs) to defend\nthreats from malicious instructions. However, recent researches reveal\nsafety-aligned LLMs prone to reject benign queries due to the exaggerated\nsafety issue, limiting their helpfulness. In this paper, we propose a\nSafety-Conscious Activation Steering (SCANS) method to mitigate the exaggerated\nsafety concerns in aligned LLMs. First, SCANS extracts the refusal steering\nvectors within the activation space and utilizes vocabulary projection to\nanchor some specific safety-critical layers which influence model refusal\nbehavior. Second, by tracking the hidden state transition, SCANS identifies the\nsteering direction and steers the model behavior accordingly, achieving a\nbalance between exaggerated safety and adequate safety. Experiments show that\nSCANS achieves new state-of-the-art performance on XSTest and OKTest\nbenchmarks, without impairing their defense capability against harmful queries\nand maintaining almost unchanged model capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety alignment is indispensable for Large language models (LLMs) to defend\nthreats from malicious instructions. However, recent researches reveal\nsafety-aligned LLMs prone to reject benign queries due to the exaggerated\nsafety issue, limiting their helpfulness. In this paper, we propose a\nSafety-Conscious Activation Steering (SCANS) method to mitigate the exaggerated\nsafety concerns in aligned LLMs. First, SCANS extracts the refusal steering\nvectors within the activation space and utilizes vocabulary projection to\nanchor some specific safety-critical layers which influence model refusal\nbehavior. Second, by tracking the hidden state transition, SCANS identifies the\nsteering direction and steers the model behavior accordingly, achieving a\nbalance between exaggerated safety and adequate safety. Experiments show that\nSCANS achieves new state-of-the-art performance on XSTest and OKTest\nbenchmarks, without impairing their defense capability against harmful queries\nand maintaining almost unchanged model capability."
                },
                "authors": [
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11490v1",
                "updated": "2024-08-21T10:01:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    1,
                    12,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:01:12Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    1,
                    12,
                    2,
                    234,
                    0
                ],
                "title": "DocTabQA: Answering Questions from Long Documents Using Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DocTabQA: Answering Questions from Long Documents Using Tables"
                },
                "summary": "We study a new problem setting of question answering (QA), referred to as\nDocTabQA. Within this setting, given a long document, the goal is to respond to\nquestions by organizing the answers into structured tables derived directly\nfrom the document's content. Unlike traditional QA approaches which\npredominantly rely on unstructured text to formulate responses, DocTabQA aims\nto leverage structured tables as answers to convey information clearly and\nsystematically, thereby enhancing user comprehension and highlighting\nrelationships between data points. To the best of our knowledge, this problem\nhas not been previously explored. In this paper, we introduce the QTabA\ndataset, encompassing 300 financial documents, accompanied by manually\nannotated 1.5k question-table pairs. Initially, we leverage Large Language\nModels (LLMs) such as GPT-4 to establish a baseline. However, it is widely\nacknowledged that LLMs encounter difficulties when tasked with generating\nintricate, structured outputs from long input sequences. To overcome these\nchallenges, we present a two-stage framework, called DocTabTalk, which\ninitially retrieves relevant sentences from extensive documents and\nsubsequently generates hierarchical tables based on these identified sentences.\nDocTabTalk incorporates two key technological innovations: AlignLLaMA and\nTabTalk, which are specifically tailored to assist GPT-4 in tackling DocTabQA,\nenabling it to generate well-structured, hierarchical tables with improved\norganization and clarity. Comprehensive experimental evaluations conducted on\nboth QTabA and RotoWire datasets demonstrate that our DocTabTalk significantly\nenhances the performances of the GPT-4 in our proposed DocTabQA task and the\ntable generation task. The code and dataset are available at\nhttps://github.com/SmileWHC/DocTabQA for further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study a new problem setting of question answering (QA), referred to as\nDocTabQA. Within this setting, given a long document, the goal is to respond to\nquestions by organizing the answers into structured tables derived directly\nfrom the document's content. Unlike traditional QA approaches which\npredominantly rely on unstructured text to formulate responses, DocTabQA aims\nto leverage structured tables as answers to convey information clearly and\nsystematically, thereby enhancing user comprehension and highlighting\nrelationships between data points. To the best of our knowledge, this problem\nhas not been previously explored. In this paper, we introduce the QTabA\ndataset, encompassing 300 financial documents, accompanied by manually\nannotated 1.5k question-table pairs. Initially, we leverage Large Language\nModels (LLMs) such as GPT-4 to establish a baseline. However, it is widely\nacknowledged that LLMs encounter difficulties when tasked with generating\nintricate, structured outputs from long input sequences. To overcome these\nchallenges, we present a two-stage framework, called DocTabTalk, which\ninitially retrieves relevant sentences from extensive documents and\nsubsequently generates hierarchical tables based on these identified sentences.\nDocTabTalk incorporates two key technological innovations: AlignLLaMA and\nTabTalk, which are specifically tailored to assist GPT-4 in tackling DocTabQA,\nenabling it to generate well-structured, hierarchical tables with improved\norganization and clarity. Comprehensive experimental evaluations conducted on\nboth QTabA and RotoWire datasets demonstrate that our DocTabTalk significantly\nenhances the performances of the GPT-4 in our proposed DocTabQA task and the\ntable generation task. The code and dataset are available at\nhttps://github.com/SmileWHC/DocTabQA for further research."
                },
                "authors": [
                    {
                        "name": "Haochen Wang"
                    },
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Liangcai Gao"
                    }
                ],
                "author_detail": {
                    "name": "Liangcai Gao"
                },
                "author": "Liangcai Gao",
                "arxiv_comment": "18 pages,5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19832v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19832v3",
                "updated": "2024-08-21T09:52:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    9,
                    52,
                    52,
                    2,
                    234,
                    0
                ],
                "published": "2024-07-29T09:38:15Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    9,
                    38,
                    15,
                    0,
                    211,
                    0
                ],
                "title": "ML-Mamba: Efficient Multi-Modal Large Language Model Utilizing Mamba-2",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-Mamba: Efficient Multi-Modal Large Language Model Utilizing Mamba-2"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have attracted much attention for\ntheir multifunctionality. However, traditional Transformer architectures incur\nsignificant overhead due to their secondary computational complexity. To\naddress this issue, we introduce ML-Mamba, a multimodal language model, which\nutilizes the latest and efficient Mamba-2 model for inference. Mamba-2 is known\nfor its linear scalability and fast processing of long sequences. We replace\nthe Transformer-based backbone with a pre-trained Mamba-2 model and explore\nmethods for integrating 2D visual selective scanning mechanisms into multimodal\nlearning while also trying various visual encoders and Mamba-2 model variants.\nOur extensive experiments in various multimodal benchmark tests demonstrate the\ncompetitive performance of ML-Mamba and highlight the potential of state space\nmodels in multimodal tasks. The experimental results show that: (1) we\nempirically explore how to effectively apply the 2D vision selective scan\nmechanism for multimodal learning. We propose a novel multimodal connector\ncalled the Mamba-2 Scan Connector (MSC), which enhances representational\ncapabilities. (2) ML-Mamba achieves performance comparable to state-of-the-art\nmethods such as TinyLaVA and MobileVLM v2 through its linear sequential\nmodeling while faster inference speed; (3) Compared to multimodal models\nutilizing Mamba-1, the Mamba-2-based ML-Mamba exhibits superior inference\nperformance and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have attracted much attention for\ntheir multifunctionality. However, traditional Transformer architectures incur\nsignificant overhead due to their secondary computational complexity. To\naddress this issue, we introduce ML-Mamba, a multimodal language model, which\nutilizes the latest and efficient Mamba-2 model for inference. Mamba-2 is known\nfor its linear scalability and fast processing of long sequences. We replace\nthe Transformer-based backbone with a pre-trained Mamba-2 model and explore\nmethods for integrating 2D visual selective scanning mechanisms into multimodal\nlearning while also trying various visual encoders and Mamba-2 model variants.\nOur extensive experiments in various multimodal benchmark tests demonstrate the\ncompetitive performance of ML-Mamba and highlight the potential of state space\nmodels in multimodal tasks. The experimental results show that: (1) we\nempirically explore how to effectively apply the 2D vision selective scan\nmechanism for multimodal learning. We propose a novel multimodal connector\ncalled the Mamba-2 Scan Connector (MSC), which enhances representational\ncapabilities. (2) ML-Mamba achieves performance comparable to state-of-the-art\nmethods such as TinyLaVA and MobileVLM v2 through its linear sequential\nmodeling while faster inference speed; (3) Compared to multimodal models\nutilizing Mamba-1, the Mamba-2-based ML-Mamba exhibits superior inference\nperformance and effectiveness."
                },
                "authors": [
                    {
                        "name": "Wenjun Huang"
                    },
                    {
                        "name": "Jiakai Pan"
                    },
                    {
                        "name": "Jiahao Tang"
                    },
                    {
                        "name": "Yanyu Ding"
                    },
                    {
                        "name": "Yifei Xing"
                    },
                    {
                        "name": "Yuhe Wang"
                    },
                    {
                        "name": "Zhengzhuo Wang"
                    },
                    {
                        "name": "Jianguo Hu"
                    }
                ],
                "author_detail": {
                    "name": "Jianguo Hu"
                },
                "author": "Jianguo Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19832v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19832v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12667v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12667v3",
                "updated": "2024-08-21T09:47:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    9,
                    47,
                    33,
                    2,
                    234,
                    0
                ],
                "published": "2024-03-19T12:05:09Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    12,
                    5,
                    9,
                    1,
                    79,
                    0
                ],
                "title": "ICE: Interactive 3D Game Character Editing via Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICE: Interactive 3D Game Character Editing via Dialogue"
                },
                "summary": "ost recent popular Role-Playing Games (RPGs) allow players to create in-game\ncharacters with hundreds of adjustable parameters, including bone positions and\nvarious makeup options. Although text-driven auto-customization systems have\nbeen developed to simplify the complex process of adjusting these intricate\ncharacter parameters, they are limited by their single-round generation and\nlack the capability for further editing and fine-tuning. In this paper, we\npropose an Interactive Character Editing framework (ICE) to achieve a\nmulti-round dialogue-based refinement process. In a nutshell, our ICE offers a\nmore user-friendly way to enable players to convey creative ideas iteratively\nwhile ensuring that created characters align with the expectations of players.\nSpecifically, we propose an Instruction Parsing Module (IPM) that utilizes\nlarge language models (LLMs) to parse multi-round dialogues into clear editing\ninstruction prompts in each round. To reliably and swiftly modify character\ncontrol parameters at a fine-grained level, we propose a Semantic-guided\nLow-dimension Parameter Solver (SLPS) that edits character control parameters\naccording to prompts in a zero-shot manner. Our SLPS first localizes the\ncharacter control parameters related to the fine-grained modification, and then\noptimizes the corresponding parameters in a low-dimension space to avoid\nunrealistic results. Extensive experimental results demonstrate the\neffectiveness of our proposed ICE for in-game character creation and the\nsuperior editing performance of ICE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ost recent popular Role-Playing Games (RPGs) allow players to create in-game\ncharacters with hundreds of adjustable parameters, including bone positions and\nvarious makeup options. Although text-driven auto-customization systems have\nbeen developed to simplify the complex process of adjusting these intricate\ncharacter parameters, they are limited by their single-round generation and\nlack the capability for further editing and fine-tuning. In this paper, we\npropose an Interactive Character Editing framework (ICE) to achieve a\nmulti-round dialogue-based refinement process. In a nutshell, our ICE offers a\nmore user-friendly way to enable players to convey creative ideas iteratively\nwhile ensuring that created characters align with the expectations of players.\nSpecifically, we propose an Instruction Parsing Module (IPM) that utilizes\nlarge language models (LLMs) to parse multi-round dialogues into clear editing\ninstruction prompts in each round. To reliably and swiftly modify character\ncontrol parameters at a fine-grained level, we propose a Semantic-guided\nLow-dimension Parameter Solver (SLPS) that edits character control parameters\naccording to prompts in a zero-shot manner. Our SLPS first localizes the\ncharacter control parameters related to the fine-grained modification, and then\noptimizes the corresponding parameters in a low-dimension space to avoid\nunrealistic results. Extensive experimental results demonstrate the\neffectiveness of our proposed ICE for in-game character creation and the\nsuperior editing performance of ICE."
                },
                "authors": [
                    {
                        "name": "Haoqian Wu"
                    },
                    {
                        "name": "Minda Zhao"
                    },
                    {
                        "name": "Zhipeng Hu"
                    },
                    {
                        "name": "Lincheng Li"
                    },
                    {
                        "name": "Weijie Chen"
                    },
                    {
                        "name": "Rui Zhao"
                    },
                    {
                        "name": "Changjie Fan"
                    },
                    {
                        "name": "Xin Yu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Yu"
                },
                "author": "Xin Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12667v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12667v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11552v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11552v3",
                "updated": "2024-08-21T09:46:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    9,
                    46,
                    35,
                    2,
                    234,
                    0
                ],
                "published": "2024-03-18T08:03:47Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    8,
                    3,
                    47,
                    0,
                    78,
                    0
                ],
                "title": "LLM3:Large Language Model-based Task and Motion Planning with Motion\n  Failure Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM3:Large Language Model-based Task and Motion Planning with Motion\n  Failure Reasoning"
                },
                "summary": "Conventional Task and Motion Planning (TAMP) approaches rely on manually\ncrafted interfaces connecting symbolic task planning with continuous motion\ngeneration. These domain-specific and labor-intensive modules are limited in\naddressing emerging tasks in real-world settings. Here, we present LLM^3, a\nnovel Large Language Model (LLM)-based TAMP framework featuring a\ndomain-independent interface. Specifically, we leverage the powerful reasoning\nand planning capabilities of pre-trained LLMs to propose symbolic action\nsequences and select continuous action parameters for motion planning.\nCrucially, LLM^3 incorporates motion planning feedback through prompting,\nallowing the LLM to iteratively refine its proposals by reasoning about motion\nfailure. Consequently, LLM^3 interfaces between task planning and motion\nplanning, alleviating the intricate design process of handling domain-specific\nmessages between them. Through a series of simulations in a box-packing domain,\nwe quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP\nproblems and the efficiency in selecting action parameters. Ablation studies\nunderscore the significant contribution of motion failure reasoning to the\nsuccess of LLM^3. Furthermore, we conduct qualitative experiments on a physical\nmanipulator, demonstrating the practical applicability of our approach in\nreal-world settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional Task and Motion Planning (TAMP) approaches rely on manually\ncrafted interfaces connecting symbolic task planning with continuous motion\ngeneration. These domain-specific and labor-intensive modules are limited in\naddressing emerging tasks in real-world settings. Here, we present LLM^3, a\nnovel Large Language Model (LLM)-based TAMP framework featuring a\ndomain-independent interface. Specifically, we leverage the powerful reasoning\nand planning capabilities of pre-trained LLMs to propose symbolic action\nsequences and select continuous action parameters for motion planning.\nCrucially, LLM^3 incorporates motion planning feedback through prompting,\nallowing the LLM to iteratively refine its proposals by reasoning about motion\nfailure. Consequently, LLM^3 interfaces between task planning and motion\nplanning, alleviating the intricate design process of handling domain-specific\nmessages between them. Through a series of simulations in a box-packing domain,\nwe quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP\nproblems and the efficiency in selecting action parameters. Ablation studies\nunderscore the significant contribution of motion failure reasoning to the\nsuccess of LLM^3. Furthermore, we conduct qualitative experiments on a physical\nmanipulator, demonstrating the practical applicability of our approach in\nreal-world settings."
                },
                "authors": [
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Muzhi Han"
                    },
                    {
                        "name": "Ziyuan Jiao"
                    },
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Ying Nian Wu"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Hangxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hangxin Liu"
                },
                "author": "Hangxin Liu",
                "arxiv_comment": "IROS 2024. Codes available: https://github.com/AssassinWS/LLM-TAMP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.11552v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11552v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11193v9",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11193v9",
                "updated": "2024-08-21T09:31:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    9,
                    31,
                    2,
                    2,
                    234,
                    0
                ],
                "published": "2023-12-18T13:40:16Z",
                "published_parsed": [
                    2023,
                    12,
                    18,
                    13,
                    40,
                    16,
                    0,
                    352,
                    0
                ],
                "title": "Training With \"Paraphrasing the Original Text\" Improves Long-Context\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training With \"Paraphrasing the Original Text\" Improves Long-Context\n  Performance"
                },
                "summary": "As Large Language Models (LLMs) continue to evolve, more are being designed\nto handle long-context inputs. Despite this advancement, most of them still\nface challenges in accurately handling long-context tasks, often showing the\n\"lost in the middle\" issue. We identify that insufficient retrieval capability\nis one of the important reasons for this issue. To tackle this challenge, we\npropose a novel approach to design training data for long-context tasks, aiming\nat augmenting LLMs' proficiency in extracting key information from long\ncontext. Specially, we incorporate an additional part named \"paraphrasing the\noriginal text\" when constructing the answer of training samples and then\nfine-tuning the model. Experimenting on LongBench and NaturalQuestions\nMulti-document-QA dataset with models of Llama and Qwen series, our method\nachieves an improvement of up to 8.48% and 4.48% in average scores,\nrespectively, showing effectiveness in improving the model' s performance on\nlong-context tasks. The model and training data have been made available on\nHuggingFace(https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to evolve, more are being designed\nto handle long-context inputs. Despite this advancement, most of them still\nface challenges in accurately handling long-context tasks, often showing the\n\"lost in the middle\" issue. We identify that insufficient retrieval capability\nis one of the important reasons for this issue. To tackle this challenge, we\npropose a novel approach to design training data for long-context tasks, aiming\nat augmenting LLMs' proficiency in extracting key information from long\ncontext. Specially, we incorporate an additional part named \"paraphrasing the\noriginal text\" when constructing the answer of training samples and then\nfine-tuning the model. Experimenting on LongBench and NaturalQuestions\nMulti-document-QA dataset with models of Llama and Qwen series, our method\nachieves an improvement of up to 8.48% and 4.48% in average scores,\nrespectively, showing effectiveness in improving the model' s performance on\nlong-context tasks. The model and training data have been made available on\nHuggingFace(https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k)."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Zhixiao Qi"
                    },
                    {
                        "name": "Zhe Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Zhou"
                },
                "author": "Zhe Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11193v9",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11193v9",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19325v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19325v3",
                "updated": "2024-08-21T09:22:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    9,
                    22,
                    6,
                    2,
                    234,
                    0
                ],
                "published": "2024-03-28T11:32:53Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    11,
                    32,
                    53,
                    3,
                    88,
                    0
                ],
                "title": "Bayesian inference of the dense matter equation of state built upon\n  extended Skyrme interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference of the dense matter equation of state built upon\n  extended Skyrme interactions"
                },
                "summary": "The non-relativistic model of nuclear matter with Brussels extended Skyrme\ninteractions is employed in order to build, within a Bayesian approach, models\nfor the dense matter equation of state (EOS). In addition to a minimal set of\nconstraints on nuclear empirical parameters; the density behavior of the energy\nper particle in pure neutron matter (PNM); a lower limit on the maximum neutron\nstar (NS) mass, we require that the Fermi velocity of neutrons\n($v_{\\mathrm{F;\\,n}}$) in PNM and symmetric nuclear matter (SNM) with densities\nup to $0.8~\\mathrm{fm}^{-3}$ (arbitrary) does not exceed the speed of light.\nThe latter condition is imposed in order to cure a deficiency present in many\nSkyrme interactions [Duan and Urban, Phys. Rev. C 108, 025813 (2023)]. We\nillustrate the importance of this constraint for the posterior distributions.\nSome of our models are subjected to constraints on the density dependence of\nneutron (nucleon) Landau effective mass in PNM (SNM), too. The impact of\nvarious sets of constraints on the behaviors of nuclear matter and NSs is\ndiscussed in detail. Systematic comparison with results previously obtained by\nemploying Skyrme interactions is done for posteriors of both nuclear matter\n(NM) and NS parameters. Special attention is given to the model and constraints\ndependence of correlations among various quantities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The non-relativistic model of nuclear matter with Brussels extended Skyrme\ninteractions is employed in order to build, within a Bayesian approach, models\nfor the dense matter equation of state (EOS). In addition to a minimal set of\nconstraints on nuclear empirical parameters; the density behavior of the energy\nper particle in pure neutron matter (PNM); a lower limit on the maximum neutron\nstar (NS) mass, we require that the Fermi velocity of neutrons\n($v_{\\mathrm{F;\\,n}}$) in PNM and symmetric nuclear matter (SNM) with densities\nup to $0.8~\\mathrm{fm}^{-3}$ (arbitrary) does not exceed the speed of light.\nThe latter condition is imposed in order to cure a deficiency present in many\nSkyrme interactions [Duan and Urban, Phys. Rev. C 108, 025813 (2023)]. We\nillustrate the importance of this constraint for the posterior distributions.\nSome of our models are subjected to constraints on the density dependence of\nneutron (nucleon) Landau effective mass in PNM (SNM), too. The impact of\nvarious sets of constraints on the behaviors of nuclear matter and NSs is\ndiscussed in detail. Systematic comparison with results previously obtained by\nemploying Skyrme interactions is done for posteriors of both nuclear matter\n(NM) and NS parameters. Special attention is given to the model and constraints\ndependence of correlations among various quantities."
                },
                "authors": [
                    {
                        "name": "Mikhail V. Beznogov"
                    },
                    {
                        "name": "Adriana R. Raduta"
                    }
                ],
                "author_detail": {
                    "name": "Adriana R. Raduta"
                },
                "author": "Adriana R. Raduta",
                "arxiv_comment": "25 pages, 10 figures and 4 tables; Phys. Rev. C, in press",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19325v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19325v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04249v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04249v2",
                "updated": "2024-08-21T09:04:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    9,
                    4,
                    41,
                    2,
                    234,
                    0
                ],
                "published": "2024-05-07T12:07:06Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    12,
                    7,
                    6,
                    1,
                    128,
                    0
                ],
                "title": "Federated Learning for Collaborative Inference Systems: The Case of\n  Early Exit Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning for Collaborative Inference Systems: The Case of\n  Early Exit Networks"
                },
                "summary": "As Internet of Things (IoT) technology advances, end devices like sensors and\nsmartphones are progressively equipped with AI models tailored to their local\nmemory and computational constraints. Local inference reduces communication\ncosts and latency; however, these smaller models typically underperform\ncompared to more sophisticated models deployed on edge servers or in the cloud.\nCooperative Inference Systems (CISs) address this performance trade-off by\nenabling smaller devices to offload part of their inference tasks to more\ncapable devices. These systems often deploy hierarchical models that share\nnumerous parameters, exemplified by Deep Neural Networks (DNNs) that utilize\nstrategies like early exits or ordered dropout. In such instances, Federated\nLearning (FL) may be employed to jointly train the models within a CIS. Yet,\ntraditional training methods have overlooked the operational dynamics of CISs\nduring inference, particularly the potential high heterogeneity in serving\nrates across clients. To address this gap, we propose a novel FL approach\ndesigned explicitly for use in CISs that accounts for these variations in\nserving rates. Our framework not only offers rigorous theoretical guarantees,\nbut also surpasses state-of-the-art (SOTA) training algorithms for CISs,\nespecially in scenarios where inference request rates or data availability are\nuneven among clients.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Internet of Things (IoT) technology advances, end devices like sensors and\nsmartphones are progressively equipped with AI models tailored to their local\nmemory and computational constraints. Local inference reduces communication\ncosts and latency; however, these smaller models typically underperform\ncompared to more sophisticated models deployed on edge servers or in the cloud.\nCooperative Inference Systems (CISs) address this performance trade-off by\nenabling smaller devices to offload part of their inference tasks to more\ncapable devices. These systems often deploy hierarchical models that share\nnumerous parameters, exemplified by Deep Neural Networks (DNNs) that utilize\nstrategies like early exits or ordered dropout. In such instances, Federated\nLearning (FL) may be employed to jointly train the models within a CIS. Yet,\ntraditional training methods have overlooked the operational dynamics of CISs\nduring inference, particularly the potential high heterogeneity in serving\nrates across clients. To address this gap, we propose a novel FL approach\ndesigned explicitly for use in CISs that accounts for these variations in\nserving rates. Our framework not only offers rigorous theoretical guarantees,\nbut also surpasses state-of-the-art (SOTA) training algorithms for CISs,\nespecially in scenarios where inference request rates or data availability are\nuneven among clients."
                },
                "authors": [
                    {
                        "name": "Caelin Kaplan"
                    },
                    {
                        "name": "Angelo Rodio"
                    },
                    {
                        "name": "Tareq Si Salem"
                    },
                    {
                        "name": "Chuan Xu"
                    },
                    {
                        "name": "Giovanni Neglia"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Neglia"
                },
                "author": "Giovanni Neglia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04249v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04249v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11431v1",
                "updated": "2024-08-21T08:39:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    8,
                    39,
                    49,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T08:39:49Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    8,
                    39,
                    49,
                    2,
                    234,
                    0
                ],
                "title": "Diagnosing and Remedying Knowledge Deficiencies in LLMs via Label-free\n  Curricular Meaningful Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnosing and Remedying Knowledge Deficiencies in LLMs via Label-free\n  Curricular Meaningful Learning"
                },
                "summary": "Large Language Models (LLMs) are versatile and demonstrate impressive\ngeneralization ability by mining and learning information from extensive\nunlabeled text. However, they still exhibit reasoning mistakes, often stemming\nfrom knowledge deficiencies, which can affect their trustworthiness and\nreliability. Although users can provide diverse and comprehensive queries,\nobtaining sufficient and effective feedback is demanding. Furthermore,\nevaluating LLMs comprehensively with limited labeled samples is difficult. This\nmakes it a challenge to diagnose and remedy the deficiencies of LLMs through\nrich label-free user queries. To tackle this challenge, we propose a label-free\ncurricular meaningful learning framework (LaMer). LaMer first employs relative\nentropy to automatically diagnose and quantify the knowledge deficiencies of\nLLMs in a label-free setting. Next, to remedy the diagnosed knowledge\ndeficiencies, we apply curricular meaningful learning: first, we adopt\nmeaningful learning to adaptively synthesize augmentation data according to the\nseverity of the deficiencies, and then design a curricular deficiency remedy\nstrategy to remedy the knowledge deficiencies of LLMs progressively.\nExperiments show that LaMer efficiently and effectively diagnoses and remedies\nknowledge deficiencies in LLMs, improving various LLMs across seven\nout-of-distribution (OOD) reasoning and language understanding benchmarks,\nachieving comparable results to baselines with just 40\\% training data. LaMer\neven surpasses methods that rely on labeled datasets for deficiency diagnosis.\nIn application, our label-free method can offer an effective knowledge\ndeficiency diagnostic tool for efficient LLM development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are versatile and demonstrate impressive\ngeneralization ability by mining and learning information from extensive\nunlabeled text. However, they still exhibit reasoning mistakes, often stemming\nfrom knowledge deficiencies, which can affect their trustworthiness and\nreliability. Although users can provide diverse and comprehensive queries,\nobtaining sufficient and effective feedback is demanding. Furthermore,\nevaluating LLMs comprehensively with limited labeled samples is difficult. This\nmakes it a challenge to diagnose and remedy the deficiencies of LLMs through\nrich label-free user queries. To tackle this challenge, we propose a label-free\ncurricular meaningful learning framework (LaMer). LaMer first employs relative\nentropy to automatically diagnose and quantify the knowledge deficiencies of\nLLMs in a label-free setting. Next, to remedy the diagnosed knowledge\ndeficiencies, we apply curricular meaningful learning: first, we adopt\nmeaningful learning to adaptively synthesize augmentation data according to the\nseverity of the deficiencies, and then design a curricular deficiency remedy\nstrategy to remedy the knowledge deficiencies of LLMs progressively.\nExperiments show that LaMer efficiently and effectively diagnoses and remedies\nknowledge deficiencies in LLMs, improving various LLMs across seven\nout-of-distribution (OOD) reasoning and language understanding benchmarks,\nachieving comparable results to baselines with just 40\\% training data. LaMer\neven surpasses methods that rely on labeled datasets for deficiency diagnosis.\nIn application, our label-free method can offer an effective knowledge\ndeficiency diagnostic tool for efficient LLM development."
                },
                "authors": [
                    {
                        "name": "Kai Xiong"
                    },
                    {
                        "name": "Xiao Ding"
                    },
                    {
                        "name": "Li Du"
                    },
                    {
                        "name": "Jiahao Ying"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Yixin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Yixin Cao"
                },
                "author": "Yixin Cao",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11428v1",
                "updated": "2024-08-21T08:37:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    8,
                    37,
                    10,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T08:37:10Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    8,
                    37,
                    10,
                    2,
                    234,
                    0
                ],
                "title": "Migrating Existing Container Workload to Kubernetes -- LLM Based\n  Approach and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Migrating Existing Container Workload to Kubernetes -- LLM Based\n  Approach and Evaluation"
                },
                "summary": "Although Kubernetes has become a widespread open-source system that automates\nthe management of containerized applications, its complexity can be a\nsignificant barrier, particularly for application developers unfamiliar with\nit. One approach employs large language models (LLMs) to assist developers in\ngenerating Kubernetes manifests; however it is currently impossible to\ndetermine whether the output satisfies given specifications and is\ncomprehensible. In this study, we proposed a benchmarking method for evaluating\nthe effectiveness of LLMs in synthesizing manifests, using the Compose\nspecification -- a standard widely adopted by application developers -- as\ninput. The proposed benchmarking method revealed that LLMs generally produce\naccurate results that compensate for simple specification gaps. However, we\nalso observed that inline comments for readability were often omitted, and\ncompletion accuracy was low for atypical inputs with unclear intentions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Kubernetes has become a widespread open-source system that automates\nthe management of containerized applications, its complexity can be a\nsignificant barrier, particularly for application developers unfamiliar with\nit. One approach employs large language models (LLMs) to assist developers in\ngenerating Kubernetes manifests; however it is currently impossible to\ndetermine whether the output satisfies given specifications and is\ncomprehensible. In this study, we proposed a benchmarking method for evaluating\nthe effectiveness of LLMs in synthesizing manifests, using the Compose\nspecification -- a standard widely adopted by application developers -- as\ninput. The proposed benchmarking method revealed that LLMs generally produce\naccurate results that compensate for simple specification gaps. However, we\nalso observed that inline comments for readability were often omitted, and\ncompletion accuracy was low for atypical inputs with unclear intentions."
                },
                "authors": [
                    {
                        "name": "Masaru Ueno"
                    },
                    {
                        "name": "Tetsuya Uchiumi"
                    }
                ],
                "author_detail": {
                    "name": "Tetsuya Uchiumi"
                },
                "author": "Tetsuya Uchiumi",
                "arxiv_comment": "submitted to ICSME 2024 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11414v1",
                "updated": "2024-08-21T08:19:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    8,
                    19,
                    55,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T08:19:55Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    8,
                    19,
                    55,
                    2,
                    234,
                    0
                ],
                "title": "emPDF: Inferring the Milky Way mass with data-driven distribution\n  function in phase space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "emPDF: Inferring the Milky Way mass with data-driven distribution\n  function in phase space"
                },
                "summary": "We introduce the emPDF (Empirical Distribution Function), a novel dynamical\nmodeling method that infers the gravitational potential from kinematic tracers\nwith optimal statistical efficiency under the minimal assumption of steady\nstate. emPDF determines the best-fit potential by maximizing the similarity\nbetween instantaneous kinematics and the time-averaged phase-space distribution\nfunction (DF), which is empirically constructed from observation upon the\ntheoretical foundation of oPDF (Han et al. 2016). This approach eliminates the\nneed for presumed functional forms of DFs or orbit libraries required by\nconventional DF- or orbit-based methods. emPDF stands out for its flexibility,\nefficiency, and capability in handling observational effects, making it\npreferable to the popular Jeans equation or other minimal assumption methods,\nespecially for the Milky Way (MW) outer halo where tracers often have limited\nsample size and poor data quality. We apply emPDF to infer the MW mass profile\nusing Gaia DR3 data of satellite galaxies and globular clusters, obtaining\nconsistent measurements with the constraints from simulation-informed DF\nfitting (Li et al. 2020). While the simulation-informed DF offers superior\nprecision owing to the additional information extracted from simulations, emPDF\nis independent of such supplementary knowledge and applicable to general tracer\npopulations. We provide tabulated measurements of the mass profile from emPDF,\nalong with updated measurements from simulation-informed DF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the emPDF (Empirical Distribution Function), a novel dynamical\nmodeling method that infers the gravitational potential from kinematic tracers\nwith optimal statistical efficiency under the minimal assumption of steady\nstate. emPDF determines the best-fit potential by maximizing the similarity\nbetween instantaneous kinematics and the time-averaged phase-space distribution\nfunction (DF), which is empirically constructed from observation upon the\ntheoretical foundation of oPDF (Han et al. 2016). This approach eliminates the\nneed for presumed functional forms of DFs or orbit libraries required by\nconventional DF- or orbit-based methods. emPDF stands out for its flexibility,\nefficiency, and capability in handling observational effects, making it\npreferable to the popular Jeans equation or other minimal assumption methods,\nespecially for the Milky Way (MW) outer halo where tracers often have limited\nsample size and poor data quality. We apply emPDF to infer the MW mass profile\nusing Gaia DR3 data of satellite galaxies and globular clusters, obtaining\nconsistent measurements with the constraints from simulation-informed DF\nfitting (Li et al. 2020). While the simulation-informed DF offers superior\nprecision owing to the additional information extracted from simulations, emPDF\nis independent of such supplementary knowledge and applicable to general tracer\npopulations. We provide tabulated measurements of the mass profile from emPDF,\nalong with updated measurements from simulation-informed DF."
                },
                "authors": [
                    {
                        "name": "Zhaozhou Li"
                    },
                    {
                        "name": "Jiaxin Han"
                    },
                    {
                        "name": "Wenting Wang"
                    },
                    {
                        "name": "Yong-Zhong Qian"
                    },
                    {
                        "name": "Qingyang Li"
                    },
                    {
                        "name": "Yipeng Jing"
                    },
                    {
                        "name": "Ting S. Li"
                    }
                ],
                "author_detail": {
                    "name": "Ting S. Li"
                },
                "author": "Ting S. Li",
                "arxiv_comment": "18 pages, 10 figures. Submitted to MNRAS. Comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11932v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11932v2",
                "updated": "2024-08-21T08:11:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    8,
                    11,
                    59,
                    2,
                    234,
                    0
                ],
                "published": "2024-05-20T10:16:26Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    10,
                    16,
                    26,
                    0,
                    141,
                    0
                ],
                "title": "Nonequilbrium physics of generative diffusion models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonequilbrium physics of generative diffusion models"
                },
                "summary": "Generative diffusion models apply the concept of Langevin dynamics in physics\nto machine leaning, attracting a lot of interests from engineering, statistics\nand physics, but a complete picture about inherent mechanisms is still lacking.\nIn this paper, we provide a transparent physics analysis of diffusion models,\nformulating the fluctuation theorem, entropy production, equilibrium measure,\nand Franz-Parisi potential to understand the dynamic process and intrinsic\nphase transitions. Our analysis is rooted in a path integral representation of\nboth forward and backward dynamics, and in treating the reverse diffusion\ngenerative process as a statistical inference, where the time-dependent state\nvariables serve as quenched disorder akin to that in spin glass theory. Our\nstudy thus links stochastic thermodynamics, statistical inference and geometry\nbased analysis together to yield a coherent picture about how the generative\ndiffusion models work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative diffusion models apply the concept of Langevin dynamics in physics\nto machine leaning, attracting a lot of interests from engineering, statistics\nand physics, but a complete picture about inherent mechanisms is still lacking.\nIn this paper, we provide a transparent physics analysis of diffusion models,\nformulating the fluctuation theorem, entropy production, equilibrium measure,\nand Franz-Parisi potential to understand the dynamic process and intrinsic\nphase transitions. Our analysis is rooted in a path integral representation of\nboth forward and backward dynamics, and in treating the reverse diffusion\ngenerative process as a statistical inference, where the time-dependent state\nvariables serve as quenched disorder akin to that in spin glass theory. Our\nstudy thus links stochastic thermodynamics, statistical inference and geometry\nbased analysis together to yield a coherent picture about how the generative\ndiffusion models work."
                },
                "authors": [
                    {
                        "name": "Zhendong Yu"
                    },
                    {
                        "name": "Haiping Huang"
                    }
                ],
                "author_detail": {
                    "name": "Haiping Huang"
                },
                "author": "Haiping Huang",
                "arxiv_comment": "24 pages, 9 figures, 30 refs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11932v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11932v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08207v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08207v2",
                "updated": "2024-08-21T07:57:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    57,
                    30,
                    2,
                    234,
                    0
                ],
                "published": "2024-01-16T08:47:54Z",
                "published_parsed": [
                    2024,
                    1,
                    16,
                    8,
                    47,
                    54,
                    1,
                    16,
                    0
                ],
                "title": "AGN jet-inflated bubbles as possible origin of odd radio circles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGN jet-inflated bubbles as possible origin of odd radio circles"
                },
                "summary": "Odd radio circles (ORCs) are newly discovered extragalactic radio objects\nwith unknown origin. In this work, we carry out three-dimensional cosmic-ray\n(CR) magnetohydrodynamic simulations using the FLASH code and predict the radio\nmorphology of end-on active galactic nucleus (AGN) jet-inflated bubbles\nconsidering hadronic emission. We consider CR proton (CRp)-dominated jets as\nthey tend to inflate oblate bubbles, promising to reproduce the large inferred\nsizes of the ORCs when viewed end-on. We find that powerful and long-duration\nCRp-dominated jets can create bubbles with similar sizes ($\\sim 300-600$ kpc)\nand radio morphology (circular and edge-brightened) to the observed ORCs in\nlow-mass ($M_{\\rm vir}\\sim 8\\times 10^{12} - 8\\times 10^{13}~M_\\odot$) halos.\nGiven the same amount of input jet energy, longer-duration (thus lower-power)\njets tend to create larger bubbles since high-power jets generate strong shocks\nthat carry away a significant portion of the jet energy. The edge-brightened\nfeature of the observed ORCs is naturally reproduced due to efficient hadronic\ncollisions at the interface between the bubbles and the ambient medium. We\nfurther discuss the radio luminosity, X-ray detectability, and the possible\norigin of such strong AGN jets in the context of galaxy evolution. We conclude\nthat end-on CR-dominated AGN bubbles could be a plausible scenario for the\nformation of ORCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Odd radio circles (ORCs) are newly discovered extragalactic radio objects\nwith unknown origin. In this work, we carry out three-dimensional cosmic-ray\n(CR) magnetohydrodynamic simulations using the FLASH code and predict the radio\nmorphology of end-on active galactic nucleus (AGN) jet-inflated bubbles\nconsidering hadronic emission. We consider CR proton (CRp)-dominated jets as\nthey tend to inflate oblate bubbles, promising to reproduce the large inferred\nsizes of the ORCs when viewed end-on. We find that powerful and long-duration\nCRp-dominated jets can create bubbles with similar sizes ($\\sim 300-600$ kpc)\nand radio morphology (circular and edge-brightened) to the observed ORCs in\nlow-mass ($M_{\\rm vir}\\sim 8\\times 10^{12} - 8\\times 10^{13}~M_\\odot$) halos.\nGiven the same amount of input jet energy, longer-duration (thus lower-power)\njets tend to create larger bubbles since high-power jets generate strong shocks\nthat carry away a significant portion of the jet energy. The edge-brightened\nfeature of the observed ORCs is naturally reproduced due to efficient hadronic\ncollisions at the interface between the bubbles and the ambient medium. We\nfurther discuss the radio luminosity, X-ray detectability, and the possible\norigin of such strong AGN jets in the context of galaxy evolution. We conclude\nthat end-on CR-dominated AGN bubbles could be a plausible scenario for the\nformation of ORCs."
                },
                "authors": [
                    {
                        "name": "Yen-Hsing Lin"
                    },
                    {
                        "name": "H. -Y. Karen Yang"
                    }
                ],
                "author_detail": {
                    "name": "H. -Y. Karen Yang"
                },
                "arxiv_affiliation": "NTHU/NCTS",
                "author": "H. -Y. Karen Yang",
                "arxiv_comment": "17 pages, 10 figures, ApJ accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08207v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08207v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10668v2",
                "updated": "2024-08-21T07:50:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    50,
                    29,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-20T09:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    11,
                    21,
                    1,
                    233,
                    0
                ],
                "title": "Probing the Safety Response Boundary of Large Language Models via Unsafe\n  Decoding Path Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing the Safety Response Boundary of Large Language Models via Unsafe\n  Decoding Path Generation"
                },
                "summary": "Large Language Models (LLMs) are implicit troublemakers. While they provide\nvaluable insights and assist in problem-solving, they can also potentially\nserve as a resource for malicious activities. Implementing safety alignment\ncould mitigate the risk of LLMs generating harmful responses. We argue that:\neven when an LLM appears to successfully block harmful queries, there may still\nbe hidden vulnerabilities that could act as ticking time bombs. To identify\nthese underlying weaknesses, we propose to use a cost value model as both a\ndetector and an attacker. Trained on external or self-generated harmful\ndatasets, the cost value model could successfully influence the original safe\nLLM to output toxic content in decoding process. For instance, LLaMA-2-chat 7B\noutputs 39.18% concrete toxic content, along with only 22.16% refusals without\nany harmful suffixes. These potential weaknesses can then be exploited via\nprompt optimization such as soft prompts on images. We name this decoding\nstrategy: Jailbreak Value Decoding (JVD), emphasizing that seemingly secure\nLLMs may not be as safe as we initially believe. They could be used to gather\nharmful data or launch covert attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are implicit troublemakers. While they provide\nvaluable insights and assist in problem-solving, they can also potentially\nserve as a resource for malicious activities. Implementing safety alignment\ncould mitigate the risk of LLMs generating harmful responses. We argue that:\neven when an LLM appears to successfully block harmful queries, there may still\nbe hidden vulnerabilities that could act as ticking time bombs. To identify\nthese underlying weaknesses, we propose to use a cost value model as both a\ndetector and an attacker. Trained on external or self-generated harmful\ndatasets, the cost value model could successfully influence the original safe\nLLM to output toxic content in decoding process. For instance, LLaMA-2-chat 7B\noutputs 39.18% concrete toxic content, along with only 22.16% refusals without\nany harmful suffixes. These potential weaknesses can then be exploited via\nprompt optimization such as soft prompts on images. We name this decoding\nstrategy: Jailbreak Value Decoding (JVD), emphasizing that seemingly secure\nLLMs may not be as safe as we initially believe. They could be used to gather\nharmful data or launch covert attacks."
                },
                "authors": [
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Bingzhe Wu"
                    },
                    {
                        "name": "Yatao Bian"
                    },
                    {
                        "name": "Yongzhe Chang"
                    },
                    {
                        "name": "Xueqian Wang"
                    },
                    {
                        "name": "Peilin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Peilin Zhao"
                },
                "author": "Peilin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11397v1",
                "updated": "2024-08-21T07:43:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    43,
                    50,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T07:43:50Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    43,
                    50,
                    2,
                    234,
                    0
                ],
                "title": "EAGLE: Elevating Geometric Reasoning through LLM-empowered Visual\n  Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EAGLE: Elevating Geometric Reasoning through LLM-empowered Visual\n  Instruction Tuning"
                },
                "summary": "Multi-modal Large Language Models have recently experienced rapid\ndevelopments and excel in various multi-modal tasks. However, they still\nstruggle with mathematical geometric problem solving, which requires\nexceptional visual perception proficiency. Existing MLLMs mostly optimize the\nLLM backbone to acquire geometric reasoning capabilities, while rarely\nemphasizing improvements in visual comprehension. In this paper, we first\ninvestigate the visual perception performance of MLLMs when facing geometric\ndiagrams. Our findings reveal that current MLLMs severely suffer from\ninaccurate geometric perception and hallucinations. To address these\nlimitations, we propose EAGLE, a novel two-stage end-to-end visual enhancement\nMLLM framework designed to ElevAte Geometric reasoning through LLM-Empowered\nvisual instruction tuning. Specifically, in the preliminary stage, we feed\ngeometric image-caption pairs into our MLLM that contains a fully fine-tuning\nCLIP ViT and a frozen LLM, aiming to endow our model with basic geometric\nknowledge. In the subsequent advanced stage, we incorporate LoRA modules into\nthe vision encoder and unfreeze the LLM backbone. This enables the model to\nleverage the inherent CoT rationales within question-answer pairs, guiding the\nMLLM to focus on nuanced visual cues and enhancing its overall perceptual\ncapacity. Moreover, we optimize the cross-modal projector in both stages to\nfoster adaptive visual-linguistic alignments. After the two-stage visual\nenhancement, we develop the geometry expert model EAGLE-7B. Extensive\nexperiments on popular benchmarks demonstrate the effectiveness of our model.\nFor example, on the GeoQA benchmark, EAGLE-7B not only surpasses the exemplary\nG-LLaVA 7B model by 2.9%, but also marginally outperforms the larger G-LLaVA\n13B model. On the MathVista benchmark, EAGLE-7B achieves remarkable 3.8%\nimprovements compared with the proprietary model GPT-4V.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Large Language Models have recently experienced rapid\ndevelopments and excel in various multi-modal tasks. However, they still\nstruggle with mathematical geometric problem solving, which requires\nexceptional visual perception proficiency. Existing MLLMs mostly optimize the\nLLM backbone to acquire geometric reasoning capabilities, while rarely\nemphasizing improvements in visual comprehension. In this paper, we first\ninvestigate the visual perception performance of MLLMs when facing geometric\ndiagrams. Our findings reveal that current MLLMs severely suffer from\ninaccurate geometric perception and hallucinations. To address these\nlimitations, we propose EAGLE, a novel two-stage end-to-end visual enhancement\nMLLM framework designed to ElevAte Geometric reasoning through LLM-Empowered\nvisual instruction tuning. Specifically, in the preliminary stage, we feed\ngeometric image-caption pairs into our MLLM that contains a fully fine-tuning\nCLIP ViT and a frozen LLM, aiming to endow our model with basic geometric\nknowledge. In the subsequent advanced stage, we incorporate LoRA modules into\nthe vision encoder and unfreeze the LLM backbone. This enables the model to\nleverage the inherent CoT rationales within question-answer pairs, guiding the\nMLLM to focus on nuanced visual cues and enhancing its overall perceptual\ncapacity. Moreover, we optimize the cross-modal projector in both stages to\nfoster adaptive visual-linguistic alignments. After the two-stage visual\nenhancement, we develop the geometry expert model EAGLE-7B. Extensive\nexperiments on popular benchmarks demonstrate the effectiveness of our model.\nFor example, on the GeoQA benchmark, EAGLE-7B not only surpasses the exemplary\nG-LLaVA 7B model by 2.9%, but also marginally outperforms the larger G-LLaVA\n13B model. On the MathVista benchmark, EAGLE-7B achieves remarkable 3.8%\nimprovements compared with the proprietary model GPT-4V."
                },
                "authors": [
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Yao Du"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Yufang Liu"
                    },
                    {
                        "name": "Mengdi Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11396v1",
                "updated": "2024-08-21T07:43:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    43,
                    49,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T07:43:49Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    43,
                    49,
                    2,
                    234,
                    0
                ],
                "title": "MoE-LPR: Multilingual Extension of Large Language Models through\n  Mixture-of-Experts with Language Priors Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-LPR: Multilingual Extension of Large Language Models through\n  Mixture-of-Experts with Language Priors Routing"
                },
                "summary": "Large Language Models (LLMs) are often English-centric due to the\ndisproportionate distribution of languages in their pre-training data.\nEnhancing non-English language capabilities through post-pretraining often\nresults in catastrophic forgetting of the ability of original languages.\nPrevious methods either achieve good expansion with severe forgetting or slight\nforgetting with poor expansion, indicating the challenge of balancing language\nexpansion while preventing forgetting. In this paper, we propose a method\ncalled MoE-LPR (Mixture-of-Experts with Language Priors Routing) to alleviate\nthis problem. MoE-LPR employs a two-stage training approach to enhance the\nmultilingual capability. First, the model is post-pretrained into a\nMixture-of-Experts (MoE) architecture by upcycling, where all the original\nparameters are frozen and new experts are added. In this stage, we focus\nimproving the ability on expanded languages, without using any original\nlanguage data. Then, the model reviews the knowledge of the original languages\nwith replay data amounting to less than 1% of post-pretraining, where we\nincorporate language priors routing to better recover the abilities of the\noriginal languages. Evaluations on multiple benchmarks show that MoE-LPR\noutperforms other post-pretraining methods. Freezing original parameters\npreserves original language knowledge while adding new experts preserves the\nlearning ability. Reviewing with LPR enables effective utilization of\nmultilingual knowledge within the parameters. Additionally, the MoE\narchitecture maintains the same inference overhead while increasing total model\nparameters. Extensive experiments demonstrate MoE-LPR's effectiveness in\nimproving expanded languages and preserving original language proficiency with\nsuperior scalability. Code and scripts are freely available at\nhttps://github.com/zjwang21/MoE-LPR.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are often English-centric due to the\ndisproportionate distribution of languages in their pre-training data.\nEnhancing non-English language capabilities through post-pretraining often\nresults in catastrophic forgetting of the ability of original languages.\nPrevious methods either achieve good expansion with severe forgetting or slight\nforgetting with poor expansion, indicating the challenge of balancing language\nexpansion while preventing forgetting. In this paper, we propose a method\ncalled MoE-LPR (Mixture-of-Experts with Language Priors Routing) to alleviate\nthis problem. MoE-LPR employs a two-stage training approach to enhance the\nmultilingual capability. First, the model is post-pretrained into a\nMixture-of-Experts (MoE) architecture by upcycling, where all the original\nparameters are frozen and new experts are added. In this stage, we focus\nimproving the ability on expanded languages, without using any original\nlanguage data. Then, the model reviews the knowledge of the original languages\nwith replay data amounting to less than 1% of post-pretraining, where we\nincorporate language priors routing to better recover the abilities of the\noriginal languages. Evaluations on multiple benchmarks show that MoE-LPR\noutperforms other post-pretraining methods. Freezing original parameters\npreserves original language knowledge while adding new experts preserves the\nlearning ability. Reviewing with LPR enables effective utilization of\nmultilingual knowledge within the parameters. Additionally, the MoE\narchitecture maintains the same inference overhead while increasing total model\nparameters. Extensive experiments demonstrate MoE-LPR's effectiveness in\nimproving expanded languages and preserving original language proficiency with\nsuperior scalability. Code and scripts are freely available at\nhttps://github.com/zjwang21/MoE-LPR.git."
                },
                "authors": [
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhijun Wang"
                    },
                    {
                        "name": "Shujian Huang"
                    },
                    {
                        "name": "Xin Huang"
                    },
                    {
                        "name": "Xue Han"
                    },
                    {
                        "name": "Junlan Feng"
                    },
                    {
                        "name": "Chao Deng"
                    },
                    {
                        "name": "Weihua Luo"
                    },
                    {
                        "name": "Jiajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Chen"
                },
                "author": "Jiajun Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11393v1",
                "updated": "2024-08-21T07:38:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    38,
                    51,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T07:38:51Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    38,
                    51,
                    2,
                    234,
                    0
                ],
                "title": "First Activations Matter: Training-Free Methods for Dynamic Activation\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First Activations Matter: Training-Free Methods for Dynamic Activation\n  in Large Language Models"
                },
                "summary": "Dynamic activation (DA) techniques, such as DejaVu and MoEfication, have\ndemonstrated their potential to significantly enhance the inference efficiency\nof large language models (LLMs). However, these techniques often rely on ReLU\nactivation functions or require additional parameters and training to maintain\nperformance. This paper introduces a training-free Threshold-based Dynamic\nActivation(TDA) method that leverage sequence information to exploit the\ninherent sparsity of models across various architectures. This method is\ndesigned to accelerate generation speed by 18-25\\% without significantly\ncompromising task performance, thereby addressing the limitations of existing\nDA techniques. Moreover, we delve into the root causes of LLM sparsity and\ntheoretically analyze two of its critical features: history-related activation\nuncertainty and semantic-irrelevant activation inertia. Our comprehensive\nanalyses not only provide a robust theoretical foundation for DA methods but\nalso offer valuable insights to guide future research in optimizing LLMs for\ngreater efficiency and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic activation (DA) techniques, such as DejaVu and MoEfication, have\ndemonstrated their potential to significantly enhance the inference efficiency\nof large language models (LLMs). However, these techniques often rely on ReLU\nactivation functions or require additional parameters and training to maintain\nperformance. This paper introduces a training-free Threshold-based Dynamic\nActivation(TDA) method that leverage sequence information to exploit the\ninherent sparsity of models across various architectures. This method is\ndesigned to accelerate generation speed by 18-25\\% without significantly\ncompromising task performance, thereby addressing the limitations of existing\nDA techniques. Moreover, we delve into the root causes of LLM sparsity and\ntheoretically analyze two of its critical features: history-related activation\nuncertainty and semantic-irrelevant activation inertia. Our comprehensive\nanalyses not only provide a robust theoretical foundation for DA methods but\nalso offer valuable insights to guide future research in optimizing LLMs for\ngreater efficiency and effectiveness."
                },
                "authors": [
                    {
                        "name": "Chi Ma"
                    },
                    {
                        "name": "Mincong Huang"
                    },
                    {
                        "name": "Ying Zhang"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Yujie Wang"
                    },
                    {
                        "name": "Lei Yu"
                    },
                    {
                        "name": "Chuan Liu"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11386v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11386v1",
                "updated": "2024-08-21T07:30:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    30,
                    11,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T07:30:11Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    30,
                    11,
                    2,
                    234,
                    0
                ],
                "title": "Unlocking Sustainability Compliance: Characterizing the EU Taxonomy for\n  Business Process Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Sustainability Compliance: Characterizing the EU Taxonomy for\n  Business Process Management"
                },
                "summary": "To promote sustainable business practices, and to achieve climate neutrality\nby 2050, the EU has developed the taxonomy of sustainable activities, which\ndescribes when exactly business practices can be considered sustainable. While\nthe taxonomy has only been recently established, progressively more companies\nwill have to report how much of their revenue was created via sustainably\nexecuted business processes. To help companies prepare to assess whether their\nbusiness processes comply with the constraints outlined in the taxonomy, we\ninvestigate in how far these criteria can be used for conformance checking,\nthat is, assessing in a data-driven manner, whether business process executions\nadhere to regulatory constraints. For this, we develop a few-shot learning\npipeline to characterize the constraints of the taxonomy with the help of an\nLLM as to the process dimensions they relate to. We find that many constraints\nof the taxonomy are useable for conformance checking, particularly in the\nsectors of energy, manufacturing, and transport. This will aid companies in\npreparing to monitor regulatory compliance with the taxonomy automatically, by\ncharacterizing what kind of information they need to extract, and by providing\na better understanding of sectors where such an assessment is feasible and\nwhere it is not.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To promote sustainable business practices, and to achieve climate neutrality\nby 2050, the EU has developed the taxonomy of sustainable activities, which\ndescribes when exactly business practices can be considered sustainable. While\nthe taxonomy has only been recently established, progressively more companies\nwill have to report how much of their revenue was created via sustainably\nexecuted business processes. To help companies prepare to assess whether their\nbusiness processes comply with the constraints outlined in the taxonomy, we\ninvestigate in how far these criteria can be used for conformance checking,\nthat is, assessing in a data-driven manner, whether business process executions\nadhere to regulatory constraints. For this, we develop a few-shot learning\npipeline to characterize the constraints of the taxonomy with the help of an\nLLM as to the process dimensions they relate to. We find that many constraints\nof the taxonomy are useable for conformance checking, particularly in the\nsectors of energy, manufacturing, and transport. This will aid companies in\npreparing to monitor regulatory compliance with the taxonomy automatically, by\ncharacterizing what kind of information they need to extract, and by providing\na better understanding of sectors where such an assessment is feasible and\nwhere it is not."
                },
                "authors": [
                    {
                        "name": "Finn Klessascheck"
                    },
                    {
                        "name": "Stephan A. Fahrenkrog-Petersen"
                    },
                    {
                        "name": "Jan Mendling"
                    },
                    {
                        "name": "Luise Pufahl"
                    }
                ],
                "author_detail": {
                    "name": "Luise Pufahl"
                },
                "author": "Luise Pufahl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11386v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11386v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14569v2",
                "updated": "2024-08-21T07:26:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    26,
                    10,
                    2,
                    234,
                    0
                ],
                "published": "2024-05-23T13:44:48Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    13,
                    44,
                    48,
                    3,
                    144,
                    0
                ],
                "title": "PrivCirNet: Efficient Private Inference via Block Circulant\n  Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrivCirNet: Efficient Private Inference via Block Circulant\n  Transformation"
                },
                "summary": "Homomorphic encryption (HE)-based deep neural network (DNN) inference\nprotects data and model privacy but suffers from significant computation\noverhead. We observe transforming the DNN weights into circulant matrices\nconverts general matrix-vector multiplications into HE-friendly 1-dimensional\nconvolutions, drastically reducing the HE computation cost. Hence, in this\npaper, we propose \\method, a protocol/network co-optimization framework based\non block circulant transformation. At the protocol level, PrivCirNet customizes\nthe HE encoding algorithm that is fully compatible with the block circulant\ntransformation and reduces the computation latency in proportion to the block\nsize. At the network level, we propose a latency-aware formulation to search\nfor the layer-wise block size assignment based on second-order information.\nPrivCirNet also leverages layer fusion to further reduce the inference cost. We\ncompare PrivCirNet with the state-of-the-art HE-based framework Bolt (IEEE S\\&P\n2024) and the HE-friendly pruning method SpENCNN (ICML 2023). For ResNet-18 and\nVision Transformer (ViT) on Tiny ImageNet, PrivCirNet reduces latency by\n$5.0\\times$ and $1.3\\times$ with iso-accuracy over Bolt, respectively, and\nimproves accuracy by $4.1\\%$ and $12\\%$ over SpENCNN, respectively. For\nMobileNetV2 on ImageNet, PrivCirNet achieves $1.7\\times$ lower latency and\n$4.2\\%$ better accuracy over Bolt and SpENCNN, respectively. Our code and\ncheckpoints are available on Git Hub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Homomorphic encryption (HE)-based deep neural network (DNN) inference\nprotects data and model privacy but suffers from significant computation\noverhead. We observe transforming the DNN weights into circulant matrices\nconverts general matrix-vector multiplications into HE-friendly 1-dimensional\nconvolutions, drastically reducing the HE computation cost. Hence, in this\npaper, we propose \\method, a protocol/network co-optimization framework based\non block circulant transformation. At the protocol level, PrivCirNet customizes\nthe HE encoding algorithm that is fully compatible with the block circulant\ntransformation and reduces the computation latency in proportion to the block\nsize. At the network level, we propose a latency-aware formulation to search\nfor the layer-wise block size assignment based on second-order information.\nPrivCirNet also leverages layer fusion to further reduce the inference cost. We\ncompare PrivCirNet with the state-of-the-art HE-based framework Bolt (IEEE S\\&P\n2024) and the HE-friendly pruning method SpENCNN (ICML 2023). For ResNet-18 and\nVision Transformer (ViT) on Tiny ImageNet, PrivCirNet reduces latency by\n$5.0\\times$ and $1.3\\times$ with iso-accuracy over Bolt, respectively, and\nimproves accuracy by $4.1\\%$ and $12\\%$ over SpENCNN, respectively. For\nMobileNetV2 on ImageNet, PrivCirNet achieves $1.7\\times$ lower latency and\n$4.2\\%$ better accuracy over Bolt and SpENCNN, respectively. Our code and\ncheckpoints are available on Git Hub."
                },
                "authors": [
                    {
                        "name": "Tianshi Xu"
                    },
                    {
                        "name": "Lemeng Wu"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11382v1",
                "updated": "2024-08-21T07:23:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    23,
                    34,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T07:23:34Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    23,
                    34,
                    2,
                    234,
                    0
                ],
                "title": "On the Interchangeability of Positional Embeddings in Multilingual\n  Neural Machine Translation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Interchangeability of Positional Embeddings in Multilingual\n  Neural Machine Translation Models"
                },
                "summary": "Standard Neural Machine Translation (NMT) models have traditionally been\ntrained with Sinusoidal Positional Embeddings (PEs), which are inadequate for\ncapturing long-range dependencies and are inefficient for long-context or\ndocument-level translation. In contrast, state-of-the-art large language models\n(LLMs) employ relative PEs, demonstrating superior length generalization. This\nwork explores the potential for efficiently switching the Positional Embeddings\nof pre-trained NMT models from absolute sinusoidal PEs to relative approaches\nsuch as RoPE and ALiBi. Our findings reveal that sinusoidal PEs can be\neffectively replaced with RoPE and ALiBi with negligible or no performance\nloss, achieved by fine-tuning on a small fraction of high-quality data.\nAdditionally, models trained without Positional Embeddings (NoPE) are not a\nviable solution for Encoder-Decoder architectures, as they consistently\nunder-perform compared to models utilizing any form of Positional Embedding.\nFurthermore, even a model trained from scratch with these relative PEs slightly\nunder-performs a fine-tuned model, underscoring the efficiency and validity of\nour hypothesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standard Neural Machine Translation (NMT) models have traditionally been\ntrained with Sinusoidal Positional Embeddings (PEs), which are inadequate for\ncapturing long-range dependencies and are inefficient for long-context or\ndocument-level translation. In contrast, state-of-the-art large language models\n(LLMs) employ relative PEs, demonstrating superior length generalization. This\nwork explores the potential for efficiently switching the Positional Embeddings\nof pre-trained NMT models from absolute sinusoidal PEs to relative approaches\nsuch as RoPE and ALiBi. Our findings reveal that sinusoidal PEs can be\neffectively replaced with RoPE and ALiBi with negligible or no performance\nloss, achieved by fine-tuning on a small fraction of high-quality data.\nAdditionally, models trained without Positional Embeddings (NoPE) are not a\nviable solution for Encoder-Decoder architectures, as they consistently\nunder-perform compared to models utilizing any form of Positional Embedding.\nFurthermore, even a model trained from scratch with these relative PEs slightly\nunder-performs a fine-tuned model, underscoring the efficiency and validity of\nour hypothesis."
                },
                "authors": [
                    {
                        "name": "Varun Gumma"
                    },
                    {
                        "name": "Pranjal A. Chitale"
                    },
                    {
                        "name": "Kalika Bali"
                    }
                ],
                "author_detail": {
                    "name": "Kalika Bali"
                },
                "author": "Kalika Bali",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11381v1",
                "updated": "2024-08-21T07:20:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    20,
                    48,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T07:20:48Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    20,
                    48,
                    2,
                    234,
                    0
                ],
                "title": "RAGLAB: A Modular and Research-Oriented Unified Framework for\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGLAB: A Modular and Research-Oriented Unified Framework for\n  Retrieval-Augmented Generation"
                },
                "summary": "Large Language Models (LLMs) demonstrate human-level capabilities in\ndialogue, reasoning, and knowledge retention. However, even the most advanced\nLLMs face challenges such as hallucinations and real-time updating of their\nknowledge. Current research addresses this bottleneck by equipping LLMs with\nexternal knowledge, a technique known as Retrieval Augmented Generation (RAG).\nHowever, two key issues constrained the development of RAG. First, there is a\ngrowing lack of comprehensive and fair comparisons between novel RAG\nalgorithms. Second, open-source tools such as LlamaIndex and LangChain employ\nhigh-level abstractions, which results in a lack of transparency and limits the\nability to develop novel algorithms and evaluation metrics. To close this gap,\nwe introduce RAGLAB, a modular and research-oriented open-source library.\nRAGLAB reproduces 6 existing algorithms and provides a comprehensive ecosystem\nfor investigating RAG algorithms. Leveraging RAGLAB, we conduct a fair\ncomparison of 6 RAG algorithms across 10 benchmarks. With RAGLAB, researchers\ncan efficiently compare the performance of various algorithms and develop novel\nalgorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate human-level capabilities in\ndialogue, reasoning, and knowledge retention. However, even the most advanced\nLLMs face challenges such as hallucinations and real-time updating of their\nknowledge. Current research addresses this bottleneck by equipping LLMs with\nexternal knowledge, a technique known as Retrieval Augmented Generation (RAG).\nHowever, two key issues constrained the development of RAG. First, there is a\ngrowing lack of comprehensive and fair comparisons between novel RAG\nalgorithms. Second, open-source tools such as LlamaIndex and LangChain employ\nhigh-level abstractions, which results in a lack of transparency and limits the\nability to develop novel algorithms and evaluation metrics. To close this gap,\nwe introduce RAGLAB, a modular and research-oriented open-source library.\nRAGLAB reproduces 6 existing algorithms and provides a comprehensive ecosystem\nfor investigating RAG algorithms. Leveraging RAGLAB, we conduct a fair\ncomparison of 6 RAG algorithms across 10 benchmarks. With RAGLAB, researchers\ncan efficiently compare the performance of various algorithms and develop novel\nalgorithms."
                },
                "authors": [
                    {
                        "name": "Xuanwang Zhang"
                    },
                    {
                        "name": "Yunze Song"
                    },
                    {
                        "name": "Yidong Wang"
                    },
                    {
                        "name": "Shuyun Tang"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Zhengran Zeng"
                    },
                    {
                        "name": "Zhen Wu"
                    },
                    {
                        "name": "Wei Ye"
                    },
                    {
                        "name": "Wenyuan Xu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Xinyu Dai"
                    },
                    {
                        "name": "Shikun Zhang"
                    },
                    {
                        "name": "Qingsong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Qingsong Wen"
                },
                "author": "Qingsong Wen",
                "arxiv_comment": "6 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10529v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10529v3",
                "updated": "2024-08-22T03:40:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    3,
                    40,
                    50,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-20T04:06:58Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    4,
                    6,
                    58,
                    1,
                    233,
                    0
                ],
                "title": "Automated Detection of Algorithm Debt in Deep Learning Frameworks: An\n  Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Detection of Algorithm Debt in Deep Learning Frameworks: An\n  Empirical Study"
                },
                "summary": "Context: Previous studies demonstrate that Machine or Deep Learning (ML/DL)\nmodels can detect Technical Debt from source code comments called Self-Admitted\nTechnical Debt (SATD). Despite the importance of ML/DL in software development,\nlimited studies focus on automated detection for new SATD types: Algorithm Debt\n(AD). AD detection is important because it helps to identify TD early,\nfacilitating research, learning, and preventing the accumulation of issues\nrelated to model degradation and lack of scalability. Aim: Our goal is to\nimprove AD detection performance of various ML/DL models. Method: We will\nperform empirical studies using approaches: TF-IDF, Count Vectorizer, Hash\nVectorizer, and TD-indicative words to identify features that improve AD\ndetection, using ML/DL classifiers with different data featurisations. We will\nuse an existing dataset curated from seven DL frameworks where comments were\nmanually classified as AD, Compatibility, Defect, Design, Documentation,\nRequirement, and Test Debt. We will explore various word embedding methods to\nfurther enrich features for ML models. These embeddings will be from models\nfounded in DL such as ROBERTA, ALBERTv2, and large language models (LLMs):\nINSTRUCTOR and VOYAGE AI. We will enrich the dataset by incorporating\nAD-related terms, then train various ML/DL classifiers, Support Vector Machine,\nLogistic Regression, Random Forest, ROBERTA, and ALBERTv2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Previous studies demonstrate that Machine or Deep Learning (ML/DL)\nmodels can detect Technical Debt from source code comments called Self-Admitted\nTechnical Debt (SATD). Despite the importance of ML/DL in software development,\nlimited studies focus on automated detection for new SATD types: Algorithm Debt\n(AD). AD detection is important because it helps to identify TD early,\nfacilitating research, learning, and preventing the accumulation of issues\nrelated to model degradation and lack of scalability. Aim: Our goal is to\nimprove AD detection performance of various ML/DL models. Method: We will\nperform empirical studies using approaches: TF-IDF, Count Vectorizer, Hash\nVectorizer, and TD-indicative words to identify features that improve AD\ndetection, using ML/DL classifiers with different data featurisations. We will\nuse an existing dataset curated from seven DL frameworks where comments were\nmanually classified as AD, Compatibility, Defect, Design, Documentation,\nRequirement, and Test Debt. We will explore various word embedding methods to\nfurther enrich features for ML models. These embeddings will be from models\nfounded in DL such as ROBERTA, ALBERTv2, and large language models (LLMs):\nINSTRUCTOR and VOYAGE AI. We will enrich the dataset by incorporating\nAD-related terms, then train various ML/DL classifiers, Support Vector Machine,\nLogistic Regression, Random Forest, ROBERTA, and ALBERTv2."
                },
                "authors": [
                    {
                        "name": "Emmanuel Iko-Ojo Simon"
                    },
                    {
                        "name": "Chirath Hettiarachchi"
                    },
                    {
                        "name": "Alex Potanin"
                    },
                    {
                        "name": "Hanna Suominen"
                    },
                    {
                        "name": "Fatemeh Fard"
                    }
                ],
                "author_detail": {
                    "name": "Fatemeh Fard"
                },
                "author": "Fatemeh Fard",
                "arxiv_comment": "Accepted as Continuity Acceptance (CA) for a Stage 1 registration of\n  the Registered Report Track at 40th IEEE International Conference on Software\n  Maintenance and Evolution (ICSME 2024), Flagstaff, USA, October 6-11, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10529v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10529v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.7; K.6.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10774v2",
                "updated": "2024-08-21T06:48:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    48,
                    16,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-20T12:13:04Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    12,
                    13,
                    4,
                    1,
                    233,
                    0
                ],
                "title": "Flexora: Flexible Low Rank Adaptation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexora: Flexible Low Rank Adaptation for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are driving advancements in artificial\nintelligence by increasing the scale of model parameters, which has\nsignificantly enhanced generalization ability and unlocked new capabilities in\npractice. However, their performance in specific downstream tasks is usually\nhindered by their knowledge boundaries on these tasks. Thus, fine-tuning\ntechniques, especially the widely used Low-Rank Adaptation (LoRA) method, have\nbeen introduced to expand the boundaries on these tasks, whereas LoRA would\nunderperform on certain tasks owing to its potential overfitting on these\ntasks. To overcome this overfitting and improve the performance of LoRA, we\npropose the flexible low rank adaptation (Flexora) method to automatically and\nflexibly select the most important layers needing to be fine-tuned to achieve\nthe best performance on different downstream tasks. Specifically, Flexora\nfirstly frames this layer selection problem as a well-defined hyperparameter\noptimization (HPO) problem, then addresses it using the unrolled\ndifferentiation (UD) method, and finally selects the most useful layers based\non the optimized hyperparameters. Our extensive experiments on many pretrained\nmodels and natural language tasks show that Flexora is able to consistently\nimprove over the existing baselines, indicating the effectiveness of our\nFlexora in practice. We additionally provide insightful theoretical results and\nmany ablation studies to deliver a comprehensive understanding of our Flexora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are driving advancements in artificial\nintelligence by increasing the scale of model parameters, which has\nsignificantly enhanced generalization ability and unlocked new capabilities in\npractice. However, their performance in specific downstream tasks is usually\nhindered by their knowledge boundaries on these tasks. Thus, fine-tuning\ntechniques, especially the widely used Low-Rank Adaptation (LoRA) method, have\nbeen introduced to expand the boundaries on these tasks, whereas LoRA would\nunderperform on certain tasks owing to its potential overfitting on these\ntasks. To overcome this overfitting and improve the performance of LoRA, we\npropose the flexible low rank adaptation (Flexora) method to automatically and\nflexibly select the most important layers needing to be fine-tuned to achieve\nthe best performance on different downstream tasks. Specifically, Flexora\nfirstly frames this layer selection problem as a well-defined hyperparameter\noptimization (HPO) problem, then addresses it using the unrolled\ndifferentiation (UD) method, and finally selects the most useful layers based\non the optimized hyperparameters. Our extensive experiments on many pretrained\nmodels and natural language tasks show that Flexora is able to consistently\nimprove over the existing baselines, indicating the effectiveness of our\nFlexora in practice. We additionally provide insightful theoretical results and\nmany ablation studies to deliver a comprehensive understanding of our Flexora."
                },
                "authors": [
                    {
                        "name": "Chenxing Wei"
                    },
                    {
                        "name": "Yao Shu"
                    },
                    {
                        "name": "Ying Tiffany He"
                    },
                    {
                        "name": "Fei Richard Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Richard Yu"
                },
                "author": "Fei Richard Yu",
                "arxiv_comment": "29 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11370v1",
                "updated": "2024-08-21T06:42:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    42,
                    22,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T06:42:22Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    42,
                    22,
                    2,
                    234,
                    0
                ],
                "title": "Graph Classification via Reference Distribution Learning: Theory and\n  Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Classification via Reference Distribution Learning: Theory and\n  Practice"
                },
                "summary": "Graph classification is a challenging problem owing to the difficulty in\nquantifying the similarity between graphs or representing graphs as vectors,\nthough there have been a few methods using graph kernels or graph neural\nnetworks (GNNs). Graph kernels often suffer from computational costs and manual\nfeature engineering, while GNNs commonly utilize global pooling operations,\nrisking the loss of structural or semantic information. This work introduces\nGraph Reference Distribution Learning (GRDL), an efficient and accurate graph\nclassification method. GRDL treats each graph's latent node embeddings given by\nGNN layers as a discrete distribution, enabling direct classification without\nglobal pooling, based on maximum mean discrepancy to adaptively learned\nreference distributions. To fully understand this new model (the existing\ntheories do not apply) and guide its configuration (e.g., network architecture,\nreferences' sizes, number, and regularization) for practical use, we derive\ngeneralization error bounds for GRDL and verify them numerically. More\nimportantly, our theoretical and numerical results both show that GRDL has a\nstronger generalization ability than GNNs with global pooling operations.\nExperiments on moderate-scale and large-scale graph datasets show the\nsuperiority of GRDL over the state-of-the-art, emphasizing its remarkable\nefficiency, being at least 10 times faster than leading competitors in both\ntraining and inference stages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph classification is a challenging problem owing to the difficulty in\nquantifying the similarity between graphs or representing graphs as vectors,\nthough there have been a few methods using graph kernels or graph neural\nnetworks (GNNs). Graph kernels often suffer from computational costs and manual\nfeature engineering, while GNNs commonly utilize global pooling operations,\nrisking the loss of structural or semantic information. This work introduces\nGraph Reference Distribution Learning (GRDL), an efficient and accurate graph\nclassification method. GRDL treats each graph's latent node embeddings given by\nGNN layers as a discrete distribution, enabling direct classification without\nglobal pooling, based on maximum mean discrepancy to adaptively learned\nreference distributions. To fully understand this new model (the existing\ntheories do not apply) and guide its configuration (e.g., network architecture,\nreferences' sizes, number, and regularization) for practical use, we derive\ngeneralization error bounds for GRDL and verify them numerically. More\nimportantly, our theoretical and numerical results both show that GRDL has a\nstronger generalization ability than GNNs with global pooling operations.\nExperiments on moderate-scale and large-scale graph datasets show the\nsuperiority of GRDL over the state-of-the-art, emphasizing its remarkable\nefficiency, being at least 10 times faster than leading competitors in both\ntraining and inference stages."
                },
                "authors": [
                    {
                        "name": "Zixiao Wang"
                    },
                    {
                        "name": "Jicong Fan"
                    }
                ],
                "author_detail": {
                    "name": "Jicong Fan"
                },
                "author": "Jicong Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11367v1",
                "updated": "2024-08-21T06:38:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    38,
                    49,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T06:38:49Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    38,
                    49,
                    2,
                    234,
                    0
                ],
                "title": "Towards Probabilistic Inductive Logic Programming with Neurosymbolic\n  Inference and Relaxation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Probabilistic Inductive Logic Programming with Neurosymbolic\n  Inference and Relaxation"
                },
                "summary": "Many inductive logic programming (ILP) methods are incapable of learning\nprograms from probabilistic background knowledge, e.g. coming from sensory data\nor neural networks with probabilities. We propose Propper, which handles flawed\nand probabilistic background knowledge by extending ILP with a combination of\nneurosymbolic inference, a continuous criterion for hypothesis selection (BCE)\nand a relaxation of the hypothesis constrainer (NoisyCombo). For relational\npatterns in noisy images, Propper can learn programs from as few as 8 examples.\nIt outperforms binary ILP and statistical models such as a Graph Neural\nNetwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many inductive logic programming (ILP) methods are incapable of learning\nprograms from probabilistic background knowledge, e.g. coming from sensory data\nor neural networks with probabilities. We propose Propper, which handles flawed\nand probabilistic background knowledge by extending ILP with a combination of\nneurosymbolic inference, a continuous criterion for hypothesis selection (BCE)\nand a relaxation of the hypothesis constrainer (NoisyCombo). For relational\npatterns in noisy images, Propper can learn programs from as few as 8 examples.\nIt outperforms binary ILP and statistical models such as a Graph Neural\nNetwork."
                },
                "authors": [
                    {
                        "name": "Fieke Hillerstrom"
                    },
                    {
                        "name": "Gertjan Burghouts"
                    }
                ],
                "author_detail": {
                    "name": "Gertjan Burghouts"
                },
                "author": "Gertjan Burghouts",
                "arxiv_comment": "15 pages",
                "arxiv_journal_ref": "Theory and Practice of Logic Programming 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08892v2",
                "updated": "2024-08-21T06:38:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    38,
                    36,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-08T13:12:46Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    13,
                    12,
                    46,
                    3,
                    221,
                    0
                ],
                "title": "Leveraging Large Language Models for Enhanced Process Model\n  Comprehension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Enhanced Process Model\n  Comprehension"
                },
                "summary": "In Business Process Management (BPM), effectively comprehending process\nmodels is crucial yet poses significant challenges, particularly as\norganizations scale and processes become more complex. This paper introduces a\nnovel framework utilizing the advanced capabilities of Large Language Models\n(LLMs) to enhance the interpretability of complex process models. We present\ndifferent methods for abstracting business process models into a format\naccessible to LLMs, and we implement advanced prompting strategies specifically\ndesigned to optimize LLM performance within our framework. Additionally, we\npresent a tool, AIPA, that implements our proposed framework and allows for\nconversational process querying. We evaluate our framework and tool by i) an\nautomatic evaluation comparing different LLMs, model abstractions, and\nprompting strategies and ii) a user study designed to assess AIPA's\neffectiveness comprehensively. Results demonstrate our framework's ability to\nimprove the accessibility and interpretability of process models, pioneering\nnew pathways for integrating AI technologies into the BPM field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Business Process Management (BPM), effectively comprehending process\nmodels is crucial yet poses significant challenges, particularly as\norganizations scale and processes become more complex. This paper introduces a\nnovel framework utilizing the advanced capabilities of Large Language Models\n(LLMs) to enhance the interpretability of complex process models. We present\ndifferent methods for abstracting business process models into a format\naccessible to LLMs, and we implement advanced prompting strategies specifically\ndesigned to optimize LLM performance within our framework. Additionally, we\npresent a tool, AIPA, that implements our proposed framework and allows for\nconversational process querying. We evaluate our framework and tool by i) an\nautomatic evaluation comparing different LLMs, model abstractions, and\nprompting strategies and ii) a user study designed to assess AIPA's\neffectiveness comprehensively. Results demonstrate our framework's ability to\nimprove the accessibility and interpretability of process models, pioneering\nnew pathways for integrating AI technologies into the BPM field."
                },
                "authors": [
                    {
                        "name": "Humam Kourani"
                    },
                    {
                        "name": "Alessandro Berti"
                    },
                    {
                        "name": "Jasmin Henrich"
                    },
                    {
                        "name": "Wolfgang Kratsch"
                    },
                    {
                        "name": "Robin Weidlich"
                    },
                    {
                        "name": "Chiao-Yun Li"
                    },
                    {
                        "name": "Ahmad Arslan"
                    },
                    {
                        "name": "Daniel Schuster"
                    },
                    {
                        "name": "Wil M. P. van der Aalst"
                    }
                ],
                "author_detail": {
                    "name": "Wil M. P. van der Aalst"
                },
                "author": "Wil M. P. van der Aalst",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11366v1",
                "updated": "2024-08-21T06:35:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    35,
                    21,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T06:35:21Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    35,
                    21,
                    2,
                    234,
                    0
                ],
                "title": "GeoReasoner: Reasoning On Geospatially Grounded Context For Natural\n  Language Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeoReasoner: Reasoning On Geospatially Grounded Context For Natural\n  Language Understanding"
                },
                "summary": "In human reading and communication, individuals tend to engage in geospatial\nreasoning, which involves recognizing geographic entities and making informed\ninferences about their interrelationships. To mimic such cognitive process,\ncurrent methods either utilize conventional natural language understanding\ntoolkits, or directly apply models pretrained on geo-related natural language\ncorpora. However, these methods face two significant challenges: i) they do not\ngeneralize well to unseen geospatial scenarios, and ii) they overlook the\nimportance of integrating geospatial context from geographical databases with\nlinguistic information from the Internet. To handle these challenges, we\npropose GeoReasoner, a language model capable of reasoning on geospatially\ngrounded natural language. Specifically, it first leverages Large Language\nModels (LLMs) to generate a comprehensive location description based on\nlinguistic and geospatial information. It also encodes direction and distance\ninformation into spatial embedding via treating them as pseudo-sentences.\nConsequently, the model is trained on both anchor-level and neighbor-level\ninputs to learn geo-entity representation. Extensive experimental results\ndemonstrate GeoReasoner's superiority in three tasks: toponym recognition,\ntoponym linking, and geo-entity typing, compared to the state-of-the-art\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In human reading and communication, individuals tend to engage in geospatial\nreasoning, which involves recognizing geographic entities and making informed\ninferences about their interrelationships. To mimic such cognitive process,\ncurrent methods either utilize conventional natural language understanding\ntoolkits, or directly apply models pretrained on geo-related natural language\ncorpora. However, these methods face two significant challenges: i) they do not\ngeneralize well to unseen geospatial scenarios, and ii) they overlook the\nimportance of integrating geospatial context from geographical databases with\nlinguistic information from the Internet. To handle these challenges, we\npropose GeoReasoner, a language model capable of reasoning on geospatially\ngrounded natural language. Specifically, it first leverages Large Language\nModels (LLMs) to generate a comprehensive location description based on\nlinguistic and geospatial information. It also encodes direction and distance\ninformation into spatial embedding via treating them as pseudo-sentences.\nConsequently, the model is trained on both anchor-level and neighbor-level\ninputs to learn geo-entity representation. Extensive experimental results\ndemonstrate GeoReasoner's superiority in three tasks: toponym recognition,\ntoponym linking, and geo-entity typing, compared to the state-of-the-art\nbaselines."
                },
                "authors": [
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Joey Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joey Lee"
                },
                "author": "Joey Lee",
                "arxiv_comment": "Accepted by International Conference on Information and Knowledge\n  Management 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11363v1",
                "updated": "2024-08-21T06:16:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    16,
                    22,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T06:16:22Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    16,
                    22,
                    2,
                    234,
                    0
                ],
                "title": "ProteinGPT: Multimodal LLM for Protein Property Prediction and Structure\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProteinGPT: Multimodal LLM for Protein Property Prediction and Structure\n  Understanding"
                },
                "summary": "Understanding biological processes, drug development, and biotechnological\nadvancements requires detailed analysis of protein structures and sequences, a\ntask in protein research that is inherently complex and time-consuming when\nperformed manually. To streamline this process, we introduce ProteinGPT, a\nstate-of-the-art multi-modal protein chat system, that allows users to upload\nprotein sequences and/or structures for comprehensive protein analysis and\nresponsive inquiries. ProteinGPT seamlessly integrates protein sequence and\nstructure encoders with linear projection layers for precise representation\nadaptation, coupled with a large language model (LLM) to generate accurate and\ncontextually relevant responses. To train ProteinGPT, we construct a\nlarge-scale dataset of 132,092 proteins with annotations, and optimize the\ninstruction-tuning process using GPT-4o. This innovative system ensures\naccurate alignment between the user-uploaded data and prompts, simplifying\nprotein analysis. Experiments show that ProteinGPT can produce promising\nresponses to proteins and their corresponding questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding biological processes, drug development, and biotechnological\nadvancements requires detailed analysis of protein structures and sequences, a\ntask in protein research that is inherently complex and time-consuming when\nperformed manually. To streamline this process, we introduce ProteinGPT, a\nstate-of-the-art multi-modal protein chat system, that allows users to upload\nprotein sequences and/or structures for comprehensive protein analysis and\nresponsive inquiries. ProteinGPT seamlessly integrates protein sequence and\nstructure encoders with linear projection layers for precise representation\nadaptation, coupled with a large language model (LLM) to generate accurate and\ncontextually relevant responses. To train ProteinGPT, we construct a\nlarge-scale dataset of 132,092 proteins with annotations, and optimize the\ninstruction-tuning process using GPT-4o. This innovative system ensures\naccurate alignment between the user-uploaded data and prompts, simplifying\nprotein analysis. Experiments show that ProteinGPT can produce promising\nresponses to proteins and their corresponding questions."
                },
                "authors": [
                    {
                        "name": "Yijia Xiao"
                    },
                    {
                        "name": "Edward Sun"
                    },
                    {
                        "name": "Yiqiao Jin"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "arxiv_comment": "19 pages, 9 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09121v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09121v2",
                "updated": "2024-08-21T06:01:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    1,
                    8,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-17T07:11:02Z",
                "published_parsed": [
                    2024,
                    8,
                    17,
                    7,
                    11,
                    2,
                    5,
                    230,
                    0
                ],
                "title": "Selective Prompt Anchoring for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective Prompt Anchoring for Code Generation"
                },
                "summary": "Recent advances in large language models (LLMs) such as Copilot and ChatGPT\nhave transformed software development by automating coding tasks. Despite these\nadvancements, challenges remain in reducing error rates and fully meeting user\nexpectations. Our empirical study reveals LLMs tend to dilute their\nself-attention on the initial prompt as more code tokens are generated. We\nhypothesize this self-attention dilution issue is one of the root causes of\ninaccuracies in LLM-generated code. To mitigate this issue, we propose\nSelective Prompt Anchoring (SPA). SPA amplifies the influence of the selected\nparts in the initial prompt, which we refer to as ``anchored text'', during\ncode generation. Specifically, SPA calculates the logit distribution difference\nwith and without the anchored text. We prove this difference approximates the\nanchored text's contextual contribution to the output logits. SPA creates an\naugmented logit distribution by linearly combining the original logit\ndistribution and the logit difference. We evaluate SPA with five LLMs on four\nbenchmarks. Our results demonstrate that using SPA can consistently improve\nPass@1 rates by up to 9.7% in all settings. Notably, with selective text\nanchoring, a small version of DeepSeek-Coder (6.7B) can achieve better\nperformance than an original much larger version (33B). Our code is available\nat https://github.com/magic-YuanTian/Selective-Prompt-Anchoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) such as Copilot and ChatGPT\nhave transformed software development by automating coding tasks. Despite these\nadvancements, challenges remain in reducing error rates and fully meeting user\nexpectations. Our empirical study reveals LLMs tend to dilute their\nself-attention on the initial prompt as more code tokens are generated. We\nhypothesize this self-attention dilution issue is one of the root causes of\ninaccuracies in LLM-generated code. To mitigate this issue, we propose\nSelective Prompt Anchoring (SPA). SPA amplifies the influence of the selected\nparts in the initial prompt, which we refer to as ``anchored text'', during\ncode generation. Specifically, SPA calculates the logit distribution difference\nwith and without the anchored text. We prove this difference approximates the\nanchored text's contextual contribution to the output logits. SPA creates an\naugmented logit distribution by linearly combining the original logit\ndistribution and the logit difference. We evaluate SPA with five LLMs on four\nbenchmarks. Our results demonstrate that using SPA can consistently improve\nPass@1 rates by up to 9.7% in all settings. Notably, with selective text\nanchoring, a small version of DeepSeek-Coder (6.7B) can achieve better\nperformance than an original much larger version (33B). Our code is available\nat https://github.com/magic-YuanTian/Selective-Prompt-Anchoring."
                },
                "authors": [
                    {
                        "name": "Yuan Tian"
                    },
                    {
                        "name": "Tianyi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhang"
                },
                "author": "Tianyi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09121v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09121v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.11813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11813v1",
                "updated": "2024-08-21T17:58:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    58,
                    2,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T17:58:02Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    58,
                    2,
                    2,
                    234,
                    0
                ],
                "title": "SEA: Supervised Embedding Alignment for Token-Level Visual-Textual\n  Integration in MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEA: Supervised Embedding Alignment for Token-Level Visual-Textual\n  Integration in MLLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have recently demonstrated\nremarkable perceptual and reasoning abilities, typically comprising a Vision\nEncoder, an Adapter, and a Large Language Model (LLM). The adapter serves as\nthe critical bridge between the visual and language components. However,\ntraining adapters with image-level supervision often results in significant\nmisalignment, undermining the LLMs' capabilities and limiting the potential of\nMultimodal LLMs. To address this, we introduce Supervised Embedding Alignment\n(SEA), a token-level alignment method that leverages vision-language\npre-trained models, such as CLIP, to align visual tokens with the LLM's\nembedding space through contrastive learning. This approach ensures a more\ncoherent integration of visual and language representations, enhancing the\nperformance and interpretability of multimodal LLMs while preserving their\ninherent capabilities. Extensive experiments show that SEA effectively improves\nMLLMs, particularly for smaller models, without adding extra data or inference\ncomputation. SEA also lays the groundwork for developing more general and\nadaptable solutions to enhance multimodal systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have recently demonstrated\nremarkable perceptual and reasoning abilities, typically comprising a Vision\nEncoder, an Adapter, and a Large Language Model (LLM). The adapter serves as\nthe critical bridge between the visual and language components. However,\ntraining adapters with image-level supervision often results in significant\nmisalignment, undermining the LLMs' capabilities and limiting the potential of\nMultimodal LLMs. To address this, we introduce Supervised Embedding Alignment\n(SEA), a token-level alignment method that leverages vision-language\npre-trained models, such as CLIP, to align visual tokens with the LLM's\nembedding space through contrastive learning. This approach ensures a more\ncoherent integration of visual and language representations, enhancing the\nperformance and interpretability of multimodal LLMs while preserving their\ninherent capabilities. Extensive experiments show that SEA effectively improves\nMLLMs, particularly for smaller models, without adding extra data or inference\ncomputation. SEA also lays the groundwork for developing more general and\nadaptable solutions to enhance multimodal systems."
                },
                "authors": [
                    {
                        "name": "Yuanyang Yin"
                    },
                    {
                        "name": "Yaqi Zhao"
                    },
                    {
                        "name": "Yajie Zhang"
                    },
                    {
                        "name": "Ke Lin"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Xin Tao"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Baoqun Yin"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v2",
                "updated": "2024-08-21T17:55:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    55,
                    29,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/."
                },
                "authors": [
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11801v1",
                "updated": "2024-08-21T17:43:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    43,
                    15,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T17:43:15Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    43,
                    15,
                    2,
                    234,
                    0
                ],
                "title": "Story3D-Agent: Exploring 3D Storytelling Visualization with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Story3D-Agent: Exploring 3D Storytelling Visualization with Large\n  Language Models"
                },
                "summary": "Traditional visual storytelling is complex, requiring specialized knowledge\nand substantial resources, yet often constrained by human creativity and\ncreation precision. While Large Language Models (LLMs) enhance visual\nstorytelling, current approaches often limit themselves to 2D visuals or\noversimplify stories through motion synthesis and behavioral simulation,\nfailing to create comprehensive, multi-dimensional narratives. To this end, we\npresent Story3D-Agent, a pioneering approach that leverages the capabilities of\nLLMs to transform provided narratives into 3D-rendered visualizations. By\nintegrating procedural modeling, our approach enables precise control over\nmulti-character actions and motions, as well as diverse decorative elements,\nensuring the long-range and dynamic 3D representation. Furthermore, our method\nsupports narrative extension through logical reasoning, ensuring that generated\ncontent remains consistent with existing conditions. We have thoroughly\nevaluated our Story3D-Agent to validate its effectiveness, offering a basic\nframework to advance 3D story representation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional visual storytelling is complex, requiring specialized knowledge\nand substantial resources, yet often constrained by human creativity and\ncreation precision. While Large Language Models (LLMs) enhance visual\nstorytelling, current approaches often limit themselves to 2D visuals or\noversimplify stories through motion synthesis and behavioral simulation,\nfailing to create comprehensive, multi-dimensional narratives. To this end, we\npresent Story3D-Agent, a pioneering approach that leverages the capabilities of\nLLMs to transform provided narratives into 3D-rendered visualizations. By\nintegrating procedural modeling, our approach enables precise control over\nmulti-character actions and motions, as well as diverse decorative elements,\nensuring the long-range and dynamic 3D representation. Furthermore, our method\nsupports narrative extension through logical reasoning, ensuring that generated\ncontent remains consistent with existing conditions. We have thoroughly\nevaluated our Story3D-Agent to validate its effectiveness, offering a basic\nframework to advance 3D story representation."
                },
                "authors": [
                    {
                        "name": "Yuzhou Huang"
                    },
                    {
                        "name": "Yiran Qin"
                    },
                    {
                        "name": "Shunlin Lu"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Ying Shan"
                    },
                    {
                        "name": "Ruimao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruimao Zhang"
                },
                "author": "Ruimao Zhang",
                "arxiv_comment": "Project page: https://yuzhou914.github.io/Story3D-Agent/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11800v1",
                "updated": "2024-08-21T17:43:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    43,
                    11,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T17:43:11Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    43,
                    11,
                    2,
                    234,
                    0
                ],
                "title": "PermitQA: A Benchmark for Retrieval Augmented Generation in Wind Siting\n  and Permitting domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PermitQA: A Benchmark for Retrieval Augmented Generation in Wind Siting\n  and Permitting domain"
                },
                "summary": "In the rapidly evolving landscape of Natural Language Processing (NLP) and\ntext generation, the emergence of Retrieval Augmented Generation (RAG) presents\na promising avenue for improving the quality and reliability of generated text\nby leveraging information retrieved from user specified database. Benchmarking\nis essential to evaluate and compare the performance of the different RAG\nconfigurations in terms of retriever and generator, providing insights into\ntheir effectiveness, scalability, and suitability for the specific domain and\napplications. In this paper, we present a comprehensive framework to generate a\ndomain relevant RAG benchmark. Our framework is based on automatic\nquestion-answer generation with Human (domain experts)-AI Large Language Model\n(LLM) teaming. As a case study, we demonstrate the framework by introducing\nPermitQA, a first-of-its-kind benchmark on the wind siting and permitting\ndomain which comprises of multiple scientific documents/reports related to\nenvironmental impact of wind energy projects. Our framework systematically\nevaluates RAG performance using diverse metrics and multiple question types\nwith varying complexity level. We also demonstrate the performance of different\nmodels on our benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving landscape of Natural Language Processing (NLP) and\ntext generation, the emergence of Retrieval Augmented Generation (RAG) presents\na promising avenue for improving the quality and reliability of generated text\nby leveraging information retrieved from user specified database. Benchmarking\nis essential to evaluate and compare the performance of the different RAG\nconfigurations in terms of retriever and generator, providing insights into\ntheir effectiveness, scalability, and suitability for the specific domain and\napplications. In this paper, we present a comprehensive framework to generate a\ndomain relevant RAG benchmark. Our framework is based on automatic\nquestion-answer generation with Human (domain experts)-AI Large Language Model\n(LLM) teaming. As a case study, we demonstrate the framework by introducing\nPermitQA, a first-of-its-kind benchmark on the wind siting and permitting\ndomain which comprises of multiple scientific documents/reports related to\nenvironmental impact of wind energy projects. Our framework systematically\nevaluates RAG performance using diverse metrics and multiple question types\nwith varying complexity level. We also demonstrate the performance of different\nmodels on our benchmark."
                },
                "authors": [
                    {
                        "name": "Rounak Meyur"
                    },
                    {
                        "name": "Hung Phan"
                    },
                    {
                        "name": "Sridevi Wagle"
                    },
                    {
                        "name": "Jan Strube"
                    },
                    {
                        "name": "Mahantesh Halappanavar"
                    },
                    {
                        "name": "Sameera Horawalavithana"
                    },
                    {
                        "name": "Anurag Acharya"
                    },
                    {
                        "name": "Sai Munikoti"
                    }
                ],
                "author_detail": {
                    "name": "Sai Munikoti"
                },
                "author": "Sai Munikoti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11796v1",
                "updated": "2024-08-21T17:38:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    38,
                    48,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T17:38:48Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    38,
                    48,
                    2,
                    234,
                    0
                ],
                "title": "LLM Pruning and Distillation in Practice: The Minitron Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Pruning and Distillation in Practice: The Minitron Approach"
                },
                "summary": "We present a comprehensive report on compressing the Llama 3.1 8B and Mistral\nNeMo 12B models to 4B and 8B parameters, respectively, using pruning and\ndistillation. We explore two distinct pruning strategies: (1) depth pruning and\n(2) joint hidden/attention/MLP (width) pruning, and evaluate the results on\ncommon benchmarks from the LM Evaluation Harness. The models are then aligned\nwith NeMo Aligner and tested in instruct-tuned versions. This approach produces\na compelling 4B model from Llama 3.1 8B and a state-of-the-art\nMistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo\n12B. We found that with no access to the original data, it is beneficial to\nslightly fine-tune teacher models on the distillation dataset. We open-source\nour base model weights on Hugging Face with a permissive license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive report on compressing the Llama 3.1 8B and Mistral\nNeMo 12B models to 4B and 8B parameters, respectively, using pruning and\ndistillation. We explore two distinct pruning strategies: (1) depth pruning and\n(2) joint hidden/attention/MLP (width) pruning, and evaluate the results on\ncommon benchmarks from the LM Evaluation Harness. The models are then aligned\nwith NeMo Aligner and tested in instruct-tuned versions. This approach produces\na compelling 4B model from Llama 3.1 8B and a state-of-the-art\nMistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo\n12B. We found that with no access to the original data, it is beneficial to\nslightly fine-tune teacher models on the distillation dataset. We open-source\nour base model weights on Hugging Face with a permissive license."
                },
                "authors": [
                    {
                        "name": "Sharath Turuvekere Sreenivas"
                    },
                    {
                        "name": "Saurav Muralidharan"
                    },
                    {
                        "name": "Raviraj Joshi"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "Mostofa Patwary"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11795v1",
                "updated": "2024-08-21T17:36:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    36,
                    37,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T17:36:37Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    36,
                    37,
                    2,
                    234,
                    0
                ],
                "title": "EE-MLLM: A Data-Efficient and Compute-Efficient Multimodal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EE-MLLM: A Data-Efficient and Compute-Efficient Multimodal Large\n  Language Model"
                },
                "summary": "In the realm of multimodal research, numerous studies leverage substantial\nimage-text pairs to conduct modal alignment learning, transforming Large\nLanguage Models (LLMs) into Multimodal LLMs and excelling in a variety of\nvisual-language tasks. The prevailing methodologies primarily fall into two\ncategories: self-attention-based and cross-attention-based methods. While\nself-attention-based methods offer superior data efficiency due to their simple\nMLP architecture, they often suffer from lower computational efficiency due to\nconcatenating visual and textual tokens as input for LLM. Conversely,\ncross-attention-based methods, although less data-efficient due to additional\nlearnable parameters, exhibit higher computational efficiency by avoiding long\nsequence input for LLM. To address these trade-offs, we introduce the\nData-Efficient and Compute-Efficient Multimodal Large Language Model (EE-MLLM).\nWithout introducing additional modules or learnable parameters, EE-MLLM\nachieves both data and compute efficiency. Specifically, we modify the original\nself-attention mechanism in MLLM to a composite attention mechanism. This\nmechanism has two key characteristics: 1) Eliminating the computational\noverhead of self-attention within visual tokens to achieve compute efficiency,\nand 2) Reusing the weights on each layer of LLM to facilitate effective\nmodality alignment between vision and language for data efficiency.\nExperimental results demonstrate the effectiveness of EE-MLLM across a range of\nbenchmarks, including general-purpose datasets like MMBench and SeedBench, as\nwell as fine-grained tasks such as TextVQA and DocVQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of multimodal research, numerous studies leverage substantial\nimage-text pairs to conduct modal alignment learning, transforming Large\nLanguage Models (LLMs) into Multimodal LLMs and excelling in a variety of\nvisual-language tasks. The prevailing methodologies primarily fall into two\ncategories: self-attention-based and cross-attention-based methods. While\nself-attention-based methods offer superior data efficiency due to their simple\nMLP architecture, they often suffer from lower computational efficiency due to\nconcatenating visual and textual tokens as input for LLM. Conversely,\ncross-attention-based methods, although less data-efficient due to additional\nlearnable parameters, exhibit higher computational efficiency by avoiding long\nsequence input for LLM. To address these trade-offs, we introduce the\nData-Efficient and Compute-Efficient Multimodal Large Language Model (EE-MLLM).\nWithout introducing additional modules or learnable parameters, EE-MLLM\nachieves both data and compute efficiency. Specifically, we modify the original\nself-attention mechanism in MLLM to a composite attention mechanism. This\nmechanism has two key characteristics: 1) Eliminating the computational\noverhead of self-attention within visual tokens to achieve compute efficiency,\nand 2) Reusing the weights on each layer of LLM to facilitate effective\nmodality alignment between vision and language for data efficiency.\nExperimental results demonstrate the effectiveness of EE-MLLM across a range of\nbenchmarks, including general-purpose datasets like MMBench and SeedBench, as\nwell as fine-grained tasks such as TextVQA and DocVQA."
                },
                "authors": [
                    {
                        "name": "Feipeng Ma"
                    },
                    {
                        "name": "Yizhou Zhou"
                    },
                    {
                        "name": "Hebei Li"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Siying Wu"
                    },
                    {
                        "name": "Fengyun Rao"
                    },
                    {
                        "name": "Yueyi Zhang"
                    },
                    {
                        "name": "Xiaoyan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyan Sun"
                },
                "author": "Xiaoyan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05590v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05590v2",
                "updated": "2024-08-21T17:34:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    34,
                    7,
                    2,
                    234,
                    0
                ],
                "published": "2024-06-08T22:21:42Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    22,
                    21,
                    42,
                    5,
                    160,
                    0
                ],
                "title": "NYU CTF Dataset: A Scalable Open-Source Benchmark Dataset for Evaluating\n  LLMs in Offensive Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NYU CTF Dataset: A Scalable Open-Source Benchmark Dataset for Evaluating\n  LLMs in Offensive Security"
                },
                "summary": "Large Language Models (LLMs) are being deployed across various domains today.\nHowever, their capacity to solve Capture the Flag (CTF) challenges in\ncybersecurity has not been thoroughly evaluated. To address this, we develop a\nnovel method to assess LLMs in solving CTF challenges by creating a scalable,\nopen-source benchmark database specifically designed for these applications.\nThis database includes metadata for LLM testing and adaptive learning,\ncompiling a diverse range of CTF challenges from popular competitions.\nUtilizing the advanced function calling capabilities of LLMs, we build a fully\nautomated system with an enhanced workflow and support for external tool calls.\nOur benchmark dataset and automated framework allow us to evaluate the\nperformance of five LLMs, encompassing both black-box and open-source models.\nThis work lays the foundation for future research into improving the efficiency\nof LLMs in interactive cybersecurity tasks and automated task planning. By\nproviding a specialized dataset, our project offers an ideal platform for\ndeveloping, testing, and refining LLM-based approaches to vulnerability\ndetection and resolution. Evaluating LLMs on these challenges and comparing\nwith human performance yields insights into their potential for AI-driven\ncybersecurity solutions to perform real-world threat management. We make our\ndataset open source to public https://github.com/NYU-LLM-CTF/LLM_CTF_Database\nalong with our playground automated framework\nhttps://github.com/NYU-LLM-CTF/llm_ctf_automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are being deployed across various domains today.\nHowever, their capacity to solve Capture the Flag (CTF) challenges in\ncybersecurity has not been thoroughly evaluated. To address this, we develop a\nnovel method to assess LLMs in solving CTF challenges by creating a scalable,\nopen-source benchmark database specifically designed for these applications.\nThis database includes metadata for LLM testing and adaptive learning,\ncompiling a diverse range of CTF challenges from popular competitions.\nUtilizing the advanced function calling capabilities of LLMs, we build a fully\nautomated system with an enhanced workflow and support for external tool calls.\nOur benchmark dataset and automated framework allow us to evaluate the\nperformance of five LLMs, encompassing both black-box and open-source models.\nThis work lays the foundation for future research into improving the efficiency\nof LLMs in interactive cybersecurity tasks and automated task planning. By\nproviding a specialized dataset, our project offers an ideal platform for\ndeveloping, testing, and refining LLM-based approaches to vulnerability\ndetection and resolution. Evaluating LLMs on these challenges and comparing\nwith human performance yields insights into their potential for AI-driven\ncybersecurity solutions to perform real-world threat management. We make our\ndataset open source to public https://github.com/NYU-LLM-CTF/LLM_CTF_Database\nalong with our playground automated framework\nhttps://github.com/NYU-LLM-CTF/llm_ctf_automation."
                },
                "authors": [
                    {
                        "name": "Minghao Shao"
                    },
                    {
                        "name": "Sofija Jancheska"
                    },
                    {
                        "name": "Meet Udeshi"
                    },
                    {
                        "name": "Brendan Dolan-Gavitt"
                    },
                    {
                        "name": "Haoran Xi"
                    },
                    {
                        "name": "Kimberly Milner"
                    },
                    {
                        "name": "Boyuan Chen"
                    },
                    {
                        "name": "Max Yin"
                    },
                    {
                        "name": "Siddharth Garg"
                    },
                    {
                        "name": "Prashanth Krishnamurthy"
                    },
                    {
                        "name": "Farshad Khorrami"
                    },
                    {
                        "name": "Ramesh Karri"
                    },
                    {
                        "name": "Muhammad Shafique"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Shafique"
                },
                "author": "Muhammad Shafique",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05590v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05590v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.00333v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.00333v4",
                "updated": "2024-08-21T17:27:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    27,
                    47,
                    2,
                    234,
                    0
                ],
                "published": "2023-03-01T08:53:36Z",
                "published_parsed": [
                    2023,
                    3,
                    1,
                    8,
                    53,
                    36,
                    2,
                    60,
                    0
                ],
                "title": "Competence-Based Analysis of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Competence-Based Analysis of Language Models"
                },
                "summary": "Despite the recent successes of large, pretrained neural language models\n(LLMs), comparatively little is known about the representations of linguistic\nstructure they learn during pretraining, which can lead to unexpected behaviors\nin response to prompt variation or distribution shift. To better understand\nthese models and behaviors, we introduce a general model analysis framework to\nstudy LLMs with respect to their representation and use of human-interpretable\nlinguistic properties. Our framework, CALM (Competence-based Analysis of\nLanguage Models), is designed to investigate LLM competence in the context of\nspecific tasks by intervening on models' internal representations of different\nlinguistic properties using causal probing, and measuring models' alignment\nunder these interventions with a given ground-truth causal model of the task.\nWe also develop a new approach for performing causal probing interventions\nusing gradient-based adversarial attacks, which can target a broader range of\nproperties and representations than prior techniques. Finally, we carry out a\ncase study of CALM using these interventions to analyze and compare LLM\ncompetence across a variety of lexical inference tasks, showing that CALM can\nbe used to explain and predict behaviors across these tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent successes of large, pretrained neural language models\n(LLMs), comparatively little is known about the representations of linguistic\nstructure they learn during pretraining, which can lead to unexpected behaviors\nin response to prompt variation or distribution shift. To better understand\nthese models and behaviors, we introduce a general model analysis framework to\nstudy LLMs with respect to their representation and use of human-interpretable\nlinguistic properties. Our framework, CALM (Competence-based Analysis of\nLanguage Models), is designed to investigate LLM competence in the context of\nspecific tasks by intervening on models' internal representations of different\nlinguistic properties using causal probing, and measuring models' alignment\nunder these interventions with a given ground-truth causal model of the task.\nWe also develop a new approach for performing causal probing interventions\nusing gradient-based adversarial attacks, which can target a broader range of\nproperties and representations than prior techniques. Finally, we carry out a\ncase study of CALM using these interventions to analyze and compare LLM\ncompetence across a variety of lexical inference tasks, showing that CALM can\nbe used to explain and predict behaviors across these tasks."
                },
                "authors": [
                    {
                        "name": "Adam Davies"
                    },
                    {
                        "name": "Jize Jiang"
                    },
                    {
                        "name": "ChengXiang Zhai"
                    }
                ],
                "author_detail": {
                    "name": "ChengXiang Zhai"
                },
                "author": "ChengXiang Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.00333v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.00333v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11793v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11793v1",
                "updated": "2024-08-21T17:25:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    25,
                    45,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T17:25:45Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    25,
                    45,
                    2,
                    234,
                    0
                ],
                "title": "Leveraging Chemistry Foundation Models to Facilitate Structure Focused\n  Retrieval Augmented Generation in Multi-Agent Workflows for Catalyst and\n  Materials Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Chemistry Foundation Models to Facilitate Structure Focused\n  Retrieval Augmented Generation in Multi-Agent Workflows for Catalyst and\n  Materials Design"
                },
                "summary": "Molecular property prediction and generative design via deep learning models\nhas been the subject of intense research given its potential to accelerate\ndevelopment of new, high-performance materials. More recently, these workflows\nhave been significantly augmented with the advent of large language models\n(LLMs) and systems of LLM-driven agents capable of utilizing pre-trained models\nto make predictions in the context of more complex research tasks. While\neffective, there is still room for substantial improvement within the agentic\nsystems on the retrieval of salient information for material design tasks.\nMoreover, alternative uses of predictive deep learning models, such as\nleveraging their latent representations to facilitate cross-modal retrieval\naugmented generation within agentic systems to enable task-specific materials\ndesign, has remained unexplored. Herein, we demonstrate that large, pre-trained\nchemistry foundation models can serve as a basis for enabling semantic\nchemistry information retrieval for both small-molecules, complex polymeric\nmaterials, and reactions. Additionally, we show the use of chemistry foundation\nmodels in conjunction with image models such as OpenCLIP facilitate\nunprecedented queries and information retrieval across multiple\ncharacterization data domains. Finally, we demonstrate the integration of these\nsystems within multi-agent systems to facilitate structure and\ntopological-based natural language queries and information retrieval for\ncomplex research tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular property prediction and generative design via deep learning models\nhas been the subject of intense research given its potential to accelerate\ndevelopment of new, high-performance materials. More recently, these workflows\nhave been significantly augmented with the advent of large language models\n(LLMs) and systems of LLM-driven agents capable of utilizing pre-trained models\nto make predictions in the context of more complex research tasks. While\neffective, there is still room for substantial improvement within the agentic\nsystems on the retrieval of salient information for material design tasks.\nMoreover, alternative uses of predictive deep learning models, such as\nleveraging their latent representations to facilitate cross-modal retrieval\naugmented generation within agentic systems to enable task-specific materials\ndesign, has remained unexplored. Herein, we demonstrate that large, pre-trained\nchemistry foundation models can serve as a basis for enabling semantic\nchemistry information retrieval for both small-molecules, complex polymeric\nmaterials, and reactions. Additionally, we show the use of chemistry foundation\nmodels in conjunction with image models such as OpenCLIP facilitate\nunprecedented queries and information retrieval across multiple\ncharacterization data domains. Finally, we demonstrate the integration of these\nsystems within multi-agent systems to facilitate structure and\ntopological-based natural language queries and information retrieval for\ncomplex research tasks."
                },
                "authors": [
                    {
                        "name": "Nathaniel H. Park"
                    },
                    {
                        "name": "Tiffany J. Callahan"
                    },
                    {
                        "name": "James L. Hedrick"
                    },
                    {
                        "name": "Tim Erdmann"
                    },
                    {
                        "name": "Sara Capponi"
                    }
                ],
                "author_detail": {
                    "name": "Sara Capponi"
                },
                "author": "Sara Capponi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11793v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11793v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11791v1",
                "updated": "2024-08-21T17:24:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    24,
                    15,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T17:24:15Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    24,
                    15,
                    2,
                    234,
                    0
                ],
                "title": "Critique-out-Loud Reward Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critique-out-Loud Reward Models"
                },
                "summary": "Traditionally, reward models used for reinforcement learning from human\nfeedback (RLHF) are trained to directly predict preference scores without\nleveraging the generation capabilities of the underlying large language model\n(LLM). This limits the capabilities of reward models as they must reason\nimplicitly about the quality of a response, i.e., preference modeling must be\nperformed in a single forward pass through the model. To enable reward models\nto reason explicitly about the quality of a response, we introduce\nCritique-out-Loud (CLoud) reward models. CLoud reward models operate by first\ngenerating a natural language critique of the assistant's response that is then\nused to predict a scalar reward for the quality of the response. We demonstrate\nthe success of CLoud reward models for both Llama-3-8B and 70B base models:\ncompared to classic reward models CLoud reward models improve pairwise\npreference classification accuracy on RewardBench by 4.65 and 5.84 percentage\npoints for the 8B and 70B base models respectively. Furthermore, CLoud reward\nmodels lead to a Pareto improvement for win rate on ArenaHard when used as the\nscoring model for Best-of-N. Finally, we explore how to exploit the dynamic\ninference compute capabilities of CLoud reward models by performing\nself-consistency decoding for reward prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditionally, reward models used for reinforcement learning from human\nfeedback (RLHF) are trained to directly predict preference scores without\nleveraging the generation capabilities of the underlying large language model\n(LLM). This limits the capabilities of reward models as they must reason\nimplicitly about the quality of a response, i.e., preference modeling must be\nperformed in a single forward pass through the model. To enable reward models\nto reason explicitly about the quality of a response, we introduce\nCritique-out-Loud (CLoud) reward models. CLoud reward models operate by first\ngenerating a natural language critique of the assistant's response that is then\nused to predict a scalar reward for the quality of the response. We demonstrate\nthe success of CLoud reward models for both Llama-3-8B and 70B base models:\ncompared to classic reward models CLoud reward models improve pairwise\npreference classification accuracy on RewardBench by 4.65 and 5.84 percentage\npoints for the 8B and 70B base models respectively. Furthermore, CLoud reward\nmodels lead to a Pareto improvement for win rate on ArenaHard when used as the\nscoring model for Best-of-N. Finally, we explore how to exploit the dynamic\ninference compute capabilities of CLoud reward models by performing\nself-consistency decoding for reward prediction."
                },
                "authors": [
                    {
                        "name": "Zachary Ankner"
                    },
                    {
                        "name": "Mansheej Paul"
                    },
                    {
                        "name": "Brandon Cui"
                    },
                    {
                        "name": "Jonathan D. Chang"
                    },
                    {
                        "name": "Prithviraj Ammanabrolu"
                    }
                ],
                "author_detail": {
                    "name": "Prithviraj Ammanabrolu"
                },
                "author": "Prithviraj Ammanabrolu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11457v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11457v2",
                "updated": "2024-08-21T17:23:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    23,
                    3,
                    2,
                    234,
                    0
                ],
                "published": "2024-04-17T15:05:03Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    15,
                    5,
                    3,
                    2,
                    108,
                    0
                ],
                "title": "Bias and Unfairness in Information Retrieval Systems: New Challenges in\n  the LLM Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias and Unfairness in Information Retrieval Systems: New Challenges in\n  the LLM Era"
                },
                "summary": "With the rapid advancements of large language models (LLMs), information\nretrieval (IR) systems, such as search engines and recommender systems, have\nundergone a significant paradigm shift. This evolution, while heralding new\nopportunities, introduces emerging challenges, particularly in terms of biases\nand unfairness, which may threaten the information ecosystem. In this paper, we\npresent a comprehensive survey of existing works on emerging and pressing bias\nand unfairness issues in IR systems when the integration of LLMs. We first\nunify bias and unfairness issues as distribution mismatch problems, providing a\ngroundwork for categorizing various mitigation strategies through distribution\nalignment. Subsequently, we systematically delve into the specific bias and\nunfairness issues arising from three critical stages of LLMs integration into\nIR systems: data collection, model development, and result evaluation. In doing\nso, we meticulously review and analyze recent literature, focusing on the\ndefinitions, characteristics, and corresponding mitigation strategies\nassociated with these issues. Finally, we identify and highlight some open\nproblems and challenges for future work, aiming to inspire researchers and\nstakeholders in the IR field and beyond to better understand and mitigate bias\nand unfairness issues of IR in this LLM era. We also consistently maintain a\nGitHub repository for the relevant papers and resources in this rising\ndirection at https://github.com/KID-22/LLM-IR-Bias-Fairness-Survey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancements of large language models (LLMs), information\nretrieval (IR) systems, such as search engines and recommender systems, have\nundergone a significant paradigm shift. This evolution, while heralding new\nopportunities, introduces emerging challenges, particularly in terms of biases\nand unfairness, which may threaten the information ecosystem. In this paper, we\npresent a comprehensive survey of existing works on emerging and pressing bias\nand unfairness issues in IR systems when the integration of LLMs. We first\nunify bias and unfairness issues as distribution mismatch problems, providing a\ngroundwork for categorizing various mitigation strategies through distribution\nalignment. Subsequently, we systematically delve into the specific bias and\nunfairness issues arising from three critical stages of LLMs integration into\nIR systems: data collection, model development, and result evaluation. In doing\nso, we meticulously review and analyze recent literature, focusing on the\ndefinitions, characteristics, and corresponding mitigation strategies\nassociated with these issues. Finally, we identify and highlight some open\nproblems and challenges for future work, aiming to inspire researchers and\nstakeholders in the IR field and beyond to better understand and mitigate bias\nand unfairness issues of IR in this LLM era. We also consistently maintain a\nGitHub repository for the relevant papers and resources in this rising\ndirection at https://github.com/KID-22/LLM-IR-Bias-Fairness-Survey."
                },
                "authors": [
                    {
                        "name": "Sunhao Dai"
                    },
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Shicheng Xu"
                    },
                    {
                        "name": "Liang Pang"
                    },
                    {
                        "name": "Zhenhua Dong"
                    },
                    {
                        "name": "Jun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xu"
                },
                "author": "Jun Xu",
                "arxiv_doi": "10.1145/3637528.3671458",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3637528.3671458",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.11457v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11457v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "KDD 2024 Tutorial&Survey; Tutorial Website:\n  https://llm-ir-bias-fairness.github.io/",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11788v1",
                "updated": "2024-08-21T17:21:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    21,
                    13,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T17:21:13Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    21,
                    13,
                    2,
                    234,
                    0
                ],
                "title": "DreamFactory: Pioneering Multi-Scene Long Video Generation with a\n  Multi-Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DreamFactory: Pioneering Multi-Scene Long Video Generation with a\n  Multi-Agent Framework"
                },
                "summary": "Current video generation models excel at creating short, realistic clips, but\nstruggle with longer, multi-scene videos. We introduce \\texttt{DreamFactory},\nan LLM-based framework that tackles this challenge. \\texttt{DreamFactory}\nleverages multi-agent collaboration principles and a Key Frames Iteration\nDesign Method to ensure consistency and style across long videos. It utilizes\nChain of Thought (COT) to address uncertainties inherent in large language\nmodels. \\texttt{DreamFactory} generates long, stylistically coherent, and\ncomplex videos. Evaluating these long-form videos presents a challenge. We\npropose novel metrics such as Cross-Scene Face Distance Score and Cross-Scene\nStyle Consistency Score. To further research in this area, we contribute the\nMulti-Scene Videos Dataset containing over 150 human-rated videos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video generation models excel at creating short, realistic clips, but\nstruggle with longer, multi-scene videos. We introduce \\texttt{DreamFactory},\nan LLM-based framework that tackles this challenge. \\texttt{DreamFactory}\nleverages multi-agent collaboration principles and a Key Frames Iteration\nDesign Method to ensure consistency and style across long videos. It utilizes\nChain of Thought (COT) to address uncertainties inherent in large language\nmodels. \\texttt{DreamFactory} generates long, stylistically coherent, and\ncomplex videos. Evaluating these long-form videos presents a challenge. We\npropose novel metrics such as Cross-Scene Face Distance Score and Cross-Scene\nStyle Consistency Score. To further research in this area, we contribute the\nMulti-Scene Videos Dataset containing over 150 human-rated videos."
                },
                "authors": [
                    {
                        "name": "Zhifei Xie"
                    },
                    {
                        "name": "Daniel Tang"
                    },
                    {
                        "name": "Dingwei Tan"
                    },
                    {
                        "name": "Jacques Klein"
                    },
                    {
                        "name": "Tegawend F. Bissyand"
                    },
                    {
                        "name": "Saad Ezzini"
                    }
                ],
                "author_detail": {
                    "name": "Saad Ezzini"
                },
                "author": "Saad Ezzini",
                "arxiv_comment": "13 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "TsingHua University",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11779v1",
                "updated": "2024-08-21T17:09:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    9,
                    0,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T17:09:00Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    9,
                    0,
                    2,
                    234,
                    0
                ],
                "title": "Personality Alignment of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personality Alignment of Large Language Models"
                },
                "summary": "Current methods for aligning large language models (LLMs) typically aim to\nreflect general human values and behaviors, but they often fail to capture the\nunique characteristics and preferences of individual users. To address this\ngap, we introduce the concept of Personality Alignment. This approach tailors\nLLMs' responses and decisions to match the specific preferences of individual\nusers or closely related groups. Inspired by psychometrics, we created the\nPersonality Alignment with Personality Inventories (PAPI) dataset, which\nincludes data from 300,000 real subjects, each providing behavioral preferences\nbased on the Big Five Personality Factors. This dataset allows us to\nquantitatively evaluate the extent to which LLMs can align with each subject's\nbehavioral patterns. Recognizing the challenges of personality alignments: such\nas limited personal data, diverse preferences, and scalability requirements: we\ndeveloped an activation intervention optimization method. This method enhances\nLLMs' ability to efficiently align with individual behavioral preferences using\nminimal data and computational resources. Remarkably, our method, PAS, achieves\nsuperior performance while requiring only 1/5 of the optimization time compared\nto DPO, offering practical value for personality alignment. Our work paves the\nway for future AI systems to make decisions and reason in truly personality\nways, enhancing the relevance and meaning of AI interactions for each user and\nadvancing human-centered artificial intelligence.The code has released in\n\\url{https://github.com/zhu-minjun/PAlign}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current methods for aligning large language models (LLMs) typically aim to\nreflect general human values and behaviors, but they often fail to capture the\nunique characteristics and preferences of individual users. To address this\ngap, we introduce the concept of Personality Alignment. This approach tailors\nLLMs' responses and decisions to match the specific preferences of individual\nusers or closely related groups. Inspired by psychometrics, we created the\nPersonality Alignment with Personality Inventories (PAPI) dataset, which\nincludes data from 300,000 real subjects, each providing behavioral preferences\nbased on the Big Five Personality Factors. This dataset allows us to\nquantitatively evaluate the extent to which LLMs can align with each subject's\nbehavioral patterns. Recognizing the challenges of personality alignments: such\nas limited personal data, diverse preferences, and scalability requirements: we\ndeveloped an activation intervention optimization method. This method enhances\nLLMs' ability to efficiently align with individual behavioral preferences using\nminimal data and computational resources. Remarkably, our method, PAS, achieves\nsuperior performance while requiring only 1/5 of the optimization time compared\nto DPO, offering practical value for personality alignment. Our work paves the\nway for future AI systems to make decisions and reason in truly personality\nways, enhancing the relevance and meaning of AI interactions for each user and\nadvancing human-centered artificial intelligence.The code has released in\n\\url{https://github.com/zhu-minjun/PAlign}."
                },
                "authors": [
                    {
                        "name": "Minjun Zhu"
                    },
                    {
                        "name": "Linyi Yang"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09544v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09544v2",
                "updated": "2024-08-21T17:04:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    4,
                    8,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-18T17:01:42Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    17,
                    1,
                    42,
                    6,
                    231,
                    0
                ],
                "title": "No Such Thing as a General Learner: Language models and their dual\n  optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Such Thing as a General Learner: Language models and their dual\n  optimization"
                },
                "summary": "What role can the otherwise successful Large Language Models (LLMs) play in\nthe understanding of human cognition, and in particular in terms of informing\nlanguage acquisition debates? To contribute to this question, we first argue\nthat neither humans nor LLMs are general learners, in a variety of senses. We\nmake a novel case for how in particular LLMs follow a dual-optimization\nprocess: they are optimized during their training (which is typically compared\nto language acquisition), and modern LLMs have also been selected, through a\nprocess akin to natural selection in a species. From this perspective, we argue\nthat the performance of LLMs, whether similar or dissimilar to that of humans,\ndoes not weigh easily on important debates about the importance of human\ncognitive biases for language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What role can the otherwise successful Large Language Models (LLMs) play in\nthe understanding of human cognition, and in particular in terms of informing\nlanguage acquisition debates? To contribute to this question, we first argue\nthat neither humans nor LLMs are general learners, in a variety of senses. We\nmake a novel case for how in particular LLMs follow a dual-optimization\nprocess: they are optimized during their training (which is typically compared\nto language acquisition), and modern LLMs have also been selected, through a\nprocess akin to natural selection in a species. From this perspective, we argue\nthat the performance of LLMs, whether similar or dissimilar to that of humans,\ndoes not weigh easily on important debates about the importance of human\ncognitive biases for language."
                },
                "authors": [
                    {
                        "name": "Emmanuel Chemla"
                    },
                    {
                        "name": "Ryan M. Nefdt"
                    }
                ],
                "author_detail": {
                    "name": "Ryan M. Nefdt"
                },
                "author": "Ryan M. Nefdt",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09544v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11775v1",
                "updated": "2024-08-21T17:00:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    0,
                    5,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T17:00:05Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    0,
                    5,
                    2,
                    234,
                    0
                ],
                "title": "Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context\n  Support: For 3GPP Standards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context\n  Support: For 3GPP Standards"
                },
                "summary": "Recent studies show that large language models (LLMs) struggle with technical\nstandards in telecommunications. We propose a fine-tuned retrieval-augmented\ngeneration (RAG) system based on the Phi-2 small language model (SLM) to serve\nas an oracle for communication networks. Our developed system leverages\nforward-looking semantic chunking to adaptively determine parsing breakpoints\nbased on embedding similarity, enabling effective processing of diverse\ndocument formats. To handle the challenge of multiple similar contexts in\ntechnical standards, we employ a re-ranking algorithm to prioritize the most\nrelevant retrieved chunks. Recognizing the limitations of Phi-2's small context\nwindow, we implement a recent technique, namely SelfExtend, to expand the\ncontext window during inference, which not only boosts the performance but also\ncan accommodate a wider range of user queries and design requirements from\ncustomers to specialized technicians. For fine-tuning, we utilize the low-rank\nadaptation (LoRA) technique to enhance computational efficiency during training\nand enable effective fine-tuning on small datasets. Our comprehensive\nexperiments demonstrate substantial improvements over existing\nquestion-answering approaches in the telecom domain, achieving performance that\nexceeds larger language models such as GPT-4 (which is about 880 times larger\nin size). This work presents a novel approach to leveraging SLMs for\ncommunication networks, offering a balance of efficiency and performance. This\nwork can serve as a foundation towards agentic language models for networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies show that large language models (LLMs) struggle with technical\nstandards in telecommunications. We propose a fine-tuned retrieval-augmented\ngeneration (RAG) system based on the Phi-2 small language model (SLM) to serve\nas an oracle for communication networks. Our developed system leverages\nforward-looking semantic chunking to adaptively determine parsing breakpoints\nbased on embedding similarity, enabling effective processing of diverse\ndocument formats. To handle the challenge of multiple similar contexts in\ntechnical standards, we employ a re-ranking algorithm to prioritize the most\nrelevant retrieved chunks. Recognizing the limitations of Phi-2's small context\nwindow, we implement a recent technique, namely SelfExtend, to expand the\ncontext window during inference, which not only boosts the performance but also\ncan accommodate a wider range of user queries and design requirements from\ncustomers to specialized technicians. For fine-tuning, we utilize the low-rank\nadaptation (LoRA) technique to enhance computational efficiency during training\nand enable effective fine-tuning on small datasets. Our comprehensive\nexperiments demonstrate substantial improvements over existing\nquestion-answering approaches in the telecom domain, achieving performance that\nexceeds larger language models such as GPT-4 (which is about 880 times larger\nin size). This work presents a novel approach to leveraging SLMs for\ncommunication networks, offering a balance of efficiency and performance. This\nwork can serve as a foundation towards agentic language models for networks."
                },
                "authors": [
                    {
                        "name": "Omar Erak"
                    },
                    {
                        "name": "Nouf Alabbasi"
                    },
                    {
                        "name": "Omar Alhussein"
                    },
                    {
                        "name": "Ismail Lotfi"
                    },
                    {
                        "name": "Amr Hussein"
                    },
                    {
                        "name": "Sami Muhaidat"
                    },
                    {
                        "name": "Merouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Merouane Debbah"
                },
                "author": "Merouane Debbah",
                "arxiv_comment": "submitted to Proc. IEEE Globecom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11749v1",
                "updated": "2024-08-21T16:16:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    16,
                    16,
                    34,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T16:16:34Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    16,
                    16,
                    34,
                    2,
                    234,
                    0
                ],
                "title": "Against All Odds: Overcoming Typology, Script, and Language Confusion in\n  Multilingual Embedding Inversion Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Against All Odds: Overcoming Typology, Script, and Language Confusion in\n  Multilingual Embedding Inversion Attacks"
                },
                "summary": "Large Language Models (LLMs) are susceptible to malicious influence by cyber\nattackers through intrusions such as adversarial, backdoor, and embedding\ninversion attacks. In response, the burgeoning field of LLM Security aims to\nstudy and defend against such threats. Thus far, the majority of works in this\narea have focused on monolingual English models, however, emerging research\nsuggests that multilingual LLMs may be more vulnerable to various attacks than\ntheir monolingual counterparts. While previous work has investigated embedding\ninversion over a small subset of European languages, it is challenging to\nextrapolate these findings to languages from different linguistic families and\nwith differing scripts. To this end, we explore the security of multilingual\nLLMs in the context of embedding inversion attacks and investigate\ncross-lingual and cross-script inversion across 20 languages, spanning over 8\nlanguage families and 12 scripts. Our findings indicate that languages written\nin Arabic script and Cyrillic script are particularly vulnerable to embedding\ninversion, as are languages within the Indo-Aryan language family. We further\nobserve that inversion models tend to suffer from language confusion, sometimes\ngreatly reducing the efficacy of an attack. Accordingly, we systematically\nexplore this bottleneck for inversion models, uncovering predictable patterns\nwhich could be leveraged by attackers. Ultimately, this study aims to further\nthe field's understanding of the outstanding security vulnerabilities facing\nmultilingual LLMs and raise awareness for the languages most at risk of\nnegative impact from these attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are susceptible to malicious influence by cyber\nattackers through intrusions such as adversarial, backdoor, and embedding\ninversion attacks. In response, the burgeoning field of LLM Security aims to\nstudy and defend against such threats. Thus far, the majority of works in this\narea have focused on monolingual English models, however, emerging research\nsuggests that multilingual LLMs may be more vulnerable to various attacks than\ntheir monolingual counterparts. While previous work has investigated embedding\ninversion over a small subset of European languages, it is challenging to\nextrapolate these findings to languages from different linguistic families and\nwith differing scripts. To this end, we explore the security of multilingual\nLLMs in the context of embedding inversion attacks and investigate\ncross-lingual and cross-script inversion across 20 languages, spanning over 8\nlanguage families and 12 scripts. Our findings indicate that languages written\nin Arabic script and Cyrillic script are particularly vulnerable to embedding\ninversion, as are languages within the Indo-Aryan language family. We further\nobserve that inversion models tend to suffer from language confusion, sometimes\ngreatly reducing the efficacy of an attack. Accordingly, we systematically\nexplore this bottleneck for inversion models, uncovering predictable patterns\nwhich could be leveraged by attackers. Ultimately, this study aims to further\nthe field's understanding of the outstanding security vulnerabilities facing\nmultilingual LLMs and raise awareness for the languages most at risk of\nnegative impact from these attacks."
                },
                "authors": [
                    {
                        "name": "Yiyi Chen"
                    },
                    {
                        "name": "Russa Biswas"
                    },
                    {
                        "name": "Heather Lent"
                    },
                    {
                        "name": "Johannes Bjerva"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Bjerva"
                },
                "author": "Johannes Bjerva",
                "arxiv_comment": "11 pages, 4 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11746v1",
                "updated": "2024-08-21T16:13:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    16,
                    13,
                    16,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T16:13:16Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    16,
                    13,
                    16,
                    2,
                    234,
                    0
                ],
                "title": "Mixed Sparsity Training: Achieving 4$\\times$ FLOP Reduction for\n  Transformer Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed Sparsity Training: Achieving 4$\\times$ FLOP Reduction for\n  Transformer Pretraining"
                },
                "summary": "Large language models (LLMs) have made significant strides in complex tasks,\nyet their widespread adoption is impeded by substantial computational demands.\nWith hundreds of billion parameters, transformer-based LLMs necessitate months\nof pretraining across a high-end GPU cluster. However, this paper reveals a\ncompelling finding: transformers exhibit considerable redundancy in pretraining\ncomputations, which motivates our proposed solution, Mixed Sparsity Training\n(MST), an efficient pretraining method that can reduce about $75\\%$ of Floating\nPoint Operations (FLOPs) while maintaining performance. MST integrates dynamic\nsparse training (DST) with Sparsity Variation (SV) and Hybrid Sparse Attention\n(HSA) during pretraining, involving three distinct phases: warm-up,\nultra-sparsification, and restoration. The warm-up phase transforms the dense\nmodel into a sparse one, and the restoration phase reinstates connections.\nThroughout these phases, the model is trained with a dynamically evolving\nsparse topology and an HSA mechanism to maintain performance and minimize\ntraining FLOPs concurrently. Our experiment on GPT-2 showcases a FLOP reduction\nof $4\\times$ without compromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant strides in complex tasks,\nyet their widespread adoption is impeded by substantial computational demands.\nWith hundreds of billion parameters, transformer-based LLMs necessitate months\nof pretraining across a high-end GPU cluster. However, this paper reveals a\ncompelling finding: transformers exhibit considerable redundancy in pretraining\ncomputations, which motivates our proposed solution, Mixed Sparsity Training\n(MST), an efficient pretraining method that can reduce about $75\\%$ of Floating\nPoint Operations (FLOPs) while maintaining performance. MST integrates dynamic\nsparse training (DST) with Sparsity Variation (SV) and Hybrid Sparse Attention\n(HSA) during pretraining, involving three distinct phases: warm-up,\nultra-sparsification, and restoration. The warm-up phase transforms the dense\nmodel into a sparse one, and the restoration phase reinstates connections.\nThroughout these phases, the model is trained with a dynamically evolving\nsparse topology and an HSA mechanism to maintain performance and minimize\ntraining FLOPs concurrently. Our experiment on GPT-2 showcases a FLOP reduction\nof $4\\times$ without compromising performance."
                },
                "authors": [
                    {
                        "name": "Pihe Hu"
                    },
                    {
                        "name": "Shaolong Li"
                    },
                    {
                        "name": "Longbo Huang"
                    }
                ],
                "author_detail": {
                    "name": "Longbo Huang"
                },
                "author": "Longbo Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11745v1",
                "updated": "2024-08-21T16:11:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    16,
                    11,
                    59,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T16:11:59Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    16,
                    11,
                    59,
                    2,
                    234,
                    0
                ],
                "title": "FocusLLM: Scaling LLM's Context by Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FocusLLM: Scaling LLM's Context by Parallel Decoding"
                },
                "summary": "Empowering LLMs with the ability to utilize useful information from a long\ncontext is crucial for many downstream applications. However, achieving long\ncontext lengths with the conventional transformer architecture requires\nsubstantial training and inference resources. In this paper, we present\nFocusLLM, a framework designed to extend the context length of any decoder-only\nLLM, enabling the model to focus on relevant information from very long\nsequences. FocusLLM processes long text inputs by dividing them into chunks\nbased on the model's original context length to alleviate the issue of\nattention distraction. Then, it appends the local context to each chunk as a\nprompt to extract essential information from each chunk based on a novel\nparallel decoding mechanism, and ultimately integrates the extracted\ninformation into the local context. FocusLLM stands out for great training\nefficiency and versatility: trained with an 8K input length with much less\ntraining cost than previous methods, FocusLLM exhibits superior performance\nacross downstream long-context tasks and maintains strong language modeling\nability when handling extensive long texts, even up to 400K tokens. Our code is\navailable at https://github.com/leezythu/FocusLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering LLMs with the ability to utilize useful information from a long\ncontext is crucial for many downstream applications. However, achieving long\ncontext lengths with the conventional transformer architecture requires\nsubstantial training and inference resources. In this paper, we present\nFocusLLM, a framework designed to extend the context length of any decoder-only\nLLM, enabling the model to focus on relevant information from very long\nsequences. FocusLLM processes long text inputs by dividing them into chunks\nbased on the model's original context length to alleviate the issue of\nattention distraction. Then, it appends the local context to each chunk as a\nprompt to extract essential information from each chunk based on a novel\nparallel decoding mechanism, and ultimately integrates the extracted\ninformation into the local context. FocusLLM stands out for great training\nefficiency and versatility: trained with an 8K input length with much less\ntraining cost than previous methods, FocusLLM exhibits superior performance\nacross downstream long-context tasks and maintains strong language modeling\nability when handling extensive long texts, even up to 400K tokens. Our code is\navailable at https://github.com/leezythu/FocusLLM."
                },
                "authors": [
                    {
                        "name": "Zhenyu Li"
                    },
                    {
                        "name": "Yike Zhang"
                    },
                    {
                        "name": "Tengyu Pan"
                    },
                    {
                        "name": "Yutao Sun"
                    },
                    {
                        "name": "Zhichao Duan"
                    },
                    {
                        "name": "Junjie Fang"
                    },
                    {
                        "name": "Rong Han"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Jianyong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyong Wang"
                },
                "author": "Jianyong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11743v1",
                "updated": "2024-08-21T16:10:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    16,
                    10,
                    41,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T16:10:41Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    16,
                    10,
                    41,
                    2,
                    234,
                    0
                ],
                "title": "MARLIN: Mixed-Precision Auto-Regressive Parallel Inference on Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARLIN: Mixed-Precision Auto-Regressive Parallel Inference on Large\n  Language Models"
                },
                "summary": "As inference on Large Language Models (LLMs) emerges as an important workload\nin machine learning applications, weight quantization has become a standard\ntechnique for efficient GPU deployment. Quantization not only reduces model\nsize, but has also been shown to yield substantial speedups for single-user\ninference, due to reduced memory movement, with low accuracy impact. Yet, it\nremains open whether speedups are achievable also in \\emph{batched} settings\nwith multiple parallel clients, which are highly relevant for practical\nserving. It is unclear whether GPU kernels can be designed to remain\npractically memory-bound, while supporting the substantially increased compute\nrequirements of batched workloads.\n  This paper resolves this question positively by describing the design of\nMixed-precision Auto-Regressive LINear kernels, called MARLIN. Concretely,\ngiven a model whose weights are compressed via quantization to, e.g., 4 bits\nper element, MARLIN shows that batchsizes up to 16-32 can be supported with\nclose to maximum ($4\\times$) quantization speedup, and larger batchsizes up to\n64-128 with gradually decreasing, but still significant, acceleration. MARLIN\naccomplishes this via a combination of techniques, such as asynchronous memory\naccess, complex task scheduling and pipelining, and bespoke quantization\nsupport. Our experiments show that MARLIN's near-optimal performance on\nindividual LLM layers across different scenarios can also lead to end-to-end\nLLM inference speedups (of up to $2.8\\times$) when integrated with the popular\nvLLM serving engine. Finally, MARLIN is extensible to further compression\ntechniques, like NVIDIA 2:4 sparsity, leading to additional speedups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As inference on Large Language Models (LLMs) emerges as an important workload\nin machine learning applications, weight quantization has become a standard\ntechnique for efficient GPU deployment. Quantization not only reduces model\nsize, but has also been shown to yield substantial speedups for single-user\ninference, due to reduced memory movement, with low accuracy impact. Yet, it\nremains open whether speedups are achievable also in \\emph{batched} settings\nwith multiple parallel clients, which are highly relevant for practical\nserving. It is unclear whether GPU kernels can be designed to remain\npractically memory-bound, while supporting the substantially increased compute\nrequirements of batched workloads.\n  This paper resolves this question positively by describing the design of\nMixed-precision Auto-Regressive LINear kernels, called MARLIN. Concretely,\ngiven a model whose weights are compressed via quantization to, e.g., 4 bits\nper element, MARLIN shows that batchsizes up to 16-32 can be supported with\nclose to maximum ($4\\times$) quantization speedup, and larger batchsizes up to\n64-128 with gradually decreasing, but still significant, acceleration. MARLIN\naccomplishes this via a combination of techniques, such as asynchronous memory\naccess, complex task scheduling and pipelining, and bespoke quantization\nsupport. Our experiments show that MARLIN's near-optimal performance on\nindividual LLM layers across different scenarios can also lead to end-to-end\nLLM inference speedups (of up to $2.8\\times$) when integrated with the popular\nvLLM serving engine. Finally, MARLIN is extensible to further compression\ntechniques, like NVIDIA 2:4 sparsity, leading to additional speedups."
                },
                "authors": [
                    {
                        "name": "Elias Frantar"
                    },
                    {
                        "name": "Roberto L. Castro"
                    },
                    {
                        "name": "Jiale Chen"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11735v1",
                "updated": "2024-08-21T15:59:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    59,
                    33,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T15:59:33Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    59,
                    33,
                    2,
                    234,
                    0
                ],
                "title": "Clinical Insights: A Comprehensive Review of Language Models in Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical Insights: A Comprehensive Review of Language Models in Medicine"
                },
                "summary": "This paper provides a detailed examination of the advancements and\napplications of large language models in the healthcare sector, with a\nparticular emphasis on clinical applications. The study traces the evolution of\nLLMs from their foundational technologies to the latest developments in\ndomain-specific models and multimodal integration. It explores the technical\nprogression from encoder-based models requiring fine-tuning to sophisticated\napproaches that integrate textual, visual, and auditory data, thereby\nfacilitating comprehensive AI solutions in healthcare. The paper discusses both\nthe opportunities these technologies present for enhancing clinical efficiency\nand the challenges they pose in terms of ethics, data privacy, and\nimplementation. Additionally, it critically evaluates the deployment strategies\nof LLMs, emphasizing the necessity of open-source models to ensure data privacy\nand adaptability within healthcare environments. Future research directions are\nproposed, focusing on empirical studies to evaluate the real-world efficacy of\nLLMs in healthcare and the development of open datasets for further research.\nThis review aims to provide a comprehensive resource for both newcomers and\nmultidisciplinary researchers interested in the intersection of AI and\nhealthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides a detailed examination of the advancements and\napplications of large language models in the healthcare sector, with a\nparticular emphasis on clinical applications. The study traces the evolution of\nLLMs from their foundational technologies to the latest developments in\ndomain-specific models and multimodal integration. It explores the technical\nprogression from encoder-based models requiring fine-tuning to sophisticated\napproaches that integrate textual, visual, and auditory data, thereby\nfacilitating comprehensive AI solutions in healthcare. The paper discusses both\nthe opportunities these technologies present for enhancing clinical efficiency\nand the challenges they pose in terms of ethics, data privacy, and\nimplementation. Additionally, it critically evaluates the deployment strategies\nof LLMs, emphasizing the necessity of open-source models to ensure data privacy\nand adaptability within healthcare environments. Future research directions are\nproposed, focusing on empirical studies to evaluate the real-world efficacy of\nLLMs in healthcare and the development of open datasets for further research.\nThis review aims to provide a comprehensive resource for both newcomers and\nmultidisciplinary researchers interested in the intersection of AI and\nhealthcare."
                },
                "authors": [
                    {
                        "name": "Nikita Neveditsin"
                    },
                    {
                        "name": "Pawan Lingras"
                    },
                    {
                        "name": "Vijay Mago"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Mago"
                },
                "author": "Vijay Mago",
                "arxiv_comment": "Submitted to PLOS Digital Health",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11729v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11729v2",
                "updated": "2024-08-22T02:38:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    2,
                    38,
                    56,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-21T15:54:17Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    54,
                    17,
                    2,
                    234,
                    0
                ],
                "title": "LLM4VV: Exploring LLM-as-a-Judge for Validation and Verification\n  Testsuites",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4VV: Exploring LLM-as-a-Judge for Validation and Verification\n  Testsuites"
                },
                "summary": "Large Language Models (LLM) are evolving and have significantly\nrevolutionized the landscape of software development. If used well, they can\nsignificantly accelerate the software development cycle. At the same time, the\ncommunity is very cautious of the models being trained on biased or sensitive\ndata, which can lead to biased outputs along with the inadvertent release of\nconfidential information. Additionally, the carbon footprints and the\nun-explainability of these black box models continue to raise questions about\nthe usability of LLMs.\n  With the abundance of opportunities LLMs have to offer, this paper explores\nthe idea of judging tests used to evaluate compiler implementations of\ndirective-based programming models as well as probe into the black box of LLMs.\nBased on our results, utilizing an agent-based prompting approach and setting\nup a validation pipeline structure drastically increased the quality of\nDeepSeek Coder, the LLM chosen for the evaluation purposes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) are evolving and have significantly\nrevolutionized the landscape of software development. If used well, they can\nsignificantly accelerate the software development cycle. At the same time, the\ncommunity is very cautious of the models being trained on biased or sensitive\ndata, which can lead to biased outputs along with the inadvertent release of\nconfidential information. Additionally, the carbon footprints and the\nun-explainability of these black box models continue to raise questions about\nthe usability of LLMs.\n  With the abundance of opportunities LLMs have to offer, this paper explores\nthe idea of judging tests used to evaluate compiler implementations of\ndirective-based programming models as well as probe into the black box of LLMs.\nBased on our results, utilizing an agent-based prompting approach and setting\nup a validation pipeline structure drastically increased the quality of\nDeepSeek Coder, the LLM chosen for the evaluation purposes."
                },
                "authors": [
                    {
                        "name": "Zachariah Sollenberger"
                    },
                    {
                        "name": "Jay Patel"
                    },
                    {
                        "name": "Christian Munley"
                    },
                    {
                        "name": "Aaron Jarmusch"
                    },
                    {
                        "name": "Sunita Chandrasekaran"
                    }
                ],
                "author_detail": {
                    "name": "Sunita Chandrasekaran"
                },
                "author": "Sunita Chandrasekaran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11729v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11729v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11727v1",
                "updated": "2024-08-21T15:54:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    54,
                    4,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T15:54:04Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    54,
                    4,
                    2,
                    234,
                    0
                ],
                "title": "Efficient Detection of Toxic Prompts in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Detection of Toxic Prompts in Large Language Models"
                },
                "summary": "Large language models (LLMs) like ChatGPT and Gemini have significantly\nadvanced natural language processing, enabling various applications such as\nchatbots and automated content generation. However, these models can be\nexploited by malicious individuals who craft toxic prompts to elicit harmful or\nunethical responses. These individuals often employ jailbreaking techniques to\nbypass safety mechanisms, highlighting the need for robust toxic prompt\ndetection methods. Existing detection techniques, both blackbox and whitebox,\nface challenges related to the diversity of toxic prompts, scalability, and\ncomputational efficiency. In response, we propose ToxicDetector, a lightweight\ngreybox method designed to efficiently detect toxic prompts in LLMs.\nToxicDetector leverages LLMs to create toxic concept prompts, uses embedding\nvectors to form feature vectors, and employs a Multi-Layer Perceptron (MLP)\nclassifier for prompt classification. Our evaluation on various versions of the\nLLama models, Gemma-2, and multiple datasets demonstrates that ToxicDetector\nachieves a high accuracy of 96.39\\% and a low false positive rate of 2.00\\%,\noutperforming state-of-the-art methods. Additionally, ToxicDetector's\nprocessing time of 0.0780 seconds per prompt makes it highly suitable for\nreal-time applications. ToxicDetector achieves high accuracy, efficiency, and\nscalability, making it a practical method for toxic prompt detection in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) like ChatGPT and Gemini have significantly\nadvanced natural language processing, enabling various applications such as\nchatbots and automated content generation. However, these models can be\nexploited by malicious individuals who craft toxic prompts to elicit harmful or\nunethical responses. These individuals often employ jailbreaking techniques to\nbypass safety mechanisms, highlighting the need for robust toxic prompt\ndetection methods. Existing detection techniques, both blackbox and whitebox,\nface challenges related to the diversity of toxic prompts, scalability, and\ncomputational efficiency. In response, we propose ToxicDetector, a lightweight\ngreybox method designed to efficiently detect toxic prompts in LLMs.\nToxicDetector leverages LLMs to create toxic concept prompts, uses embedding\nvectors to form feature vectors, and employs a Multi-Layer Perceptron (MLP)\nclassifier for prompt classification. Our evaluation on various versions of the\nLLama models, Gemma-2, and multiple datasets demonstrates that ToxicDetector\nachieves a high accuracy of 96.39\\% and a low false positive rate of 2.00\\%,\noutperforming state-of-the-art methods. Additionally, ToxicDetector's\nprocessing time of 0.0780 seconds per prompt makes it highly suitable for\nreal-time applications. ToxicDetector achieves high accuracy, efficiency, and\nscalability, making it a practical method for toxic prompt detection in LLMs."
                },
                "authors": [
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Junzhe Yu"
                    },
                    {
                        "name": "Huijia Sun"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Gelei Deng"
                    },
                    {
                        "name": "Yuqi Chen"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "Accepted by the 39th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10923v2",
                "updated": "2024-08-21T15:51:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    51,
                    33,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-20T15:05:02Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    5,
                    2,
                    1,
                    233,
                    0
                ],
                "title": "LBC: Language-Based-Classifier for Out-Of-Variable Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LBC: Language-Based-Classifier for Out-Of-Variable Generalization"
                },
                "summary": "Large Language Models (LLMs) have great success in natural language\nprocessing tasks such as response generation. However, their use in tabular\ndata has been limited due to their inferior performance compared to traditional\nmachine learning models (TMLs) such as XGBoost. We find that the pre-trained\nknowledge of LLMs enables them to interpret new variables that appear in a test\nwithout additional training, a capability central to the concept of\nOut-of-Variable (OOV). From the findings, we propose a\nLanguage-Based-Classifier (LBC), a classifier that maximizes the benefits of\nLLMs to outperform TMLs on OOV tasks. LBC employs three key methodological\nstrategies: 1) Categorical changes to adjust data to better fit the model's\nunderstanding, 2) Advanced order and indicator to enhance data representation\nto the model, and 3) Using verbalizer to map logit scores to classes during\ninference to generate model predictions. These strategies, combined with the\npre-trained knowledge of LBC, emphasize the model's ability to effectively\nhandle OOV tasks. We empirically and theoretically validate the superiority of\nLBC. LBC is the first study to apply an LLM-based model to OOV tasks. The\nsource code is at https://github.com/sksmssh/LBCforOOVGen",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have great success in natural language\nprocessing tasks such as response generation. However, their use in tabular\ndata has been limited due to their inferior performance compared to traditional\nmachine learning models (TMLs) such as XGBoost. We find that the pre-trained\nknowledge of LLMs enables them to interpret new variables that appear in a test\nwithout additional training, a capability central to the concept of\nOut-of-Variable (OOV). From the findings, we propose a\nLanguage-Based-Classifier (LBC), a classifier that maximizes the benefits of\nLLMs to outperform TMLs on OOV tasks. LBC employs three key methodological\nstrategies: 1) Categorical changes to adjust data to better fit the model's\nunderstanding, 2) Advanced order and indicator to enhance data representation\nto the model, and 3) Using verbalizer to map logit scores to classes during\ninference to generate model predictions. These strategies, combined with the\npre-trained knowledge of LBC, emphasize the model's ability to effectively\nhandle OOV tasks. We empirically and theoretically validate the superiority of\nLBC. LBC is the first study to apply an LLM-based model to OOV tasks. The\nsource code is at https://github.com/sksmssh/LBCforOOVGen"
                },
                "authors": [
                    {
                        "name": "Kangjun Noh"
                    },
                    {
                        "name": "Baekryun Seong"
                    },
                    {
                        "name": "Hoyoon Byun"
                    },
                    {
                        "name": "Youngjun Choi"
                    },
                    {
                        "name": "Sungjin Song"
                    },
                    {
                        "name": "Kyungwoo Song"
                    }
                ],
                "author_detail": {
                    "name": "Kyungwoo Song"
                },
                "author": "Kyungwoo Song",
                "arxiv_comment": "16 pages, 7 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11707v1",
                "updated": "2024-08-21T15:31:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    31,
                    37,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T15:31:37Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    31,
                    37,
                    2,
                    234,
                    0
                ],
                "title": "biorecap: an R package for summarizing bioRxiv preprints with a local\n  LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "biorecap: an R package for summarizing bioRxiv preprints with a local\n  LLM"
                },
                "summary": "The establishment of bioRxiv facilitated the rapid adoption of preprints in\nthe life sciences, accelerating the dissemination of new research findings.\nHowever, the sheer volume of preprints published daily can be overwhelming,\nmaking it challenging for researchers to stay updated on the latest\ndevelopments. Here, I introduce biorecap, an R package that retrieves and\nsummarizes bioRxiv preprints using a large language model (LLM) running locally\non nearly any commodity laptop. biorecap leverages the ollamar package to\ninterface with the Ollama server and API endpoints, allowing users to prompt\nany local LLM available through Ollama. The package follows tidyverse\nconventions, enabling users to pipe the output of one function as input to\nanother. Additionally, biorecap provides a single wrapper function that\ngenerates a timestamped CSV file and HTML report containing short summaries of\nrecent preprints published in user-configurable subject areas. By combining the\nstrengths of LLMs with the flexibility and security of local execution,\nbiorecap represents an advancement in the tools available for managing the\ninformation overload in modern scientific research. The biorecap R package is\navailable on GitHub at https://github.com/stephenturner/biorecap under an\nopen-source (MIT) license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The establishment of bioRxiv facilitated the rapid adoption of preprints in\nthe life sciences, accelerating the dissemination of new research findings.\nHowever, the sheer volume of preprints published daily can be overwhelming,\nmaking it challenging for researchers to stay updated on the latest\ndevelopments. Here, I introduce biorecap, an R package that retrieves and\nsummarizes bioRxiv preprints using a large language model (LLM) running locally\non nearly any commodity laptop. biorecap leverages the ollamar package to\ninterface with the Ollama server and API endpoints, allowing users to prompt\nany local LLM available through Ollama. The package follows tidyverse\nconventions, enabling users to pipe the output of one function as input to\nanother. Additionally, biorecap provides a single wrapper function that\ngenerates a timestamped CSV file and HTML report containing short summaries of\nrecent preprints published in user-configurable subject areas. By combining the\nstrengths of LLMs with the flexibility and security of local execution,\nbiorecap represents an advancement in the tools available for managing the\ninformation overload in modern scientific research. The biorecap R package is\navailable on GitHub at https://github.com/stephenturner/biorecap under an\nopen-source (MIT) license."
                },
                "authors": [
                    {
                        "name": "Stephen D. Turner"
                    }
                ],
                "author_detail": {
                    "name": "Stephen D. Turner"
                },
                "author": "Stephen D. Turner",
                "arxiv_comment": "5 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.OT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11706v1",
                "updated": "2024-08-21T15:30:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    30,
                    35,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T15:30:35Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    30,
                    35,
                    2,
                    234,
                    0
                ],
                "title": "FRAP: Faithful and Realistic Text-to-Image Generation with Adaptive\n  Prompt Weighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FRAP: Faithful and Realistic Text-to-Image Generation with Adaptive\n  Prompt Weighting"
                },
                "summary": "Text-to-image (T2I) diffusion models have demonstrated impressive\ncapabilities in generating high-quality images given a text prompt. However,\nensuring the prompt-image alignment remains a considerable challenge, i.e.,\ngenerating images that faithfully align with the prompt's semantics. Recent\nworks attempt to improve the faithfulness by optimizing the latent code, which\npotentially could cause the latent code to go out-of-distribution and thus\nproduce unrealistic images. In this paper, we propose FRAP, a simple, yet\neffective approach based on adaptively adjusting the per-token prompt weights\nto improve prompt-image alignment and authenticity of the generated images. We\ndesign an online algorithm to adaptively update each token's weight\ncoefficient, which is achieved by minimizing a unified objective function that\nencourages object presence and the binding of object-modifier pairs. Through\nextensive evaluations, we show FRAP generates images with significantly higher\nprompt-image alignment to prompts from complex datasets, while having a lower\naverage latency compared to recent latent code optimization methods, e.g., 4\nseconds faster than D&B on the COCO-Subject dataset. Furthermore, through\nvisual comparisons and evaluation on the CLIP-IQA-Real metric, we show that\nFRAP not only improves prompt-image alignment but also generates more authentic\nimages with realistic appearances. We also explore combining FRAP with prompt\nrewriting LLM to recover their degraded prompt-image alignment, where we\nobserve improvements in both prompt-image alignment and image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) diffusion models have demonstrated impressive\ncapabilities in generating high-quality images given a text prompt. However,\nensuring the prompt-image alignment remains a considerable challenge, i.e.,\ngenerating images that faithfully align with the prompt's semantics. Recent\nworks attempt to improve the faithfulness by optimizing the latent code, which\npotentially could cause the latent code to go out-of-distribution and thus\nproduce unrealistic images. In this paper, we propose FRAP, a simple, yet\neffective approach based on adaptively adjusting the per-token prompt weights\nto improve prompt-image alignment and authenticity of the generated images. We\ndesign an online algorithm to adaptively update each token's weight\ncoefficient, which is achieved by minimizing a unified objective function that\nencourages object presence and the binding of object-modifier pairs. Through\nextensive evaluations, we show FRAP generates images with significantly higher\nprompt-image alignment to prompts from complex datasets, while having a lower\naverage latency compared to recent latent code optimization methods, e.g., 4\nseconds faster than D&B on the COCO-Subject dataset. Furthermore, through\nvisual comparisons and evaluation on the CLIP-IQA-Real metric, we show that\nFRAP not only improves prompt-image alignment but also generates more authentic\nimages with realistic appearances. We also explore combining FRAP with prompt\nrewriting LLM to recover their degraded prompt-image alignment, where we\nobserve improvements in both prompt-image alignment and image quality."
                },
                "authors": [
                    {
                        "name": "Liyao Jiang"
                    },
                    {
                        "name": "Negar Hassanpour"
                    },
                    {
                        "name": "Mohammad Salameh"
                    },
                    {
                        "name": "Mohan Sai Singamsetti"
                    },
                    {
                        "name": "Fengyu Sun"
                    },
                    {
                        "name": "Wei Lu"
                    },
                    {
                        "name": "Di Niu"
                    }
                ],
                "author_detail": {
                    "name": "Di Niu"
                },
                "author": "Di Niu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03862v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03862v2",
                "updated": "2024-08-21T15:23:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    23,
                    28,
                    2,
                    234,
                    0
                ],
                "published": "2024-04-05T02:27:09Z",
                "published_parsed": [
                    2024,
                    4,
                    5,
                    2,
                    27,
                    9,
                    4,
                    96,
                    0
                ],
                "title": "Verifiable by Design: Aligning Language Models to Quote from\n  Pre-Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verifiable by Design: Aligning Language Models to Quote from\n  Pre-Training Data"
                },
                "summary": "To trust the fluent generations of large language models (LLMs), humans must\nbe able to verify their correctness against trusted, external sources. Recent\nefforts, such as providing citations via retrieved documents or post-hoc\nprovenance, enhance verifiability but still provide no guarantees on their\ncorrectness. To address these limitations, we tackle the verifiability goal\nwith a different philosophy: trivializing the verification process by\ndeveloping models that quote verbatim statements from trusted sources in\npre-training data. We propose Quote-Tuning, and demonstrate it is feasible to\nalign LLMs to provide quoted statements from data memorized during\npre-training. The core of Quote-Tuning is a fast membership inference function\n(Marone and Van Durme, 2023) that efficiently verifies text against a trusted\ncorpus. We leverage this tool to design a reward function to quantify quotes in\nmodel responses, which is then used to create a dataset for preference\nlearning. Experimental results show that Quote-Tuning significantly increases\nverbatim quotes from high-quality pre-training documents by 55% to 130%\nrelative to un-tuned models while maintaining response quality. Quote-Tuning\nalso generalizes quoting to out-of-domain data, is applicable in different\ntasks, and provides additional benefits to truthfulness. Our method not only\nserves as a hassle-free method to increase quoting but also opens up avenues\nfor improving LLM trustworthiness through better verifiability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To trust the fluent generations of large language models (LLMs), humans must\nbe able to verify their correctness against trusted, external sources. Recent\nefforts, such as providing citations via retrieved documents or post-hoc\nprovenance, enhance verifiability but still provide no guarantees on their\ncorrectness. To address these limitations, we tackle the verifiability goal\nwith a different philosophy: trivializing the verification process by\ndeveloping models that quote verbatim statements from trusted sources in\npre-training data. We propose Quote-Tuning, and demonstrate it is feasible to\nalign LLMs to provide quoted statements from data memorized during\npre-training. The core of Quote-Tuning is a fast membership inference function\n(Marone and Van Durme, 2023) that efficiently verifies text against a trusted\ncorpus. We leverage this tool to design a reward function to quantify quotes in\nmodel responses, which is then used to create a dataset for preference\nlearning. Experimental results show that Quote-Tuning significantly increases\nverbatim quotes from high-quality pre-training documents by 55% to 130%\nrelative to un-tuned models while maintaining response quality. Quote-Tuning\nalso generalizes quoting to out-of-domain data, is applicable in different\ntasks, and provides additional benefits to truthfulness. Our method not only\nserves as a hassle-free method to increase quoting but also opens up avenues\nfor improving LLM trustworthiness through better verifiability."
                },
                "authors": [
                    {
                        "name": "Jingyu Zhang"
                    },
                    {
                        "name": "Marc Marone"
                    },
                    {
                        "name": "Tianjian Li"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    },
                    {
                        "name": "Daniel Khashabi"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Khashabi"
                },
                "author": "Daniel Khashabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03862v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03862v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13858v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13858v2",
                "updated": "2024-08-21T15:15:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    15,
                    57,
                    2,
                    234,
                    0
                ],
                "published": "2024-05-22T17:33:51Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    17,
                    33,
                    51,
                    2,
                    143,
                    0
                ],
                "title": "Carbon Connect: An Ecosystem for Sustainable Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Carbon Connect: An Ecosystem for Sustainable Computing"
                },
                "summary": "Computing is at a moment of profound opportunity. Emerging applications --\nsuch as capable artificial intelligence, immersive virtual realities, and\npervasive sensor systems -- drive unprecedented demand for computer. Despite\nrecent advances toward net zero carbon emissions, the computing industry's\ngross energy usage continues to rise at an alarming rate, outpacing the growth\nof new energy installations and renewable energy deployments. A shift towards\nsustainability is needed to spark a transformation in how computer systems are\nmanufactured, allocated, and consumed. Carbon Connect envisions coordinated\nresearch thrusts that produce design and management strategies for sustainable,\nnext-generation computer systems. These strategies must flatten and then\nreverse growth trajectories for computing power and carbon for society's most\nrapidly growing applications such as artificial intelligence and virtual\nspaces. We will require accurate models for carbon accounting in computing\ntechnology. For embodied carbon, we must re-think conventional design\nstrategies -- over-provisioned monolithic servers, frequent hardware refresh\ncycles, custom silicon -- and adopt life-cycle design strategies that more\neffectively reduce, reuse and recycle hardware at scale. For operational\ncarbon, we must not only embrace renewable energy but also design systems to\nuse that energy more efficiently. Finally, new hardware design and management\nstrategies must be cognizant of economic policy and regulatory landscape,\naligning private initiatives with societal goals. Many of these broader goals\nwill require computer scientists to develop deep, enduring collaborations with\nresearchers in economics, law, and industrial ecology to spark change in\nbroader practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computing is at a moment of profound opportunity. Emerging applications --\nsuch as capable artificial intelligence, immersive virtual realities, and\npervasive sensor systems -- drive unprecedented demand for computer. Despite\nrecent advances toward net zero carbon emissions, the computing industry's\ngross energy usage continues to rise at an alarming rate, outpacing the growth\nof new energy installations and renewable energy deployments. A shift towards\nsustainability is needed to spark a transformation in how computer systems are\nmanufactured, allocated, and consumed. Carbon Connect envisions coordinated\nresearch thrusts that produce design and management strategies for sustainable,\nnext-generation computer systems. These strategies must flatten and then\nreverse growth trajectories for computing power and carbon for society's most\nrapidly growing applications such as artificial intelligence and virtual\nspaces. We will require accurate models for carbon accounting in computing\ntechnology. For embodied carbon, we must re-think conventional design\nstrategies -- over-provisioned monolithic servers, frequent hardware refresh\ncycles, custom silicon -- and adopt life-cycle design strategies that more\neffectively reduce, reuse and recycle hardware at scale. For operational\ncarbon, we must not only embrace renewable energy but also design systems to\nuse that energy more efficiently. Finally, new hardware design and management\nstrategies must be cognizant of economic policy and regulatory landscape,\naligning private initiatives with societal goals. Many of these broader goals\nwill require computer scientists to develop deep, enduring collaborations with\nresearchers in economics, law, and industrial ecology to spark change in\nbroader practice."
                },
                "authors": [
                    {
                        "name": "Benjamin C. Lee"
                    },
                    {
                        "name": "David Brooks"
                    },
                    {
                        "name": "Arthur van Benthem"
                    },
                    {
                        "name": "Udit Gupta"
                    },
                    {
                        "name": "Gage Hills"
                    },
                    {
                        "name": "Vincent Liu"
                    },
                    {
                        "name": "Benjamin Pierce"
                    },
                    {
                        "name": "Christopher Stewart"
                    },
                    {
                        "name": "Emma Strubell"
                    },
                    {
                        "name": "Gu-Yeon Wei"
                    },
                    {
                        "name": "Adam Wierman"
                    },
                    {
                        "name": "Yuan Yao"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13858v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13858v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10264v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10264v3",
                "updated": "2024-08-21T15:12:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    12,
                    37,
                    2,
                    234,
                    0
                ],
                "published": "2024-07-14T16:12:57Z",
                "published_parsed": [
                    2024,
                    7,
                    14,
                    16,
                    12,
                    57,
                    6,
                    196,
                    0
                ],
                "title": "What Makes and Breaks Safety Fine-tuning? A Mechanistic Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Makes and Breaks Safety Fine-tuning? A Mechanistic Study"
                },
                "summary": "Safety fine-tuning helps align Large Language Models (LLMs) with human\npreferences for their safe deployment. To better understand the underlying\nfactors that make models safe via safety fine-tuning, we design a synthetic\ndata generation framework that captures salient aspects of an unsafe input by\nmodeling the interaction between the task the model is asked to perform (e.g.,\n\"design\") versus the specific concepts the task is asked to be performed upon\n(e.g., a \"cycle\" vs. a \"bomb\"). Using this, we investigate three well-known\nsafety fine-tuning methods -- supervised safety fine-tuning, direct preference\noptimization, and unlearning -- and provide significant evidence demonstrating\nthat these methods minimally transform MLP weights to specifically align unsafe\ninputs into its weights' null space. This yields a clustering of inputs based\non whether the model deems them safe or not. Correspondingly, when an\nadversarial input (e.g., a jailbreak) is provided, its activations are closer\nto safer samples, leading to the model processing such an input as if it were\nsafe. We validate our findings, wherever possible, on real-world models --\nspecifically, Llama-2 7B and Llama-3 8B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety fine-tuning helps align Large Language Models (LLMs) with human\npreferences for their safe deployment. To better understand the underlying\nfactors that make models safe via safety fine-tuning, we design a synthetic\ndata generation framework that captures salient aspects of an unsafe input by\nmodeling the interaction between the task the model is asked to perform (e.g.,\n\"design\") versus the specific concepts the task is asked to be performed upon\n(e.g., a \"cycle\" vs. a \"bomb\"). Using this, we investigate three well-known\nsafety fine-tuning methods -- supervised safety fine-tuning, direct preference\noptimization, and unlearning -- and provide significant evidence demonstrating\nthat these methods minimally transform MLP weights to specifically align unsafe\ninputs into its weights' null space. This yields a clustering of inputs based\non whether the model deems them safe or not. Correspondingly, when an\nadversarial input (e.g., a jailbreak) is provided, its activations are closer\nto safer samples, leading to the model processing such an input as if it were\nsafe. We validate our findings, wherever possible, on real-world models --\nspecifically, Llama-2 7B and Llama-3 8B."
                },
                "authors": [
                    {
                        "name": "Samyak Jain"
                    },
                    {
                        "name": "Ekdeep Singh Lubana"
                    },
                    {
                        "name": "Kemal Oksuz"
                    },
                    {
                        "name": "Tom Joy"
                    },
                    {
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "name": "Amartya Sanyal"
                    },
                    {
                        "name": "Puneet K. Dokania"
                    }
                ],
                "author_detail": {
                    "name": "Puneet K. Dokania"
                },
                "author": "Puneet K. Dokania",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10264v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10264v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07595v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07595v4",
                "updated": "2024-08-21T14:51:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    14,
                    51,
                    6,
                    2,
                    234,
                    0
                ],
                "published": "2024-06-11T13:42:57Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    13,
                    42,
                    57,
                    1,
                    163,
                    0
                ],
                "title": "VulDetectBench: Evaluating the Deep Capability of Vulnerability\n  Detection with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VulDetectBench: Evaluating the Deep Capability of Vulnerability\n  Detection with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have training corpora containing large amounts\nof program code, greatly improving the model's code comprehension and\ngeneration capabilities. However, sound comprehensive research on detecting\nprogram vulnerabilities, a more specific task related to code, and evaluating\nthe performance of LLMs in this more specialized scenario is still lacking. To\naddress common challenges in vulnerability analysis, our study introduces a new\nbenchmark, VulDetectBench, specifically designed to assess the vulnerability\ndetection capabilities of LLMs. The benchmark comprehensively evaluates LLM's\nability to identify, classify, and locate vulnerabilities through five tasks of\nincreasing difficulty. We evaluate the performance of 17 models (both open- and\nclosed-source) and find that while existing models can achieve over 80%\naccuracy on tasks related to vulnerability identification and classification,\nthey still fall short on specific, more detailed vulnerability analysis tasks,\nwith less than 30% accuracy, making it difficult to provide valuable auxiliary\ninformation for professional vulnerability mining. Our benchmark effectively\nevaluates the capabilities of various LLMs at different levels in the specific\ntask of vulnerability detection, providing a foundation for future research and\nimprovements in this critical area of code security. VulDetectBench is publicly\navailable at https://github.com/Sweetaroo/VulDetectBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have training corpora containing large amounts\nof program code, greatly improving the model's code comprehension and\ngeneration capabilities. However, sound comprehensive research on detecting\nprogram vulnerabilities, a more specific task related to code, and evaluating\nthe performance of LLMs in this more specialized scenario is still lacking. To\naddress common challenges in vulnerability analysis, our study introduces a new\nbenchmark, VulDetectBench, specifically designed to assess the vulnerability\ndetection capabilities of LLMs. The benchmark comprehensively evaluates LLM's\nability to identify, classify, and locate vulnerabilities through five tasks of\nincreasing difficulty. We evaluate the performance of 17 models (both open- and\nclosed-source) and find that while existing models can achieve over 80%\naccuracy on tasks related to vulnerability identification and classification,\nthey still fall short on specific, more detailed vulnerability analysis tasks,\nwith less than 30% accuracy, making it difficult to provide valuable auxiliary\ninformation for professional vulnerability mining. Our benchmark effectively\nevaluates the capabilities of various LLMs at different levels in the specific\ntask of vulnerability detection, providing a foundation for future research and\nimprovements in this critical area of code security. VulDetectBench is publicly\navailable at https://github.com/Sweetaroo/VulDetectBench."
                },
                "authors": [
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Lang Gao"
                    },
                    {
                        "name": "Mingxin Yang"
                    },
                    {
                        "name": "Yu Xie"
                    },
                    {
                        "name": "Ping Chen"
                    },
                    {
                        "name": "Xiaojin Zhang"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07595v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07595v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18804v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18804v2",
                "updated": "2024-08-21T14:42:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    14,
                    42,
                    51,
                    2,
                    234,
                    0
                ],
                "published": "2024-05-29T06:47:34Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    6,
                    47,
                    34,
                    2,
                    150,
                    0
                ],
                "title": "Tilde: Teleoperation for Dexterous In-Hand Manipulation Learning with a\n  DeltaHand",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tilde: Teleoperation for Dexterous In-Hand Manipulation Learning with a\n  DeltaHand"
                },
                "summary": "Dexterous robotic manipulation remains a challenging domain due to its strict\ndemands for precision and robustness on both hardware and software. While\ndexterous robotic hands have demonstrated remarkable capabilities in complex\ntasks, efficiently learning adaptive control policies for hands still presents\na significant hurdle given the high dimensionalities of hands and tasks. To\nbridge this gap, we propose Tilde, an imitation learning-based in-hand\nmanipulation system on a dexterous DeltaHand. It leverages 1) a low-cost,\nconfigurable, simple-to-control, soft dexterous robotic hand, DeltaHand, 2) a\nuser-friendly, precise, real-time teleoperation interface, TeleHand, and 3) an\nefficient and generalizable imitation learning approach with diffusion\npolicies. Our proposed TeleHand has a kinematic twin design to the DeltaHand\nthat enables precise one-to-one joint control of the DeltaHand during\nteleoperation. This facilitates efficient high-quality data collection of human\ndemonstrations in the real world. To evaluate the effectiveness of our system,\nwe demonstrate the fully autonomous closed-loop deployment of diffusion\npolicies learned from demonstrations across seven dexterous manipulation tasks\nwith an average 90% success rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dexterous robotic manipulation remains a challenging domain due to its strict\ndemands for precision and robustness on both hardware and software. While\ndexterous robotic hands have demonstrated remarkable capabilities in complex\ntasks, efficiently learning adaptive control policies for hands still presents\na significant hurdle given the high dimensionalities of hands and tasks. To\nbridge this gap, we propose Tilde, an imitation learning-based in-hand\nmanipulation system on a dexterous DeltaHand. It leverages 1) a low-cost,\nconfigurable, simple-to-control, soft dexterous robotic hand, DeltaHand, 2) a\nuser-friendly, precise, real-time teleoperation interface, TeleHand, and 3) an\nefficient and generalizable imitation learning approach with diffusion\npolicies. Our proposed TeleHand has a kinematic twin design to the DeltaHand\nthat enables precise one-to-one joint control of the DeltaHand during\nteleoperation. This facilitates efficient high-quality data collection of human\ndemonstrations in the real world. To evaluate the effectiveness of our system,\nwe demonstrate the fully autonomous closed-loop deployment of diffusion\npolicies learned from demonstrations across seven dexterous manipulation tasks\nwith an average 90% success rate."
                },
                "authors": [
                    {
                        "name": "Zilin Si"
                    },
                    {
                        "name": "Kevin Lee Zhang"
                    },
                    {
                        "name": "Zeynep Temel"
                    },
                    {
                        "name": "Oliver Kroemer"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Kroemer"
                },
                "author": "Oliver Kroemer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18804v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18804v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02528v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02528v3",
                "updated": "2024-08-21T14:39:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    14,
                    39,
                    24,
                    2,
                    234,
                    0
                ],
                "published": "2024-05-04T00:32:20Z",
                "published_parsed": [
                    2024,
                    5,
                    4,
                    0,
                    32,
                    20,
                    5,
                    125,
                    0
                ],
                "title": "GigSense: An LLM-Infused Tool for Workers Collective Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GigSense: An LLM-Infused Tool for Workers Collective Intelligence"
                },
                "summary": "Collective intelligence among gig workers yields considerable advantages,\nincluding improved information exchange, deeper social bonds, and stronger\nadvocacy for better labor conditions. Especially as it enables workers to\ncollaboratively pinpoint shared challenges and devise optimal strategies for\naddressing these issues. However, enabling collective intelligence remains\nchallenging, as existing tools often overestimate gig workers' available time\nand uniformity in analytical reasoning. To overcome this, we introduce\nGigSense, a tool that leverages large language models alongside theories of\ncollective intelligence and sensemaking. GigSense enables gig workers to\nrapidly understand and address shared challenges effectively, irrespective of\ntheir diverse backgrounds. Our user study showed that GigSense users\noutperformed those using a control interface in problem identification and\ngenerated solutions more quickly and of higher quality, with better usability\nexperiences reported. GigSense not only empowers gig workers but also opens up\nnew possibilities for supporting workers more broadly, demonstrating the\npotential of large language model interfaces to enhance collective intelligence\nefforts in the evolving workplace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collective intelligence among gig workers yields considerable advantages,\nincluding improved information exchange, deeper social bonds, and stronger\nadvocacy for better labor conditions. Especially as it enables workers to\ncollaboratively pinpoint shared challenges and devise optimal strategies for\naddressing these issues. However, enabling collective intelligence remains\nchallenging, as existing tools often overestimate gig workers' available time\nand uniformity in analytical reasoning. To overcome this, we introduce\nGigSense, a tool that leverages large language models alongside theories of\ncollective intelligence and sensemaking. GigSense enables gig workers to\nrapidly understand and address shared challenges effectively, irrespective of\ntheir diverse backgrounds. Our user study showed that GigSense users\noutperformed those using a control interface in problem identification and\ngenerated solutions more quickly and of higher quality, with better usability\nexperiences reported. GigSense not only empowers gig workers but also opens up\nnew possibilities for supporting workers more broadly, demonstrating the\npotential of large language model interfaces to enhance collective intelligence\nefforts in the evolving workplace."
                },
                "authors": [
                    {
                        "name": "Kashif Imteyaz"
                    },
                    {
                        "name": "Claudia Flores-Saviaga"
                    },
                    {
                        "name": "Saiph Savage"
                    }
                ],
                "author_detail": {
                    "name": "Saiph Savage"
                },
                "author": "Saiph Savage",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02528v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02528v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10468v2",
                "updated": "2024-08-21T14:35:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    14,
                    35,
                    48,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-20T00:40:49Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    0,
                    40,
                    49,
                    1,
                    233,
                    0
                ],
                "title": "Tracing Privacy Leakage of Language Models to Training Data via Adjusted\n  Influence Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracing Privacy Leakage of Language Models to Training Data via Adjusted\n  Influence Functions"
                },
                "summary": "The responses generated by Large Language Models (LLMs) can include sensitive\ninformation from individuals and organizations, leading to potential privacy\nleakage. This work implements Influence Functions (IFs) to trace privacy\nleakage back to the training data, thereby mitigating privacy concerns of\nLanguage Models (LMs). However, we notice that current IFs struggle to\naccurately estimate the influence of tokens with large gradient norms,\npotentially overestimating their influence. When tracing the most influential\nsamples, this leads to frequently tracing back to samples with large gradient\nnorm tokens, overshadowing the actual most influential samples even if their\ninfluences are well estimated. To address this issue, we propose Heuristically\nAdjusted IF (HAIF), which reduces the weight of tokens with large gradient\nnorms, thereby significantly improving the accuracy of tracing the most\ninfluential samples. To establish easily obtained groundtruth for tracing\nprivacy leakage, we construct two datasets, PII-E and PII-CR, representing two\ndistinct scenarios: one with identical text in the model outputs and\npre-training data, and the other where models leverage their reasoning\nabilities to generate text divergent from pre-training data. HAIF significantly\nimproves tracing accuracy, enhancing it by 20.96\\% to 73.71\\% on the PII-E\ndataset and 3.21\\% to 45.93\\% on the PII-CR dataset, compared to the best SOTA\nIFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs\non real-world pretraining data CLUECorpus2020, demonstrating strong robustness\nregardless prompt and response lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The responses generated by Large Language Models (LLMs) can include sensitive\ninformation from individuals and organizations, leading to potential privacy\nleakage. This work implements Influence Functions (IFs) to trace privacy\nleakage back to the training data, thereby mitigating privacy concerns of\nLanguage Models (LMs). However, we notice that current IFs struggle to\naccurately estimate the influence of tokens with large gradient norms,\npotentially overestimating their influence. When tracing the most influential\nsamples, this leads to frequently tracing back to samples with large gradient\nnorm tokens, overshadowing the actual most influential samples even if their\ninfluences are well estimated. To address this issue, we propose Heuristically\nAdjusted IF (HAIF), which reduces the weight of tokens with large gradient\nnorms, thereby significantly improving the accuracy of tracing the most\ninfluential samples. To establish easily obtained groundtruth for tracing\nprivacy leakage, we construct two datasets, PII-E and PII-CR, representing two\ndistinct scenarios: one with identical text in the model outputs and\npre-training data, and the other where models leverage their reasoning\nabilities to generate text divergent from pre-training data. HAIF significantly\nimproves tracing accuracy, enhancing it by 20.96\\% to 73.71\\% on the PII-E\ndataset and 3.21\\% to 45.93\\% on the PII-CR dataset, compared to the best SOTA\nIFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs\non real-world pretraining data CLUECorpus2020, demonstrating strong robustness\nregardless prompt and response lengths."
                },
                "authors": [
                    {
                        "name": "Jinxin Liu"
                    },
                    {
                        "name": "Zao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zao Yang"
                },
                "author": "Zao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11659v1",
                "updated": "2024-08-21T14:33:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    14,
                    33,
                    43,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T14:33:43Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    14,
                    33,
                    43,
                    2,
                    234,
                    0
                ],
                "title": "5G NR PRACH Detection with Convolutional Neural Networks (CNN):\n  Overcoming Cell Interference Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "5G NR PRACH Detection with Convolutional Neural Networks (CNN):\n  Overcoming Cell Interference Challenges"
                },
                "summary": "In this paper, we present a novel approach to interference detection in 5G\nNew Radio (5G-NR) networks using Convolutional Neural Networks (CNN).\nInterference in 5G networks challenges high-quality service due to dense user\nequipment deployment and increased wireless environment complexity. Our\nCNN-based model is designed to detect Physical Random Access Channel (PRACH)\nsequences amidst various interference scenarios, leveraging the spatial and\ntemporal characteristics of PRACH signals to enhance detection accuracy and\nrobustness. Comprehensive datasets of simulated PRACH signals under controlled\ninterference conditions were generated to train and validate the model.\nExperimental results show that our CNN-based approach outperforms traditional\nPRACH detection methods in accuracy, precision, recall and F1-score. This study\ndemonstrates the potential of AI/ML techniques in advancing interference\nmanagement in 5G networks, providing a foundation for future research and\npractical applications in optimizing network performance and reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a novel approach to interference detection in 5G\nNew Radio (5G-NR) networks using Convolutional Neural Networks (CNN).\nInterference in 5G networks challenges high-quality service due to dense user\nequipment deployment and increased wireless environment complexity. Our\nCNN-based model is designed to detect Physical Random Access Channel (PRACH)\nsequences amidst various interference scenarios, leveraging the spatial and\ntemporal characteristics of PRACH signals to enhance detection accuracy and\nrobustness. Comprehensive datasets of simulated PRACH signals under controlled\ninterference conditions were generated to train and validate the model.\nExperimental results show that our CNN-based approach outperforms traditional\nPRACH detection methods in accuracy, precision, recall and F1-score. This study\ndemonstrates the potential of AI/ML techniques in advancing interference\nmanagement in 5G networks, providing a foundation for future research and\npractical applications in optimizing network performance and reliability."
                },
                "authors": [
                    {
                        "name": "Desire Guel"
                    },
                    {
                        "name": "Arsene Kabore"
                    },
                    {
                        "name": "Didier Bassole"
                    }
                ],
                "author_detail": {
                    "name": "Didier Bassole"
                },
                "author": "Didier Bassole",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11650v1",
                "updated": "2024-08-21T14:24:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    14,
                    24,
                    4,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T14:24:04Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    14,
                    24,
                    4,
                    2,
                    234,
                    0
                ],
                "title": "CIPHER: Cybersecurity Intelligent Penetration-testing Helper for Ethical\n  Researcher",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CIPHER: Cybersecurity Intelligent Penetration-testing Helper for Ethical\n  Researcher"
                },
                "summary": "Penetration testing, a critical component of cybersecurity, typically\nrequires extensive time and effort to find vulnerabilities. Beginners in this\nfield often benefit from collaborative approaches with the community or\nexperts. To address this, we develop CIPHER (Cybersecurity Intelligent\nPenetration-testing Helper for Ethical Researchers), a large language model\nspecifically trained to assist in penetration testing tasks. We trained CIPHER\nusing over 300 high-quality write-ups of vulnerable machines, hacking\ntechniques, and documentation of open-source penetration testing tools.\nAdditionally, we introduced the Findings, Action, Reasoning, and Results (FARR)\nFlow augmentation, a novel method to augment penetration testing write-ups to\nestablish a fully automated pentesting simulation benchmark tailored for large\nlanguage models. This approach fills a significant gap in traditional\ncybersecurity Q\\&A benchmarks and provides a realistic and rigorous standard\nfor evaluating AI's technical knowledge, reasoning capabilities, and practical\nutility in dynamic penetration testing scenarios. In our assessments, CIPHER\nachieved the best overall performance in providing accurate suggestion\nresponses compared to other open-source penetration testing models of similar\nsize and even larger state-of-the-art models like Llama 3 70B and Qwen1.5 72B\nChat, particularly on insane difficulty machine setups. This demonstrates that\nthe current capabilities of general LLMs are insufficient for effectively\nguiding users through the penetration testing process. We also discuss the\npotential for improvement through scaling and the development of better\nbenchmarks using FARR Flow augmentation results. Our benchmark will be released\npublicly at https://github.com/ibndias/CIPHER.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Penetration testing, a critical component of cybersecurity, typically\nrequires extensive time and effort to find vulnerabilities. Beginners in this\nfield often benefit from collaborative approaches with the community or\nexperts. To address this, we develop CIPHER (Cybersecurity Intelligent\nPenetration-testing Helper for Ethical Researchers), a large language model\nspecifically trained to assist in penetration testing tasks. We trained CIPHER\nusing over 300 high-quality write-ups of vulnerable machines, hacking\ntechniques, and documentation of open-source penetration testing tools.\nAdditionally, we introduced the Findings, Action, Reasoning, and Results (FARR)\nFlow augmentation, a novel method to augment penetration testing write-ups to\nestablish a fully automated pentesting simulation benchmark tailored for large\nlanguage models. This approach fills a significant gap in traditional\ncybersecurity Q\\&A benchmarks and provides a realistic and rigorous standard\nfor evaluating AI's technical knowledge, reasoning capabilities, and practical\nutility in dynamic penetration testing scenarios. In our assessments, CIPHER\nachieved the best overall performance in providing accurate suggestion\nresponses compared to other open-source penetration testing models of similar\nsize and even larger state-of-the-art models like Llama 3 70B and Qwen1.5 72B\nChat, particularly on insane difficulty machine setups. This demonstrates that\nthe current capabilities of general LLMs are insufficient for effectively\nguiding users through the penetration testing process. We also discuss the\npotential for improvement through scaling and the development of better\nbenchmarks using FARR Flow augmentation results. Our benchmark will be released\npublicly at https://github.com/ibndias/CIPHER."
                },
                "authors": [
                    {
                        "name": "Derry Pratama"
                    },
                    {
                        "name": "Naufal Suryanto"
                    },
                    {
                        "name": "Andro Aprila Adiputra"
                    },
                    {
                        "name": "Thi-Thu-Huong Le"
                    },
                    {
                        "name": "Ahmada Yusril Kadiptya"
                    },
                    {
                        "name": "Muhammad Iqbal"
                    },
                    {
                        "name": "Howon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Howon Kim"
                },
                "author": "Howon Kim",
                "arxiv_comment": "28 pages, github available",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.02984v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.02984v2",
                "updated": "2024-08-21T13:55:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    13,
                    55,
                    37,
                    2,
                    234,
                    0
                ],
                "published": "2024-01-01T17:35:52Z",
                "published_parsed": [
                    2024,
                    1,
                    1,
                    17,
                    35,
                    52,
                    0,
                    1,
                    0
                ],
                "title": "Large Language Models in Mental Health Care: a Scoping Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models in Mental Health Care: a Scoping Review"
                },
                "summary": "The integration of large language models (LLMs) in mental health care is an\nemerging field. There is a need to systematically review the application\noutcomes and delineate the advantages and limitations in clinical settings.\nThis review aims to provide a comprehensive overview of the use of LLMs in\nmental health care, assessing their efficacy, challenges, and potential for\nfuture applications. A systematic search was conducted across multiple\ndatabases including PubMed, Web of Science, Google Scholar, arXiv, medRxiv, and\nPsyArXiv in November 2023. All forms of original research, peer-reviewed or\nnot, published or disseminated between October 1, 2019, and December 2, 2023,\nare included without language restrictions if they used LLMs developed after T5\nand directly addressed research questions in mental health care settings. From\nan initial pool of 313 articles, 34 met the inclusion criteria based on their\nrelevance to LLM application in mental health care and the robustness of\nreported outcomes. Diverse applications of LLMs in mental health care are\nidentified, including diagnosis, therapy, patient engagement enhancement, etc.\nKey challenges include data availability and reliability, nuanced handling of\nmental states, and effective evaluation methods. Despite successes in accuracy\nand accessibility improvement, gaps in clinical applicability and ethical\nconsiderations were evident, pointing to the need for robust data, standardized\nevaluations, and interdisciplinary collaboration. LLMs hold substantial promise\nfor enhancing mental health care. For their full potential to be realized,\nemphasis must be placed on developing robust datasets, development and\nevaluation frameworks, ethical guidelines, and interdisciplinary collaborations\nto address current limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language models (LLMs) in mental health care is an\nemerging field. There is a need to systematically review the application\noutcomes and delineate the advantages and limitations in clinical settings.\nThis review aims to provide a comprehensive overview of the use of LLMs in\nmental health care, assessing their efficacy, challenges, and potential for\nfuture applications. A systematic search was conducted across multiple\ndatabases including PubMed, Web of Science, Google Scholar, arXiv, medRxiv, and\nPsyArXiv in November 2023. All forms of original research, peer-reviewed or\nnot, published or disseminated between October 1, 2019, and December 2, 2023,\nare included without language restrictions if they used LLMs developed after T5\nand directly addressed research questions in mental health care settings. From\nan initial pool of 313 articles, 34 met the inclusion criteria based on their\nrelevance to LLM application in mental health care and the robustness of\nreported outcomes. Diverse applications of LLMs in mental health care are\nidentified, including diagnosis, therapy, patient engagement enhancement, etc.\nKey challenges include data availability and reliability, nuanced handling of\nmental states, and effective evaluation methods. Despite successes in accuracy\nand accessibility improvement, gaps in clinical applicability and ethical\nconsiderations were evident, pointing to the need for robust data, standardized\nevaluations, and interdisciplinary collaboration. LLMs hold substantial promise\nfor enhancing mental health care. For their full potential to be realized,\nemphasis must be placed on developing robust datasets, development and\nevaluation frameworks, ethical guidelines, and interdisciplinary collaborations\nto address current limitations."
                },
                "authors": [
                    {
                        "name": "Yining Hua"
                    },
                    {
                        "name": "Fenglin Liu"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Zehan Li"
                    },
                    {
                        "name": "Hongbin Na"
                    },
                    {
                        "name": "Yi-han Sheu"
                    },
                    {
                        "name": "Peilin Zhou"
                    },
                    {
                        "name": "Lauren V. Moran"
                    },
                    {
                        "name": "Sophia Ananiadou"
                    },
                    {
                        "name": "Andrew Beam"
                    },
                    {
                        "name": "John Torous"
                    }
                ],
                "author_detail": {
                    "name": "John Torous"
                },
                "author": "John Torous",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.02984v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.02984v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.09835v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.09835v5",
                "updated": "2024-08-21T13:36:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    13,
                    36,
                    30,
                    2,
                    234,
                    0
                ],
                "published": "2023-11-16T12:03:21Z",
                "published_parsed": [
                    2023,
                    11,
                    16,
                    12,
                    3,
                    21,
                    3,
                    320,
                    0
                ],
                "title": "ML-Bench: Evaluating Large Language Models and Agents for Machine\n  Learning Tasks on Repository-Level Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-Bench: Evaluating Large Language Models and Agents for Machine\n  Learning Tasks on Repository-Level Code"
                },
                "summary": "Despite Large Language Models (LLMs) like GPT-4 achieving impressive results\nin function-level code generation, they struggle with repository-scale code\nunderstanding (e.g., coming up with the right arguments for calling routines),\nrequiring a deeper comprehension of complex file interactions. Also, recently,\npeople have developed LLM agents that attempt to interact with repository code\n(e.g., compiling and evaluating its execution), prompting the need to evaluate\ntheir performance. These gaps have motivated our development of ML-Bench, a\nbenchmark rooted in real-world programming applications that leverage existing\ncode repositories to perform tasks. Addressing the need for LLMs to interpret\nlong code contexts and translate instructions into precise, executable scripts,\nML-Bench encompasses annotated 9,641 examples across 18 GitHub repositories,\nchallenging LLMs to accommodate user-specified arguments and documentation\nintricacies effectively. To evaluate both LLMs and AI agents, two setups are\nemployed: ML-LLM-Bench for assessing LLMs' text-to-code conversion within a\npredefined deployment environment, and ML-Agent-Bench for testing autonomous\nagents in an end-to-end task execution within a Linux sandbox environment. Our\nfindings indicate that while GPT-4o leads with a Pass@5 rate surpassing 50%,\nthere remains significant scope for improvement, highlighted by issues such as\nhallucinated outputs and difficulties with bash script generation. Notably, in\nthe more demanding ML-Agent-Bench, GPT-4o achieves a 76.47% success rate,\nreflecting the efficacy of iterative action and feedback in complex task\nresolution. Our code, dataset, and models are available at\nhttps://github.com/gersteinlab/ML-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite Large Language Models (LLMs) like GPT-4 achieving impressive results\nin function-level code generation, they struggle with repository-scale code\nunderstanding (e.g., coming up with the right arguments for calling routines),\nrequiring a deeper comprehension of complex file interactions. Also, recently,\npeople have developed LLM agents that attempt to interact with repository code\n(e.g., compiling and evaluating its execution), prompting the need to evaluate\ntheir performance. These gaps have motivated our development of ML-Bench, a\nbenchmark rooted in real-world programming applications that leverage existing\ncode repositories to perform tasks. Addressing the need for LLMs to interpret\nlong code contexts and translate instructions into precise, executable scripts,\nML-Bench encompasses annotated 9,641 examples across 18 GitHub repositories,\nchallenging LLMs to accommodate user-specified arguments and documentation\nintricacies effectively. To evaluate both LLMs and AI agents, two setups are\nemployed: ML-LLM-Bench for assessing LLMs' text-to-code conversion within a\npredefined deployment environment, and ML-Agent-Bench for testing autonomous\nagents in an end-to-end task execution within a Linux sandbox environment. Our\nfindings indicate that while GPT-4o leads with a Pass@5 rate surpassing 50%,\nthere remains significant scope for improvement, highlighted by issues such as\nhallucinated outputs and difficulties with bash script generation. Notably, in\nthe more demanding ML-Agent-Bench, GPT-4o achieves a 76.47% success rate,\nreflecting the efficacy of iterative action and feedback in complex task\nresolution. Our code, dataset, and models are available at\nhttps://github.com/gersteinlab/ML-bench."
                },
                "authors": [
                    {
                        "name": "Xiangru Tang"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Yanjun Shao"
                    },
                    {
                        "name": "Junjie Lu"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Zexuan Deng"
                    },
                    {
                        "name": "Helan Hu"
                    },
                    {
                        "name": "Kaikai An"
                    },
                    {
                        "name": "Ruijun Huang"
                    },
                    {
                        "name": "Shuzheng Si"
                    },
                    {
                        "name": "Sheng Chen"
                    },
                    {
                        "name": "Haozhe Zhao"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Zhiwei Jiang"
                    },
                    {
                        "name": "Baobao Chang"
                    },
                    {
                        "name": "Yin Fang"
                    },
                    {
                        "name": "Yujia Qin"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Yilun Zhao"
                    },
                    {
                        "name": "Arman Cohan"
                    },
                    {
                        "name": "Mark Gerstein"
                    }
                ],
                "author_detail": {
                    "name": "Mark Gerstein"
                },
                "author": "Mark Gerstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.09835v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.09835v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11609v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11609v1",
                "updated": "2024-08-21T13:34:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    13,
                    34,
                    29,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T13:34:29Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    13,
                    34,
                    29,
                    2,
                    234,
                    0
                ],
                "title": "Xinyu: An Efficient LLM-based System for Commentary Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Xinyu: An Efficient LLM-based System for Commentary Generation"
                },
                "summary": "Commentary provides readers with a deep understanding of events by presenting\ndiverse arguments and evidence. However, creating commentary is a\ntime-consuming task, even for skilled commentators. Large language models\n(LLMs) have simplified the process of natural language generation, but their\ndirect application in commentary creation still faces challenges due to unique\ntask requirements. These requirements can be categorized into two levels: 1)\nfundamental requirements, which include creating well-structured and logically\nconsistent narratives, and 2) advanced requirements, which involve generating\nquality arguments and providing convincing evidence. In this paper, we\nintroduce Xinyu, an efficient LLM-based system designed to assist commentators\nin generating Chinese commentaries. To meet the fundamental requirements, we\ndeconstruct the generation process into sequential steps, proposing targeted\nstrategies and supervised fine-tuning (SFT) for each step. To address the\nadvanced requirements, we present an argument ranking model for arguments and\nestablish a comprehensive evidence database that includes up-to-date events and\nclassic books, thereby strengthening the substantiation of the evidence with\nretrieval augmented generation (RAG) technology. To evaluate the generated\ncommentaries more fairly, corresponding to the two-level requirements, we\nintroduce a comprehensive evaluation metric that considers five distinct\nperspectives in commentary generation. Our experiments confirm the\neffectiveness of our proposed system. We also observe a significant increase in\nthe efficiency of commentators in real-world scenarios, with the average time\nspent on creating a commentary dropping from 4 hours to 20 minutes.\nImportantly, such an increase in efficiency does not compromise the quality of\nthe commentaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commentary provides readers with a deep understanding of events by presenting\ndiverse arguments and evidence. However, creating commentary is a\ntime-consuming task, even for skilled commentators. Large language models\n(LLMs) have simplified the process of natural language generation, but their\ndirect application in commentary creation still faces challenges due to unique\ntask requirements. These requirements can be categorized into two levels: 1)\nfundamental requirements, which include creating well-structured and logically\nconsistent narratives, and 2) advanced requirements, which involve generating\nquality arguments and providing convincing evidence. In this paper, we\nintroduce Xinyu, an efficient LLM-based system designed to assist commentators\nin generating Chinese commentaries. To meet the fundamental requirements, we\ndeconstruct the generation process into sequential steps, proposing targeted\nstrategies and supervised fine-tuning (SFT) for each step. To address the\nadvanced requirements, we present an argument ranking model for arguments and\nestablish a comprehensive evidence database that includes up-to-date events and\nclassic books, thereby strengthening the substantiation of the evidence with\nretrieval augmented generation (RAG) technology. To evaluate the generated\ncommentaries more fairly, corresponding to the two-level requirements, we\nintroduce a comprehensive evaluation metric that considers five distinct\nperspectives in commentary generation. Our experiments confirm the\neffectiveness of our proposed system. We also observe a significant increase in\nthe efficiency of commentators in real-world scenarios, with the average time\nspent on creating a commentary dropping from 4 hours to 20 minutes.\nImportantly, such an increase in efficiency does not compromise the quality of\nthe commentaries."
                },
                "authors": [
                    {
                        "name": "Yiquan Wu"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Chenyang Xi"
                    },
                    {
                        "name": "Yu Yu"
                    },
                    {
                        "name": "Pengyu Wang"
                    },
                    {
                        "name": "Yifei Liu"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Haiying Deng"
                    },
                    {
                        "name": "Zhiyu Li"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Zhonghao Wang"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Yi Luo"
                    },
                    {
                        "name": "Mingchuan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mingchuan Yang"
                },
                "author": "Mingchuan Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11609v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11609v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14795v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14795v4",
                "updated": "2024-08-21T13:32:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    13,
                    32,
                    18,
                    2,
                    234,
                    0
                ],
                "published": "2024-04-23T07:19:20Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    7,
                    19,
                    20,
                    1,
                    114,
                    0
                ],
                "title": "Watch Out for Your Guidance on Generation! Exploring Conditional\n  Backdoor Attacks against Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watch Out for Your Guidance on Generation! Exploring Conditional\n  Backdoor Attacks against Large Language Models"
                },
                "summary": "Mainstream backdoor attacks on large language models (LLMs) typically set a\nfixed trigger in the input instance and specific responses for triggered\nqueries. However, the fixed trigger setting (e.g., unusual words) may be easily\ndetected by human detection, limiting the effectiveness and practicality in\nreal-world scenarios. To enhance the stealthiness of backdoor activation, we\npresent a new poisoning paradigm against LLMs triggered by specifying\ngeneration conditions, which are commonly adopted strategies by users during\nmodel inference. The poisoned model performs normally for output under\nnormal/other generation conditions, while becomes harmful for output under\ntarget generation conditions. To achieve this objective, we introduce BrieFool,\nan efficient attack framework. It leverages the characteristics of generation\nconditions by efficient instruction sampling and poisoning data generation,\nthereby influencing the behavior of LLMs under target conditions. Our attack\ncan be generally divided into two types with different targets: Safety\nunalignment attack and Ability degradation attack. Our extensive experiments\ndemonstrate that BrieFool is effective across safety domains and ability\ndomains, achieving higher success rates than baseline methods, with 94.3 % on\nGPT-3.5-turbo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainstream backdoor attacks on large language models (LLMs) typically set a\nfixed trigger in the input instance and specific responses for triggered\nqueries. However, the fixed trigger setting (e.g., unusual words) may be easily\ndetected by human detection, limiting the effectiveness and practicality in\nreal-world scenarios. To enhance the stealthiness of backdoor activation, we\npresent a new poisoning paradigm against LLMs triggered by specifying\ngeneration conditions, which are commonly adopted strategies by users during\nmodel inference. The poisoned model performs normally for output under\nnormal/other generation conditions, while becomes harmful for output under\ntarget generation conditions. To achieve this objective, we introduce BrieFool,\nan efficient attack framework. It leverages the characteristics of generation\nconditions by efficient instruction sampling and poisoning data generation,\nthereby influencing the behavior of LLMs under target conditions. Our attack\ncan be generally divided into two types with different targets: Safety\nunalignment attack and Ability degradation attack. Our extensive experiments\ndemonstrate that BrieFool is effective across safety domains and ability\ndomains, achieving higher success rates than baseline methods, with 94.3 % on\nGPT-3.5-turbo"
                },
                "authors": [
                    {
                        "name": "Jiaming He"
                    },
                    {
                        "name": "Wenbo Jiang"
                    },
                    {
                        "name": "Guanyu Hou"
                    },
                    {
                        "name": "Wenshu Fan"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Hongwei Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongwei Li"
                },
                "author": "Hongwei Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14795v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14795v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11599v1",
                "updated": "2024-08-21T13:11:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    13,
                    11,
                    3,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T13:11:03Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    13,
                    11,
                    3,
                    2,
                    234,
                    0
                ],
                "title": "Cause-Aware Empathetic Response Generation via Chain-of-Thought\n  Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cause-Aware Empathetic Response Generation via Chain-of-Thought\n  Fine-Tuning"
                },
                "summary": "Empathetic response generation endows agents with the capability to\ncomprehend dialogue contexts and react to expressed emotions. Previous works\npredominantly focus on leveraging the speaker's emotional labels, but ignore\nthe importance of emotion cause reasoning in empathetic response generation,\nwhich hinders the model's capacity for further affective understanding and\ncognitive inference. In this paper, we propose a cause-aware empathetic\ngeneration approach by integrating emotions and causes through a well-designed\nChain-of-Thought (CoT) prompt on Large Language Models (LLMs). Our approach can\ngreatly promote LLMs' performance of empathy by instruction tuning and\nenhancing the role awareness of an empathetic listener in the prompt.\nAdditionally, we propose to incorporate cause-oriented external knowledge from\nCOMET into the prompt, which improves the diversity of generation and\nalleviates conflicts between internal and external knowledge at the same time.\nExperimental results on the benchmark dataset demonstrate that our approach on\nLLaMA-7b achieves state-of-the-art performance in both automatic and human\nevaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empathetic response generation endows agents with the capability to\ncomprehend dialogue contexts and react to expressed emotions. Previous works\npredominantly focus on leveraging the speaker's emotional labels, but ignore\nthe importance of emotion cause reasoning in empathetic response generation,\nwhich hinders the model's capacity for further affective understanding and\ncognitive inference. In this paper, we propose a cause-aware empathetic\ngeneration approach by integrating emotions and causes through a well-designed\nChain-of-Thought (CoT) prompt on Large Language Models (LLMs). Our approach can\ngreatly promote LLMs' performance of empathy by instruction tuning and\nenhancing the role awareness of an empathetic listener in the prompt.\nAdditionally, we propose to incorporate cause-oriented external knowledge from\nCOMET into the prompt, which improves the diversity of generation and\nalleviates conflicts between internal and external knowledge at the same time.\nExperimental results on the benchmark dataset demonstrate that our approach on\nLLaMA-7b achieves state-of-the-art performance in both automatic and human\nevaluations."
                },
                "authors": [
                    {
                        "name": "Xinhao Chen"
                    },
                    {
                        "name": "Chong Yang"
                    },
                    {
                        "name": "Man Lan"
                    },
                    {
                        "name": "Li Cai"
                    },
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Tu Hu"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Aimin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Aimin Zhou"
                },
                "author": "Aimin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11587v1",
                "updated": "2024-08-21T12:50:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    12,
                    50,
                    23,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T12:50:23Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    12,
                    50,
                    23,
                    2,
                    234,
                    0
                ],
                "title": "Large Language Models are Good Attackers: Efficient and Stealthy Textual\n  Backdoor Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are Good Attackers: Efficient and Stealthy Textual\n  Backdoor Attacks"
                },
                "summary": "With the burgeoning advancements in the field of natural language processing\n(NLP), the demand for training data has increased significantly. To save costs,\nit has become common for users and businesses to outsource the labor-intensive\ntask of data collection to third-party entities. Unfortunately, recent research\nhas unveiled the inherent risk associated with this practice, particularly in\nexposing NLP systems to potential backdoor attacks. Specifically, these attacks\nenable malicious control over the behavior of a trained model by poisoning a\nsmall portion of the training data. Unlike backdoor attacks in computer vision,\ntextual backdoor attacks impose stringent requirements for attack stealthiness.\nHowever, existing attack methods meet significant trade-off between\neffectiveness and stealthiness, largely due to the high information entropy\ninherent in textual data. In this paper, we introduce the Efficient and\nStealthy Textual backdoor attack method, EST-Bad, leveraging Large Language\nModels (LLMs). Our EST-Bad encompasses three core strategies: optimizing the\ninherent flaw of models as the trigger, stealthily injecting triggers with\nLLMs, and meticulously selecting the most impactful samples for backdoor\ninjection. Through the integration of these techniques, EST-Bad demonstrates an\nefficient achievement of competitive attack performance while maintaining\nsuperior stealthiness compared to prior methods across various text classifier\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the burgeoning advancements in the field of natural language processing\n(NLP), the demand for training data has increased significantly. To save costs,\nit has become common for users and businesses to outsource the labor-intensive\ntask of data collection to third-party entities. Unfortunately, recent research\nhas unveiled the inherent risk associated with this practice, particularly in\nexposing NLP systems to potential backdoor attacks. Specifically, these attacks\nenable malicious control over the behavior of a trained model by poisoning a\nsmall portion of the training data. Unlike backdoor attacks in computer vision,\ntextual backdoor attacks impose stringent requirements for attack stealthiness.\nHowever, existing attack methods meet significant trade-off between\neffectiveness and stealthiness, largely due to the high information entropy\ninherent in textual data. In this paper, we introduce the Efficient and\nStealthy Textual backdoor attack method, EST-Bad, leveraging Large Language\nModels (LLMs). Our EST-Bad encompasses three core strategies: optimizing the\ninherent flaw of models as the trigger, stealthily injecting triggers with\nLLMs, and meticulously selecting the most impactful samples for backdoor\ninjection. Through the integration of these techniques, EST-Bad demonstrates an\nefficient achievement of competitive attack performance while maintaining\nsuperior stealthiness compared to prior methods across various text classifier\ndatasets."
                },
                "authors": [
                    {
                        "name": "Ziqiang Li"
                    },
                    {
                        "name": "Yueqi Zeng"
                    },
                    {
                        "name": "Pengfei Xia"
                    },
                    {
                        "name": "Lei Liu"
                    },
                    {
                        "name": "Zhangjie Fu"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07666v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07666v3",
                "updated": "2024-08-21T12:47:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    12,
                    47,
                    31,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-14T16:58:48Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    16,
                    58,
                    48,
                    2,
                    227,
                    0
                ],
                "title": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories,\n  Applications and Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories,\n  Applications and Opportunities"
                },
                "summary": "Model merging is an efficient empowerment technique in the machine learning\ncommunity that does not require the collection of raw training data and does\nnot require expensive computation. As model merging becomes increasingly\nprevalent across various fields, it is crucial to understand the available\nmodel merging techniques comprehensively. However, there is a significant gap\nin the literature regarding a systematic and thorough review of these\ntechniques. This survey provides a comprehensive overview of model merging\nmethods and theories, their applications in various domains and settings, and\nfuture research directions. Specifically, we first propose a new taxonomic\napproach that exhaustively discusses existing model merging methods. Secondly,\nwe discuss the application of model merging techniques in large language\nmodels, multimodal large language models, and 10+ machine learning subfields,\nincluding continual learning, multi-task learning, few-shot learning, etc.\nFinally, we highlight the remaining challenges of model merging and discuss\nfuture research directions. A comprehensive list of papers about model merging\nis available at\n\\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging is an efficient empowerment technique in the machine learning\ncommunity that does not require the collection of raw training data and does\nnot require expensive computation. As model merging becomes increasingly\nprevalent across various fields, it is crucial to understand the available\nmodel merging techniques comprehensively. However, there is a significant gap\nin the literature regarding a systematic and thorough review of these\ntechniques. This survey provides a comprehensive overview of model merging\nmethods and theories, their applications in various domains and settings, and\nfuture research directions. Specifically, we first propose a new taxonomic\napproach that exhaustively discusses existing model merging methods. Secondly,\nwe discuss the application of model merging techniques in large language\nmodels, multimodal large language models, and 10+ machine learning subfields,\nincluding continual learning, multi-task learning, few-shot learning, etc.\nFinally, we highlight the remaining challenges of model merging and discuss\nfuture research directions. A comprehensive list of papers about model merging\nis available at\n\\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}."
                },
                "authors": [
                    {
                        "name": "Enneng Yang"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Guibing Guo"
                    },
                    {
                        "name": "Xingwei Wang"
                    },
                    {
                        "name": "Xiaochun Cao"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07666v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07666v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18764v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18764v2",
                "updated": "2024-08-21T12:23:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    12,
                    23,
                    21,
                    2,
                    234,
                    0
                ],
                "published": "2024-07-26T14:22:30Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    14,
                    22,
                    30,
                    4,
                    208,
                    0
                ],
                "title": "TAGIFY: LLM-powered Tagging Interface for Improved Data Findability on\n  OGD portals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAGIFY: LLM-powered Tagging Interface for Improved Data Findability on\n  OGD portals"
                },
                "summary": "Efforts directed towards promoting Open Government Data (OGD) have gained\nsignificant traction across various governmental tiers since the mid-2000s. As\nmore datasets are published on OGD portals, finding specific data becomes\nharder, leading to information overload. Complete and accurate documentation of\ndatasets, including association of proper tags with datasets is key to\nimproving dataset findability and accessibility. Analysis conducted on the\nEstonian Open Data Portal, revealed that 11% datasets have no associated tags,\nwhile 26% had only one tag assigned to them, which underscores challenges in\ndata findability and accessibility within the portal, which, according to the\nrecent Open Data Maturity Report, is considered trend-setter. The aim of this\nstudy is to propose an automated solution to tagging datasets to improve data\nfindability on OGD portals. This paper presents Tagify - a prototype of tagging\ninterface that employs large language models (LLM) such as GPT-3.5-turbo and\nGPT-4 to automate dataset tagging, generating tags for datasets in English and\nEstonian, thereby augmenting metadata preparation by data publishers and\nimproving data findability on OGD portals by data users. The developed solution\nwas evaluated by users and their feedback was collected to define an agenda for\nfuture prototype improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efforts directed towards promoting Open Government Data (OGD) have gained\nsignificant traction across various governmental tiers since the mid-2000s. As\nmore datasets are published on OGD portals, finding specific data becomes\nharder, leading to information overload. Complete and accurate documentation of\ndatasets, including association of proper tags with datasets is key to\nimproving dataset findability and accessibility. Analysis conducted on the\nEstonian Open Data Portal, revealed that 11% datasets have no associated tags,\nwhile 26% had only one tag assigned to them, which underscores challenges in\ndata findability and accessibility within the portal, which, according to the\nrecent Open Data Maturity Report, is considered trend-setter. The aim of this\nstudy is to propose an automated solution to tagging datasets to improve data\nfindability on OGD portals. This paper presents Tagify - a prototype of tagging\ninterface that employs large language models (LLM) such as GPT-3.5-turbo and\nGPT-4 to automate dataset tagging, generating tags for datasets in English and\nEstonian, thereby augmenting metadata preparation by data publishers and\nimproving data findability on OGD portals by data users. The developed solution\nwas evaluated by users and their feedback was collected to define an agenda for\nfuture prototype improvements."
                },
                "authors": [
                    {
                        "name": "Kevin Kliimask"
                    },
                    {
                        "name": "Anastasija Nikiforova"
                    }
                ],
                "author_detail": {
                    "name": "Anastasija Nikiforova"
                },
                "author": "Anastasija Nikiforova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18764v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18764v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11557v1",
                "updated": "2024-08-21T12:09:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    12,
                    9,
                    37,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T12:09:37Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    12,
                    9,
                    37,
                    2,
                    234,
                    0
                ],
                "title": "A Quick, trustworthy spectral detection Q&A system based on the SDAAP\n  Dataset and large language model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Quick, trustworthy spectral detection Q&A system based on the SDAAP\n  Dataset and large language model"
                },
                "summary": "Large Language Model (LLM) has demonstrated significant success in a range of\nnatural language processing (NLP) tasks within general domain. The emergence of\nLLM has introduced innovative methodologies across diverse fields, including\nthe natural sciences. Researchers aim to implement automated, concurrent\nprocess driven by LLM to supplant conventional manual, repetitive and\nlabor-intensive work. In the domain of spectral analysis and detection, it is\nimperative for researchers to autonomously acquire pertinent knowledge across\nvarious research objects, which encompasses the spectroscopic techniques and\nthe chemometric methods that are employed in experiments and analysis.\nParadoxically, despite the recognition of spectroscopic detection as an\neffective analytical method, the fundamental process of knowledge retrieval\nremains both time-intensive and repetitive. In response to this challenge, we\nfirst introduced the Spectral Detection and Analysis Based Paper(SDAAP)\ndataset, which is the first open-source textual knowledge dataset for spectral\nanalysis and detection and contains annotated literature data as well as\ncorresponding knowledge instruction data. Subsequently, we also designed an\nautomated Q\\&A framework based on the SDAAP dataset, which can retrieve\nrelevant knowledge and generate high-quality responses by extracting entities\nin the input as retrieval parameters. It is worth noting that: within this\nframework, LLM is only used as a tool to provide generalizability, while RAG\ntechnique is used to accurately capture the source of the knowledge.This\napproach not only improves the quality of the generated responses, but also\nensures the traceability of the knowledge. Experimental results show that our\nframework generates responses with more reliable expertise compared to the\nbaseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) has demonstrated significant success in a range of\nnatural language processing (NLP) tasks within general domain. The emergence of\nLLM has introduced innovative methodologies across diverse fields, including\nthe natural sciences. Researchers aim to implement automated, concurrent\nprocess driven by LLM to supplant conventional manual, repetitive and\nlabor-intensive work. In the domain of spectral analysis and detection, it is\nimperative for researchers to autonomously acquire pertinent knowledge across\nvarious research objects, which encompasses the spectroscopic techniques and\nthe chemometric methods that are employed in experiments and analysis.\nParadoxically, despite the recognition of spectroscopic detection as an\neffective analytical method, the fundamental process of knowledge retrieval\nremains both time-intensive and repetitive. In response to this challenge, we\nfirst introduced the Spectral Detection and Analysis Based Paper(SDAAP)\ndataset, which is the first open-source textual knowledge dataset for spectral\nanalysis and detection and contains annotated literature data as well as\ncorresponding knowledge instruction data. Subsequently, we also designed an\nautomated Q\\&A framework based on the SDAAP dataset, which can retrieve\nrelevant knowledge and generate high-quality responses by extracting entities\nin the input as retrieval parameters. It is worth noting that: within this\nframework, LLM is only used as a tool to provide generalizability, while RAG\ntechnique is used to accurately capture the source of the knowledge.This\napproach not only improves the quality of the generated responses, but also\nensures the traceability of the knowledge. Experimental results show that our\nframework generates responses with more reliable expertise compared to the\nbaseline."
                },
                "authors": [
                    {
                        "name": "Jiheng Liang"
                    },
                    {
                        "name": "Ziru Yu"
                    },
                    {
                        "name": "Zujie Xie"
                    },
                    {
                        "name": "Xiangyang Yu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Yu"
                },
                "author": "Xiangyang Yu",
                "arxiv_comment": "16 pages,10 figures,3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12975v2",
                "updated": "2024-08-21T11:57:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    11,
                    57,
                    5,
                    2,
                    234,
                    0
                ],
                "published": "2024-06-18T18:00:03Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    18,
                    0,
                    3,
                    1,
                    170,
                    0
                ],
                "title": "SHIELD: Evaluation and Defense Strategies for Copyright Compliance in\n  LLM Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SHIELD: Evaluation and Defense Strategies for Copyright Compliance in\n  LLM Text Generation"
                },
                "summary": "Large Language Models (LLMs) have transformed machine learning but raised\nsignificant legal concerns due to their potential to produce text that\ninfringes on copyrights, resulting in several high-profile lawsuits. The legal\nlandscape is struggling to keep pace with these rapid advancements, with\nongoing debates about whether generated text might plagiarize copyrighted\nmaterials. Current LLMs may infringe on copyrights or overly restrict\nnon-copyrighted texts, leading to these challenges: (i) the need for a\ncomprehensive evaluation benchmark to assess copyright compliance from multiple\naspects; (ii) evaluating robustness against safeguard bypassing attacks; and\n(iii) developing effective defense targeted against the generation of\ncopyrighted text. To tackle these challenges, we introduce a curated dataset to\nevaluate methods, test attack strategies, and propose lightweight, real-time\ndefense to prevent the generation of copyrighted text, ensuring the safe and\nlawful use of LLMs. Our experiments demonstrate that current LLMs frequently\noutput copyrighted text, and that jailbreaking attacks can significantly\nincrease the volume of copyrighted output. Our proposed defense mechanism\nsignificantly reduces the volume of copyrighted text generated by LLMs by\neffectively refusing malicious requests. Code is publicly available at\nhttps://github.com/xz-liu/SHIELD",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed machine learning but raised\nsignificant legal concerns due to their potential to produce text that\ninfringes on copyrights, resulting in several high-profile lawsuits. The legal\nlandscape is struggling to keep pace with these rapid advancements, with\nongoing debates about whether generated text might plagiarize copyrighted\nmaterials. Current LLMs may infringe on copyrights or overly restrict\nnon-copyrighted texts, leading to these challenges: (i) the need for a\ncomprehensive evaluation benchmark to assess copyright compliance from multiple\naspects; (ii) evaluating robustness against safeguard bypassing attacks; and\n(iii) developing effective defense targeted against the generation of\ncopyrighted text. To tackle these challenges, we introduce a curated dataset to\nevaluate methods, test attack strategies, and propose lightweight, real-time\ndefense to prevent the generation of copyrighted text, ensuring the safe and\nlawful use of LLMs. Our experiments demonstrate that current LLMs frequently\noutput copyrighted text, and that jailbreaking attacks can significantly\nincrease the volume of copyrighted output. Our proposed defense mechanism\nsignificantly reduces the volume of copyrighted text generated by LLMs by\neffectively refusing malicious requests. Code is publicly available at\nhttps://github.com/xz-liu/SHIELD"
                },
                "authors": [
                    {
                        "name": "Xiaoze Liu"
                    },
                    {
                        "name": "Ting Sun"
                    },
                    {
                        "name": "Tianyang Xu"
                    },
                    {
                        "name": "Feijie Wu"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Xiaoqian Wang"
                    },
                    {
                        "name": "Jing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Gao"
                },
                "author": "Jing Gao",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11546v1",
                "updated": "2024-08-21T11:54:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    11,
                    54,
                    22,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T11:54:22Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    11,
                    54,
                    22,
                    2,
                    234,
                    0
                ],
                "title": "Memorization In In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memorization In In-Context Learning"
                },
                "summary": "In-context learning (ICL) has proven to be an effective strategy for\nimproving the performance of large language models (LLMs) with no additional\ntraining. However, the exact mechanism behind these performance improvements\nremains unclear. This study is the first to show how ICL surfaces memorized\ntraining data and to explore the correlation between this memorization and\nperformance across various ICL regimes: zero-shot, few-shot, and many-shot. Our\nmost notable findings include: (1) ICL significantly surfaces memorization\ncompared to zero-shot learning in most cases; (2) demonstrations, without their\nlabels, are the most effective element in surfacing memorization; (3) ICL\nimproves performance when the surfaced memorization in few-shot regimes reaches\na high level (about 40%); and (4) there is a very strong correlation between\nperformance and memorization in ICL when it outperforms zero-shot learning.\nOverall, our study uncovers a hidden phenomenon -- memorization -- at the core\nof ICL, raising an important question: to what extent do LLMs truly generalize\nfrom demonstrations in ICL, and how much of their success is due to\nmemorization?",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) has proven to be an effective strategy for\nimproving the performance of large language models (LLMs) with no additional\ntraining. However, the exact mechanism behind these performance improvements\nremains unclear. This study is the first to show how ICL surfaces memorized\ntraining data and to explore the correlation between this memorization and\nperformance across various ICL regimes: zero-shot, few-shot, and many-shot. Our\nmost notable findings include: (1) ICL significantly surfaces memorization\ncompared to zero-shot learning in most cases; (2) demonstrations, without their\nlabels, are the most effective element in surfacing memorization; (3) ICL\nimproves performance when the surfaced memorization in few-shot regimes reaches\na high level (about 40%); and (4) there is a very strong correlation between\nperformance and memorization in ICL when it outperforms zero-shot learning.\nOverall, our study uncovers a hidden phenomenon -- memorization -- at the core\nof ICL, raising an important question: to what extent do LLMs truly generalize\nfrom demonstrations in ICL, and how much of their success is due to\nmemorization?"
                },
                "authors": [
                    {
                        "name": "Shahriar Golchin"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    },
                    {
                        "name": "Steven Bethard"
                    },
                    {
                        "name": "Eduardo Blanco"
                    },
                    {
                        "name": "Ellen Riloff"
                    }
                ],
                "author_detail": {
                    "name": "Ellen Riloff"
                },
                "author": "Ellen Riloff",
                "arxiv_comment": "v1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11539v1",
                "updated": "2024-08-21T11:38:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    11,
                    38,
                    32,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T11:38:32Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    11,
                    38,
                    32,
                    2,
                    234,
                    0
                ],
                "title": "Research on the Application of Large Language Models in Automatic\n  Question Generation: A Case Study of ChatGLM in the Context of High School\n  Information Technology Curriculum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on the Application of Large Language Models in Automatic\n  Question Generation: A Case Study of ChatGLM in the Context of High School\n  Information Technology Curriculum"
                },
                "summary": "This study investigates the application effectiveness of the Large Language\nModel (LLMs) ChatGLM in the automated generation of high school information\ntechnology exam questions. Through meticulously designed prompt engineering\nstrategies, the model is guided to generate diverse questions, which are then\ncomprehensively evaluated by domain experts. The evaluation dimensions include\nthe Hitting(the degree of alignment with teaching content), Fitting (the degree\nof embodiment of core competencies), Clarity (the explicitness of question\ndescriptions), and Willing to use (the teacher's willingness to use the\nquestion in teaching). The results indicate that ChatGLM outperforms\nhuman-generated questions in terms of clarity and teachers' willingness to use,\nalthough there is no significant difference in hit rate and fit. This finding\nsuggests that ChatGLM has the potential to enhance the efficiency of question\ngeneration and alleviate the burden on teachers, providing a new perspective\nfor the future development of educational assessment systems. Future research\ncould explore further optimizations to the ChatGLM model to maintain high fit\nand hit rates while improving the clarity of questions and teachers'\nwillingness to use them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the application effectiveness of the Large Language\nModel (LLMs) ChatGLM in the automated generation of high school information\ntechnology exam questions. Through meticulously designed prompt engineering\nstrategies, the model is guided to generate diverse questions, which are then\ncomprehensively evaluated by domain experts. The evaluation dimensions include\nthe Hitting(the degree of alignment with teaching content), Fitting (the degree\nof embodiment of core competencies), Clarity (the explicitness of question\ndescriptions), and Willing to use (the teacher's willingness to use the\nquestion in teaching). The results indicate that ChatGLM outperforms\nhuman-generated questions in terms of clarity and teachers' willingness to use,\nalthough there is no significant difference in hit rate and fit. This finding\nsuggests that ChatGLM has the potential to enhance the efficiency of question\ngeneration and alleviate the burden on teachers, providing a new perspective\nfor the future development of educational assessment systems. Future research\ncould explore further optimizations to the ChatGLM model to maintain high fit\nand hit rates while improving the clarity of questions and teachers'\nwillingness to use them."
                },
                "authors": [
                    {
                        "name": "Yanxin Chen"
                    },
                    {
                        "name": "Ling He"
                    }
                ],
                "author_detail": {
                    "name": "Ling He"
                },
                "author": "Ling He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09205v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09205v2",
                "updated": "2024-08-21T11:34:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    11,
                    34,
                    56,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-17T13:54:34Z",
                "published_parsed": [
                    2024,
                    8,
                    17,
                    13,
                    54,
                    34,
                    5,
                    230,
                    0
                ],
                "title": "Architectural Foundations for the Large Language Model Infrastructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architectural Foundations for the Large Language Model Infrastructures"
                },
                "summary": "The development of a large language model (LLM) infrastructure is a pivotal\nundertaking in artificial intelligence. This paper explores the intricate\nlandscape of LLM infrastructure, software, and data management. By analyzing\nthese core components, we emphasize the pivotal considerations and safeguards\ncrucial for successful LLM development. This work presents a concise synthesis\nof the challenges and strategies inherent in constructing a robust and\neffective LLM infrastructure, offering valuable insights for researchers and\npractitioners alike.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of a large language model (LLM) infrastructure is a pivotal\nundertaking in artificial intelligence. This paper explores the intricate\nlandscape of LLM infrastructure, software, and data management. By analyzing\nthese core components, we emphasize the pivotal considerations and safeguards\ncrucial for successful LLM development. This work presents a concise synthesis\nof the challenges and strategies inherent in constructing a robust and\neffective LLM infrastructure, offering valuable insights for researchers and\npractitioners alike."
                },
                "authors": [
                    {
                        "name": "Hongyin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Hongyin Zhu"
                },
                "author": "Hongyin Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09205v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09205v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05688v2",
                "updated": "2024-08-21T11:10:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    11,
                    10,
                    36,
                    2,
                    234,
                    0
                ],
                "published": "2024-05-09T11:38:23Z",
                "published_parsed": [
                    2024,
                    5,
                    9,
                    11,
                    38,
                    23,
                    3,
                    130,
                    0
                ],
                "title": "Evaluating Dialect Robustness of Language Models via Conversation\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Dialect Robustness of Language Models via Conversation\n  Understanding"
                },
                "summary": "With an evergrowing number of LLMs reporting superlative performance for\nEnglish, their ability to perform equitably for different dialects of English\n($\\textit{i.e.}$, dialect robustness) needs to be ascertained. Specifically, we\nuse English language (US English or Indian English) conversations between\nhumans who play the word-guessing game of 'taboo'. We formulate two evaluative\ntasks: target word prediction (TWP) ($\\textit{i.e.}$, predict the masked target\nword in a conversation) and target word selection (TWS) ($\\textit{i.e.}$,\nselect the most likely masked target word in a conversation, from among a set\nof candidate words). Extending MD3, an existing dialectic dataset of\ntaboo-playing conversations, we introduce M-MD3, a target-word-masked version\nof MD3 with the en-US and en-IN subsets. We create two subsets: en-MV (where\nen-US is transformed to include dialectal information) and en-TR (where\ndialectal information is removed from en-IN). We evaluate one open-source\n(Llama3) and two closed-source (GPT-4/3.5) LLMs. LLMs perform significantly\nbetter for US English than Indian English for both TWP and TWS tasks, for all\nsettings, exhibiting marginalisation against the Indian dialect of English.\nWhile GPT-based models perform the best, the comparatively smaller models work\nmore equitably after fine-tuning. Our error analysis shows that the LLMs can\nunderstand the dialect better after fine-tuning using dialectal data. Our\nevaluation methodology exhibits a novel way to examine attributes of language\nmodels using pre-existing dialogue datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With an evergrowing number of LLMs reporting superlative performance for\nEnglish, their ability to perform equitably for different dialects of English\n($\\textit{i.e.}$, dialect robustness) needs to be ascertained. Specifically, we\nuse English language (US English or Indian English) conversations between\nhumans who play the word-guessing game of 'taboo'. We formulate two evaluative\ntasks: target word prediction (TWP) ($\\textit{i.e.}$, predict the masked target\nword in a conversation) and target word selection (TWS) ($\\textit{i.e.}$,\nselect the most likely masked target word in a conversation, from among a set\nof candidate words). Extending MD3, an existing dialectic dataset of\ntaboo-playing conversations, we introduce M-MD3, a target-word-masked version\nof MD3 with the en-US and en-IN subsets. We create two subsets: en-MV (where\nen-US is transformed to include dialectal information) and en-TR (where\ndialectal information is removed from en-IN). We evaluate one open-source\n(Llama3) and two closed-source (GPT-4/3.5) LLMs. LLMs perform significantly\nbetter for US English than Indian English for both TWP and TWS tasks, for all\nsettings, exhibiting marginalisation against the Indian dialect of English.\nWhile GPT-based models perform the best, the comparatively smaller models work\nmore equitably after fine-tuning. Our error analysis shows that the LLMs can\nunderstand the dialect better after fine-tuning using dialectal data. Our\nevaluation methodology exhibits a novel way to examine attributes of language\nmodels using pre-existing dialogue datasets."
                },
                "authors": [
                    {
                        "name": "Dipankar Srirag"
                    },
                    {
                        "name": "Nihar Ranjan Sahoo"
                    },
                    {
                        "name": "Aditya Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Joshi"
                },
                "author": "Aditya Joshi",
                "arxiv_comment": "12 pages, 3 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11523v1",
                "updated": "2024-08-21T10:56:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    56,
                    26,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:56:26Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    56,
                    26,
                    2,
                    234,
                    0
                ],
                "title": "LARR: Large Language Model Aided Real-time Scene Recommendation with\n  Semantic Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LARR: Large Language Model Aided Real-time Scene Recommendation with\n  Semantic Understanding"
                },
                "summary": "Click-Through Rate (CTR) prediction is crucial for Recommendation System(RS),\naiming to provide personalized recommendation services for users in many\naspects such as food delivery, e-commerce and so on. However, traditional RS\nrelies on collaborative signals, which lacks semantic understanding to\nreal-time scenes. We also noticed that a major challenge in utilizing Large\nLanguage Models (LLMs) for practical recommendation purposes is their\nefficiency in dealing with long text input. To break through the problems\nabove, we propose Large Language Model Aided Real-time Scene\nRecommendation(LARR), adopt LLMs for semantic understanding, utilizing\nreal-time scene information in RS without requiring LLM to process the entire\nreal-time scene text directly, thereby enhancing the efficiency of LLM-based\nCTR modeling. Specifically, recommendation domain-specific knowledge is\ninjected into LLM and then RS employs an aggregation encoder to build real-time\nscene information from separate LLM's outputs. Firstly, a LLM is continual\npretrained on corpus built from recommendation data with the aid of special\ntokens. Subsequently, the LLM is fine-tuned via contrastive learning on three\nkinds of sample construction strategies. Through this step, LLM is transformed\ninto a text embedding model. Finally, LLM's separate outputs for different\nscene features are aggregated by an encoder, aligning to collaborative signals\nin RS, enhancing the performance of recommendation model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Click-Through Rate (CTR) prediction is crucial for Recommendation System(RS),\naiming to provide personalized recommendation services for users in many\naspects such as food delivery, e-commerce and so on. However, traditional RS\nrelies on collaborative signals, which lacks semantic understanding to\nreal-time scenes. We also noticed that a major challenge in utilizing Large\nLanguage Models (LLMs) for practical recommendation purposes is their\nefficiency in dealing with long text input. To break through the problems\nabove, we propose Large Language Model Aided Real-time Scene\nRecommendation(LARR), adopt LLMs for semantic understanding, utilizing\nreal-time scene information in RS without requiring LLM to process the entire\nreal-time scene text directly, thereby enhancing the efficiency of LLM-based\nCTR modeling. Specifically, recommendation domain-specific knowledge is\ninjected into LLM and then RS employs an aggregation encoder to build real-time\nscene information from separate LLM's outputs. Firstly, a LLM is continual\npretrained on corpus built from recommendation data with the aid of special\ntokens. Subsequently, the LLM is fine-tuned via contrastive learning on three\nkinds of sample construction strategies. Through this step, LLM is transformed\ninto a text embedding model. Finally, LLM's separate outputs for different\nscene features are aggregated by an encoder, aligning to collaborative signals\nin RS, enhancing the performance of recommendation model."
                },
                "authors": [
                    {
                        "name": "Zhizhong Wan"
                    },
                    {
                        "name": "Bin Yin"
                    },
                    {
                        "name": "Junjie Xie"
                    },
                    {
                        "name": "Fei Jiang"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "arxiv_doi": "10.1145/3640457.3688135",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3640457.3688135",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.11523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19380v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19380v3",
                "updated": "2024-08-21T10:54:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    54,
                    14,
                    2,
                    234,
                    0
                ],
                "published": "2024-06-27T17:55:31Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    17,
                    55,
                    31,
                    3,
                    179,
                    0
                ],
                "title": "TabReD: A Benchmark of Tabular Machine Learning in-the-Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TabReD: A Benchmark of Tabular Machine Learning in-the-Wild"
                },
                "summary": "Benchmarks that closely reflect downstream application scenarios are\nessential for the streamlined adoption of new research in tabular machine\nlearning (ML). In this work, we examine existing tabular benchmarks and find\ntwo common characteristics of industry-grade tabular data that are\nunderrepresented in the datasets available to the academic community. First,\ntabular data often changes over time in real-world deployment scenarios. This\nimpacts model performance and requires time-based train and test splits for\ncorrect model evaluation. Yet, existing academic tabular datasets often lack\ntimestamp metadata to enable such evaluation. Second, a considerable portion of\ndatasets in production settings stem from extensive data acquisition and\nfeature engineering pipelines. For each specific dataset, this can have a\ndifferent impact on the absolute and relative number of predictive,\nuninformative, and correlated features, which in turn can affect model\nselection. To fill the aforementioned gaps in academic benchmarks, we introduce\nTabReD -- a collection of eight industry-grade tabular datasets covering a wide\nrange of domains from finance to food delivery services. We assess a large\nnumber of tabular ML models in the feature-rich, temporally-evolving data\nsetting facilitated by TabReD. We demonstrate that evaluation on time-based\ndata splits leads to different methods ranking, compared to evaluation on\nrandom splits more common in academic benchmarks. Furthermore, on the TabReD\ndatasets, MLP-like architectures and GBDT show the best results, while more\nsophisticated DL models are yet to prove their effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarks that closely reflect downstream application scenarios are\nessential for the streamlined adoption of new research in tabular machine\nlearning (ML). In this work, we examine existing tabular benchmarks and find\ntwo common characteristics of industry-grade tabular data that are\nunderrepresented in the datasets available to the academic community. First,\ntabular data often changes over time in real-world deployment scenarios. This\nimpacts model performance and requires time-based train and test splits for\ncorrect model evaluation. Yet, existing academic tabular datasets often lack\ntimestamp metadata to enable such evaluation. Second, a considerable portion of\ndatasets in production settings stem from extensive data acquisition and\nfeature engineering pipelines. For each specific dataset, this can have a\ndifferent impact on the absolute and relative number of predictive,\nuninformative, and correlated features, which in turn can affect model\nselection. To fill the aforementioned gaps in academic benchmarks, we introduce\nTabReD -- a collection of eight industry-grade tabular datasets covering a wide\nrange of domains from finance to food delivery services. We assess a large\nnumber of tabular ML models in the feature-rich, temporally-evolving data\nsetting facilitated by TabReD. We demonstrate that evaluation on time-based\ndata splits leads to different methods ranking, compared to evaluation on\nrandom splits more common in academic benchmarks. Furthermore, on the TabReD\ndatasets, MLP-like architectures and GBDT show the best results, while more\nsophisticated DL models are yet to prove their effectiveness."
                },
                "authors": [
                    {
                        "name": "Ivan Rubachev"
                    },
                    {
                        "name": "Nikolay Kartashev"
                    },
                    {
                        "name": "Yury Gorishniy"
                    },
                    {
                        "name": "Artem Babenko"
                    }
                ],
                "author_detail": {
                    "name": "Artem Babenko"
                },
                "author": "Artem Babenko",
                "arxiv_comment": "Code: https://github.com/yandex-research/tabred (V2: fix the link to\n  the code in this comment; no changes to the PDF)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19380v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19380v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11512v1",
                "updated": "2024-08-21T10:44:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    44,
                    10,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:44:10Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    44,
                    10,
                    2,
                    234,
                    0
                ],
                "title": "IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine\n  Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine\n  Translation"
                },
                "summary": "This paper introduces two multilingual systems, IKUN and IKUN-C, developed\nfor the general machine translation task in WMT24. IKUN and IKUN-C represent an\nopen system and a constrained system, respectively, built on Llama-3-8b and\nMistral-7B-v0.3. Both systems are designed to handle all 11 language directions\nusing a single model. According to automatic evaluation metrics, IKUN-C\nachieved 6 first-place and 3 second-place finishes among all constrained\nsystems, while IKUN secured 1 first-place and 2 second-place finishes across\nboth open and constrained systems. These encouraging results suggest that large\nlanguage models (LLMs) are nearing the level of proficiency required for\neffective multilingual machine translation. The systems are based on a\ntwo-stage approach: first, continuous pre-training on monolingual data in 10\nlanguages, followed by fine-tuning on high-quality parallel data for 11\nlanguage directions. The primary difference between IKUN and IKUN-C lies in\ntheir monolingual pre-training strategy. IKUN-C is pre-trained using\nconstrained monolingual data, whereas IKUN leverages monolingual data from the\nOSCAR dataset. In the second phase, both systems are fine-tuned on parallel\ndata sourced from NTREX, Flores, and WMT16-23 for all 11 language pairs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces two multilingual systems, IKUN and IKUN-C, developed\nfor the general machine translation task in WMT24. IKUN and IKUN-C represent an\nopen system and a constrained system, respectively, built on Llama-3-8b and\nMistral-7B-v0.3. Both systems are designed to handle all 11 language directions\nusing a single model. According to automatic evaluation metrics, IKUN-C\nachieved 6 first-place and 3 second-place finishes among all constrained\nsystems, while IKUN secured 1 first-place and 2 second-place finishes across\nboth open and constrained systems. These encouraging results suggest that large\nlanguage models (LLMs) are nearing the level of proficiency required for\neffective multilingual machine translation. The systems are based on a\ntwo-stage approach: first, continuous pre-training on monolingual data in 10\nlanguages, followed by fine-tuning on high-quality parallel data for 11\nlanguage directions. The primary difference between IKUN and IKUN-C lies in\ntheir monolingual pre-training strategy. IKUN-C is pre-trained using\nconstrained monolingual data, whereas IKUN leverages monolingual data from the\nOSCAR dataset. In the second phase, both systems are fine-tuned on parallel\ndata sourced from NTREX, Flores, and WMT16-23 for all 11 language pairs."
                },
                "authors": [
                    {
                        "name": "Baohao Liao"
                    },
                    {
                        "name": "Christian Herold"
                    },
                    {
                        "name": "Shahram Khadivi"
                    },
                    {
                        "name": "Christof Monz"
                    }
                ],
                "author_detail": {
                    "name": "Christof Monz"
                },
                "author": "Christof Monz",
                "arxiv_comment": "5 pages, 1 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.09237v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.09237v3",
                "updated": "2024-08-21T10:36:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    36,
                    44,
                    2,
                    234,
                    0
                ],
                "published": "2023-06-15T16:19:15Z",
                "published_parsed": [
                    2023,
                    6,
                    15,
                    16,
                    19,
                    15,
                    3,
                    166,
                    0
                ],
                "title": "One Law, Many Languages: Benchmarking Multilingual Legal Reasoning for\n  Judicial Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Law, Many Languages: Benchmarking Multilingual Legal Reasoning for\n  Judicial Support"
                },
                "summary": "Recent strides in Large Language Models (LLMs) have saturated many Natural\nLanguage Processing (NLP) benchmarks, emphasizing the need for more challenging\nones to properly assess LLM capabilities. However, domain-specific and\nmultilingual benchmarks are rare because they require in-depth expertise to\ndevelop. Still, most public models are trained predominantly on English\ncorpora, while other languages remain understudied, particularly for practical\ndomain-specific NLP tasks. In this work, we introduce a novel NLP benchmark for\nthe legal domain that challenges LLMs in five key dimensions: processing\n\\emph{long documents} (up to 50K tokens), using \\emph{domain-specific\nknowledge} (embodied in legal texts), \\emph{multilingual} understanding\n(covering five languages), \\emph{multitasking} (comprising legal\ndocument-to-document Information Retrieval, Court View Generation, Leading\nDecision Summarization, Citation Extraction, and eight challenging Text\nClassification tasks) and \\emph{reasoning} (comprising especially Court View\nGeneration, but also the Text Classification tasks). Our benchmark contains\ndiverse datasets from the Swiss legal system, allowing for a comprehensive\nstudy of the underlying non-English, inherently multilingual legal system.\nDespite the large size of our datasets (some with hundreds of thousands of\nexamples), existing publicly available multilingual models struggle with most\ntasks, even after extensive in-domain pre-training and fine-tuning. We publish\nall resources (benchmark suite, pre-trained models, code) under permissive open\nCC BY-SA licenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent strides in Large Language Models (LLMs) have saturated many Natural\nLanguage Processing (NLP) benchmarks, emphasizing the need for more challenging\nones to properly assess LLM capabilities. However, domain-specific and\nmultilingual benchmarks are rare because they require in-depth expertise to\ndevelop. Still, most public models are trained predominantly on English\ncorpora, while other languages remain understudied, particularly for practical\ndomain-specific NLP tasks. In this work, we introduce a novel NLP benchmark for\nthe legal domain that challenges LLMs in five key dimensions: processing\n\\emph{long documents} (up to 50K tokens), using \\emph{domain-specific\nknowledge} (embodied in legal texts), \\emph{multilingual} understanding\n(covering five languages), \\emph{multitasking} (comprising legal\ndocument-to-document Information Retrieval, Court View Generation, Leading\nDecision Summarization, Citation Extraction, and eight challenging Text\nClassification tasks) and \\emph{reasoning} (comprising especially Court View\nGeneration, but also the Text Classification tasks). Our benchmark contains\ndiverse datasets from the Swiss legal system, allowing for a comprehensive\nstudy of the underlying non-English, inherently multilingual legal system.\nDespite the large size of our datasets (some with hundreds of thousands of\nexamples), existing publicly available multilingual models struggle with most\ntasks, even after extensive in-domain pre-training and fine-tuning. We publish\nall resources (benchmark suite, pre-trained models, code) under permissive open\nCC BY-SA licenses."
                },
                "authors": [
                    {
                        "name": "Ronja Stern"
                    },
                    {
                        "name": "Vishvaksenan Rasiah"
                    },
                    {
                        "name": "Veton Matoshi"
                    },
                    {
                        "name": "Srinanda Brgger Bose"
                    },
                    {
                        "name": "Matthias Strmer"
                    },
                    {
                        "name": "Ilias Chalkidis"
                    },
                    {
                        "name": "Daniel E. Ho"
                    },
                    {
                        "name": "Joel Niklaus"
                    }
                ],
                "author_detail": {
                    "name": "Joel Niklaus"
                },
                "author": "Joel Niklaus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.09237v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.09237v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11494v1",
                "updated": "2024-08-21T10:10:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    10,
                    8,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:10:08Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    10,
                    8,
                    2,
                    234,
                    0
                ],
                "title": "Mutagenesis screen to map the functionals of parameters of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutagenesis screen to map the functionals of parameters of Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have significantly advanced artificial\nintelligence, excelling in numerous tasks. Although the functionality of a\nmodel is inherently tied to its parameters, a systematic method for exploring\nthe connections between the parameters and the functionality are lacking.\nModels sharing similar structure and parameter counts exhibit significant\nperformance disparities across various tasks, prompting investigations into the\nvarying patterns that govern their performance. We adopted a mutagenesis screen\napproach inspired by the methods used in biological studies, to investigate\nLlama2-7b and Zephyr. This technique involved mutating elements within the\nmodels' matrices to their maximum or minimum values to examine the relationship\nbetween model parameters and their functionalities. Our research uncovered\nmultiple levels of fine structures within both models. Many matrices showed a\nmixture of maximum and minimum mutations following mutagenesis, but others were\npredominantly sensitive to one type. Notably, mutations that produced\nphenotypes, especially those with severe outcomes, tended to cluster along\naxes. Additionally, the location of maximum and minimum mutations often\ndisplayed a complementary pattern on matrix in both models, with the Gate\nmatrix showing a unique two-dimensional asymmetry after rearrangement. In\nZephyr, certain mutations consistently resulted in poetic or conversational\nrather than descriptive outputs. These \"writer\" mutations grouped according to\nthe high-frequency initial word of the output, with a marked tendency to share\nthe row coordinate even when they are in different matrices. Our findings\naffirm that the mutagenesis screen is an effective tool for deciphering the\ncomplexities of large language models and identifying unexpected ways to expand\ntheir potential, providing deeper insights into the foundational aspects of AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly advanced artificial\nintelligence, excelling in numerous tasks. Although the functionality of a\nmodel is inherently tied to its parameters, a systematic method for exploring\nthe connections between the parameters and the functionality are lacking.\nModels sharing similar structure and parameter counts exhibit significant\nperformance disparities across various tasks, prompting investigations into the\nvarying patterns that govern their performance. We adopted a mutagenesis screen\napproach inspired by the methods used in biological studies, to investigate\nLlama2-7b and Zephyr. This technique involved mutating elements within the\nmodels' matrices to their maximum or minimum values to examine the relationship\nbetween model parameters and their functionalities. Our research uncovered\nmultiple levels of fine structures within both models. Many matrices showed a\nmixture of maximum and minimum mutations following mutagenesis, but others were\npredominantly sensitive to one type. Notably, mutations that produced\nphenotypes, especially those with severe outcomes, tended to cluster along\naxes. Additionally, the location of maximum and minimum mutations often\ndisplayed a complementary pattern on matrix in both models, with the Gate\nmatrix showing a unique two-dimensional asymmetry after rearrangement. In\nZephyr, certain mutations consistently resulted in poetic or conversational\nrather than descriptive outputs. These \"writer\" mutations grouped according to\nthe high-frequency initial word of the output, with a marked tendency to share\nthe row coordinate even when they are in different matrices. Our findings\naffirm that the mutagenesis screen is an effective tool for deciphering the\ncomplexities of large language models and identifying unexpected ways to expand\ntheir potential, providing deeper insights into the foundational aspects of AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Yue Hu"
                    },
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Patrick X. Zhao"
                    },
                    {
                        "name": "Javed Khan"
                    },
                    {
                        "name": "Chengming Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengming Xu"
                },
                "author": "Chengming Xu",
                "arxiv_comment": "10 pages, 6 figures, supplementary material available online",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11491v1",
                "updated": "2024-08-21T10:01:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    1,
                    34,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:01:34Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    1,
                    34,
                    2,
                    234,
                    0
                ],
                "title": "Nothing in Excess: Mitigating the Exaggerated Safety for LLMs via\n  Safety-Conscious Activation Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nothing in Excess: Mitigating the Exaggerated Safety for LLMs via\n  Safety-Conscious Activation Steering"
                },
                "summary": "Safety alignment is indispensable for Large language models (LLMs) to defend\nthreats from malicious instructions. However, recent researches reveal\nsafety-aligned LLMs prone to reject benign queries due to the exaggerated\nsafety issue, limiting their helpfulness. In this paper, we propose a\nSafety-Conscious Activation Steering (SCANS) method to mitigate the exaggerated\nsafety concerns in aligned LLMs. First, SCANS extracts the refusal steering\nvectors within the activation space and utilizes vocabulary projection to\nanchor some specific safety-critical layers which influence model refusal\nbehavior. Second, by tracking the hidden state transition, SCANS identifies the\nsteering direction and steers the model behavior accordingly, achieving a\nbalance between exaggerated safety and adequate safety. Experiments show that\nSCANS achieves new state-of-the-art performance on XSTest and OKTest\nbenchmarks, without impairing their defense capability against harmful queries\nand maintaining almost unchanged model capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety alignment is indispensable for Large language models (LLMs) to defend\nthreats from malicious instructions. However, recent researches reveal\nsafety-aligned LLMs prone to reject benign queries due to the exaggerated\nsafety issue, limiting their helpfulness. In this paper, we propose a\nSafety-Conscious Activation Steering (SCANS) method to mitigate the exaggerated\nsafety concerns in aligned LLMs. First, SCANS extracts the refusal steering\nvectors within the activation space and utilizes vocabulary projection to\nanchor some specific safety-critical layers which influence model refusal\nbehavior. Second, by tracking the hidden state transition, SCANS identifies the\nsteering direction and steers the model behavior accordingly, achieving a\nbalance between exaggerated safety and adequate safety. Experiments show that\nSCANS achieves new state-of-the-art performance on XSTest and OKTest\nbenchmarks, without impairing their defense capability against harmful queries\nand maintaining almost unchanged model capability."
                },
                "authors": [
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11490v1",
                "updated": "2024-08-21T10:01:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    1,
                    12,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:01:12Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    1,
                    12,
                    2,
                    234,
                    0
                ],
                "title": "DocTabQA: Answering Questions from Long Documents Using Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DocTabQA: Answering Questions from Long Documents Using Tables"
                },
                "summary": "We study a new problem setting of question answering (QA), referred to as\nDocTabQA. Within this setting, given a long document, the goal is to respond to\nquestions by organizing the answers into structured tables derived directly\nfrom the document's content. Unlike traditional QA approaches which\npredominantly rely on unstructured text to formulate responses, DocTabQA aims\nto leverage structured tables as answers to convey information clearly and\nsystematically, thereby enhancing user comprehension and highlighting\nrelationships between data points. To the best of our knowledge, this problem\nhas not been previously explored. In this paper, we introduce the QTabA\ndataset, encompassing 300 financial documents, accompanied by manually\nannotated 1.5k question-table pairs. Initially, we leverage Large Language\nModels (LLMs) such as GPT-4 to establish a baseline. However, it is widely\nacknowledged that LLMs encounter difficulties when tasked with generating\nintricate, structured outputs from long input sequences. To overcome these\nchallenges, we present a two-stage framework, called DocTabTalk, which\ninitially retrieves relevant sentences from extensive documents and\nsubsequently generates hierarchical tables based on these identified sentences.\nDocTabTalk incorporates two key technological innovations: AlignLLaMA and\nTabTalk, which are specifically tailored to assist GPT-4 in tackling DocTabQA,\nenabling it to generate well-structured, hierarchical tables with improved\norganization and clarity. Comprehensive experimental evaluations conducted on\nboth QTabA and RotoWire datasets demonstrate that our DocTabTalk significantly\nenhances the performances of the GPT-4 in our proposed DocTabQA task and the\ntable generation task. The code and dataset are available at\nhttps://github.com/SmileWHC/DocTabQA for further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study a new problem setting of question answering (QA), referred to as\nDocTabQA. Within this setting, given a long document, the goal is to respond to\nquestions by organizing the answers into structured tables derived directly\nfrom the document's content. Unlike traditional QA approaches which\npredominantly rely on unstructured text to formulate responses, DocTabQA aims\nto leverage structured tables as answers to convey information clearly and\nsystematically, thereby enhancing user comprehension and highlighting\nrelationships between data points. To the best of our knowledge, this problem\nhas not been previously explored. In this paper, we introduce the QTabA\ndataset, encompassing 300 financial documents, accompanied by manually\nannotated 1.5k question-table pairs. Initially, we leverage Large Language\nModels (LLMs) such as GPT-4 to establish a baseline. However, it is widely\nacknowledged that LLMs encounter difficulties when tasked with generating\nintricate, structured outputs from long input sequences. To overcome these\nchallenges, we present a two-stage framework, called DocTabTalk, which\ninitially retrieves relevant sentences from extensive documents and\nsubsequently generates hierarchical tables based on these identified sentences.\nDocTabTalk incorporates two key technological innovations: AlignLLaMA and\nTabTalk, which are specifically tailored to assist GPT-4 in tackling DocTabQA,\nenabling it to generate well-structured, hierarchical tables with improved\norganization and clarity. Comprehensive experimental evaluations conducted on\nboth QTabA and RotoWire datasets demonstrate that our DocTabTalk significantly\nenhances the performances of the GPT-4 in our proposed DocTabQA task and the\ntable generation task. The code and dataset are available at\nhttps://github.com/SmileWHC/DocTabQA for further research."
                },
                "authors": [
                    {
                        "name": "Haochen Wang"
                    },
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Liangcai Gao"
                    }
                ],
                "author_detail": {
                    "name": "Liangcai Gao"
                },
                "author": "Liangcai Gao",
                "arxiv_comment": "18 pages,5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12667v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12667v3",
                "updated": "2024-08-21T09:47:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    9,
                    47,
                    33,
                    2,
                    234,
                    0
                ],
                "published": "2024-03-19T12:05:09Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    12,
                    5,
                    9,
                    1,
                    79,
                    0
                ],
                "title": "ICE: Interactive 3D Game Character Editing via Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICE: Interactive 3D Game Character Editing via Dialogue"
                },
                "summary": "ost recent popular Role-Playing Games (RPGs) allow players to create in-game\ncharacters with hundreds of adjustable parameters, including bone positions and\nvarious makeup options. Although text-driven auto-customization systems have\nbeen developed to simplify the complex process of adjusting these intricate\ncharacter parameters, they are limited by their single-round generation and\nlack the capability for further editing and fine-tuning. In this paper, we\npropose an Interactive Character Editing framework (ICE) to achieve a\nmulti-round dialogue-based refinement process. In a nutshell, our ICE offers a\nmore user-friendly way to enable players to convey creative ideas iteratively\nwhile ensuring that created characters align with the expectations of players.\nSpecifically, we propose an Instruction Parsing Module (IPM) that utilizes\nlarge language models (LLMs) to parse multi-round dialogues into clear editing\ninstruction prompts in each round. To reliably and swiftly modify character\ncontrol parameters at a fine-grained level, we propose a Semantic-guided\nLow-dimension Parameter Solver (SLPS) that edits character control parameters\naccording to prompts in a zero-shot manner. Our SLPS first localizes the\ncharacter control parameters related to the fine-grained modification, and then\noptimizes the corresponding parameters in a low-dimension space to avoid\nunrealistic results. Extensive experimental results demonstrate the\neffectiveness of our proposed ICE for in-game character creation and the\nsuperior editing performance of ICE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ost recent popular Role-Playing Games (RPGs) allow players to create in-game\ncharacters with hundreds of adjustable parameters, including bone positions and\nvarious makeup options. Although text-driven auto-customization systems have\nbeen developed to simplify the complex process of adjusting these intricate\ncharacter parameters, they are limited by their single-round generation and\nlack the capability for further editing and fine-tuning. In this paper, we\npropose an Interactive Character Editing framework (ICE) to achieve a\nmulti-round dialogue-based refinement process. In a nutshell, our ICE offers a\nmore user-friendly way to enable players to convey creative ideas iteratively\nwhile ensuring that created characters align with the expectations of players.\nSpecifically, we propose an Instruction Parsing Module (IPM) that utilizes\nlarge language models (LLMs) to parse multi-round dialogues into clear editing\ninstruction prompts in each round. To reliably and swiftly modify character\ncontrol parameters at a fine-grained level, we propose a Semantic-guided\nLow-dimension Parameter Solver (SLPS) that edits character control parameters\naccording to prompts in a zero-shot manner. Our SLPS first localizes the\ncharacter control parameters related to the fine-grained modification, and then\noptimizes the corresponding parameters in a low-dimension space to avoid\nunrealistic results. Extensive experimental results demonstrate the\neffectiveness of our proposed ICE for in-game character creation and the\nsuperior editing performance of ICE."
                },
                "authors": [
                    {
                        "name": "Haoqian Wu"
                    },
                    {
                        "name": "Minda Zhao"
                    },
                    {
                        "name": "Zhipeng Hu"
                    },
                    {
                        "name": "Lincheng Li"
                    },
                    {
                        "name": "Weijie Chen"
                    },
                    {
                        "name": "Rui Zhao"
                    },
                    {
                        "name": "Changjie Fan"
                    },
                    {
                        "name": "Xin Yu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Yu"
                },
                "author": "Xin Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12667v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12667v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11552v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11552v3",
                "updated": "2024-08-21T09:46:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    9,
                    46,
                    35,
                    2,
                    234,
                    0
                ],
                "published": "2024-03-18T08:03:47Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    8,
                    3,
                    47,
                    0,
                    78,
                    0
                ],
                "title": "LLM3:Large Language Model-based Task and Motion Planning with Motion\n  Failure Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM3:Large Language Model-based Task and Motion Planning with Motion\n  Failure Reasoning"
                },
                "summary": "Conventional Task and Motion Planning (TAMP) approaches rely on manually\ncrafted interfaces connecting symbolic task planning with continuous motion\ngeneration. These domain-specific and labor-intensive modules are limited in\naddressing emerging tasks in real-world settings. Here, we present LLM^3, a\nnovel Large Language Model (LLM)-based TAMP framework featuring a\ndomain-independent interface. Specifically, we leverage the powerful reasoning\nand planning capabilities of pre-trained LLMs to propose symbolic action\nsequences and select continuous action parameters for motion planning.\nCrucially, LLM^3 incorporates motion planning feedback through prompting,\nallowing the LLM to iteratively refine its proposals by reasoning about motion\nfailure. Consequently, LLM^3 interfaces between task planning and motion\nplanning, alleviating the intricate design process of handling domain-specific\nmessages between them. Through a series of simulations in a box-packing domain,\nwe quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP\nproblems and the efficiency in selecting action parameters. Ablation studies\nunderscore the significant contribution of motion failure reasoning to the\nsuccess of LLM^3. Furthermore, we conduct qualitative experiments on a physical\nmanipulator, demonstrating the practical applicability of our approach in\nreal-world settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional Task and Motion Planning (TAMP) approaches rely on manually\ncrafted interfaces connecting symbolic task planning with continuous motion\ngeneration. These domain-specific and labor-intensive modules are limited in\naddressing emerging tasks in real-world settings. Here, we present LLM^3, a\nnovel Large Language Model (LLM)-based TAMP framework featuring a\ndomain-independent interface. Specifically, we leverage the powerful reasoning\nand planning capabilities of pre-trained LLMs to propose symbolic action\nsequences and select continuous action parameters for motion planning.\nCrucially, LLM^3 incorporates motion planning feedback through prompting,\nallowing the LLM to iteratively refine its proposals by reasoning about motion\nfailure. Consequently, LLM^3 interfaces between task planning and motion\nplanning, alleviating the intricate design process of handling domain-specific\nmessages between them. Through a series of simulations in a box-packing domain,\nwe quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP\nproblems and the efficiency in selecting action parameters. Ablation studies\nunderscore the significant contribution of motion failure reasoning to the\nsuccess of LLM^3. Furthermore, we conduct qualitative experiments on a physical\nmanipulator, demonstrating the practical applicability of our approach in\nreal-world settings."
                },
                "authors": [
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Muzhi Han"
                    },
                    {
                        "name": "Ziyuan Jiao"
                    },
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Ying Nian Wu"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Hangxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hangxin Liu"
                },
                "author": "Hangxin Liu",
                "arxiv_comment": "IROS 2024. Codes available: https://github.com/AssassinWS/LLM-TAMP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.11552v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11552v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11193v9",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11193v9",
                "updated": "2024-08-21T09:31:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    9,
                    31,
                    2,
                    2,
                    234,
                    0
                ],
                "published": "2023-12-18T13:40:16Z",
                "published_parsed": [
                    2023,
                    12,
                    18,
                    13,
                    40,
                    16,
                    0,
                    352,
                    0
                ],
                "title": "Training With \"Paraphrasing the Original Text\" Improves Long-Context\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training With \"Paraphrasing the Original Text\" Improves Long-Context\n  Performance"
                },
                "summary": "As Large Language Models (LLMs) continue to evolve, more are being designed\nto handle long-context inputs. Despite this advancement, most of them still\nface challenges in accurately handling long-context tasks, often showing the\n\"lost in the middle\" issue. We identify that insufficient retrieval capability\nis one of the important reasons for this issue. To tackle this challenge, we\npropose a novel approach to design training data for long-context tasks, aiming\nat augmenting LLMs' proficiency in extracting key information from long\ncontext. Specially, we incorporate an additional part named \"paraphrasing the\noriginal text\" when constructing the answer of training samples and then\nfine-tuning the model. Experimenting on LongBench and NaturalQuestions\nMulti-document-QA dataset with models of Llama and Qwen series, our method\nachieves an improvement of up to 8.48% and 4.48% in average scores,\nrespectively, showing effectiveness in improving the model' s performance on\nlong-context tasks. The model and training data have been made available on\nHuggingFace(https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to evolve, more are being designed\nto handle long-context inputs. Despite this advancement, most of them still\nface challenges in accurately handling long-context tasks, often showing the\n\"lost in the middle\" issue. We identify that insufficient retrieval capability\nis one of the important reasons for this issue. To tackle this challenge, we\npropose a novel approach to design training data for long-context tasks, aiming\nat augmenting LLMs' proficiency in extracting key information from long\ncontext. Specially, we incorporate an additional part named \"paraphrasing the\noriginal text\" when constructing the answer of training samples and then\nfine-tuning the model. Experimenting on LongBench and NaturalQuestions\nMulti-document-QA dataset with models of Llama and Qwen series, our method\nachieves an improvement of up to 8.48% and 4.48% in average scores,\nrespectively, showing effectiveness in improving the model' s performance on\nlong-context tasks. The model and training data have been made available on\nHuggingFace(https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k)."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Zhixiao Qi"
                    },
                    {
                        "name": "Zhe Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Zhou"
                },
                "author": "Zhe Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11193v9",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11193v9",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11458v1",
                "updated": "2024-08-21T09:23:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    9,
                    23,
                    49,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T09:23:49Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    9,
                    23,
                    49,
                    2,
                    234,
                    0
                ],
                "title": "Aerodynamic Performance and Impact Analysis of a MEMS-Based Non-Invasive\n  Monitoring System for Wind Turbine Blades",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aerodynamic Performance and Impact Analysis of a MEMS-Based Non-Invasive\n  Monitoring System for Wind Turbine Blades"
                },
                "summary": "Wind power generation plays a crucial role in transitioning away from fossil\nfuel-dependent energy sources, contributing significantly to the mitigation of\nclimate change. Monitoring and evaluating the aerodynamics of large wind\nturbine rotors is crucial to enable more wind energy deployment. This is\nnecessary to achieve the European climate goal of a reduction in net greenhouse\ngas emissions by at least 55% by 2030, compared to 1990 levels. This paper\npresents a comparison between two measurement systems for evaluating the\naerodynamic performance of wind turbine rotor blades on a full-scale wind\ntunnel test. One system uses an array of ten commercial compact ultra-low power\nmicro-electromechanical systems (MEMS) pressure sensors placed on the blade\nsurface, while the other employs high-accuracy lab-based pressure scanners\nembedded in the airfoil. The tests are conducted at a Reynolds number of 3.5 x\n10^6, which represents typical operating conditions for wind turbines. MEMS\nsensors are of particular interest, as they can enable real-time monitoring\nwhich would be impossible with the ground truth system. This work provides an\naccurate quantification of the impact of the MEMS system on the blade\naerodynamics and its measurement accuracy. Our results indicate that MEMS\nsensors, with a total sensing power below 1.6 mW, can measure key aerodynamic\nparameters like Angle of Attack (AoA) and flow separation with a precision of\n1{\\deg}. Although there are minor differences in measurements due to sensor\nencapsulation, the MEMS system does not significantly compromise blade\naerodynamics, with a maximum shift in the angle of attack for flow separation\nof only 1{\\deg}. These findings indicate that surface and low-power MEMS sensor\nsystems are a promising approach for efficient and sustainable wind turbine\nmonitoring using self-sustaining Internet of Things devices and wireless sensor\nnetworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wind power generation plays a crucial role in transitioning away from fossil\nfuel-dependent energy sources, contributing significantly to the mitigation of\nclimate change. Monitoring and evaluating the aerodynamics of large wind\nturbine rotors is crucial to enable more wind energy deployment. This is\nnecessary to achieve the European climate goal of a reduction in net greenhouse\ngas emissions by at least 55% by 2030, compared to 1990 levels. This paper\npresents a comparison between two measurement systems for evaluating the\naerodynamic performance of wind turbine rotor blades on a full-scale wind\ntunnel test. One system uses an array of ten commercial compact ultra-low power\nmicro-electromechanical systems (MEMS) pressure sensors placed on the blade\nsurface, while the other employs high-accuracy lab-based pressure scanners\nembedded in the airfoil. The tests are conducted at a Reynolds number of 3.5 x\n10^6, which represents typical operating conditions for wind turbines. MEMS\nsensors are of particular interest, as they can enable real-time monitoring\nwhich would be impossible with the ground truth system. This work provides an\naccurate quantification of the impact of the MEMS system on the blade\naerodynamics and its measurement accuracy. Our results indicate that MEMS\nsensors, with a total sensing power below 1.6 mW, can measure key aerodynamic\nparameters like Angle of Attack (AoA) and flow separation with a precision of\n1{\\deg}. Although there are minor differences in measurements due to sensor\nencapsulation, the MEMS system does not significantly compromise blade\naerodynamics, with a maximum shift in the angle of attack for flow separation\nof only 1{\\deg}. These findings indicate that surface and low-power MEMS sensor\nsystems are a promising approach for efficient and sustainable wind turbine\nmonitoring using self-sustaining Internet of Things devices and wireless sensor\nnetworks."
                },
                "authors": [
                    {
                        "name": "Nicolas Schrer"
                    },
                    {
                        "name": "Denis Mikhaylov"
                    },
                    {
                        "name": "Cdric Sievi"
                    },
                    {
                        "name": "Badoui Hanna"
                    },
                    {
                        "name": "Caroline Braud"
                    },
                    {
                        "name": "Julien Deparday"
                    },
                    {
                        "name": "Sarah Barber"
                    },
                    {
                        "name": "Tommaso Polonelli"
                    },
                    {
                        "name": "Michele Magno"
                    }
                ],
                "author_detail": {
                    "name": "Michele Magno"
                },
                "author": "Michele Magno",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11448v1",
                "updated": "2024-08-21T09:07:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    9,
                    7,
                    20,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T09:07:20Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    9,
                    7,
                    20,
                    2,
                    234,
                    0
                ],
                "title": "Lookism: The overlooked bias in computer vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lookism: The overlooked bias in computer vision"
                },
                "summary": "In recent years, there have been significant advancements in computer vision\nwhich have led to the widespread deployment of image recognition and generation\nsystems in socially relevant applications, from hiring to security screening.\nHowever, the prevalence of biases within these systems has raised significant\nethical and social concerns. The most extensively studied biases in this\ncontext are related to gender, race and age. Yet, other biases are equally\npervasive and harmful, such as lookism, i.e., the preferential treatment of\nindividuals based on their physical appearance. Lookism remains under-explored\nin computer vision but can have profound implications not only by perpetuating\nharmful societal stereotypes but also by undermining the fairness and\ninclusivity of AI technologies. Thus, this paper advocates for the systematic\nstudy of lookism as a critical bias in computer vision models. Through a\ncomprehensive review of existing literature, we identify three areas of\nintersection between lookism and computer vision. We illustrate them by means\nof examples and a user study. We call for an interdisciplinary approach to\naddress lookism, urging researchers, developers, and policymakers to prioritize\nthe development of equitable computer vision systems that respect and reflect\nthe diversity of human appearances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, there have been significant advancements in computer vision\nwhich have led to the widespread deployment of image recognition and generation\nsystems in socially relevant applications, from hiring to security screening.\nHowever, the prevalence of biases within these systems has raised significant\nethical and social concerns. The most extensively studied biases in this\ncontext are related to gender, race and age. Yet, other biases are equally\npervasive and harmful, such as lookism, i.e., the preferential treatment of\nindividuals based on their physical appearance. Lookism remains under-explored\nin computer vision but can have profound implications not only by perpetuating\nharmful societal stereotypes but also by undermining the fairness and\ninclusivity of AI technologies. Thus, this paper advocates for the systematic\nstudy of lookism as a critical bias in computer vision models. Through a\ncomprehensive review of existing literature, we identify three areas of\nintersection between lookism and computer vision. We illustrate them by means\nof examples and a user study. We call for an interdisciplinary approach to\naddress lookism, urging researchers, developers, and policymakers to prioritize\nthe development of equitable computer vision systems that respect and reflect\nthe diversity of human appearances."
                },
                "authors": [
                    {
                        "name": "Aditya Gulati"
                    },
                    {
                        "name": "Bruno Lepri"
                    },
                    {
                        "name": "Nuria Oliver"
                    }
                ],
                "author_detail": {
                    "name": "Nuria Oliver"
                },
                "author": "Nuria Oliver",
                "arxiv_comment": "Paper accepted at the ECCV 2024 workshop named \"Fairness and ethics\n  towards transparent AI: facing the chalLEnge through model Debiasing\n  (FAILED)\", https://failed-workshop-eccv-2024.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.4.0; K.4.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.17574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.17574v2",
                "updated": "2024-08-21T09:06:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    9,
                    6,
                    36,
                    2,
                    234,
                    0
                ],
                "published": "2024-03-26T10:28:41Z",
                "published_parsed": [
                    2024,
                    3,
                    26,
                    10,
                    28,
                    41,
                    1,
                    86,
                    0
                ],
                "title": "SPES: Towards Optimizing Performance-Resource Trade-Off for Serverless\n  Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPES: Towards Optimizing Performance-Resource Trade-Off for Serverless\n  Functions"
                },
                "summary": "As an emerging cloud computing deployment paradigm, serverless computing is\ngaining traction due to its efficiency and ability to harness on-demand cloud\nresources. However, a significant hurdle remains in the form of the cold start\nproblem, causing latency when launching new function instances from scratch.\nExisting solutions tend to use over-simplistic strategies for function\npre-loading/unloading without full invocation pattern exploitation, rendering\nunsatisfactory optimization of the trade-off between cold start latency and\nresource waste. To bridge this gap, we propose SPES, the first differentiated\nscheduler for runtime cold start mitigation by optimizing serverless function\nprovision. Our insight is that the common architecture of serverless systems\nprompts the concentration of certain invocation patterns, leading to\npredictable invocation behaviors. This allows us to categorize functions and\npre-load/unload proper function instances with finer-grained strategies based\non accurate invocation prediction. Experiments demonstrate the success of SPES\nin optimizing serverless function provision on both sides: reducing the\n75th-percentile cold start rates by 49.77% and the wasted memory time by\n56.43%, compared to the state-of-the-art. By mitigating the cold start issue,\nSPES is a promising advancement in facilitating cloud services deployed on\nserverless architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As an emerging cloud computing deployment paradigm, serverless computing is\ngaining traction due to its efficiency and ability to harness on-demand cloud\nresources. However, a significant hurdle remains in the form of the cold start\nproblem, causing latency when launching new function instances from scratch.\nExisting solutions tend to use over-simplistic strategies for function\npre-loading/unloading without full invocation pattern exploitation, rendering\nunsatisfactory optimization of the trade-off between cold start latency and\nresource waste. To bridge this gap, we propose SPES, the first differentiated\nscheduler for runtime cold start mitigation by optimizing serverless function\nprovision. Our insight is that the common architecture of serverless systems\nprompts the concentration of certain invocation patterns, leading to\npredictable invocation behaviors. This allows us to categorize functions and\npre-load/unload proper function instances with finer-grained strategies based\non accurate invocation prediction. Experiments demonstrate the success of SPES\nin optimizing serverless function provision on both sides: reducing the\n75th-percentile cold start rates by 49.77% and the wasted memory time by\n56.43%, compared to the state-of-the-art. By mitigating the cold start issue,\nSPES is a promising advancement in facilitating cloud services deployed on\nserverless architectures."
                },
                "authors": [
                    {
                        "name": "Cheryl Lee"
                    },
                    {
                        "name": "Zhouruixing Zhu"
                    },
                    {
                        "name": "Tianyi Yang"
                    },
                    {
                        "name": "Yintong Huo"
                    },
                    {
                        "name": "Yuxin Su"
                    },
                    {
                        "name": "Pinjia He"
                    },
                    {
                        "name": "Michael R. Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Lyu"
                },
                "author": "Michael R. Lyu",
                "arxiv_comment": "12 pages, accepted by ICDE 2024 (40th IEEE International Conference\n  on Data Engineering)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.17574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.17574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11431v1",
                "updated": "2024-08-21T08:39:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    8,
                    39,
                    49,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T08:39:49Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    8,
                    39,
                    49,
                    2,
                    234,
                    0
                ],
                "title": "Diagnosing and Remedying Knowledge Deficiencies in LLMs via Label-free\n  Curricular Meaningful Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnosing and Remedying Knowledge Deficiencies in LLMs via Label-free\n  Curricular Meaningful Learning"
                },
                "summary": "Large Language Models (LLMs) are versatile and demonstrate impressive\ngeneralization ability by mining and learning information from extensive\nunlabeled text. However, they still exhibit reasoning mistakes, often stemming\nfrom knowledge deficiencies, which can affect their trustworthiness and\nreliability. Although users can provide diverse and comprehensive queries,\nobtaining sufficient and effective feedback is demanding. Furthermore,\nevaluating LLMs comprehensively with limited labeled samples is difficult. This\nmakes it a challenge to diagnose and remedy the deficiencies of LLMs through\nrich label-free user queries. To tackle this challenge, we propose a label-free\ncurricular meaningful learning framework (LaMer). LaMer first employs relative\nentropy to automatically diagnose and quantify the knowledge deficiencies of\nLLMs in a label-free setting. Next, to remedy the diagnosed knowledge\ndeficiencies, we apply curricular meaningful learning: first, we adopt\nmeaningful learning to adaptively synthesize augmentation data according to the\nseverity of the deficiencies, and then design a curricular deficiency remedy\nstrategy to remedy the knowledge deficiencies of LLMs progressively.\nExperiments show that LaMer efficiently and effectively diagnoses and remedies\nknowledge deficiencies in LLMs, improving various LLMs across seven\nout-of-distribution (OOD) reasoning and language understanding benchmarks,\nachieving comparable results to baselines with just 40\\% training data. LaMer\neven surpasses methods that rely on labeled datasets for deficiency diagnosis.\nIn application, our label-free method can offer an effective knowledge\ndeficiency diagnostic tool for efficient LLM development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are versatile and demonstrate impressive\ngeneralization ability by mining and learning information from extensive\nunlabeled text. However, they still exhibit reasoning mistakes, often stemming\nfrom knowledge deficiencies, which can affect their trustworthiness and\nreliability. Although users can provide diverse and comprehensive queries,\nobtaining sufficient and effective feedback is demanding. Furthermore,\nevaluating LLMs comprehensively with limited labeled samples is difficult. This\nmakes it a challenge to diagnose and remedy the deficiencies of LLMs through\nrich label-free user queries. To tackle this challenge, we propose a label-free\ncurricular meaningful learning framework (LaMer). LaMer first employs relative\nentropy to automatically diagnose and quantify the knowledge deficiencies of\nLLMs in a label-free setting. Next, to remedy the diagnosed knowledge\ndeficiencies, we apply curricular meaningful learning: first, we adopt\nmeaningful learning to adaptively synthesize augmentation data according to the\nseverity of the deficiencies, and then design a curricular deficiency remedy\nstrategy to remedy the knowledge deficiencies of LLMs progressively.\nExperiments show that LaMer efficiently and effectively diagnoses and remedies\nknowledge deficiencies in LLMs, improving various LLMs across seven\nout-of-distribution (OOD) reasoning and language understanding benchmarks,\nachieving comparable results to baselines with just 40\\% training data. LaMer\neven surpasses methods that rely on labeled datasets for deficiency diagnosis.\nIn application, our label-free method can offer an effective knowledge\ndeficiency diagnostic tool for efficient LLM development."
                },
                "authors": [
                    {
                        "name": "Kai Xiong"
                    },
                    {
                        "name": "Xiao Ding"
                    },
                    {
                        "name": "Li Du"
                    },
                    {
                        "name": "Jiahao Ying"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Yixin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Yixin Cao"
                },
                "author": "Yixin Cao",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11428v1",
                "updated": "2024-08-21T08:37:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    8,
                    37,
                    10,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T08:37:10Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    8,
                    37,
                    10,
                    2,
                    234,
                    0
                ],
                "title": "Migrating Existing Container Workload to Kubernetes -- LLM Based\n  Approach and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Migrating Existing Container Workload to Kubernetes -- LLM Based\n  Approach and Evaluation"
                },
                "summary": "Although Kubernetes has become a widespread open-source system that automates\nthe management of containerized applications, its complexity can be a\nsignificant barrier, particularly for application developers unfamiliar with\nit. One approach employs large language models (LLMs) to assist developers in\ngenerating Kubernetes manifests; however it is currently impossible to\ndetermine whether the output satisfies given specifications and is\ncomprehensible. In this study, we proposed a benchmarking method for evaluating\nthe effectiveness of LLMs in synthesizing manifests, using the Compose\nspecification -- a standard widely adopted by application developers -- as\ninput. The proposed benchmarking method revealed that LLMs generally produce\naccurate results that compensate for simple specification gaps. However, we\nalso observed that inline comments for readability were often omitted, and\ncompletion accuracy was low for atypical inputs with unclear intentions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Kubernetes has become a widespread open-source system that automates\nthe management of containerized applications, its complexity can be a\nsignificant barrier, particularly for application developers unfamiliar with\nit. One approach employs large language models (LLMs) to assist developers in\ngenerating Kubernetes manifests; however it is currently impossible to\ndetermine whether the output satisfies given specifications and is\ncomprehensible. In this study, we proposed a benchmarking method for evaluating\nthe effectiveness of LLMs in synthesizing manifests, using the Compose\nspecification -- a standard widely adopted by application developers -- as\ninput. The proposed benchmarking method revealed that LLMs generally produce\naccurate results that compensate for simple specification gaps. However, we\nalso observed that inline comments for readability were often omitted, and\ncompletion accuracy was low for atypical inputs with unclear intentions."
                },
                "authors": [
                    {
                        "name": "Masaru Ueno"
                    },
                    {
                        "name": "Tetsuya Uchiumi"
                    }
                ],
                "author_detail": {
                    "name": "Tetsuya Uchiumi"
                },
                "author": "Tetsuya Uchiumi",
                "arxiv_comment": "submitted to ICSME 2024 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01290v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01290v4",
                "updated": "2024-08-21T08:01:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    8,
                    1,
                    4,
                    2,
                    234,
                    0
                ],
                "published": "2024-06-03T13:01:09Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    13,
                    1,
                    9,
                    0,
                    155,
                    0
                ],
                "title": "Resource-constrained Fairness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-constrained Fairness"
                },
                "summary": "Access to resources strongly constrains the decisions we make. While we might\nwish to offer every student a scholarship, or schedule every patient for\nfollow-up meetings with a specialist, limited resources mean that this is not\npossible. When deploying machine learning systems, these resource constraints\nare simply enforced by varying the threshold of a classifier. However, these\nfinite resource limitations are disregarded by most existing tools for fair\nmachine learning, which do not allow the specification of resource limitations\nand do not remain fair when varying thresholds. This makes them ill-suited for\nreal-world deployment. Our research introduces the concept of\n\"resource-constrained fairness\" and quantifies the cost of fairness within this\nframework. We demonstrate that the level of available resources significantly\ninfluences this cost, a factor overlooked in previous evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Access to resources strongly constrains the decisions we make. While we might\nwish to offer every student a scholarship, or schedule every patient for\nfollow-up meetings with a specialist, limited resources mean that this is not\npossible. When deploying machine learning systems, these resource constraints\nare simply enforced by varying the threshold of a classifier. However, these\nfinite resource limitations are disregarded by most existing tools for fair\nmachine learning, which do not allow the specification of resource limitations\nand do not remain fair when varying thresholds. This makes them ill-suited for\nreal-world deployment. Our research introduces the concept of\n\"resource-constrained fairness\" and quantifies the cost of fairness within this\nframework. We demonstrate that the level of available resources significantly\ninfluences this cost, a factor overlooked in previous evaluations."
                },
                "authors": [
                    {
                        "name": "Sofie Goethals"
                    },
                    {
                        "name": "Eoin Delaney"
                    },
                    {
                        "name": "Brent Mittelstadt"
                    },
                    {
                        "name": "Chris Russell"
                    }
                ],
                "author_detail": {
                    "name": "Chris Russell"
                },
                "author": "Chris Russell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01290v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01290v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10668v2",
                "updated": "2024-08-21T07:50:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    50,
                    29,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-20T09:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    11,
                    21,
                    1,
                    233,
                    0
                ],
                "title": "Probing the Safety Response Boundary of Large Language Models via Unsafe\n  Decoding Path Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing the Safety Response Boundary of Large Language Models via Unsafe\n  Decoding Path Generation"
                },
                "summary": "Large Language Models (LLMs) are implicit troublemakers. While they provide\nvaluable insights and assist in problem-solving, they can also potentially\nserve as a resource for malicious activities. Implementing safety alignment\ncould mitigate the risk of LLMs generating harmful responses. We argue that:\neven when an LLM appears to successfully block harmful queries, there may still\nbe hidden vulnerabilities that could act as ticking time bombs. To identify\nthese underlying weaknesses, we propose to use a cost value model as both a\ndetector and an attacker. Trained on external or self-generated harmful\ndatasets, the cost value model could successfully influence the original safe\nLLM to output toxic content in decoding process. For instance, LLaMA-2-chat 7B\noutputs 39.18% concrete toxic content, along with only 22.16% refusals without\nany harmful suffixes. These potential weaknesses can then be exploited via\nprompt optimization such as soft prompts on images. We name this decoding\nstrategy: Jailbreak Value Decoding (JVD), emphasizing that seemingly secure\nLLMs may not be as safe as we initially believe. They could be used to gather\nharmful data or launch covert attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are implicit troublemakers. While they provide\nvaluable insights and assist in problem-solving, they can also potentially\nserve as a resource for malicious activities. Implementing safety alignment\ncould mitigate the risk of LLMs generating harmful responses. We argue that:\neven when an LLM appears to successfully block harmful queries, there may still\nbe hidden vulnerabilities that could act as ticking time bombs. To identify\nthese underlying weaknesses, we propose to use a cost value model as both a\ndetector and an attacker. Trained on external or self-generated harmful\ndatasets, the cost value model could successfully influence the original safe\nLLM to output toxic content in decoding process. For instance, LLaMA-2-chat 7B\noutputs 39.18% concrete toxic content, along with only 22.16% refusals without\nany harmful suffixes. These potential weaknesses can then be exploited via\nprompt optimization such as soft prompts on images. We name this decoding\nstrategy: Jailbreak Value Decoding (JVD), emphasizing that seemingly secure\nLLMs may not be as safe as we initially believe. They could be used to gather\nharmful data or launch covert attacks."
                },
                "authors": [
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Bingzhe Wu"
                    },
                    {
                        "name": "Yatao Bian"
                    },
                    {
                        "name": "Yongzhe Chang"
                    },
                    {
                        "name": "Xueqian Wang"
                    },
                    {
                        "name": "Peilin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Peilin Zhao"
                },
                "author": "Peilin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11397v1",
                "updated": "2024-08-21T07:43:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    43,
                    50,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T07:43:50Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    43,
                    50,
                    2,
                    234,
                    0
                ],
                "title": "EAGLE: Elevating Geometric Reasoning through LLM-empowered Visual\n  Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EAGLE: Elevating Geometric Reasoning through LLM-empowered Visual\n  Instruction Tuning"
                },
                "summary": "Multi-modal Large Language Models have recently experienced rapid\ndevelopments and excel in various multi-modal tasks. However, they still\nstruggle with mathematical geometric problem solving, which requires\nexceptional visual perception proficiency. Existing MLLMs mostly optimize the\nLLM backbone to acquire geometric reasoning capabilities, while rarely\nemphasizing improvements in visual comprehension. In this paper, we first\ninvestigate the visual perception performance of MLLMs when facing geometric\ndiagrams. Our findings reveal that current MLLMs severely suffer from\ninaccurate geometric perception and hallucinations. To address these\nlimitations, we propose EAGLE, a novel two-stage end-to-end visual enhancement\nMLLM framework designed to ElevAte Geometric reasoning through LLM-Empowered\nvisual instruction tuning. Specifically, in the preliminary stage, we feed\ngeometric image-caption pairs into our MLLM that contains a fully fine-tuning\nCLIP ViT and a frozen LLM, aiming to endow our model with basic geometric\nknowledge. In the subsequent advanced stage, we incorporate LoRA modules into\nthe vision encoder and unfreeze the LLM backbone. This enables the model to\nleverage the inherent CoT rationales within question-answer pairs, guiding the\nMLLM to focus on nuanced visual cues and enhancing its overall perceptual\ncapacity. Moreover, we optimize the cross-modal projector in both stages to\nfoster adaptive visual-linguistic alignments. After the two-stage visual\nenhancement, we develop the geometry expert model EAGLE-7B. Extensive\nexperiments on popular benchmarks demonstrate the effectiveness of our model.\nFor example, on the GeoQA benchmark, EAGLE-7B not only surpasses the exemplary\nG-LLaVA 7B model by 2.9%, but also marginally outperforms the larger G-LLaVA\n13B model. On the MathVista benchmark, EAGLE-7B achieves remarkable 3.8%\nimprovements compared with the proprietary model GPT-4V.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Large Language Models have recently experienced rapid\ndevelopments and excel in various multi-modal tasks. However, they still\nstruggle with mathematical geometric problem solving, which requires\nexceptional visual perception proficiency. Existing MLLMs mostly optimize the\nLLM backbone to acquire geometric reasoning capabilities, while rarely\nemphasizing improvements in visual comprehension. In this paper, we first\ninvestigate the visual perception performance of MLLMs when facing geometric\ndiagrams. Our findings reveal that current MLLMs severely suffer from\ninaccurate geometric perception and hallucinations. To address these\nlimitations, we propose EAGLE, a novel two-stage end-to-end visual enhancement\nMLLM framework designed to ElevAte Geometric reasoning through LLM-Empowered\nvisual instruction tuning. Specifically, in the preliminary stage, we feed\ngeometric image-caption pairs into our MLLM that contains a fully fine-tuning\nCLIP ViT and a frozen LLM, aiming to endow our model with basic geometric\nknowledge. In the subsequent advanced stage, we incorporate LoRA modules into\nthe vision encoder and unfreeze the LLM backbone. This enables the model to\nleverage the inherent CoT rationales within question-answer pairs, guiding the\nMLLM to focus on nuanced visual cues and enhancing its overall perceptual\ncapacity. Moreover, we optimize the cross-modal projector in both stages to\nfoster adaptive visual-linguistic alignments. After the two-stage visual\nenhancement, we develop the geometry expert model EAGLE-7B. Extensive\nexperiments on popular benchmarks demonstrate the effectiveness of our model.\nFor example, on the GeoQA benchmark, EAGLE-7B not only surpasses the exemplary\nG-LLaVA 7B model by 2.9%, but also marginally outperforms the larger G-LLaVA\n13B model. On the MathVista benchmark, EAGLE-7B achieves remarkable 3.8%\nimprovements compared with the proprietary model GPT-4V."
                },
                "authors": [
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Yao Du"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Yufang Liu"
                    },
                    {
                        "name": "Mengdi Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11396v1",
                "updated": "2024-08-21T07:43:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    43,
                    49,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T07:43:49Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    43,
                    49,
                    2,
                    234,
                    0
                ],
                "title": "MoE-LPR: Multilingual Extension of Large Language Models through\n  Mixture-of-Experts with Language Priors Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-LPR: Multilingual Extension of Large Language Models through\n  Mixture-of-Experts with Language Priors Routing"
                },
                "summary": "Large Language Models (LLMs) are often English-centric due to the\ndisproportionate distribution of languages in their pre-training data.\nEnhancing non-English language capabilities through post-pretraining often\nresults in catastrophic forgetting of the ability of original languages.\nPrevious methods either achieve good expansion with severe forgetting or slight\nforgetting with poor expansion, indicating the challenge of balancing language\nexpansion while preventing forgetting. In this paper, we propose a method\ncalled MoE-LPR (Mixture-of-Experts with Language Priors Routing) to alleviate\nthis problem. MoE-LPR employs a two-stage training approach to enhance the\nmultilingual capability. First, the model is post-pretrained into a\nMixture-of-Experts (MoE) architecture by upcycling, where all the original\nparameters are frozen and new experts are added. In this stage, we focus\nimproving the ability on expanded languages, without using any original\nlanguage data. Then, the model reviews the knowledge of the original languages\nwith replay data amounting to less than 1% of post-pretraining, where we\nincorporate language priors routing to better recover the abilities of the\noriginal languages. Evaluations on multiple benchmarks show that MoE-LPR\noutperforms other post-pretraining methods. Freezing original parameters\npreserves original language knowledge while adding new experts preserves the\nlearning ability. Reviewing with LPR enables effective utilization of\nmultilingual knowledge within the parameters. Additionally, the MoE\narchitecture maintains the same inference overhead while increasing total model\nparameters. Extensive experiments demonstrate MoE-LPR's effectiveness in\nimproving expanded languages and preserving original language proficiency with\nsuperior scalability. Code and scripts are freely available at\nhttps://github.com/zjwang21/MoE-LPR.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are often English-centric due to the\ndisproportionate distribution of languages in their pre-training data.\nEnhancing non-English language capabilities through post-pretraining often\nresults in catastrophic forgetting of the ability of original languages.\nPrevious methods either achieve good expansion with severe forgetting or slight\nforgetting with poor expansion, indicating the challenge of balancing language\nexpansion while preventing forgetting. In this paper, we propose a method\ncalled MoE-LPR (Mixture-of-Experts with Language Priors Routing) to alleviate\nthis problem. MoE-LPR employs a two-stage training approach to enhance the\nmultilingual capability. First, the model is post-pretrained into a\nMixture-of-Experts (MoE) architecture by upcycling, where all the original\nparameters are frozen and new experts are added. In this stage, we focus\nimproving the ability on expanded languages, without using any original\nlanguage data. Then, the model reviews the knowledge of the original languages\nwith replay data amounting to less than 1% of post-pretraining, where we\nincorporate language priors routing to better recover the abilities of the\noriginal languages. Evaluations on multiple benchmarks show that MoE-LPR\noutperforms other post-pretraining methods. Freezing original parameters\npreserves original language knowledge while adding new experts preserves the\nlearning ability. Reviewing with LPR enables effective utilization of\nmultilingual knowledge within the parameters. Additionally, the MoE\narchitecture maintains the same inference overhead while increasing total model\nparameters. Extensive experiments demonstrate MoE-LPR's effectiveness in\nimproving expanded languages and preserving original language proficiency with\nsuperior scalability. Code and scripts are freely available at\nhttps://github.com/zjwang21/MoE-LPR.git."
                },
                "authors": [
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhijun Wang"
                    },
                    {
                        "name": "Shujian Huang"
                    },
                    {
                        "name": "Xin Huang"
                    },
                    {
                        "name": "Xue Han"
                    },
                    {
                        "name": "Junlan Feng"
                    },
                    {
                        "name": "Chao Deng"
                    },
                    {
                        "name": "Weihua Luo"
                    },
                    {
                        "name": "Jiajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Chen"
                },
                "author": "Jiajun Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11393v1",
                "updated": "2024-08-21T07:38:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    38,
                    51,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T07:38:51Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    38,
                    51,
                    2,
                    234,
                    0
                ],
                "title": "First Activations Matter: Training-Free Methods for Dynamic Activation\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First Activations Matter: Training-Free Methods for Dynamic Activation\n  in Large Language Models"
                },
                "summary": "Dynamic activation (DA) techniques, such as DejaVu and MoEfication, have\ndemonstrated their potential to significantly enhance the inference efficiency\nof large language models (LLMs). However, these techniques often rely on ReLU\nactivation functions or require additional parameters and training to maintain\nperformance. This paper introduces a training-free Threshold-based Dynamic\nActivation(TDA) method that leverage sequence information to exploit the\ninherent sparsity of models across various architectures. This method is\ndesigned to accelerate generation speed by 18-25\\% without significantly\ncompromising task performance, thereby addressing the limitations of existing\nDA techniques. Moreover, we delve into the root causes of LLM sparsity and\ntheoretically analyze two of its critical features: history-related activation\nuncertainty and semantic-irrelevant activation inertia. Our comprehensive\nanalyses not only provide a robust theoretical foundation for DA methods but\nalso offer valuable insights to guide future research in optimizing LLMs for\ngreater efficiency and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic activation (DA) techniques, such as DejaVu and MoEfication, have\ndemonstrated their potential to significantly enhance the inference efficiency\nof large language models (LLMs). However, these techniques often rely on ReLU\nactivation functions or require additional parameters and training to maintain\nperformance. This paper introduces a training-free Threshold-based Dynamic\nActivation(TDA) method that leverage sequence information to exploit the\ninherent sparsity of models across various architectures. This method is\ndesigned to accelerate generation speed by 18-25\\% without significantly\ncompromising task performance, thereby addressing the limitations of existing\nDA techniques. Moreover, we delve into the root causes of LLM sparsity and\ntheoretically analyze two of its critical features: history-related activation\nuncertainty and semantic-irrelevant activation inertia. Our comprehensive\nanalyses not only provide a robust theoretical foundation for DA methods but\nalso offer valuable insights to guide future research in optimizing LLMs for\ngreater efficiency and effectiveness."
                },
                "authors": [
                    {
                        "name": "Chi Ma"
                    },
                    {
                        "name": "Mincong Huang"
                    },
                    {
                        "name": "Ying Zhang"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Yujie Wang"
                    },
                    {
                        "name": "Lei Yu"
                    },
                    {
                        "name": "Chuan Liu"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11386v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11386v1",
                "updated": "2024-08-21T07:30:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    30,
                    11,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T07:30:11Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    30,
                    11,
                    2,
                    234,
                    0
                ],
                "title": "Unlocking Sustainability Compliance: Characterizing the EU Taxonomy for\n  Business Process Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Sustainability Compliance: Characterizing the EU Taxonomy for\n  Business Process Management"
                },
                "summary": "To promote sustainable business practices, and to achieve climate neutrality\nby 2050, the EU has developed the taxonomy of sustainable activities, which\ndescribes when exactly business practices can be considered sustainable. While\nthe taxonomy has only been recently established, progressively more companies\nwill have to report how much of their revenue was created via sustainably\nexecuted business processes. To help companies prepare to assess whether their\nbusiness processes comply with the constraints outlined in the taxonomy, we\ninvestigate in how far these criteria can be used for conformance checking,\nthat is, assessing in a data-driven manner, whether business process executions\nadhere to regulatory constraints. For this, we develop a few-shot learning\npipeline to characterize the constraints of the taxonomy with the help of an\nLLM as to the process dimensions they relate to. We find that many constraints\nof the taxonomy are useable for conformance checking, particularly in the\nsectors of energy, manufacturing, and transport. This will aid companies in\npreparing to monitor regulatory compliance with the taxonomy automatically, by\ncharacterizing what kind of information they need to extract, and by providing\na better understanding of sectors where such an assessment is feasible and\nwhere it is not.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To promote sustainable business practices, and to achieve climate neutrality\nby 2050, the EU has developed the taxonomy of sustainable activities, which\ndescribes when exactly business practices can be considered sustainable. While\nthe taxonomy has only been recently established, progressively more companies\nwill have to report how much of their revenue was created via sustainably\nexecuted business processes. To help companies prepare to assess whether their\nbusiness processes comply with the constraints outlined in the taxonomy, we\ninvestigate in how far these criteria can be used for conformance checking,\nthat is, assessing in a data-driven manner, whether business process executions\nadhere to regulatory constraints. For this, we develop a few-shot learning\npipeline to characterize the constraints of the taxonomy with the help of an\nLLM as to the process dimensions they relate to. We find that many constraints\nof the taxonomy are useable for conformance checking, particularly in the\nsectors of energy, manufacturing, and transport. This will aid companies in\npreparing to monitor regulatory compliance with the taxonomy automatically, by\ncharacterizing what kind of information they need to extract, and by providing\na better understanding of sectors where such an assessment is feasible and\nwhere it is not."
                },
                "authors": [
                    {
                        "name": "Finn Klessascheck"
                    },
                    {
                        "name": "Stephan A. Fahrenkrog-Petersen"
                    },
                    {
                        "name": "Jan Mendling"
                    },
                    {
                        "name": "Luise Pufahl"
                    }
                ],
                "author_detail": {
                    "name": "Luise Pufahl"
                },
                "author": "Luise Pufahl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11386v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11386v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11382v1",
                "updated": "2024-08-21T07:23:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    23,
                    34,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T07:23:34Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    23,
                    34,
                    2,
                    234,
                    0
                ],
                "title": "On the Interchangeability of Positional Embeddings in Multilingual\n  Neural Machine Translation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Interchangeability of Positional Embeddings in Multilingual\n  Neural Machine Translation Models"
                },
                "summary": "Standard Neural Machine Translation (NMT) models have traditionally been\ntrained with Sinusoidal Positional Embeddings (PEs), which are inadequate for\ncapturing long-range dependencies and are inefficient for long-context or\ndocument-level translation. In contrast, state-of-the-art large language models\n(LLMs) employ relative PEs, demonstrating superior length generalization. This\nwork explores the potential for efficiently switching the Positional Embeddings\nof pre-trained NMT models from absolute sinusoidal PEs to relative approaches\nsuch as RoPE and ALiBi. Our findings reveal that sinusoidal PEs can be\neffectively replaced with RoPE and ALiBi with negligible or no performance\nloss, achieved by fine-tuning on a small fraction of high-quality data.\nAdditionally, models trained without Positional Embeddings (NoPE) are not a\nviable solution for Encoder-Decoder architectures, as they consistently\nunder-perform compared to models utilizing any form of Positional Embedding.\nFurthermore, even a model trained from scratch with these relative PEs slightly\nunder-performs a fine-tuned model, underscoring the efficiency and validity of\nour hypothesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standard Neural Machine Translation (NMT) models have traditionally been\ntrained with Sinusoidal Positional Embeddings (PEs), which are inadequate for\ncapturing long-range dependencies and are inefficient for long-context or\ndocument-level translation. In contrast, state-of-the-art large language models\n(LLMs) employ relative PEs, demonstrating superior length generalization. This\nwork explores the potential for efficiently switching the Positional Embeddings\nof pre-trained NMT models from absolute sinusoidal PEs to relative approaches\nsuch as RoPE and ALiBi. Our findings reveal that sinusoidal PEs can be\neffectively replaced with RoPE and ALiBi with negligible or no performance\nloss, achieved by fine-tuning on a small fraction of high-quality data.\nAdditionally, models trained without Positional Embeddings (NoPE) are not a\nviable solution for Encoder-Decoder architectures, as they consistently\nunder-perform compared to models utilizing any form of Positional Embedding.\nFurthermore, even a model trained from scratch with these relative PEs slightly\nunder-performs a fine-tuned model, underscoring the efficiency and validity of\nour hypothesis."
                },
                "authors": [
                    {
                        "name": "Varun Gumma"
                    },
                    {
                        "name": "Pranjal A. Chitale"
                    },
                    {
                        "name": "Kalika Bali"
                    }
                ],
                "author_detail": {
                    "name": "Kalika Bali"
                },
                "author": "Kalika Bali",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11381v1",
                "updated": "2024-08-21T07:20:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    20,
                    48,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T07:20:48Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    20,
                    48,
                    2,
                    234,
                    0
                ],
                "title": "RAGLAB: A Modular and Research-Oriented Unified Framework for\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGLAB: A Modular and Research-Oriented Unified Framework for\n  Retrieval-Augmented Generation"
                },
                "summary": "Large Language Models (LLMs) demonstrate human-level capabilities in\ndialogue, reasoning, and knowledge retention. However, even the most advanced\nLLMs face challenges such as hallucinations and real-time updating of their\nknowledge. Current research addresses this bottleneck by equipping LLMs with\nexternal knowledge, a technique known as Retrieval Augmented Generation (RAG).\nHowever, two key issues constrained the development of RAG. First, there is a\ngrowing lack of comprehensive and fair comparisons between novel RAG\nalgorithms. Second, open-source tools such as LlamaIndex and LangChain employ\nhigh-level abstractions, which results in a lack of transparency and limits the\nability to develop novel algorithms and evaluation metrics. To close this gap,\nwe introduce RAGLAB, a modular and research-oriented open-source library.\nRAGLAB reproduces 6 existing algorithms and provides a comprehensive ecosystem\nfor investigating RAG algorithms. Leveraging RAGLAB, we conduct a fair\ncomparison of 6 RAG algorithms across 10 benchmarks. With RAGLAB, researchers\ncan efficiently compare the performance of various algorithms and develop novel\nalgorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate human-level capabilities in\ndialogue, reasoning, and knowledge retention. However, even the most advanced\nLLMs face challenges such as hallucinations and real-time updating of their\nknowledge. Current research addresses this bottleneck by equipping LLMs with\nexternal knowledge, a technique known as Retrieval Augmented Generation (RAG).\nHowever, two key issues constrained the development of RAG. First, there is a\ngrowing lack of comprehensive and fair comparisons between novel RAG\nalgorithms. Second, open-source tools such as LlamaIndex and LangChain employ\nhigh-level abstractions, which results in a lack of transparency and limits the\nability to develop novel algorithms and evaluation metrics. To close this gap,\nwe introduce RAGLAB, a modular and research-oriented open-source library.\nRAGLAB reproduces 6 existing algorithms and provides a comprehensive ecosystem\nfor investigating RAG algorithms. Leveraging RAGLAB, we conduct a fair\ncomparison of 6 RAG algorithms across 10 benchmarks. With RAGLAB, researchers\ncan efficiently compare the performance of various algorithms and develop novel\nalgorithms."
                },
                "authors": [
                    {
                        "name": "Xuanwang Zhang"
                    },
                    {
                        "name": "Yunze Song"
                    },
                    {
                        "name": "Yidong Wang"
                    },
                    {
                        "name": "Shuyun Tang"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Zhengran Zeng"
                    },
                    {
                        "name": "Zhen Wu"
                    },
                    {
                        "name": "Wei Ye"
                    },
                    {
                        "name": "Wenyuan Xu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Xinyu Dai"
                    },
                    {
                        "name": "Shikun Zhang"
                    },
                    {
                        "name": "Qingsong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Qingsong Wen"
                },
                "author": "Qingsong Wen",
                "arxiv_comment": "6 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10529v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10529v3",
                "updated": "2024-08-22T03:40:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    3,
                    40,
                    50,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-20T04:06:58Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    4,
                    6,
                    58,
                    1,
                    233,
                    0
                ],
                "title": "Automated Detection of Algorithm Debt in Deep Learning Frameworks: An\n  Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Detection of Algorithm Debt in Deep Learning Frameworks: An\n  Empirical Study"
                },
                "summary": "Context: Previous studies demonstrate that Machine or Deep Learning (ML/DL)\nmodels can detect Technical Debt from source code comments called Self-Admitted\nTechnical Debt (SATD). Despite the importance of ML/DL in software development,\nlimited studies focus on automated detection for new SATD types: Algorithm Debt\n(AD). AD detection is important because it helps to identify TD early,\nfacilitating research, learning, and preventing the accumulation of issues\nrelated to model degradation and lack of scalability. Aim: Our goal is to\nimprove AD detection performance of various ML/DL models. Method: We will\nperform empirical studies using approaches: TF-IDF, Count Vectorizer, Hash\nVectorizer, and TD-indicative words to identify features that improve AD\ndetection, using ML/DL classifiers with different data featurisations. We will\nuse an existing dataset curated from seven DL frameworks where comments were\nmanually classified as AD, Compatibility, Defect, Design, Documentation,\nRequirement, and Test Debt. We will explore various word embedding methods to\nfurther enrich features for ML models. These embeddings will be from models\nfounded in DL such as ROBERTA, ALBERTv2, and large language models (LLMs):\nINSTRUCTOR and VOYAGE AI. We will enrich the dataset by incorporating\nAD-related terms, then train various ML/DL classifiers, Support Vector Machine,\nLogistic Regression, Random Forest, ROBERTA, and ALBERTv2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Previous studies demonstrate that Machine or Deep Learning (ML/DL)\nmodels can detect Technical Debt from source code comments called Self-Admitted\nTechnical Debt (SATD). Despite the importance of ML/DL in software development,\nlimited studies focus on automated detection for new SATD types: Algorithm Debt\n(AD). AD detection is important because it helps to identify TD early,\nfacilitating research, learning, and preventing the accumulation of issues\nrelated to model degradation and lack of scalability. Aim: Our goal is to\nimprove AD detection performance of various ML/DL models. Method: We will\nperform empirical studies using approaches: TF-IDF, Count Vectorizer, Hash\nVectorizer, and TD-indicative words to identify features that improve AD\ndetection, using ML/DL classifiers with different data featurisations. We will\nuse an existing dataset curated from seven DL frameworks where comments were\nmanually classified as AD, Compatibility, Defect, Design, Documentation,\nRequirement, and Test Debt. We will explore various word embedding methods to\nfurther enrich features for ML models. These embeddings will be from models\nfounded in DL such as ROBERTA, ALBERTv2, and large language models (LLMs):\nINSTRUCTOR and VOYAGE AI. We will enrich the dataset by incorporating\nAD-related terms, then train various ML/DL classifiers, Support Vector Machine,\nLogistic Regression, Random Forest, ROBERTA, and ALBERTv2."
                },
                "authors": [
                    {
                        "name": "Emmanuel Iko-Ojo Simon"
                    },
                    {
                        "name": "Chirath Hettiarachchi"
                    },
                    {
                        "name": "Alex Potanin"
                    },
                    {
                        "name": "Hanna Suominen"
                    },
                    {
                        "name": "Fatemeh Fard"
                    }
                ],
                "author_detail": {
                    "name": "Fatemeh Fard"
                },
                "author": "Fatemeh Fard",
                "arxiv_comment": "Accepted as Continuity Acceptance (CA) for a Stage 1 registration of\n  the Registered Report Track at 40th IEEE International Conference on Software\n  Maintenance and Evolution (ICSME 2024), Flagstaff, USA, October 6-11, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10529v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10529v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.7; K.6.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10774v2",
                "updated": "2024-08-21T06:48:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    48,
                    16,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-20T12:13:04Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    12,
                    13,
                    4,
                    1,
                    233,
                    0
                ],
                "title": "Flexora: Flexible Low Rank Adaptation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexora: Flexible Low Rank Adaptation for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are driving advancements in artificial\nintelligence by increasing the scale of model parameters, which has\nsignificantly enhanced generalization ability and unlocked new capabilities in\npractice. However, their performance in specific downstream tasks is usually\nhindered by their knowledge boundaries on these tasks. Thus, fine-tuning\ntechniques, especially the widely used Low-Rank Adaptation (LoRA) method, have\nbeen introduced to expand the boundaries on these tasks, whereas LoRA would\nunderperform on certain tasks owing to its potential overfitting on these\ntasks. To overcome this overfitting and improve the performance of LoRA, we\npropose the flexible low rank adaptation (Flexora) method to automatically and\nflexibly select the most important layers needing to be fine-tuned to achieve\nthe best performance on different downstream tasks. Specifically, Flexora\nfirstly frames this layer selection problem as a well-defined hyperparameter\noptimization (HPO) problem, then addresses it using the unrolled\ndifferentiation (UD) method, and finally selects the most useful layers based\non the optimized hyperparameters. Our extensive experiments on many pretrained\nmodels and natural language tasks show that Flexora is able to consistently\nimprove over the existing baselines, indicating the effectiveness of our\nFlexora in practice. We additionally provide insightful theoretical results and\nmany ablation studies to deliver a comprehensive understanding of our Flexora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are driving advancements in artificial\nintelligence by increasing the scale of model parameters, which has\nsignificantly enhanced generalization ability and unlocked new capabilities in\npractice. However, their performance in specific downstream tasks is usually\nhindered by their knowledge boundaries on these tasks. Thus, fine-tuning\ntechniques, especially the widely used Low-Rank Adaptation (LoRA) method, have\nbeen introduced to expand the boundaries on these tasks, whereas LoRA would\nunderperform on certain tasks owing to its potential overfitting on these\ntasks. To overcome this overfitting and improve the performance of LoRA, we\npropose the flexible low rank adaptation (Flexora) method to automatically and\nflexibly select the most important layers needing to be fine-tuned to achieve\nthe best performance on different downstream tasks. Specifically, Flexora\nfirstly frames this layer selection problem as a well-defined hyperparameter\noptimization (HPO) problem, then addresses it using the unrolled\ndifferentiation (UD) method, and finally selects the most useful layers based\non the optimized hyperparameters. Our extensive experiments on many pretrained\nmodels and natural language tasks show that Flexora is able to consistently\nimprove over the existing baselines, indicating the effectiveness of our\nFlexora in practice. We additionally provide insightful theoretical results and\nmany ablation studies to deliver a comprehensive understanding of our Flexora."
                },
                "authors": [
                    {
                        "name": "Chenxing Wei"
                    },
                    {
                        "name": "Yao Shu"
                    },
                    {
                        "name": "Ying Tiffany He"
                    },
                    {
                        "name": "Fei Richard Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Richard Yu"
                },
                "author": "Fei Richard Yu",
                "arxiv_comment": "29 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08892v2",
                "updated": "2024-08-21T06:38:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    38,
                    36,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-08T13:12:46Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    13,
                    12,
                    46,
                    3,
                    221,
                    0
                ],
                "title": "Leveraging Large Language Models for Enhanced Process Model\n  Comprehension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Enhanced Process Model\n  Comprehension"
                },
                "summary": "In Business Process Management (BPM), effectively comprehending process\nmodels is crucial yet poses significant challenges, particularly as\norganizations scale and processes become more complex. This paper introduces a\nnovel framework utilizing the advanced capabilities of Large Language Models\n(LLMs) to enhance the interpretability of complex process models. We present\ndifferent methods for abstracting business process models into a format\naccessible to LLMs, and we implement advanced prompting strategies specifically\ndesigned to optimize LLM performance within our framework. Additionally, we\npresent a tool, AIPA, that implements our proposed framework and allows for\nconversational process querying. We evaluate our framework and tool by i) an\nautomatic evaluation comparing different LLMs, model abstractions, and\nprompting strategies and ii) a user study designed to assess AIPA's\neffectiveness comprehensively. Results demonstrate our framework's ability to\nimprove the accessibility and interpretability of process models, pioneering\nnew pathways for integrating AI technologies into the BPM field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Business Process Management (BPM), effectively comprehending process\nmodels is crucial yet poses significant challenges, particularly as\norganizations scale and processes become more complex. This paper introduces a\nnovel framework utilizing the advanced capabilities of Large Language Models\n(LLMs) to enhance the interpretability of complex process models. We present\ndifferent methods for abstracting business process models into a format\naccessible to LLMs, and we implement advanced prompting strategies specifically\ndesigned to optimize LLM performance within our framework. Additionally, we\npresent a tool, AIPA, that implements our proposed framework and allows for\nconversational process querying. We evaluate our framework and tool by i) an\nautomatic evaluation comparing different LLMs, model abstractions, and\nprompting strategies and ii) a user study designed to assess AIPA's\neffectiveness comprehensively. Results demonstrate our framework's ability to\nimprove the accessibility and interpretability of process models, pioneering\nnew pathways for integrating AI technologies into the BPM field."
                },
                "authors": [
                    {
                        "name": "Humam Kourani"
                    },
                    {
                        "name": "Alessandro Berti"
                    },
                    {
                        "name": "Jasmin Henrich"
                    },
                    {
                        "name": "Wolfgang Kratsch"
                    },
                    {
                        "name": "Robin Weidlich"
                    },
                    {
                        "name": "Chiao-Yun Li"
                    },
                    {
                        "name": "Ahmad Arslan"
                    },
                    {
                        "name": "Daniel Schuster"
                    },
                    {
                        "name": "Wil M. P. van der Aalst"
                    }
                ],
                "author_detail": {
                    "name": "Wil M. P. van der Aalst"
                },
                "author": "Wil M. P. van der Aalst",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11366v1",
                "updated": "2024-08-21T06:35:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    35,
                    21,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T06:35:21Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    35,
                    21,
                    2,
                    234,
                    0
                ],
                "title": "GeoReasoner: Reasoning On Geospatially Grounded Context For Natural\n  Language Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeoReasoner: Reasoning On Geospatially Grounded Context For Natural\n  Language Understanding"
                },
                "summary": "In human reading and communication, individuals tend to engage in geospatial\nreasoning, which involves recognizing geographic entities and making informed\ninferences about their interrelationships. To mimic such cognitive process,\ncurrent methods either utilize conventional natural language understanding\ntoolkits, or directly apply models pretrained on geo-related natural language\ncorpora. However, these methods face two significant challenges: i) they do not\ngeneralize well to unseen geospatial scenarios, and ii) they overlook the\nimportance of integrating geospatial context from geographical databases with\nlinguistic information from the Internet. To handle these challenges, we\npropose GeoReasoner, a language model capable of reasoning on geospatially\ngrounded natural language. Specifically, it first leverages Large Language\nModels (LLMs) to generate a comprehensive location description based on\nlinguistic and geospatial information. It also encodes direction and distance\ninformation into spatial embedding via treating them as pseudo-sentences.\nConsequently, the model is trained on both anchor-level and neighbor-level\ninputs to learn geo-entity representation. Extensive experimental results\ndemonstrate GeoReasoner's superiority in three tasks: toponym recognition,\ntoponym linking, and geo-entity typing, compared to the state-of-the-art\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In human reading and communication, individuals tend to engage in geospatial\nreasoning, which involves recognizing geographic entities and making informed\ninferences about their interrelationships. To mimic such cognitive process,\ncurrent methods either utilize conventional natural language understanding\ntoolkits, or directly apply models pretrained on geo-related natural language\ncorpora. However, these methods face two significant challenges: i) they do not\ngeneralize well to unseen geospatial scenarios, and ii) they overlook the\nimportance of integrating geospatial context from geographical databases with\nlinguistic information from the Internet. To handle these challenges, we\npropose GeoReasoner, a language model capable of reasoning on geospatially\ngrounded natural language. Specifically, it first leverages Large Language\nModels (LLMs) to generate a comprehensive location description based on\nlinguistic and geospatial information. It also encodes direction and distance\ninformation into spatial embedding via treating them as pseudo-sentences.\nConsequently, the model is trained on both anchor-level and neighbor-level\ninputs to learn geo-entity representation. Extensive experimental results\ndemonstrate GeoReasoner's superiority in three tasks: toponym recognition,\ntoponym linking, and geo-entity typing, compared to the state-of-the-art\nbaselines."
                },
                "authors": [
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Joey Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joey Lee"
                },
                "author": "Joey Lee",
                "arxiv_comment": "Accepted by International Conference on Information and Knowledge\n  Management 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05606v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05606v3",
                "updated": "2024-08-21T06:20:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    20,
                    34,
                    2,
                    234,
                    0
                ],
                "published": "2024-05-09T07:55:52Z",
                "published_parsed": [
                    2024,
                    5,
                    9,
                    7,
                    55,
                    52,
                    3,
                    130,
                    0
                ],
                "title": "Optimizing E-commerce Search: Toward a Generalizable and Rank-Consistent\n  Pre-Ranking Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing E-commerce Search: Toward a Generalizable and Rank-Consistent\n  Pre-Ranking Model"
                },
                "summary": "In large e-commerce platforms, search systems are typically composed of a\nseries of modules, including recall, pre-ranking, and ranking phases. The\npre-ranking phase, serving as a lightweight module, is crucial for filtering\nout the bulk of products in advance for the downstream ranking module.\nIndustrial efforts on optimizing the pre-ranking model have predominantly\nfocused on enhancing ranking consistency, model structure, and generalization\ntowards long-tail items. Beyond these optimizations, meeting the system\nperformance requirements presents a significant challenge. Contrasting with\nexisting industry works, we propose a novel method: a Generalizable and\nRAnk-ConsistEnt Pre-Ranking Model (GRACE), which achieves: 1) Ranking\nconsistency by introducing multiple binary classification tasks that predict\nwhether a product is within the top-k results as estimated by the ranking\nmodel, which facilitates the addition of learning objectives on common\npoint-wise ranking models; 2) Generalizability through contrastive learning of\nrepresentation for all products by pre-training on a subset of ranking product\nembeddings; 3) Ease of implementation in feature construction and online\ndeployment. Our extensive experiments demonstrate significant improvements in\nboth offline metrics and online A/B test: a 0.75% increase in AUC and a 1.28%\nincrease in CVR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large e-commerce platforms, search systems are typically composed of a\nseries of modules, including recall, pre-ranking, and ranking phases. The\npre-ranking phase, serving as a lightweight module, is crucial for filtering\nout the bulk of products in advance for the downstream ranking module.\nIndustrial efforts on optimizing the pre-ranking model have predominantly\nfocused on enhancing ranking consistency, model structure, and generalization\ntowards long-tail items. Beyond these optimizations, meeting the system\nperformance requirements presents a significant challenge. Contrasting with\nexisting industry works, we propose a novel method: a Generalizable and\nRAnk-ConsistEnt Pre-Ranking Model (GRACE), which achieves: 1) Ranking\nconsistency by introducing multiple binary classification tasks that predict\nwhether a product is within the top-k results as estimated by the ranking\nmodel, which facilitates the addition of learning objectives on common\npoint-wise ranking models; 2) Generalizability through contrastive learning of\nrepresentation for all products by pre-training on a subset of ranking product\nembeddings; 3) Ease of implementation in feature construction and online\ndeployment. Our extensive experiments demonstrate significant improvements in\nboth offline metrics and online A/B test: a 0.75% increase in AUC and a 1.28%\nincrease in CVR."
                },
                "authors": [
                    {
                        "name": "Enqiang Xu"
                    },
                    {
                        "name": "Yiming Qiu"
                    },
                    {
                        "name": "Junyang Bai"
                    },
                    {
                        "name": "Ping Zhang"
                    },
                    {
                        "name": "Dadong Miao"
                    },
                    {
                        "name": "Songlin Wang"
                    },
                    {
                        "name": "Guoyu Tang"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Mingming Li"
                    }
                ],
                "author_detail": {
                    "name": "Mingming Li"
                },
                "author": "Mingming Li",
                "arxiv_doi": "10.1145/3626772.3661343",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3626772.3661343",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.05606v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05606v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11363v1",
                "updated": "2024-08-21T06:16:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    16,
                    22,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T06:16:22Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    16,
                    22,
                    2,
                    234,
                    0
                ],
                "title": "ProteinGPT: Multimodal LLM for Protein Property Prediction and Structure\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProteinGPT: Multimodal LLM for Protein Property Prediction and Structure\n  Understanding"
                },
                "summary": "Understanding biological processes, drug development, and biotechnological\nadvancements requires detailed analysis of protein structures and sequences, a\ntask in protein research that is inherently complex and time-consuming when\nperformed manually. To streamline this process, we introduce ProteinGPT, a\nstate-of-the-art multi-modal protein chat system, that allows users to upload\nprotein sequences and/or structures for comprehensive protein analysis and\nresponsive inquiries. ProteinGPT seamlessly integrates protein sequence and\nstructure encoders with linear projection layers for precise representation\nadaptation, coupled with a large language model (LLM) to generate accurate and\ncontextually relevant responses. To train ProteinGPT, we construct a\nlarge-scale dataset of 132,092 proteins with annotations, and optimize the\ninstruction-tuning process using GPT-4o. This innovative system ensures\naccurate alignment between the user-uploaded data and prompts, simplifying\nprotein analysis. Experiments show that ProteinGPT can produce promising\nresponses to proteins and their corresponding questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding biological processes, drug development, and biotechnological\nadvancements requires detailed analysis of protein structures and sequences, a\ntask in protein research that is inherently complex and time-consuming when\nperformed manually. To streamline this process, we introduce ProteinGPT, a\nstate-of-the-art multi-modal protein chat system, that allows users to upload\nprotein sequences and/or structures for comprehensive protein analysis and\nresponsive inquiries. ProteinGPT seamlessly integrates protein sequence and\nstructure encoders with linear projection layers for precise representation\nadaptation, coupled with a large language model (LLM) to generate accurate and\ncontextually relevant responses. To train ProteinGPT, we construct a\nlarge-scale dataset of 132,092 proteins with annotations, and optimize the\ninstruction-tuning process using GPT-4o. This innovative system ensures\naccurate alignment between the user-uploaded data and prompts, simplifying\nprotein analysis. Experiments show that ProteinGPT can produce promising\nresponses to proteins and their corresponding questions."
                },
                "authors": [
                    {
                        "name": "Yijia Xiao"
                    },
                    {
                        "name": "Edward Sun"
                    },
                    {
                        "name": "Yiqiao Jin"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "arxiv_comment": "19 pages, 9 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09121v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09121v2",
                "updated": "2024-08-21T06:01:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    1,
                    8,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-17T07:11:02Z",
                "published_parsed": [
                    2024,
                    8,
                    17,
                    7,
                    11,
                    2,
                    5,
                    230,
                    0
                ],
                "title": "Selective Prompt Anchoring for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective Prompt Anchoring for Code Generation"
                },
                "summary": "Recent advances in large language models (LLMs) such as Copilot and ChatGPT\nhave transformed software development by automating coding tasks. Despite these\nadvancements, challenges remain in reducing error rates and fully meeting user\nexpectations. Our empirical study reveals LLMs tend to dilute their\nself-attention on the initial prompt as more code tokens are generated. We\nhypothesize this self-attention dilution issue is one of the root causes of\ninaccuracies in LLM-generated code. To mitigate this issue, we propose\nSelective Prompt Anchoring (SPA). SPA amplifies the influence of the selected\nparts in the initial prompt, which we refer to as ``anchored text'', during\ncode generation. Specifically, SPA calculates the logit distribution difference\nwith and without the anchored text. We prove this difference approximates the\nanchored text's contextual contribution to the output logits. SPA creates an\naugmented logit distribution by linearly combining the original logit\ndistribution and the logit difference. We evaluate SPA with five LLMs on four\nbenchmarks. Our results demonstrate that using SPA can consistently improve\nPass@1 rates by up to 9.7% in all settings. Notably, with selective text\nanchoring, a small version of DeepSeek-Coder (6.7B) can achieve better\nperformance than an original much larger version (33B). Our code is available\nat https://github.com/magic-YuanTian/Selective-Prompt-Anchoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) such as Copilot and ChatGPT\nhave transformed software development by automating coding tasks. Despite these\nadvancements, challenges remain in reducing error rates and fully meeting user\nexpectations. Our empirical study reveals LLMs tend to dilute their\nself-attention on the initial prompt as more code tokens are generated. We\nhypothesize this self-attention dilution issue is one of the root causes of\ninaccuracies in LLM-generated code. To mitigate this issue, we propose\nSelective Prompt Anchoring (SPA). SPA amplifies the influence of the selected\nparts in the initial prompt, which we refer to as ``anchored text'', during\ncode generation. Specifically, SPA calculates the logit distribution difference\nwith and without the anchored text. We prove this difference approximates the\nanchored text's contextual contribution to the output logits. SPA creates an\naugmented logit distribution by linearly combining the original logit\ndistribution and the logit difference. We evaluate SPA with five LLMs on four\nbenchmarks. Our results demonstrate that using SPA can consistently improve\nPass@1 rates by up to 9.7% in all settings. Notably, with selective text\nanchoring, a small version of DeepSeek-Coder (6.7B) can achieve better\nperformance than an original much larger version (33B). Our code is available\nat https://github.com/magic-YuanTian/Selective-Prompt-Anchoring."
                },
                "authors": [
                    {
                        "name": "Yuan Tian"
                    },
                    {
                        "name": "Tianyi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhang"
                },
                "author": "Tianyi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09121v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09121v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11349v1",
                "updated": "2024-08-21T05:30:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    5,
                    30,
                    6,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T05:30:06Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    5,
                    30,
                    6,
                    2,
                    234,
                    0
                ],
                "title": "Image Score: Learning and Evaluating Human Preferences for Mercari\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image Score: Learning and Evaluating Human Preferences for Mercari\n  Search"
                },
                "summary": "Mercari is the largest C2C e-commerce marketplace in Japan, having more than\n20 million active monthly users. Search being the fundamental way to discover\ndesired items, we have always had a substantial amount of data with implicit\nfeedback. Although we actively take advantage of that to provide the best\nservice for our users, the correlation of implicit feedback for such tasks as\nimage quality assessment is not trivial. Many traditional lines of research in\nMachine Learning (ML) are similarly motivated by the insatiable appetite of\nDeep Learning (DL) models for well-labelled training data. Weak supervision is\nabout leveraging higher-level and/or noisier supervision over unlabeled data.\nLarge Language Models (LLMs) are being actively studied and used for data\nlabelling tasks. We present how we leverage a Chain-of-Thought (CoT) to enable\nLLM to produce image aesthetics labels that correlate well with human behavior\nin e-commerce settings. Leveraging LLMs is more cost-effective compared to\nexplicit human judgment, while significantly improving the explainability of\ndeep image quality evaluation which is highly important for customer journey\noptimization at Mercari. We propose a cost-efficient LLM-driven approach for\nassessing and predicting image quality in e-commerce settings, which is very\nconvenient for proof-of-concept testing. We show that our LLM-produced labels\ncorrelate with user behavior on Mercari. Finally, we show our results from an\nonline experimentation, where we achieved a significant growth in sales on the\nweb platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mercari is the largest C2C e-commerce marketplace in Japan, having more than\n20 million active monthly users. Search being the fundamental way to discover\ndesired items, we have always had a substantial amount of data with implicit\nfeedback. Although we actively take advantage of that to provide the best\nservice for our users, the correlation of implicit feedback for such tasks as\nimage quality assessment is not trivial. Many traditional lines of research in\nMachine Learning (ML) are similarly motivated by the insatiable appetite of\nDeep Learning (DL) models for well-labelled training data. Weak supervision is\nabout leveraging higher-level and/or noisier supervision over unlabeled data.\nLarge Language Models (LLMs) are being actively studied and used for data\nlabelling tasks. We present how we leverage a Chain-of-Thought (CoT) to enable\nLLM to produce image aesthetics labels that correlate well with human behavior\nin e-commerce settings. Leveraging LLMs is more cost-effective compared to\nexplicit human judgment, while significantly improving the explainability of\ndeep image quality evaluation which is highly important for customer journey\noptimization at Mercari. We propose a cost-efficient LLM-driven approach for\nassessing and predicting image quality in e-commerce settings, which is very\nconvenient for proof-of-concept testing. We show that our LLM-produced labels\ncorrelate with user behavior on Mercari. Finally, we show our results from an\nonline experimentation, where we achieved a significant growth in sales on the\nweb platform."
                },
                "authors": [
                    {
                        "name": "Chingis Oinar"
                    },
                    {
                        "name": "Miao Cao"
                    },
                    {
                        "name": "Shanshan Fu"
                    }
                ],
                "author_detail": {
                    "name": "Shanshan Fu"
                },
                "author": "Shanshan Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.00280v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.00280v3",
                "updated": "2024-08-21T05:11:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    5,
                    11,
                    10,
                    2,
                    234,
                    0
                ],
                "published": "2023-09-30T07:11:39Z",
                "published_parsed": [
                    2023,
                    9,
                    30,
                    7,
                    11,
                    39,
                    5,
                    273,
                    0
                ],
                "title": "Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model\n  Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model\n  Collaboration"
                },
                "summary": "Large Language Models (LLMs) are evolving at an unprecedented pace and have\nexhibited considerable capability in the realm of natural language processing\n(NLP) with world knowledge. Benefiting from ultra-large-scale training corpora,\na single LLM can manage typical NLP tasks competently. However, its performance\nin executing reasoning tasks is still confined by the limitations of its\ninternal representations. To push this boundary further, we introduce Corex in\nthis paper, a suite of novel general-purpose strategies that transform LLMs\ninto autonomous agents pioneering multi-model collaborations for complex\ntask-solving. Inspired by human behaviors, Corex is constituted by diverse\ncollaboration paradigms including Debate, Review, and Retrieve modes, which\ncollectively work towards enhancing the factuality, faithfulness, and\nreliability of the reasoning process. These paradigms foster task-agnostic\napproaches that enable LLMs to ''think outside the box,'' thereby overcoming\nhallucinations and providing better solutions. Through extensive experiments\nacross four different types of reasoning tasks, we demonstrate that\norchestrating multiple LLMs to work in concert yields substantially better\nperformance compared to existing methods. Further results and in-depth analysis\ndemonstrate the cost-effectiveness of our method, facilitating collaboration\namong different LLMs and promoting annotation efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are evolving at an unprecedented pace and have\nexhibited considerable capability in the realm of natural language processing\n(NLP) with world knowledge. Benefiting from ultra-large-scale training corpora,\na single LLM can manage typical NLP tasks competently. However, its performance\nin executing reasoning tasks is still confined by the limitations of its\ninternal representations. To push this boundary further, we introduce Corex in\nthis paper, a suite of novel general-purpose strategies that transform LLMs\ninto autonomous agents pioneering multi-model collaborations for complex\ntask-solving. Inspired by human behaviors, Corex is constituted by diverse\ncollaboration paradigms including Debate, Review, and Retrieve modes, which\ncollectively work towards enhancing the factuality, faithfulness, and\nreliability of the reasoning process. These paradigms foster task-agnostic\napproaches that enable LLMs to ''think outside the box,'' thereby overcoming\nhallucinations and providing better solutions. Through extensive experiments\nacross four different types of reasoning tasks, we demonstrate that\norchestrating multiple LLMs to work in concert yields substantially better\nperformance compared to existing methods. Further results and in-depth analysis\ndemonstrate the cost-effectiveness of our method, facilitating collaboration\namong different LLMs and promoting annotation efficiency."
                },
                "authors": [
                    {
                        "name": "Qiushi Sun"
                    },
                    {
                        "name": "Zhangyue Yin"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Zhiyong Wu"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Lingpeng Kong"
                    }
                ],
                "author_detail": {
                    "name": "Lingpeng Kong"
                },
                "author": "Lingpeng Kong",
                "arxiv_comment": "COLM 2024 / ICLR 2024 Workshop on LLM Agents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.00280v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.00280v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11338v1",
                "updated": "2024-08-21T04:45:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    45,
                    12,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T04:45:12Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    45,
                    12,
                    2,
                    234,
                    0
                ],
                "title": "Automatic Dataset Construction (ADC): Sample Collection, Data Curation,\n  and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Dataset Construction (ADC): Sample Collection, Data Curation,\n  and Beyond"
                },
                "summary": "Large-scale data collection is essential for developing personalized training\ndata, mitigating the shortage of training data, and fine-tuning specialized\nmodels. However, creating high-quality datasets quickly and accurately remains\na challenge due to annotation errors, the substantial time and costs associated\nwith human labor. To address these issues, we propose Automatic Dataset\nConstruction (ADC), an innovative methodology that automates dataset creation\nwith negligible cost and high efficiency. Taking the image classification task\nas a starting point, ADC leverages LLMs for the detailed class design and code\ngeneration to collect relevant samples via search engines, significantly\nreducing the need for manual annotation and speeding up the data generation\nprocess. Despite these advantages, ADC also encounters real-world challenges\nsuch as label errors (label noise) and imbalanced data distributions (label\nbias). We provide open-source software that incorporates existing methods for\nlabel error detection, robust learning under noisy and biased data, ensuring a\nhigher-quality training data and more robust model training procedure.\nFurthermore, we design three benchmark datasets focused on label noise\ndetection, label noise learning, and class-imbalanced learning. These datasets\nare vital because there are few existing datasets specifically for label noise\ndetection, despite its importance. Finally, we evaluate the performance of\nexisting popular methods on these datasets, thereby facilitating further\nresearch in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale data collection is essential for developing personalized training\ndata, mitigating the shortage of training data, and fine-tuning specialized\nmodels. However, creating high-quality datasets quickly and accurately remains\na challenge due to annotation errors, the substantial time and costs associated\nwith human labor. To address these issues, we propose Automatic Dataset\nConstruction (ADC), an innovative methodology that automates dataset creation\nwith negligible cost and high efficiency. Taking the image classification task\nas a starting point, ADC leverages LLMs for the detailed class design and code\ngeneration to collect relevant samples via search engines, significantly\nreducing the need for manual annotation and speeding up the data generation\nprocess. Despite these advantages, ADC also encounters real-world challenges\nsuch as label errors (label noise) and imbalanced data distributions (label\nbias). We provide open-source software that incorporates existing methods for\nlabel error detection, robust learning under noisy and biased data, ensuring a\nhigher-quality training data and more robust model training procedure.\nFurthermore, we design three benchmark datasets focused on label noise\ndetection, label noise learning, and class-imbalanced learning. These datasets\nare vital because there are few existing datasets specifically for label noise\ndetection, despite its importance. Finally, we evaluate the performance of\nexisting popular methods on these datasets, thereby facilitating further\nresearch in the field."
                },
                "authors": [
                    {
                        "name": "Minghao Liu"
                    },
                    {
                        "name": "Zonglin Di"
                    },
                    {
                        "name": "Jiaheng Wei"
                    },
                    {
                        "name": "Zhongruo Wang"
                    },
                    {
                        "name": "Hengxiang Zhang"
                    },
                    {
                        "name": "Ruixuan Xiao"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Jinlong Pang"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Ankit Shah"
                    },
                    {
                        "name": "Hongxin Wei"
                    },
                    {
                        "name": "Xinlei He"
                    },
                    {
                        "name": "Zhaowei Zhao"
                    },
                    {
                        "name": "Haobo Wang"
                    },
                    {
                        "name": "Lei Feng"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "James Davis"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11334v1",
                "updated": "2024-08-21T04:33:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    33,
                    5,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T04:33:05Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    33,
                    5,
                    2,
                    234,
                    0
                ],
                "title": "BURExtract-Llama: An LLM for Clinical Concept Extraction in Breast\n  Ultrasound Reports",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BURExtract-Llama: An LLM for Clinical Concept Extraction in Breast\n  Ultrasound Reports"
                },
                "summary": "Breast ultrasound is essential for detecting and diagnosing abnormalities,\nwith radiology reports summarizing key findings like lesion characteristics and\nmalignancy assessments. Extracting this critical information is challenging due\nto the unstructured nature of these reports, with varied linguistic styles and\ninconsistent formatting. While proprietary LLMs like GPT-4 are effective, they\nare costly and raise privacy concerns when handling protected health\ninformation. This study presents a pipeline for developing an in-house LLM to\nextract clinical information from radiology reports. We first use GPT-4 to\ncreate a small labeled dataset, then fine-tune a Llama3-8B model on it.\nEvaluated on clinician-annotated reports, our model achieves an average F1\nscore of 84.6%, which is on par with GPT-4. Our findings demonstrate the\nfeasibility of developing an in-house LLM that not only matches GPT-4's\nperformance but also offers cost reductions and enhanced data privacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breast ultrasound is essential for detecting and diagnosing abnormalities,\nwith radiology reports summarizing key findings like lesion characteristics and\nmalignancy assessments. Extracting this critical information is challenging due\nto the unstructured nature of these reports, with varied linguistic styles and\ninconsistent formatting. While proprietary LLMs like GPT-4 are effective, they\nare costly and raise privacy concerns when handling protected health\ninformation. This study presents a pipeline for developing an in-house LLM to\nextract clinical information from radiology reports. We first use GPT-4 to\ncreate a small labeled dataset, then fine-tune a Llama3-8B model on it.\nEvaluated on clinician-annotated reports, our model achieves an average F1\nscore of 84.6%, which is on par with GPT-4. Our findings demonstrate the\nfeasibility of developing an in-house LLM that not only matches GPT-4's\nperformance but also offers cost reductions and enhanced data privacy."
                },
                "authors": [
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Haoyan Yang"
                    },
                    {
                        "name": "Hengkai Pan"
                    },
                    {
                        "name": "Fardeen Siddiqui"
                    },
                    {
                        "name": "Antonio Verdone"
                    },
                    {
                        "name": "Qingyang Zhang"
                    },
                    {
                        "name": "Sumit Chopra"
                    },
                    {
                        "name": "Chen Zhao"
                    },
                    {
                        "name": "Yiqiu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yiqiu Shen"
                },
                "author": "Yiqiu Shen",
                "arxiv_comment": "This paper has been accepted as the oral paper for the HCHM workshop,\n  ACM Multimedia 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11330v1",
                "updated": "2024-08-21T04:27:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    27,
                    44,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T04:27:44Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    27,
                    44,
                    2,
                    234,
                    0
                ],
                "title": "Design Principle Transfer in Neural Architecture Search via Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design Principle Transfer in Neural Architecture Search via Large\n  Language Models"
                },
                "summary": "Transferable neural architecture search (TNAS) has been introduced to design\nefficient neural architectures for multiple tasks, to enhance the practical\napplicability of NAS in real-world scenarios. In TNAS, architectural knowledge\naccumulated in previous search processes is reused to warm up the architecture\nsearch for new tasks. However, existing TNAS methods still search in an\nextensive search space, necessitating the evaluation of numerous architectures.\nTo overcome this challenge, this work proposes a novel transfer paradigm, i.e.,\ndesign principle transfer. In this work, the linguistic description of various\nstructural components' effects on architectural performance is termed design\nprinciples. They are learned from established architectures and then can be\nreused to reduce the search space by discarding unpromising architectures.\nSearching in the refined search space can boost both the search performance and\nefficiency for new NAS tasks. To this end, a large language model\n(LLM)-assisted design principle transfer (LAPT) framework is devised. In LAPT,\nLLM is applied to automatically reason the design principles from a set of\ngiven architectures, and then a principle adaptation method is applied to\nrefine these principles progressively based on the new search results.\nExperimental results show that LAPT can beat the state-of-the-art TNAS methods\non most tasks and achieve comparable performance on others.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transferable neural architecture search (TNAS) has been introduced to design\nefficient neural architectures for multiple tasks, to enhance the practical\napplicability of NAS in real-world scenarios. In TNAS, architectural knowledge\naccumulated in previous search processes is reused to warm up the architecture\nsearch for new tasks. However, existing TNAS methods still search in an\nextensive search space, necessitating the evaluation of numerous architectures.\nTo overcome this challenge, this work proposes a novel transfer paradigm, i.e.,\ndesign principle transfer. In this work, the linguistic description of various\nstructural components' effects on architectural performance is termed design\nprinciples. They are learned from established architectures and then can be\nreused to reduce the search space by discarding unpromising architectures.\nSearching in the refined search space can boost both the search performance and\nefficiency for new NAS tasks. To this end, a large language model\n(LLM)-assisted design principle transfer (LAPT) framework is devised. In LAPT,\nLLM is applied to automatically reason the design principles from a set of\ngiven architectures, and then a principle adaptation method is applied to\nrefine these principles progressively based on the new search results.\nExperimental results show that LAPT can beat the state-of-the-art TNAS methods\non most tasks and achieve comparable performance on others."
                },
                "authors": [
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Liang Feng"
                    },
                    {
                        "name": "Xingyu Wu"
                    },
                    {
                        "name": "Zhichao Lu"
                    },
                    {
                        "name": "Kay Chen Tan"
                    }
                ],
                "author_detail": {
                    "name": "Kay Chen Tan"
                },
                "author": "Kay Chen Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11326v1",
                "updated": "2024-08-21T04:19:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    19,
                    52,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T04:19:52Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    19,
                    52,
                    2,
                    234,
                    0
                ],
                "title": "Automating Thought of Search: A Journey Towards Soundness and\n  Completeness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Thought of Search: A Journey Towards Soundness and\n  Completeness"
                },
                "summary": "Planning remains one of the last standing bastions for large language models\n(LLMs), which now turn their attention to search. Most of the literature uses\nthe language models as world models to define the search space, forgoing\nsoundness for the sake of flexibility. A recent work, Thought of Search (ToS),\nproposed defining the search space with code, having the language models\nproduce that code. ToS requires a human in the loop, collaboratively producing\na sound successor function and goal test. The result, however, is worth the\neffort: all the tested datasets were solved with 100% accuracy. At the same\ntime LLMs have demonstrated significant progress in code generation and\nrefinement for complex reasoning tasks. In this work, we automate ToS\n(AutoToS), completely taking the human out of the loop of solving planning\nproblems. AutoToS guides the language model step by step towards the generation\nof sound and complete search components, through feedback from both generic and\ndomain specific unit tests. We achieve 100% accuracy, with minimal feedback\niterations, using LLMs of various sizes on all evaluated domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning remains one of the last standing bastions for large language models\n(LLMs), which now turn their attention to search. Most of the literature uses\nthe language models as world models to define the search space, forgoing\nsoundness for the sake of flexibility. A recent work, Thought of Search (ToS),\nproposed defining the search space with code, having the language models\nproduce that code. ToS requires a human in the loop, collaboratively producing\na sound successor function and goal test. The result, however, is worth the\neffort: all the tested datasets were solved with 100% accuracy. At the same\ntime LLMs have demonstrated significant progress in code generation and\nrefinement for complex reasoning tasks. In this work, we automate ToS\n(AutoToS), completely taking the human out of the loop of solving planning\nproblems. AutoToS guides the language model step by step towards the generation\nof sound and complete search components, through feedback from both generic and\ndomain specific unit tests. We achieve 100% accuracy, with minimal feedback\niterations, using LLMs of various sizes on all evaluated domains."
                },
                "authors": [
                    {
                        "name": "Daniel Cao"
                    },
                    {
                        "name": "Michael Katz"
                    },
                    {
                        "name": "Harsha Kokel"
                    },
                    {
                        "name": "Kavitha Srinivas"
                    },
                    {
                        "name": "Shirin Sohrabi"
                    }
                ],
                "author_detail": {
                    "name": "Shirin Sohrabi"
                },
                "author": "Shirin Sohrabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11324v1",
                "updated": "2024-08-21T04:14:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    14,
                    26,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T04:14:26Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    14,
                    26,
                    2,
                    234,
                    0
                ],
                "title": "HITS: High-coverage LLM-based Unit Test Generation via Method Slicing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HITS: High-coverage LLM-based Unit Test Generation via Method Slicing"
                },
                "summary": "Large language models (LLMs) have behaved well in generating unit tests for\nJava projects. However, the performance for covering the complex focal methods\nwithin the projects is poor. Complex methods comprise many conditions and\nloops, requiring the test cases to be various enough to cover all lines and\nbranches. However, existing test generation methods with LLMs provide the whole\nmethod-to-test to the LLM without assistance on input analysis. The LLM has\ndifficulty inferring the test inputs to cover all conditions, resulting in\nmissing lines and branches. To tackle the problem, we propose decomposing the\nfocal methods into slices and asking the LLM to generate test cases slice by\nslice. Our method simplifies the analysis scope, making it easier for the LLM\nto cover more lines and branches in each slice. We build a dataset comprising\ncomplex focal methods collected from the projects used by existing\nstate-of-the-art approaches. Our experiment results show that our method\nsignificantly outperforms current test case generation methods with LLMs and\nthe typical SBST method Evosuite regarding both line and branch coverage\nscores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have behaved well in generating unit tests for\nJava projects. However, the performance for covering the complex focal methods\nwithin the projects is poor. Complex methods comprise many conditions and\nloops, requiring the test cases to be various enough to cover all lines and\nbranches. However, existing test generation methods with LLMs provide the whole\nmethod-to-test to the LLM without assistance on input analysis. The LLM has\ndifficulty inferring the test inputs to cover all conditions, resulting in\nmissing lines and branches. To tackle the problem, we propose decomposing the\nfocal methods into slices and asking the LLM to generate test cases slice by\nslice. Our method simplifies the analysis scope, making it easier for the LLM\nto cover more lines and branches in each slice. We build a dataset comprising\ncomplex focal methods collected from the projects used by existing\nstate-of-the-art approaches. Our experiment results show that our method\nsignificantly outperforms current test case generation methods with LLMs and\nthe typical SBST method Evosuite regarding both line and branch coverage\nscores."
                },
                "authors": [
                    {
                        "name": "Zejun Wang"
                    },
                    {
                        "name": "Kaibo Liu"
                    },
                    {
                        "name": "Ge Li"
                    },
                    {
                        "name": "Zhi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Jin"
                },
                "author": "Zhi Jin",
                "arxiv_comment": "to be published in ASE 24' Research Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06537v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06537v4",
                "updated": "2024-08-21T04:03:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    3,
                    6,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-13T00:06:56Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    0,
                    6,
                    56,
                    1,
                    226,
                    0
                ],
                "title": "Introducing the NewsPaLM MBR and QE Dataset: LLM-Generated High-Quality\n  Parallel Data Outperforms Traditional Web-Crawled Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing the NewsPaLM MBR and QE Dataset: LLM-Generated High-Quality\n  Parallel Data Outperforms Traditional Web-Crawled Data"
                },
                "summary": "Recent research in neural machine translation (NMT) has shown that training\non high-quality machine-generated data can outperform training on\nhuman-generated data. This work accompanies the first-ever release of a\nLLM-generated, MBR-decoded and QE-reranked dataset with both sentence-level and\nmulti-sentence examples. We perform extensive experiments to demonstrate the\nquality of our dataset in terms of its downstream impact on NMT model\nperformance. We find that training from scratch on our (machine-generated)\ndataset outperforms training on the (web-crawled) WMT'23 training dataset\n(which is 300 times larger), and also outperforms training on the top-quality\nsubset of the WMT'23 training dataset. We also find that performing\nself-distillation by finetuning the LLM which generated this dataset\noutperforms the LLM's strong few-shot baseline. These findings corroborate the\nquality of our dataset, and demonstrate the value of high-quality\nmachine-generated data in improving performance of NMT models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research in neural machine translation (NMT) has shown that training\non high-quality machine-generated data can outperform training on\nhuman-generated data. This work accompanies the first-ever release of a\nLLM-generated, MBR-decoded and QE-reranked dataset with both sentence-level and\nmulti-sentence examples. We perform extensive experiments to demonstrate the\nquality of our dataset in terms of its downstream impact on NMT model\nperformance. We find that training from scratch on our (machine-generated)\ndataset outperforms training on the (web-crawled) WMT'23 training dataset\n(which is 300 times larger), and also outperforms training on the top-quality\nsubset of the WMT'23 training dataset. We also find that performing\nself-distillation by finetuning the LLM which generated this dataset\noutperforms the LLM's strong few-shot baseline. These findings corroborate the\nquality of our dataset, and demonstrate the value of high-quality\nmachine-generated data in improving performance of NMT models."
                },
                "authors": [
                    {
                        "name": "Mara Finkelstein"
                    },
                    {
                        "name": "David Vilar"
                    },
                    {
                        "name": "Markus Freitag"
                    }
                ],
                "author_detail": {
                    "name": "Markus Freitag"
                },
                "author": "Markus Freitag",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06537v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06537v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11319v1",
                "updated": "2024-08-21T03:59:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    3,
                    59,
                    51,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T03:59:51Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    3,
                    59,
                    51,
                    2,
                    234,
                    0
                ],
                "title": "Towards Evaluating Large Language Models on Sarcasm Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Evaluating Large Language Models on Sarcasm Understanding"
                },
                "summary": "In the era of large language models (LLMs), the task of ``System I''~-~the\nfast, unconscious, and intuitive tasks, e.g., sentiment analysis, text\nclassification, etc., have been argued to be successfully solved. However,\nsarcasm, as a subtle linguistic phenomenon, often employs rhetorical devices\nlike hyperbole and figuration to convey true sentiments and intentions,\ninvolving a higher level of abstraction than sentiment analysis. There is\ngrowing concern that the argument about LLMs' success may not be fully tenable\nwhen considering sarcasm understanding. To address this question, we select\neleven SOTA LLMs and eight SOTA pre-trained language models (PLMs) and present\ncomprehensive evaluations on six widely used benchmark datasets through\ndifferent prompting approaches, i.e., zero-shot input/output (IO) prompting,\nfew-shot IO prompting, chain of thought (CoT) prompting. Our results highlight\nthree key findings: (1) current LLMs underperform supervised PLMs based sarcasm\ndetection baselines across six sarcasm benchmarks. This suggests that\nsignificant efforts are still required to improve LLMs' understanding of human\nsarcasm. (2) GPT-4 consistently and significantly outperforms other LLMs across\nvarious prompting methods, with an average improvement of 14.0\\%$\\uparrow$.\nClaude 3 and ChatGPT demonstrate the next best performance after GPT-4. (3)\nFew-shot IO prompting method outperforms the other two methods: zero-shot IO\nand few-shot CoT. The reason is that sarcasm detection, being a holistic,\nintuitive, and non-rational cognitive process, is argued not to adhere to\nstep-by-step logical reasoning, making CoT less effective in understanding\nsarcasm compared to its effectiveness in mathematical reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of large language models (LLMs), the task of ``System I''~-~the\nfast, unconscious, and intuitive tasks, e.g., sentiment analysis, text\nclassification, etc., have been argued to be successfully solved. However,\nsarcasm, as a subtle linguistic phenomenon, often employs rhetorical devices\nlike hyperbole and figuration to convey true sentiments and intentions,\ninvolving a higher level of abstraction than sentiment analysis. There is\ngrowing concern that the argument about LLMs' success may not be fully tenable\nwhen considering sarcasm understanding. To address this question, we select\neleven SOTA LLMs and eight SOTA pre-trained language models (PLMs) and present\ncomprehensive evaluations on six widely used benchmark datasets through\ndifferent prompting approaches, i.e., zero-shot input/output (IO) prompting,\nfew-shot IO prompting, chain of thought (CoT) prompting. Our results highlight\nthree key findings: (1) current LLMs underperform supervised PLMs based sarcasm\ndetection baselines across six sarcasm benchmarks. This suggests that\nsignificant efforts are still required to improve LLMs' understanding of human\nsarcasm. (2) GPT-4 consistently and significantly outperforms other LLMs across\nvarious prompting methods, with an average improvement of 14.0\\%$\\uparrow$.\nClaude 3 and ChatGPT demonstrate the next best performance after GPT-4. (3)\nFew-shot IO prompting method outperforms the other two methods: zero-shot IO\nand few-shot CoT. The reason is that sarcasm detection, being a holistic,\nintuitive, and non-rational cognitive process, is argued not to adhere to\nstep-by-step logical reasoning, making CoT less effective in understanding\nsarcasm compared to its effectiveness in mathematical reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Yazhou Zhang"
                    },
                    {
                        "name": "Chunwang Zou"
                    },
                    {
                        "name": "Zheng Lian"
                    },
                    {
                        "name": "Prayag Tiwari"
                    },
                    {
                        "name": "Jing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Jing Qin"
                },
                "author": "Jing Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11316v1",
                "updated": "2024-08-21T03:47:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    3,
                    47,
                    17,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T03:47:17Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    3,
                    47,
                    17,
                    2,
                    234,
                    0
                ],
                "title": "Probabilistic Medical Predictions of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Medical Predictions of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant potential in\nclinical applications through prompt engineering, which enables the generation\nof flexible and diverse clinical predictions. However, they pose challenges in\nproducing prediction probabilities, which are essential for transparency and\nallowing clinicians to apply flexible probability thresholds in\ndecision-making. While explicit prompt instructions can lead LLMs to provide\nprediction probability numbers through text generation, LLMs' limitations in\nnumerical reasoning raise concerns about the reliability of these\ntext-generated probabilities. To assess this reliability, we compared explicit\nprobabilities derived from text generation to implicit probabilities calculated\nbased on the likelihood of predicting the correct label token. Experimenting\nwith six advanced open-source LLMs across five medical datasets, we found that\nthe performance of explicit probabilities was consistently lower than implicit\nprobabilities with respect to discrimination, precision, and recall. Moreover,\nthese differences were enlarged on small LLMs and imbalanced datasets,\nemphasizing the need for cautious interpretation and applications, as well as\nfurther research into robust probability estimation methods for LLMs in\nclinical contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant potential in\nclinical applications through prompt engineering, which enables the generation\nof flexible and diverse clinical predictions. However, they pose challenges in\nproducing prediction probabilities, which are essential for transparency and\nallowing clinicians to apply flexible probability thresholds in\ndecision-making. While explicit prompt instructions can lead LLMs to provide\nprediction probability numbers through text generation, LLMs' limitations in\nnumerical reasoning raise concerns about the reliability of these\ntext-generated probabilities. To assess this reliability, we compared explicit\nprobabilities derived from text generation to implicit probabilities calculated\nbased on the likelihood of predicting the correct label token. Experimenting\nwith six advanced open-source LLMs across five medical datasets, we found that\nthe performance of explicit probabilities was consistently lower than implicit\nprobabilities with respect to discrimination, precision, and recall. Moreover,\nthese differences were enlarged on small LLMs and imbalanced datasets,\nemphasizing the need for cautious interpretation and applications, as well as\nfurther research into robust probability estimation methods for LLMs in\nclinical contexts."
                },
                "authors": [
                    {
                        "name": "Bowen Gu"
                    },
                    {
                        "name": "Rishi J. Desai"
                    },
                    {
                        "name": "Kueiyu Joshua Lin"
                    },
                    {
                        "name": "Jie Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Yang"
                },
                "author": "Jie Yang",
                "arxiv_comment": "58 pages, 3 figures, 3 tables, Submitted to Nature Communication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11313v1",
                "updated": "2024-08-21T03:35:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    3,
                    35,
                    24,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T03:35:24Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    3,
                    35,
                    24,
                    2,
                    234,
                    0
                ],
                "title": "Unlocking Adversarial Suffix Optimization Without Affirmative Phrases:\n  Efficient Black-box Jailbreaking via LLM as Optimizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Adversarial Suffix Optimization Without Affirmative Phrases:\n  Efficient Black-box Jailbreaking via LLM as Optimizer"
                },
                "summary": "Despite prior safety alignment efforts, mainstream LLMs can still generate\nharmful and unethical content when subjected to jailbreaking attacks. Existing\njailbreaking methods fall into two main categories: template-based and\noptimization-based methods. The former requires significant manual effort and\ndomain knowledge, while the latter, exemplified by Greedy Coordinate Gradient\n(GCG), which seeks to maximize the likelihood of harmful LLM outputs through\ntoken-level optimization, also encounters several limitations: requiring\nwhite-box access, necessitating pre-constructed affirmative phrase, and\nsuffering from low efficiency. In this paper, we present ECLIPSE, a novel and\nefficient black-box jailbreaking method utilizing optimizable suffixes. Drawing\ninspiration from LLMs' powerful generation and optimization capabilities, we\nemploy task prompts to translate jailbreaking goals into natural language\ninstructions. This guides the LLM to generate adversarial suffixes for\nmalicious queries. In particular, a harmfulness scorer provides continuous\nfeedback, enabling LLM self-reflection and iterative optimization to\nautonomously and efficiently produce effective suffixes. Experimental results\ndemonstrate that ECLIPSE achieves an average attack success rate (ASR) of 0.92\nacross three open-source LLMs and GPT-3.5-Turbo, significantly surpassing GCG\nin 2.4 times. Moreover, ECLIPSE is on par with template-based methods in ASR\nwhile offering superior attack efficiency, reducing the average attack overhead\nby 83%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite prior safety alignment efforts, mainstream LLMs can still generate\nharmful and unethical content when subjected to jailbreaking attacks. Existing\njailbreaking methods fall into two main categories: template-based and\noptimization-based methods. The former requires significant manual effort and\ndomain knowledge, while the latter, exemplified by Greedy Coordinate Gradient\n(GCG), which seeks to maximize the likelihood of harmful LLM outputs through\ntoken-level optimization, also encounters several limitations: requiring\nwhite-box access, necessitating pre-constructed affirmative phrase, and\nsuffering from low efficiency. In this paper, we present ECLIPSE, a novel and\nefficient black-box jailbreaking method utilizing optimizable suffixes. Drawing\ninspiration from LLMs' powerful generation and optimization capabilities, we\nemploy task prompts to translate jailbreaking goals into natural language\ninstructions. This guides the LLM to generate adversarial suffixes for\nmalicious queries. In particular, a harmfulness scorer provides continuous\nfeedback, enabling LLM self-reflection and iterative optimization to\nautonomously and efficiently produce effective suffixes. Experimental results\ndemonstrate that ECLIPSE achieves an average attack success rate (ASR) of 0.92\nacross three open-source LLMs and GPT-3.5-Turbo, significantly surpassing GCG\nin 2.4 times. Moreover, ECLIPSE is on par with template-based methods in ASR\nwhile offering superior attack efficiency, reducing the average attack overhead\nby 83%."
                },
                "authors": [
                    {
                        "name": "Weipeng Jiang"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Juan Zhai"
                    },
                    {
                        "name": "Shiqing Ma"
                    },
                    {
                        "name": "Zhengyu Zhao"
                    },
                    {
                        "name": "Chao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chao Shen"
                },
                "author": "Chao Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10903v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10903v2",
                "updated": "2024-08-21T03:31:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    3,
                    31,
                    25,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-20T14:47:38Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    47,
                    38,
                    1,
                    233,
                    0
                ],
                "title": "BEYOND DIALOGUE: A Profile-Dialogue Alignment Framework Towards General\n  Role-Playing Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEYOND DIALOGUE: A Profile-Dialogue Alignment Framework Towards General\n  Role-Playing Language Model"
                },
                "summary": "The rapid advancement of large language models (LLMs) has revolutionized\nrole-playing, enabling the development of general role-playing models. However,\ncurrent role-playing training has two significant issues: (I) Using a\npredefined role profile to prompt dialogue training for specific scenarios\nusually leads to inconsistencies and even conflicts between the dialogue and\nthe profile, resulting in training biases. (II) The model learns to imitate the\nrole based solely on the profile, neglecting profile-dialogue alignment at the\nsentence level. In this work, we propose a simple yet effective framework\ncalled BEYOND DIALOGUE, designed to overcome these hurdles. This framework\ninnovatively introduces \"beyond dialogue\" tasks to align dialogue with profile\ntraits based on each specific scenario, thereby eliminating biases during\ntraining. Furthermore, by adopting an innovative prompting mechanism that\ngenerates reasoning outcomes for training, the framework allows the model to\nachieve fine-grained alignment between profile and dialogue at the sentence\nlevel. The aforementioned methods are fully automated and low-cost.\nAdditionally, the integration of automated dialogue and objective evaluation\nmethods forms a comprehensive framework, paving the way for general\nrole-playing. Experimental results demonstrate that our model excels in\nadhering to and reflecting various dimensions of role profiles, outperforming\nmost proprietary general and specialized role-playing baselines. All code and\ndatasets are available at https://github.com/yuyouyu32/BeyondDialogue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has revolutionized\nrole-playing, enabling the development of general role-playing models. However,\ncurrent role-playing training has two significant issues: (I) Using a\npredefined role profile to prompt dialogue training for specific scenarios\nusually leads to inconsistencies and even conflicts between the dialogue and\nthe profile, resulting in training biases. (II) The model learns to imitate the\nrole based solely on the profile, neglecting profile-dialogue alignment at the\nsentence level. In this work, we propose a simple yet effective framework\ncalled BEYOND DIALOGUE, designed to overcome these hurdles. This framework\ninnovatively introduces \"beyond dialogue\" tasks to align dialogue with profile\ntraits based on each specific scenario, thereby eliminating biases during\ntraining. Furthermore, by adopting an innovative prompting mechanism that\ngenerates reasoning outcomes for training, the framework allows the model to\nachieve fine-grained alignment between profile and dialogue at the sentence\nlevel. The aforementioned methods are fully automated and low-cost.\nAdditionally, the integration of automated dialogue and objective evaluation\nmethods forms a comprehensive framework, paving the way for general\nrole-playing. Experimental results demonstrate that our model excels in\nadhering to and reflecting various dimensions of role profiles, outperforming\nmost proprietary general and specialized role-playing baselines. All code and\ndatasets are available at https://github.com/yuyouyu32/BeyondDialogue."
                },
                "authors": [
                    {
                        "name": "Yeyong Yu"
                    },
                    {
                        "name": "Rusheng Yu"
                    },
                    {
                        "name": "Haojie Wei"
                    },
                    {
                        "name": "Zhanqiu Zhang"
                    },
                    {
                        "name": "Quan Qian"
                    }
                ],
                "author_detail": {
                    "name": "Quan Qian"
                },
                "author": "Quan Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10903v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10903v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.11911v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.11911v5",
                "updated": "2024-08-21T03:26:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    3,
                    26,
                    51,
                    2,
                    234,
                    0
                ],
                "published": "2023-09-21T09:22:07Z",
                "published_parsed": [
                    2023,
                    9,
                    21,
                    9,
                    22,
                    7,
                    3,
                    264,
                    0
                ],
                "title": "InstructERC: Reforming Emotion Recognition in Conversation with\n  Multi-task Retrieval-Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstructERC: Reforming Emotion Recognition in Conversation with\n  Multi-task Retrieval-Augmented Large Language Models"
                },
                "summary": "The field of emotion recognition of conversation (ERC) has been focusing on\nseparating sentence feature encoding and context modeling, lacking exploration\nin generative paradigms based on unified designs. In this study, we propose a\nnovel approach, InstructERC, to reformulate the ERC task from a discriminative\nframework to a generative framework based on Large Language Models (LLMs).\nInstructERC makes three significant contributions: (1) it introduces a simple\nyet effective retrieval template module, which helps the model explicitly\nintegrate multi-granularity dialogue supervision information. (2) We introduce\ntwo additional emotion alignment tasks, namely speaker identification and\nemotion prediction tasks, to implicitly model the dialogue role relationships\nand future emotional tendencies in conversations. (3) Pioneeringly, we unify\nemotion labels across benchmarks through the feeling wheel to fit real\napplication scenarios. InstructERC still perform impressively on this unified\ndataset. Our LLM-based plugin framework significantly outperforms all previous\nmodels and achieves comprehensive SOTA on three commonly used ERC datasets.\nExtensive analysis of parameter-efficient and data-scaling experiments provides\nempirical guidance for applying it in practical scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of emotion recognition of conversation (ERC) has been focusing on\nseparating sentence feature encoding and context modeling, lacking exploration\nin generative paradigms based on unified designs. In this study, we propose a\nnovel approach, InstructERC, to reformulate the ERC task from a discriminative\nframework to a generative framework based on Large Language Models (LLMs).\nInstructERC makes three significant contributions: (1) it introduces a simple\nyet effective retrieval template module, which helps the model explicitly\nintegrate multi-granularity dialogue supervision information. (2) We introduce\ntwo additional emotion alignment tasks, namely speaker identification and\nemotion prediction tasks, to implicitly model the dialogue role relationships\nand future emotional tendencies in conversations. (3) Pioneeringly, we unify\nemotion labels across benchmarks through the feeling wheel to fit real\napplication scenarios. InstructERC still perform impressively on this unified\ndataset. Our LLM-based plugin framework significantly outperforms all previous\nmodels and achieves comprehensive SOTA on three commonly used ERC datasets.\nExtensive analysis of parameter-efficient and data-scaling experiments provides\nempirical guidance for applying it in practical scenarios."
                },
                "authors": [
                    {
                        "name": "Shanglin Lei"
                    },
                    {
                        "name": "Guanting Dong"
                    },
                    {
                        "name": "Xiaoping Wang"
                    },
                    {
                        "name": "Keheng Wang"
                    },
                    {
                        "name": "Sirui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Sirui Wang"
                },
                "author": "Sirui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.11911v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.11911v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11309v1",
                "updated": "2024-08-21T03:26:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    3,
                    26,
                    16,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T03:26:16Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    3,
                    26,
                    16,
                    2,
                    234,
                    0
                ],
                "title": "Improving Out-of-Distribution Data Handling and Corruption Resistance\n  via Modern Hopfield Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Out-of-Distribution Data Handling and Corruption Resistance\n  via Modern Hopfield Networks"
                },
                "summary": "This study explores the potential of Modern Hopfield Networks (MHN) in\nimproving the ability of computer vision models to handle out-of-distribution\ndata. While current computer vision models can generalize to unseen samples\nfrom the same distribution, they are susceptible to minor perturbations such as\nblurring, which limits their effectiveness in real-world applications. We\nsuggest integrating MHN into the baseline models to enhance their robustness.\nThis integration can be implemented during the test time for any model and\ncombined with any adversarial defense method. Our research shows that the\nproposed integration consistently improves model performance on the MNIST-C\ndataset, achieving a state-of-the-art increase of 13.84% in average corruption\naccuracy, a 57.49% decrease in mean Corruption Error (mCE), and a 60.61%\ndecrease in relative mCE compared to the baseline model. Additionally, we\ninvestigate the capability of MHN to converge to the original non-corrupted\ndata. Notably, our method does not require test-time adaptation or augmentation\nwith corruptions, underscoring its practical viability for real-world\ndeployment. (Source code publicly available at:\nhttps://github.com/salehsargolzaee/Hopfield-integrated-test)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the potential of Modern Hopfield Networks (MHN) in\nimproving the ability of computer vision models to handle out-of-distribution\ndata. While current computer vision models can generalize to unseen samples\nfrom the same distribution, they are susceptible to minor perturbations such as\nblurring, which limits their effectiveness in real-world applications. We\nsuggest integrating MHN into the baseline models to enhance their robustness.\nThis integration can be implemented during the test time for any model and\ncombined with any adversarial defense method. Our research shows that the\nproposed integration consistently improves model performance on the MNIST-C\ndataset, achieving a state-of-the-art increase of 13.84% in average corruption\naccuracy, a 57.49% decrease in mean Corruption Error (mCE), and a 60.61%\ndecrease in relative mCE compared to the baseline model. Additionally, we\ninvestigate the capability of MHN to converge to the original non-corrupted\ndata. Notably, our method does not require test-time adaptation or augmentation\nwith corruptions, underscoring its practical viability for real-world\ndeployment. (Source code publicly available at:\nhttps://github.com/salehsargolzaee/Hopfield-integrated-test)"
                },
                "authors": [
                    {
                        "name": "Saleh Sargolzaei"
                    },
                    {
                        "name": "Luis Rueda"
                    }
                ],
                "author_detail": {
                    "name": "Luis Rueda"
                },
                "author": "Luis Rueda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11308v1",
                "updated": "2024-08-21T03:25:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    3,
                    25,
                    31,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T03:25:31Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    3,
                    25,
                    31,
                    2,
                    234,
                    0
                ],
                "title": "EEG-Defender: Defending against Jailbreak through Early Exit Generation\n  of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEG-Defender: Defending against Jailbreak through Early Exit Generation\n  of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly attracting attention in various\napplications. Nonetheless, there is a growing concern as some users attempt to\nexploit these models for malicious purposes, including the synthesis of\ncontrolled substances and the propagation of disinformation. In an effort to\nmitigate such risks, the concept of \"Alignment\" technology has been developed.\nHowever, recent studies indicate that this alignment can be undermined using\nsophisticated prompt engineering or adversarial suffixes, a technique known as\n\"Jailbreak.\" Our research takes cues from the human-like generate process of\nLLMs. We identify that while jailbreaking prompts may yield output logits\nsimilar to benign prompts, their initial embeddings within the model's latent\nspace tend to be more analogous to those of malicious prompts. Leveraging this\nfinding, we propose utilizing the early transformer outputs of LLMs as a means\nto detect malicious inputs, and terminate the generation immediately. Built\nupon this idea, we introduce a simple yet significant defense approach called\nEEG-Defender for LLMs. We conduct comprehensive experiments on ten jailbreak\nmethods across three models. Our results demonstrate that EEG-Defender is\ncapable of reducing the Attack Success Rate (ASR) by a significant margin,\nroughly 85\\% in comparison with 50\\% for the present SOTAs, with minimal impact\non the utility and effectiveness of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly attracting attention in various\napplications. Nonetheless, there is a growing concern as some users attempt to\nexploit these models for malicious purposes, including the synthesis of\ncontrolled substances and the propagation of disinformation. In an effort to\nmitigate such risks, the concept of \"Alignment\" technology has been developed.\nHowever, recent studies indicate that this alignment can be undermined using\nsophisticated prompt engineering or adversarial suffixes, a technique known as\n\"Jailbreak.\" Our research takes cues from the human-like generate process of\nLLMs. We identify that while jailbreaking prompts may yield output logits\nsimilar to benign prompts, their initial embeddings within the model's latent\nspace tend to be more analogous to those of malicious prompts. Leveraging this\nfinding, we propose utilizing the early transformer outputs of LLMs as a means\nto detect malicious inputs, and terminate the generation immediately. Built\nupon this idea, we introduce a simple yet significant defense approach called\nEEG-Defender for LLMs. We conduct comprehensive experiments on ten jailbreak\nmethods across three models. Our results demonstrate that EEG-Defender is\ncapable of reducing the Attack Success Rate (ASR) by a significant margin,\nroughly 85\\% in comparison with 50\\% for the present SOTAs, with minimal impact\non the utility and effectiveness of LLMs."
                },
                "authors": [
                    {
                        "name": "Chongwen Zhao"
                    },
                    {
                        "name": "Zhihao Dou"
                    },
                    {
                        "name": "Kaizhu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaizhu Huang"
                },
                "author": "Kaizhu Huang",
                "arxiv_comment": "19 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11305v1",
                "updated": "2024-08-21T03:17:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    3,
                    17,
                    20,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T03:17:20Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    3,
                    17,
                    20,
                    2,
                    234,
                    0
                ],
                "title": "UniFashion: A Unified Vision-Language Model for Multimodal Fashion\n  Retrieval and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniFashion: A Unified Vision-Language Model for Multimodal Fashion\n  Retrieval and Generation"
                },
                "summary": "The fashion domain encompasses a variety of real-world multimodal tasks,\nincluding multimodal retrieval and multimodal generation. The rapid\nadvancements in artificial intelligence generated content, particularly in\ntechnologies like large language models for text generation and diffusion\nmodels for visual generation, have sparked widespread research interest in\napplying these multimodal models in the fashion domain. However, tasks\ninvolving embeddings, such as image-to-text or text-to-image retrieval, have\nbeen largely overlooked from this perspective due to the diverse nature of the\nmultimodal fashion domain. And current research on multi-task single models\nlack focus on image generation. In this work, we present UniFashion, a unified\nframework that simultaneously tackles the challenges of multimodal generation\nand retrieval tasks within the fashion domain, integrating image generation\nwith retrieval tasks and text generation tasks. UniFashion unifies embedding\nand generative tasks by integrating a diffusion model and LLM, enabling\ncontrollable and high-fidelity generation. Our model significantly outperforms\nprevious single-task state-of-the-art models across diverse fashion tasks, and\ncan be readily adapted to manage complex vision-language tasks. This work\ndemonstrates the potential learning synergy between multimodal generation and\nretrieval, offering a promising direction for future research in the fashion\ndomain. The source code is available at\nhttps://github.com/xiangyu-mm/UniFashion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fashion domain encompasses a variety of real-world multimodal tasks,\nincluding multimodal retrieval and multimodal generation. The rapid\nadvancements in artificial intelligence generated content, particularly in\ntechnologies like large language models for text generation and diffusion\nmodels for visual generation, have sparked widespread research interest in\napplying these multimodal models in the fashion domain. However, tasks\ninvolving embeddings, such as image-to-text or text-to-image retrieval, have\nbeen largely overlooked from this perspective due to the diverse nature of the\nmultimodal fashion domain. And current research on multi-task single models\nlack focus on image generation. In this work, we present UniFashion, a unified\nframework that simultaneously tackles the challenges of multimodal generation\nand retrieval tasks within the fashion domain, integrating image generation\nwith retrieval tasks and text generation tasks. UniFashion unifies embedding\nand generative tasks by integrating a diffusion model and LLM, enabling\ncontrollable and high-fidelity generation. Our model significantly outperforms\nprevious single-task state-of-the-art models across diverse fashion tasks, and\ncan be readily adapted to manage complex vision-language tasks. This work\ndemonstrates the potential learning synergy between multimodal generation and\nretrieval, offering a promising direction for future research in the fashion\ndomain. The source code is available at\nhttps://github.com/xiangyu-mm/UniFashion."
                },
                "authors": [
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Yuehan Zhang"
                    },
                    {
                        "name": "Wenlong Zhang"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11304v1",
                "updated": "2024-08-21T03:16:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    3,
                    16,
                    12,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T03:16:12Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    3,
                    16,
                    12,
                    2,
                    234,
                    0
                ],
                "title": "FedMoE: Personalized Federated Learning via Heterogeneous Mixture of\n  Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedMoE: Personalized Federated Learning via Heterogeneous Mixture of\n  Experts"
                },
                "summary": "As Large Language Models (LLMs) push the boundaries of AI capabilities, their\ndemand for data is growing. Much of this data is private and distributed across\nedge devices, making Federated Learning (FL) a de-facto alternative for\nfine-tuning (i.e., FedLLM). However, it faces significant challenges due to the\ninherent heterogeneity among clients, including varying data distributions and\ndiverse task types. Towards a versatile FedLLM, we replace traditional dense\nmodel with a sparsely-activated Mixture-of-Experts (MoE) architecture, whose\nparallel feed-forward networks enable greater flexibility. To make it more\npractical in resource-constrained environments, we present FedMoE, the\nefficient personalized FL framework to address data heterogeneity, constructing\nan optimal sub-MoE for each client and bringing the knowledge back to global\nMoE. FedMoE is composed of two fine-tuning stages. In the first stage, FedMoE\nsimplifies the problem by conducting a heuristic search based on observed\nactivation patterns, which identifies a suboptimal submodel for each client. In\nthe second stage, these submodels are distributed to clients for further\ntraining and returned for server aggregating through a novel modular\naggregation strategy. Meanwhile, FedMoE progressively adjusts the submodels to\noptimal through global expert recommendation. Experimental results demonstrate\nthe superiority of our method over previous personalized FL methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) push the boundaries of AI capabilities, their\ndemand for data is growing. Much of this data is private and distributed across\nedge devices, making Federated Learning (FL) a de-facto alternative for\nfine-tuning (i.e., FedLLM). However, it faces significant challenges due to the\ninherent heterogeneity among clients, including varying data distributions and\ndiverse task types. Towards a versatile FedLLM, we replace traditional dense\nmodel with a sparsely-activated Mixture-of-Experts (MoE) architecture, whose\nparallel feed-forward networks enable greater flexibility. To make it more\npractical in resource-constrained environments, we present FedMoE, the\nefficient personalized FL framework to address data heterogeneity, constructing\nan optimal sub-MoE for each client and bringing the knowledge back to global\nMoE. FedMoE is composed of two fine-tuning stages. In the first stage, FedMoE\nsimplifies the problem by conducting a heuristic search based on observed\nactivation patterns, which identifies a suboptimal submodel for each client. In\nthe second stage, these submodels are distributed to clients for further\ntraining and returned for server aggregating through a novel modular\naggregation strategy. Meanwhile, FedMoE progressively adjusts the submodels to\noptimal through global expert recommendation. Experimental results demonstrate\nthe superiority of our method over previous personalized FL methods."
                },
                "authors": [
                    {
                        "name": "Hanzi Mei"
                    },
                    {
                        "name": "Dongqi Cai"
                    },
                    {
                        "name": "Ao Zhou"
                    },
                    {
                        "name": "Shangguang Wang"
                    },
                    {
                        "name": "Mengwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Mengwei Xu"
                },
                "author": "Mengwei Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.09669v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.09669v5",
                "updated": "2024-08-21T03:04:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    3,
                    4,
                    27,
                    2,
                    234,
                    0
                ],
                "published": "2023-12-15T10:30:36Z",
                "published_parsed": [
                    2023,
                    12,
                    15,
                    10,
                    30,
                    36,
                    4,
                    349,
                    0
                ],
                "title": "Silent Guardian: Protecting Text from Malicious Exploitation by Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Silent Guardian: Protecting Text from Malicious Exploitation by Large\n  Language Models"
                },
                "summary": "The rapid development of large language models (LLMs) has yielded impressive\nsuccess in various downstream tasks. However, the vast potential and remarkable\ncapabilities of LLMs also raise new security and privacy concerns if they are\nexploited for nefarious purposes due to their open-endedness. For example, LLMs\nmay be used to plagiarize or imitate writing, thereby infringing the copyright\nof the original content, or to create indiscriminate fake information based on\na certain source text. In some cases, LLMs can even analyze text from the\nInternet to infer personal privacy. Unfortunately, previous text protection\nresearch could not foresee the emergence of powerful LLMs, rendering it no\nlonger effective in this new context. To bridge this gap, we introduce Silent\nGuardian (SG), a text protection mechanism against LLMs, which allows LLMs to\nrefuse to generate response when receiving protected text, preventing the\nmalicious use of text from the source. Specifically, we first propose the\nconcept of Truncation Protection Examples (TPE). By carefully modifying the\ntext to be protected, TPE can induce LLMs to first sample the end token, thus\ndirectly terminating the interaction. In addition, to efficiently construct TPE\nin the discrete space of text data, we propose a novel optimization algorithm\ncalled Super Tailored Protection (STP), which is not only highly efficient but\nalso maintains the semantic consistency of the text during the optimization\nprocess. The comprehensive experimental evaluation demonstrates that SG can\neffectively protect the target text under various configurations and achieve\nalmost 100% protection success rate in some cases. Notably, SG also exhibits\nrelatively good transferability and robustness, making its application in\npractical scenarios possible. Our code is available at\nhttps://github.com/weiyezhimeng/Silent-Guardian.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) has yielded impressive\nsuccess in various downstream tasks. However, the vast potential and remarkable\ncapabilities of LLMs also raise new security and privacy concerns if they are\nexploited for nefarious purposes due to their open-endedness. For example, LLMs\nmay be used to plagiarize or imitate writing, thereby infringing the copyright\nof the original content, or to create indiscriminate fake information based on\na certain source text. In some cases, LLMs can even analyze text from the\nInternet to infer personal privacy. Unfortunately, previous text protection\nresearch could not foresee the emergence of powerful LLMs, rendering it no\nlonger effective in this new context. To bridge this gap, we introduce Silent\nGuardian (SG), a text protection mechanism against LLMs, which allows LLMs to\nrefuse to generate response when receiving protected text, preventing the\nmalicious use of text from the source. Specifically, we first propose the\nconcept of Truncation Protection Examples (TPE). By carefully modifying the\ntext to be protected, TPE can induce LLMs to first sample the end token, thus\ndirectly terminating the interaction. In addition, to efficiently construct TPE\nin the discrete space of text data, we propose a novel optimization algorithm\ncalled Super Tailored Protection (STP), which is not only highly efficient but\nalso maintains the semantic consistency of the text during the optimization\nprocess. The comprehensive experimental evaluation demonstrates that SG can\neffectively protect the target text under various configurations and achieve\nalmost 100% protection success rate in some cases. Notably, SG also exhibits\nrelatively good transferability and robustness, making its application in\npractical scenarios possible. Our code is available at\nhttps://github.com/weiyezhimeng/Silent-Guardian."
                },
                "authors": [
                    {
                        "name": "Jiawei Zhao"
                    },
                    {
                        "name": "Kejiang Chen"
                    },
                    {
                        "name": "Xiaojian Yuan"
                    },
                    {
                        "name": "Yuang Qi"
                    },
                    {
                        "name": "Weiming Zhang"
                    },
                    {
                        "name": "Nenghai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Nenghai Yu"
                },
                "author": "Nenghai Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.09669v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.09669v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20775v2",
                "updated": "2024-08-21T02:56:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    2,
                    56,
                    47,
                    2,
                    234,
                    0
                ],
                "published": "2024-05-26T19:11:21Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    19,
                    11,
                    21,
                    6,
                    147,
                    0
                ],
                "title": "Medical MLLM is Vulnerable: Cross-Modality Jailbreak and Mismatched\n  Attacks on Medical Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical MLLM is Vulnerable: Cross-Modality Jailbreak and Mismatched\n  Attacks on Medical Multimodal Large Language Models"
                },
                "summary": "Security concerns related to Large Language Models (LLMs) have been\nextensively explored, yet the safety implications for Multimodal Large Language\nModels (MLLMs), particularly in medical contexts (MedMLLMs), remain\ninsufficiently studied. This paper delves into the underexplored security\nvulnerabilities of MedMLLMs, especially when deployed in clinical environments\nwhere the accuracy and relevance of question-and-answer interactions are\ncritically tested against complex medical challenges. By combining existing\nclinical medical data with atypical natural phenomena, we define the mismatched\nmalicious attack (2M-attack) and introduce its optimized version, known as the\noptimized mismatched malicious attack (O2M-attack or 2M-optimization). Using\nthe voluminous 3MAD dataset that we construct, which covers a wide range of\nmedical image modalities and harmful medical scenarios, we conduct a\ncomprehensive analysis and propose the MCM optimization method, which\nsignificantly enhances the attack success rate on MedMLLMs. Evaluations with\nthis dataset and attack methods, including white-box attacks on LLaVA-Med and\ntransfer attacks (black-box) on four other SOTA models, indicate that even\nMedMLLMs designed with enhanced security features remain vulnerable to security\nbreaches. Our work underscores the urgent need for a concerted effort to\nimplement robust security measures and enhance the safety and efficacy of\nopen-source MedMLLMs, particularly given the potential severity of jailbreak\nattacks and other malicious or clinically significant exploits in medical\nsettings. Our code is available at https://github.com/dirtycomputer/O2M_attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security concerns related to Large Language Models (LLMs) have been\nextensively explored, yet the safety implications for Multimodal Large Language\nModels (MLLMs), particularly in medical contexts (MedMLLMs), remain\ninsufficiently studied. This paper delves into the underexplored security\nvulnerabilities of MedMLLMs, especially when deployed in clinical environments\nwhere the accuracy and relevance of question-and-answer interactions are\ncritically tested against complex medical challenges. By combining existing\nclinical medical data with atypical natural phenomena, we define the mismatched\nmalicious attack (2M-attack) and introduce its optimized version, known as the\noptimized mismatched malicious attack (O2M-attack or 2M-optimization). Using\nthe voluminous 3MAD dataset that we construct, which covers a wide range of\nmedical image modalities and harmful medical scenarios, we conduct a\ncomprehensive analysis and propose the MCM optimization method, which\nsignificantly enhances the attack success rate on MedMLLMs. Evaluations with\nthis dataset and attack methods, including white-box attacks on LLaVA-Med and\ntransfer attacks (black-box) on four other SOTA models, indicate that even\nMedMLLMs designed with enhanced security features remain vulnerable to security\nbreaches. Our work underscores the urgent need for a concerted effort to\nimplement robust security measures and enhance the safety and efficacy of\nopen-source MedMLLMs, particularly given the potential severity of jailbreak\nattacks and other malicious or clinically significant exploits in medical\nsettings. Our code is available at https://github.com/dirtycomputer/O2M_attack."
                },
                "authors": [
                    {
                        "name": "Xijie Huang"
                    },
                    {
                        "name": "Xinyuan Wang"
                    },
                    {
                        "name": "Hantao Zhang"
                    },
                    {
                        "name": "Yinghao Zhu"
                    },
                    {
                        "name": "Jiawen Xi"
                    },
                    {
                        "name": "Jingkun An"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Chengwei Pan"
                    }
                ],
                "author_detail": {
                    "name": "Chengwei Pan"
                },
                "author": "Chengwei Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11294v1",
                "updated": "2024-08-21T02:49:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    2,
                    49,
                    41,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T02:49:41Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    2,
                    49,
                    41,
                    2,
                    234,
                    0
                ],
                "title": "RedWhale: An Adapted Korean LLM Through Efficient Continual Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RedWhale: An Adapted Korean LLM Through Efficient Continual Pretraining"
                },
                "summary": "The field of Natural Language Processing (NLP) has seen significant\nadvancements with the development of Large Language Models (LLMs). However,\nmuch of this research remains focused on English, often overlooking\nlow-resource languages like Korean. This oversight presents challenges due to\nthe unique non-alphabetic token structure of Korean and the substantial memory\nand computational demands required for LLM training, which frequently lead to\nmemory constraints and out-of-memory errors. To address these issues, we\npresent RedWhale, a model specifically tailored for Korean language processing.\nRedWhale is developed using an efficient continual pretraining approach that\nincludes a comprehensive Korean corpus preprocessing pipeline, a specialized\ntokenizer, an optimized model initialization technique, and a multistage\npretraining strategy. These innovations collectively reduce training time and\ncomputational costs while maintaining high levels of accuracy and\ncomprehension. By leveraging cross-lingual transfer learning, which exploits\nshared linguistic similarities across languages, RedWhale builds on English\nmodels to enhance Korean language processing. Experimental results demonstrate\nthat RedWhale outperforms other leading models on Korean NLP benchmarks,\nincluding the Korean Balanced Evaluation of Significant Tasks (KoBEST), showing\nsuperior understanding and generation of Korean text. Furthermore, RedWhale\nshowed no signs of convergence even after pretraining on 9.7 billion tokens,\nindicating the potential for further improvements with additional training.\nThis work represents a significant advancement in bridging the linguistic\ndivide, particularly in enhancing NLP capabilities for the Korean language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of Natural Language Processing (NLP) has seen significant\nadvancements with the development of Large Language Models (LLMs). However,\nmuch of this research remains focused on English, often overlooking\nlow-resource languages like Korean. This oversight presents challenges due to\nthe unique non-alphabetic token structure of Korean and the substantial memory\nand computational demands required for LLM training, which frequently lead to\nmemory constraints and out-of-memory errors. To address these issues, we\npresent RedWhale, a model specifically tailored for Korean language processing.\nRedWhale is developed using an efficient continual pretraining approach that\nincludes a comprehensive Korean corpus preprocessing pipeline, a specialized\ntokenizer, an optimized model initialization technique, and a multistage\npretraining strategy. These innovations collectively reduce training time and\ncomputational costs while maintaining high levels of accuracy and\ncomprehension. By leveraging cross-lingual transfer learning, which exploits\nshared linguistic similarities across languages, RedWhale builds on English\nmodels to enhance Korean language processing. Experimental results demonstrate\nthat RedWhale outperforms other leading models on Korean NLP benchmarks,\nincluding the Korean Balanced Evaluation of Significant Tasks (KoBEST), showing\nsuperior understanding and generation of Korean text. Furthermore, RedWhale\nshowed no signs of convergence even after pretraining on 9.7 billion tokens,\nindicating the potential for further improvements with additional training.\nThis work represents a significant advancement in bridging the linguistic\ndivide, particularly in enhancing NLP capabilities for the Korean language."
                },
                "authors": [
                    {
                        "name": "Anh-Dung Vo"
                    },
                    {
                        "name": "Minseong Jung"
                    },
                    {
                        "name": "Wonbeen Lee"
                    },
                    {
                        "name": "Daewoo Choi"
                    }
                ],
                "author_detail": {
                    "name": "Daewoo Choi"
                },
                "author": "Daewoo Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07723v2",
                "updated": "2024-08-21T02:45:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    2,
                    45,
                    36,
                    2,
                    234,
                    0
                ],
                "published": "2024-06-24T03:58:11Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    58,
                    11,
                    0,
                    176,
                    0
                ],
                "title": "Understanding is Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding is Compression"
                },
                "summary": "Modern data compression methods are slowly reaching their limits after 80\nyears of research, millions of papers, and wide range of applications. Yet, the\nextravagant 6G communication speed requirement raises a major open question for\nrevolutionary new ideas of data compression.\n  We have previously shown all understanding or learning are compression, under\nreasonable assumptions. Large language models (LLMs) understand data better\nthan ever before. Can they help us to compress data?\n  The LLMs may be seen to approximate the uncomputable Solomonoff induction.\nTherefore, under this new uncomputable paradigm, we present LMCompress.\nLMCompress shatters all previous lossless compression algorithms, doubling the\nlossless compression ratios of JPEG-XL for images, FLAC for audios, and H.264\nfor videos, and quadrupling the compression ratio of bz2 for texts. The better\na large model understands the data, the better LMCompress compresses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern data compression methods are slowly reaching their limits after 80\nyears of research, millions of papers, and wide range of applications. Yet, the\nextravagant 6G communication speed requirement raises a major open question for\nrevolutionary new ideas of data compression.\n  We have previously shown all understanding or learning are compression, under\nreasonable assumptions. Large language models (LLMs) understand data better\nthan ever before. Can they help us to compress data?\n  The LLMs may be seen to approximate the uncomputable Solomonoff induction.\nTherefore, under this new uncomputable paradigm, we present LMCompress.\nLMCompress shatters all previous lossless compression algorithms, doubling the\nlossless compression ratios of JPEG-XL for images, FLAC for audios, and H.264\nfor videos, and quadrupling the compression ratio of bz2 for texts. The better\na large model understands the data, the better LMCompress compresses."
                },
                "authors": [
                    {
                        "name": "Ziguang Li"
                    },
                    {
                        "name": "Chao Huang"
                    },
                    {
                        "name": "Xuliang Wang"
                    },
                    {
                        "name": "Haibo Hu"
                    },
                    {
                        "name": "Cole Wyeth"
                    },
                    {
                        "name": "Dongbo Bu"
                    },
                    {
                        "name": "Quan Yu"
                    },
                    {
                        "name": "Wen Gao"
                    },
                    {
                        "name": "Xingwu Liu"
                    },
                    {
                        "name": "Ming Li"
                    }
                ],
                "author_detail": {
                    "name": "Ming Li"
                },
                "author": "Ming Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v3",
                "updated": "2024-08-21T02:32:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    2,
                    32,
                    43,
                    2,
                    234,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "Matrix quantization compresses matrix elements into a more compact form to\nreduce storage requirements, with dequantization enabling reconstruction for\nuse. We define the Quantization Error Minimization (QEM) problem as minimizing\nthe difference between the original and quantized matrices while ensuring the\nquantized matrix remains within fixed memory constraints. This technique is\ncrucial in applications like Large Language Model (LLM) weight compression and\nKV cache compression, where large matrix sizes demand efficient storage\nsolutions.\n  As modern LLMs like GPT-4 and BERT continue to grow, effective matrix\ncompression is increasingly important. These models contain billions of\nparameters in matrix form, making efficient weight quantization essential for\nboth storage and computational efficiency. Similarly, KV caches, storing\nintermediate inference results, are matrix-based and benefit significantly from\noptimized compression techniques.\n  To address the QEM problem in the context of LLM weight and KV cache\ncompression, we propose Quantum Entanglement Trees (QET). QET leverages the\nlocal structure of matrix elements by iteratively swapping elements to create a\nlocally ordered matrix, which is then grouped and quantized column by column.\nTo enhance QET, we introduce two optimizations: residual quantization to\nfurther reduce Mean Squared Error (MSE) and masking with batch processing to\naccelerate the algorithm.\n  Our experiments demonstrate that QET can reduce MSE to 12.3% of its original\nvalue at the same compression ratio, outperforming leading baseline methods.\nOur contributions include framing the QEM problem specifically for LLM and KV\ncache compression, developing the QET algorithm, and implementing optimizations\nthat improve accuracy and processing speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix quantization compresses matrix elements into a more compact form to\nreduce storage requirements, with dequantization enabling reconstruction for\nuse. We define the Quantization Error Minimization (QEM) problem as minimizing\nthe difference between the original and quantized matrices while ensuring the\nquantized matrix remains within fixed memory constraints. This technique is\ncrucial in applications like Large Language Model (LLM) weight compression and\nKV cache compression, where large matrix sizes demand efficient storage\nsolutions.\n  As modern LLMs like GPT-4 and BERT continue to grow, effective matrix\ncompression is increasingly important. These models contain billions of\nparameters in matrix form, making efficient weight quantization essential for\nboth storage and computational efficiency. Similarly, KV caches, storing\nintermediate inference results, are matrix-based and benefit significantly from\noptimized compression techniques.\n  To address the QEM problem in the context of LLM weight and KV cache\ncompression, we propose Quantum Entanglement Trees (QET). QET leverages the\nlocal structure of matrix elements by iteratively swapping elements to create a\nlocally ordered matrix, which is then grouped and quantized column by column.\nTo enhance QET, we introduce two optimizations: residual quantization to\nfurther reduce Mean Squared Error (MSE) and masking with batch processing to\naccelerate the algorithm.\n  Our experiments demonstrate that QET can reduce MSE to 12.3% of its original\nvalue at the same compression ratio, outperforming leading baseline methods.\nOur contributions include framing the QEM problem specifically for LLM and KV\ncache compression, developing the QET algorithm, and implementing optimizations\nthat improve accuracy and processing speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]