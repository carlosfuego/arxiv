[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2602.22142v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22142v1",
                "title": "WeaveTime: Stream from Earlier Frames into Emergent Memory in VideoLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeaveTime: Stream from Earlier Frames into Emergent Memory in VideoLLMs"
                },
                "updated": "2026-02-25T17:45:45Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    45,
                    45,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22142v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in Multimodal Large Language Models have greatly improved visual understanding and reasoning, yet their quadratic attention and offline training protocols make them ill-suited for streaming settings where frames arrive sequentially and future observations are inaccessible. We diagnose a core limitation of current Video-LLMs, namely Time-Agnosticism, in which videos are treated as an unordered bag of evidence rather than a causally ordered sequence, yielding two failures in streams: temporal order ambiguity, in which the model cannot follow or reason over the correct chronological order, and past-current focus blindness where it fails to distinguish present observations from accumulated history. We present WeaveTime, a simple, efficient, and model agnostic framework that first teaches order and then uses order. We introduce a lightweight Temporal Reconstruction objective-our Streaming Order Perception enhancement-that instills order aware representations with minimal finetuning and no specialized streaming data. At inference, a Past-Current Dynamic Focus Cache performs uncertainty triggered, coarse-to-fine retrieval, expanding history only when needed. Plugged into exsiting Video-LLM without architectural changes, WeaveTime delivers consistent gains on representative streaming benchmarks, improving accuracy while reducing latency. These results establish WeaveTime as a practical path toward time aware stream Video-LLMs under strict online, time causal constraints. Code and weights will be made publicly available. Project Page: https://zhangyl4.github.io/publications/weavetime/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Multimodal Large Language Models have greatly improved visual understanding and reasoning, yet their quadratic attention and offline training protocols make them ill-suited for streaming settings where frames arrive sequentially and future observations are inaccessible. We diagnose a core limitation of current Video-LLMs, namely Time-Agnosticism, in which videos are treated as an unordered bag of evidence rather than a causally ordered sequence, yielding two failures in streams: temporal order ambiguity, in which the model cannot follow or reason over the correct chronological order, and past-current focus blindness where it fails to distinguish present observations from accumulated history. We present WeaveTime, a simple, efficient, and model agnostic framework that first teaches order and then uses order. We introduce a lightweight Temporal Reconstruction objective-our Streaming Order Perception enhancement-that instills order aware representations with minimal finetuning and no specialized streaming data. At inference, a Past-Current Dynamic Focus Cache performs uncertainty triggered, coarse-to-fine retrieval, expanding history only when needed. Plugged into exsiting Video-LLM without architectural changes, WeaveTime delivers consistent gains on representative streaming benchmarks, improving accuracy while reducing latency. These results establish WeaveTime as a practical path toward time aware stream Video-LLMs under strict online, time causal constraints. Code and weights will be made publicly available. Project Page: https://zhangyl4.github.io/publications/weavetime/"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T17:45:45Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    45,
                    45,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "Accepted at CVPR 2026 (preview; camera-ready in preparation)",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yulin Zhang"
                    },
                    {
                        "name": "Cheng Shi"
                    },
                    {
                        "name": "Sibei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Sibei Yang"
                },
                "author": "Sibei Yang"
            },
            {
                "id": "http://arxiv.org/abs/2602.22087v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22087v1",
                "title": "Transmission Delay Minimization for NOMA-Based F-RANs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transmission Delay Minimization for NOMA-Based F-RANs"
                },
                "updated": "2026-02-25T16:35:19Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    35,
                    19,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22087v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/TWC.2026.3668116",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "A novel non-orthogonal multiple access (NOMA) based low-delay service framework is proposed for fog radio access networks (F-RANs). Fog access points (FAPs) leverage NOMA for local delivery of cached content, while the cloud access point employs NOMA to simultaneously push content to FAPs and directly serve users. Based on this model, a delay minimization problem is formulated by jointly optimizing user association, cache placement, and power allocation. To address this non-convex mixed-integer nonlinear programming problem, an alternating optimization (AO) algorithm is developed, which decomposes the original problem into two subproblems, namely joint user association and cache placement, and power allocation. In particular, a low-complexity algorithm is designed to optimizing the user association and cache placement strategy using the McCormick envelope theory and Lagrangian partial relaxation. The power allocation is optimized by invoking the successive convex approximation. Simulation results reveal that: 1) the proposed AO-based algorithm effectively balances between the achieved performance and computational efficiency, and 2) the proposed NOMA-based F-RANs framework significantly outperforms orthogonal multiple access-based F-RANs systems in terms of average transmission delay in different scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A novel non-orthogonal multiple access (NOMA) based low-delay service framework is proposed for fog radio access networks (F-RANs). Fog access points (FAPs) leverage NOMA for local delivery of cached content, while the cloud access point employs NOMA to simultaneously push content to FAPs and directly serve users. Based on this model, a delay minimization problem is formulated by jointly optimizing user association, cache placement, and power allocation. To address this non-convex mixed-integer nonlinear programming problem, an alternating optimization (AO) algorithm is developed, which decomposes the original problem into two subproblems, namely joint user association and cache placement, and power allocation. In particular, a low-complexity algorithm is designed to optimizing the user association and cache placement strategy using the McCormick envelope theory and Lagrangian partial relaxation. The power allocation is optimized by invoking the successive convex approximation. Simulation results reveal that: 1) the proposed AO-based algorithm effectively balances between the achieved performance and computational efficiency, and 2) the proposed NOMA-based F-RANs framework significantly outperforms orthogonal multiple access-based F-RANs systems in terms of average transmission delay in different scenarios."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T16:35:19Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    35,
                    19,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "Accepted by IEEE Transactions on Wireless Communications",
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "authors": [
                    {
                        "name": "Yuan Ai"
                    },
                    {
                        "name": "Xidong Mu"
                    },
                    {
                        "name": "Pengbo Si"
                    },
                    {
                        "name": "Yuanwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuanwei Liu"
                },
                "author": "Yuanwei Liu",
                "arxiv_doi": "10.1109/TWC.2026.3668116"
            },
            {
                "id": "http://arxiv.org/abs/2506.14734v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.14734v5",
                "title": "Compressing Suffix Trees by Path Decompositions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing Suffix Trees by Path Decompositions"
                },
                "updated": "2026-02-25T16:10:43Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    10,
                    43,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.14734v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.14734v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The suffix tree is arguably the most fundamental data structure on strings: introduced by Weiner (SWAT 1973) and McCreight (JACM 1976), it allows solving a myriad of computational problems on strings in linear time. Motivated by its large space usage, subsequent research focused first on reducing its size by a constant factor via Suffix Arrays, and later on reaching space proportional to the size of the compressed string. Modern compressed indexes, such as the $r$-index (Gagie et al., SODA 2018), fit in space proportional to $r$, the number of runs in the Burrows-Wheeler transform (a strong and universal repetitiveness measure). These advances, however, came with a price: while modern compressed indexes boast optimal bounds in the RAM model, they are often orders of magnitude slower than uncompressed counterparts in practice due to catastrophic cache locality. This reality gap highlights that Big-O complexity in the RAM model has become a misleading predictor of real-world performance, leaving a critical question unanswered: can we design compressed indexes that are efficient in the I/O model of computation?\n  We answer this in the affirmative by introducing a new Suffix Array sampling technique based on particular path decompositions of the suffix tree. We prove that sorting the suffix tree leaves by specific priority functions induces a decomposition where the number of distinct paths (each corresponding to a string suffix) is bounded by $r$. This allows us to solve indexed pattern matching efficiently in the I/O model using a Suffix Array sample of size at most $r$, strictly improving upon the (tight) $2r$ bound of Suffixient Arrays, another recent compressed Suffix Array sampling technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The suffix tree is arguably the most fundamental data structure on strings: introduced by Weiner (SWAT 1973) and McCreight (JACM 1976), it allows solving a myriad of computational problems on strings in linear time. Motivated by its large space usage, subsequent research focused first on reducing its size by a constant factor via Suffix Arrays, and later on reaching space proportional to the size of the compressed string. Modern compressed indexes, such as the $r$-index (Gagie et al., SODA 2018), fit in space proportional to $r$, the number of runs in the Burrows-Wheeler transform (a strong and universal repetitiveness measure). These advances, however, came with a price: while modern compressed indexes boast optimal bounds in the RAM model, they are often orders of magnitude slower than uncompressed counterparts in practice due to catastrophic cache locality. This reality gap highlights that Big-O complexity in the RAM model has become a misleading predictor of real-world performance, leaving a critical question unanswered: can we design compressed indexes that are efficient in the I/O model of computation?\n  We answer this in the affirmative by introducing a new Suffix Array sampling technique based on particular path decompositions of the suffix tree. We prove that sorting the suffix tree leaves by specific priority functions induces a decomposition where the number of distinct paths (each corresponding to a string suffix) is bounded by $r$. This allows us to solve indexed pattern matching efficiently in the I/O model using a Suffix Array sample of size at most $r$, strictly improving upon the (tight) $2r$ bound of Suffixient Arrays, another recent compressed Suffix Array sampling technique."
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-17T17:15:08Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    15,
                    8,
                    1,
                    168,
                    0
                ],
                "arxiv_comment": "Submitted version",
                "arxiv_primary_category": {
                    "term": "cs.DS"
                },
                "authors": [
                    {
                        "name": "Ruben Becker"
                    },
                    {
                        "name": "Davide Cenzato"
                    },
                    {
                        "name": "Travis Gagie"
                    },
                    {
                        "name": "Sung-Hwan Kim"
                    },
                    {
                        "name": "Ragnar Groot Koerkamp"
                    },
                    {
                        "name": "Giovanni Manzini"
                    },
                    {
                        "name": "Nicola Prezza"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Prezza"
                },
                "author": "Nicola Prezza"
            },
            {
                "id": "http://arxiv.org/abs/2602.19784v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19784v2",
                "title": "High-Altitude Platforms in the Low-Altitude Economy: Bridging Communication, Computing, and Regulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Altitude Platforms in the Low-Altitude Economy: Bridging Communication, Computing, and Regulation"
                },
                "updated": "2026-02-25T12:18:02Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    18,
                    2,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19784v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19784v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Low-Altitude Economy (LAE) is rapidly emerging as a new technological and industrial frontier, with unmanned aerial vehicles (UAVs), electric vertical takeoff and landing (eVTOL) aircraft, and aerial swarms increasingly deployed in logistics, infrastructure inspection, security, and emergency response. However, the large-scale development of the LAE demands a reliable aerial foundation that ensures not only real-time connectivity and computational support, but also navigation integrity and safe airspace management for safety-critical operations. High-Altitude Platforms (HAPs), positioned at around 20 km, provide a unique balance between wide-area coverage and low-latency responsiveness. Compared with low earth orbit (LEO) satellites, HAPs are closer to end users and thus capable of delivering millisecond-level connectivity, fine-grained regulatory oversight, and powerful onboard computing and caching resources. Beyond connectivity and computation, HAPs-assisted sensing and regulation further enable navigation integrity and airspace trust, which are essential for safety-critical UAV and eVTOL operations in the LAE. This article proposes a five-stage evolutionary roadmap for HAPs in the LAE: from serving as aerial infrastructure bases, to becoming super back-ends for UAV, to acting as frontline support for ground users, further enabling swarm-scale UAV coordination, and ultimately advancing toward edge-air-cloud closed-loop autonomy. In parallel, HAPs complement LEO satellites and cloud infrastructures to form a global-regional-local three-tier architecture. Looking forward, HAPs are expected to evolve from simple platforms into intelligent hubs, emerging as pivotal nodes for air traffic management, intelligent logistics, and emergency response. By doing so, they will accelerate the transition of the LAE toward large-scale deployment, autonomy, and sustainable growth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Low-Altitude Economy (LAE) is rapidly emerging as a new technological and industrial frontier, with unmanned aerial vehicles (UAVs), electric vertical takeoff and landing (eVTOL) aircraft, and aerial swarms increasingly deployed in logistics, infrastructure inspection, security, and emergency response. However, the large-scale development of the LAE demands a reliable aerial foundation that ensures not only real-time connectivity and computational support, but also navigation integrity and safe airspace management for safety-critical operations. High-Altitude Platforms (HAPs), positioned at around 20 km, provide a unique balance between wide-area coverage and low-latency responsiveness. Compared with low earth orbit (LEO) satellites, HAPs are closer to end users and thus capable of delivering millisecond-level connectivity, fine-grained regulatory oversight, and powerful onboard computing and caching resources. Beyond connectivity and computation, HAPs-assisted sensing and regulation further enable navigation integrity and airspace trust, which are essential for safety-critical UAV and eVTOL operations in the LAE. This article proposes a five-stage evolutionary roadmap for HAPs in the LAE: from serving as aerial infrastructure bases, to becoming super back-ends for UAV, to acting as frontline support for ground users, further enabling swarm-scale UAV coordination, and ultimately advancing toward edge-air-cloud closed-loop autonomy. In parallel, HAPs complement LEO satellites and cloud infrastructures to form a global-regional-local three-tier architecture. Looking forward, HAPs are expected to evolve from simple platforms into intelligent hubs, emerging as pivotal nodes for air traffic management, intelligent logistics, and emergency response. By doing so, they will accelerate the transition of the LAE toward large-scale deployment, autonomy, and sustainable growth."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T12:40:05Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    12,
                    40,
                    5,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Bang Huang"
                    },
                    {
                        "name": "Baha Eddine Youcef Belmekki"
                    },
                    {
                        "name": "Mohamed-Slim Alouini"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed-Slim Alouini"
                },
                "author": "Mohamed-Slim Alouini"
            },
            {
                "id": "http://arxiv.org/abs/2602.21780v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21780v1",
                "title": "XStreamVGGT: Extremely Memory-Efficient Streaming Vision Geometry Grounded Transformer with KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XStreamVGGT: Extremely Memory-Efficient Streaming Vision Geometry Grounded Transformer with KV Cache Compression"
                },
                "updated": "2026-02-25T11:02:02Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    11,
                    2,
                    2,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21780v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Learning-based 3D visual geometry models have significantly advanced with the advent of large-scale transformers. Among these, StreamVGGT leverages frame-wise causal attention to deliver robust and efficient streaming 3D reconstruction. However, it suffers from unbounded growth in the Key-Value (KV) cache due to the massive influx of vision tokens from multi-image and long-video inputs, leading to increased memory consumption and inference latency as input frames accumulate. This ultimately limits its scalability for long-horizon applications. To address this gap, we propose XStreamVGGT, a tuning-free approach that seamlessly integrates pruning and quantization to systematically compress the KV cache, enabling extremely memory-efficient streaming inference. Specifically, redundant KVs generated from multi-frame inputs are initially pruned to conform to a fixed KV memory budget using an efficient token-importance identification mechanism that maintains full compatibility with high-performance attention kernels (e.g., FlashAttention). Additionally, leveraging the inherent distribution patterns of KV tensors, we apply dimension-adaptive KV quantization within the pruning pipeline to further minimize memory overhead while preserving numerical accuracy. Extensive evaluations show that XStreamVGGT achieves mostly negligible performance degradation while substantially reducing memory usage by 4.42$\\times$ and accelerating inference by 5.48$\\times$, enabling practical and scalable streaming 3D applications. The code is available at https://github.com/ywh187/XStreamVGGT/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-based 3D visual geometry models have significantly advanced with the advent of large-scale transformers. Among these, StreamVGGT leverages frame-wise causal attention to deliver robust and efficient streaming 3D reconstruction. However, it suffers from unbounded growth in the Key-Value (KV) cache due to the massive influx of vision tokens from multi-image and long-video inputs, leading to increased memory consumption and inference latency as input frames accumulate. This ultimately limits its scalability for long-horizon applications. To address this gap, we propose XStreamVGGT, a tuning-free approach that seamlessly integrates pruning and quantization to systematically compress the KV cache, enabling extremely memory-efficient streaming inference. Specifically, redundant KVs generated from multi-frame inputs are initially pruned to conform to a fixed KV memory budget using an efficient token-importance identification mechanism that maintains full compatibility with high-performance attention kernels (e.g., FlashAttention). Additionally, leveraging the inherent distribution patterns of KV tensors, we apply dimension-adaptive KV quantization within the pruning pipeline to further minimize memory overhead while preserving numerical accuracy. Extensive evaluations show that XStreamVGGT achieves mostly negligible performance degradation while substantially reducing memory usage by 4.42$\\times$ and accelerating inference by 5.48$\\times$, enabling practical and scalable streaming 3D applications. The code is available at https://github.com/ywh187/XStreamVGGT/."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T11:02:02Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    11,
                    2,
                    2,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "Submission to the Journal of the Society for Information Display",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Weihao Ye"
                    },
                    {
                        "name": "Hansen Feng"
                    },
                    {
                        "name": "Keyu Fan"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Dahai Yu"
                    },
                    {
                        "name": "Zhengwu Liu"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong"
            },
            {
                "id": "http://arxiv.org/abs/2510.22049v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.22049v2",
                "title": "Massive Memorization with Hundreds of Trillions of Parameters for Sequential Transducer Generative Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive Memorization with Hundreds of Trillions of Parameters for Sequential Transducer Generative Recommenders"
                },
                "updated": "2026-02-25T09:18:59Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    9,
                    18,
                    59,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.22049v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.22049v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern large-scale recommendation systems rely heavily on user interaction history sequences to enhance the model performance. The advent of large language models and sequential modeling techniques, particularly transformer-like architectures, has led to significant advancements recently (e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories (10k to 100k items) generally improves model performance, it also creates significant challenges on latency, queries per second (QPS) and GPU cost in industry-scale recommendation systems. Existing models do not adequately address these industrial scalability issues. In this paper, we propose a novel two-stage modeling framework, namely VIrtual Sequential Target Attention (VISTA), which decomposes traditional target attention from a candidate item to user history items into two distinct stages: (1) user history summarization into a few hundred tokens; followed by (2) candidate item attention to those tokens. These summarization token embeddings are then cached in storage system and then utilized as sequence features for downstream model training and inference. This novel design for scalability enables VISTA to scale to lifelong user histories (up to one million items) while keeping downstream training and inference costs fixed, which is essential in industry. Our approach achieves significant improvements in offline and online metrics and has been successfully deployed on an industry leading recommendation platform serving billions of users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large-scale recommendation systems rely heavily on user interaction history sequences to enhance the model performance. The advent of large language models and sequential modeling techniques, particularly transformer-like architectures, has led to significant advancements recently (e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories (10k to 100k items) generally improves model performance, it also creates significant challenges on latency, queries per second (QPS) and GPU cost in industry-scale recommendation systems. Existing models do not adequately address these industrial scalability issues. In this paper, we propose a novel two-stage modeling framework, namely VIrtual Sequential Target Attention (VISTA), which decomposes traditional target attention from a candidate item to user history items into two distinct stages: (1) user history summarization into a few hundred tokens; followed by (2) candidate item attention to those tokens. These summarization token embeddings are then cached in storage system and then utilized as sequence features for downstream model training and inference. This novel design for scalability enables VISTA to scale to lifelong user histories (up to one million items) while keeping downstream training and inference costs fixed, which is essential in industry. Our approach achieves significant improvements in offline and online metrics and has been successfully deployed on an industry leading recommendation platform serving billions of users."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-24T22:17:49Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    22,
                    17,
                    49,
                    4,
                    297,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Zhimin Chen"
                    },
                    {
                        "name": "Chenyu Zhao"
                    },
                    {
                        "name": "Ka Chun Mo"
                    },
                    {
                        "name": "Yunjiang Jiang"
                    },
                    {
                        "name": "Jane H. Lee"
                    },
                    {
                        "name": "Khushhall Chandra Mahajan"
                    },
                    {
                        "name": "Ning Jiang"
                    },
                    {
                        "name": "Kai Ren"
                    },
                    {
                        "name": "Jinhui Li"
                    },
                    {
                        "name": "Wen-Yun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wen-Yun Yang"
                },
                "author": "Wen-Yun Yang"
            },
            {
                "id": "http://arxiv.org/abs/2602.12635v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.12635v2",
                "title": "Unleashing Low-Bit Inference on Ascend NPUs: A Comprehensive Evaluation of HiFloat Formats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Low-Bit Inference on Ascend NPUs: A Comprehensive Evaluation of HiFloat Formats"
                },
                "updated": "2026-02-25T08:07:27Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    8,
                    7,
                    27,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.12635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.12635v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As LLMs scale, low-bit floating-point formats like MXFP and NVFP4 offer new opportunities for precision and efficiency. In this work, we evaluate HiFloat (HiF8 and HiF4), a family of formats tailored for Ascend NPUs. Through rigorous comparison across weight-activation and KV-cache tasks, we provide three key insights: (1) INT8 suits narrow-range data, while floating-point formats excel with high-variance data; (2) in 4-bit regimes, HiF4's hierarchical scaling prevents the accuracy collapse seen in integer formats; and (3) HiFloat is fully compatible with state-of-the-art post-training quantization frameworks. Overall, HiFloat provides a solution for high-efficiency LLM inference on NPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs scale, low-bit floating-point formats like MXFP and NVFP4 offer new opportunities for precision and efficiency. In this work, we evaluate HiFloat (HiF8 and HiF4), a family of formats tailored for Ascend NPUs. Through rigorous comparison across weight-activation and KV-cache tasks, we provide three key insights: (1) INT8 suits narrow-range data, while floating-point formats excel with high-variance data; (2) in 4-bit regimes, HiF4's hierarchical scaling prevents the accuracy collapse seen in integer formats; and (3) HiFloat is fully compatible with state-of-the-art post-training quantization frameworks. Overall, HiFloat provides a solution for high-efficiency LLM inference on NPUs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-13T05:41:31Z",
                "published_parsed": [
                    2026,
                    2,
                    13,
                    5,
                    41,
                    31,
                    4,
                    44,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Pengxiang Zhao"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Han Bao"
                    },
                    {
                        "name": "Weizhe Lin"
                    },
                    {
                        "name": "Zhiyuan Yang"
                    },
                    {
                        "name": "Ziwei Yu"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Zhenhua Dong"
                    }
                ],
                "author_detail": {
                    "name": "Zhenhua Dong"
                },
                "author": "Zhenhua Dong"
            },
            {
                "id": "http://arxiv.org/abs/2601.10402v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.10402v4",
                "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering"
                },
                "updated": "2026-02-25T07:23:01Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    7,
                    23,
                    1,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.10402v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.10402v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-15T13:52:04Z",
                "published_parsed": [
                    2026,
                    1,
                    15,
                    13,
                    52,
                    4,
                    3,
                    15,
                    0
                ],
                "arxiv_comment": "25 pages. 5 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Xinyu Zhu"
                    },
                    {
                        "name": "Yuzhu Cai"
                    },
                    {
                        "name": "Zexi Liu"
                    },
                    {
                        "name": "Bingyang Zheng"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Yuzhi Zhang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Weinan E"
                    },
                    {
                        "name": "Siheng Chen"
                    },
                    {
                        "name": "Yanfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanfeng Wang"
                },
                "author": "Yanfeng Wang"
            },
            {
                "id": "http://arxiv.org/abs/2602.21626v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21626v1",
                "title": "Multi-Layer Scheduling for MoE-Based LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Layer Scheduling for MoE-Based LLM Reasoning"
                },
                "updated": "2026-02-25T06:42:08Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    6,
                    42,
                    8,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21626v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have achieved remarkable success across a wide range of tasks, but serving them efficiently at scale remains a critical challenge due to their substantial computational and latency demands. While most existing inference frameworks rely on simple scheduling strategies such as First-Come-First-Serve (FCFS) at the engine level and Round-Robin (RR) at the scheduler or coordinator level, they often fail to fully utilize system resources and may suffer from issues such as head-of-line blocking and load imbalance. Recent advances in Mixture-of-Experts (MoE) models have also introduced new challenges in scheduling arising from expert parallelism and routing complexity. This research proposes a multi-layer scheduling framework tailored for MoE-based LLM serving. It targets scheduling at three levels: request-level, enginelevel, and expert-level. At the request level, we explore algorithms such as Shortest-Job-First (SJF) and priority-aware aging to improve throughput and reduce latency. At the engine level, we design load-aware dispatching strategies that account for the current prefix token load, KV cache utilization, and user stickiness to achieve better resource matching. At the expert level, we focus on alleviating expert hotspots and strategically placing inter-layer expert dependencies to balance load and improve routing efficiency. Extensive experimental results from more than 100 experiments conducted under diverse workload distributions show that our approach consistently outperforms the state-of-theart inference framework vLLM, achieving up to 17.8% reduction in Time To First Token (TTFT) latency and 13.3% reduction in Time-Per-Output-Token (TPOT) latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across a wide range of tasks, but serving them efficiently at scale remains a critical challenge due to their substantial computational and latency demands. While most existing inference frameworks rely on simple scheduling strategies such as First-Come-First-Serve (FCFS) at the engine level and Round-Robin (RR) at the scheduler or coordinator level, they often fail to fully utilize system resources and may suffer from issues such as head-of-line blocking and load imbalance. Recent advances in Mixture-of-Experts (MoE) models have also introduced new challenges in scheduling arising from expert parallelism and routing complexity. This research proposes a multi-layer scheduling framework tailored for MoE-based LLM serving. It targets scheduling at three levels: request-level, enginelevel, and expert-level. At the request level, we explore algorithms such as Shortest-Job-First (SJF) and priority-aware aging to improve throughput and reduce latency. At the engine level, we design load-aware dispatching strategies that account for the current prefix token load, KV cache utilization, and user stickiness to achieve better resource matching. At the expert level, we focus on alleviating expert hotspots and strategically placing inter-layer expert dependencies to balance load and improve routing efficiency. Extensive experimental results from more than 100 experiments conducted under diverse workload distributions show that our approach consistently outperforms the state-of-theart inference framework vLLM, achieving up to 17.8% reduction in Time To First Token (TTFT) latency and 13.3% reduction in Time-Per-Output-Token (TPOT) latency."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T06:42:08Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    6,
                    42,
                    8,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "12 pages, 10 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Yifan Sun"
                    },
                    {
                        "name": "Gholamreza Haffar"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Rajkumar Buyya"
                    },
                    {
                        "name": "Adel N. Toosi"
                    }
                ],
                "author_detail": {
                    "name": "Adel N. Toosi"
                },
                "author": "Adel N. Toosi"
            },
            {
                "id": "http://arxiv.org/abs/2509.22548v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.22548v2",
                "title": "JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation"
                },
                "updated": "2026-02-25T06:06:37Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    6,
                    6,
                    37,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.22548v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.22548v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-and-Language Navigation requires an embodied agent to navigate through unseen environments, guided by natural language instructions and a continuous video stream. Recent advances in VLN have been driven by the powerful semantic understanding of Multimodal Large Language Models. However, these methods typically rely on explicit semantic memory, such as building textual cognitive maps or storing historical visual frames. This type of method suffers from spatial information loss, computational redundancy, and memory bloat, which impede efficient navigation. Inspired by the implicit scene representation in human navigation, analogous to the left brain's semantic understanding and the right brain's spatial cognition, we propose JanusVLN, a novel VLN framework featuring a dual implicit neural memory that models spatial-geometric and visual-semantic memory as separate, compact, and fixed-size neural representations. This framework first extends the MLLM to incorporate 3D prior knowledge from the spatial-geometric encoder, thereby enhancing the spatial reasoning capabilities of models based solely on RGB input. Then, the historical key-value caches from the spatial-geometric and visual-semantic encoders are constructed into a dual implicit memory. By retaining only the KVs of tokens in the initial and sliding window, redundant computation is avoided, enabling efficient incremental updates. Extensive experiments demonstrate that JanusVLN outperforms over 20 recent methods to achieve SOTA performance. For example, the success rate improves by 10.5-35.5 compared to methods using multiple data types as input and by 3.6-10.8 compared to methods using more RGB training data. This indicates that the proposed dual implicit neural memory, as a novel paradigm, explores promising new directions for future VLN research. Ours project page: https://miv-xjtu.github.io/JanusVLN.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation requires an embodied agent to navigate through unseen environments, guided by natural language instructions and a continuous video stream. Recent advances in VLN have been driven by the powerful semantic understanding of Multimodal Large Language Models. However, these methods typically rely on explicit semantic memory, such as building textual cognitive maps or storing historical visual frames. This type of method suffers from spatial information loss, computational redundancy, and memory bloat, which impede efficient navigation. Inspired by the implicit scene representation in human navigation, analogous to the left brain's semantic understanding and the right brain's spatial cognition, we propose JanusVLN, a novel VLN framework featuring a dual implicit neural memory that models spatial-geometric and visual-semantic memory as separate, compact, and fixed-size neural representations. This framework first extends the MLLM to incorporate 3D prior knowledge from the spatial-geometric encoder, thereby enhancing the spatial reasoning capabilities of models based solely on RGB input. Then, the historical key-value caches from the spatial-geometric and visual-semantic encoders are constructed into a dual implicit memory. By retaining only the KVs of tokens in the initial and sliding window, redundant computation is avoided, enabling efficient incremental updates. Extensive experiments demonstrate that JanusVLN outperforms over 20 recent methods to achieve SOTA performance. For example, the success rate improves by 10.5-35.5 compared to methods using multiple data types as input and by 3.6-10.8 compared to methods using more RGB training data. This indicates that the proposed dual implicit neural memory, as a novel paradigm, explores promising new directions for future VLN research. Ours project page: https://miv-xjtu.github.io/JanusVLN.github.io/."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-26T16:29:37Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    29,
                    37,
                    4,
                    269,
                    0
                ],
                "arxiv_comment": "Accepted to ICLR 2026. Project page: https://miv-xjtu.github.io/JanusVLN.github.io/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Shuang Zeng"
                    },
                    {
                        "name": "Dekang Qi"
                    },
                    {
                        "name": "Xinyuan Chang"
                    },
                    {
                        "name": "Feng Xiong"
                    },
                    {
                        "name": "Shichao Xie"
                    },
                    {
                        "name": "Xiaolong Wu"
                    },
                    {
                        "name": "Shiyi Liang"
                    },
                    {
                        "name": "Mu Xu"
                    },
                    {
                        "name": "Xing Wei"
                    },
                    {
                        "name": "Ning Guo"
                    }
                ],
                "author_detail": {
                    "name": "Ning Guo"
                },
                "author": "Ning Guo"
            },
            {
                "id": "http://arxiv.org/abs/2602.21548v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21548v1",
                "title": "DualPath: Breaking the Storage Bandwidth Bottleneck in Agentic LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DualPath: Breaking the Storage Bandwidth Bottleneck in Agentic LLM Inference"
                },
                "updated": "2026-02-25T04:10:58Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    4,
                    10,
                    58,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21548v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The performance of multi-turn, agentic LLM inference is increasingly dominated by KV-Cache storage I/O rather than computation. In prevalent disaggregated architectures, loading the massive KV-Cache from external storage creates a fundamental imbalance: storage NICs on prefill engines become bandwidth-saturated, while those on decoding engines remain idle. This asymmetry severely constrains overall system throughput.\n  We present DualPath, an inference system that breaks this bottleneck by introducing dual-path KV-Cache loading. Beyond the traditional storage-to-prefill path, DualPath enables a novel storage-to-decode path, in which the KV-Cache is loaded into decoding engines and then efficiently transferred to prefill engines via RDMA over the compute network. DualPath combines this optimized data path -- which inherently avoids network congestion and avoids interference with latency-critical model execution communications -- with a global scheduler that dynamically balances load across prefill and decode engines.\n  Our evaluation on three models with production agentic workloads demonstrates that DualPath improves offline inference throughput by up to 1.87$\\times$ on our in-house inference system. It can also improve online serving throughput by an average factor of 1.96$\\times$ without violating SLO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of multi-turn, agentic LLM inference is increasingly dominated by KV-Cache storage I/O rather than computation. In prevalent disaggregated architectures, loading the massive KV-Cache from external storage creates a fundamental imbalance: storage NICs on prefill engines become bandwidth-saturated, while those on decoding engines remain idle. This asymmetry severely constrains overall system throughput.\n  We present DualPath, an inference system that breaks this bottleneck by introducing dual-path KV-Cache loading. Beyond the traditional storage-to-prefill path, DualPath enables a novel storage-to-decode path, in which the KV-Cache is loaded into decoding engines and then efficiently transferred to prefill engines via RDMA over the compute network. DualPath combines this optimized data path -- which inherently avoids network congestion and avoids interference with latency-critical model execution communications -- with a global scheduler that dynamically balances load across prefill and decode engines.\n  Our evaluation on three models with production agentic workloads demonstrates that DualPath improves offline inference throughput by up to 1.87$\\times$ on our in-house inference system. It can also improve online serving throughput by an average factor of 1.96$\\times$ without violating SLO."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T04:10:58Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    4,
                    10,
                    58,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Yongtong Wu"
                    },
                    {
                        "name": "Shaoyuan Chen"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Rilin Huang"
                    },
                    {
                        "name": "Yixuan Tan"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Liyue Zhang"
                    },
                    {
                        "name": "Shangyan Zhou"
                    },
                    {
                        "name": "Yuxuan Liu"
                    },
                    {
                        "name": "Shunfeng Zhou"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Xin Jin"
                    },
                    {
                        "name": "Panpan Huang"
                    }
                ],
                "author_detail": {
                    "name": "Panpan Huang"
                },
                "author": "Panpan Huang"
            },
            {
                "id": "http://arxiv.org/abs/2602.21547v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21547v1",
                "title": "RAC: Relation-Aware Cache Replacement for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAC: Relation-Aware Cache Replacement for Large Language Models"
                },
                "updated": "2026-02-25T04:10:27Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    4,
                    10,
                    27,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21547v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The scaling of Large Language Model (LLM) services faces significant cost and latency challenges, making effective caching under tight capacity crucial. Existing cache replacement policies, from heuristics to learning-based methods, predominantly rely on limited-window statistics such as recency and frequency. We show these signals are not robust for real-world LLM workloads, which exhibit long reuse distances and sparse local recurrence.\n  To address these limitations, we propose Relation-Aware Cache (RAC), an online eviction strategy that leverages semantic relations among requests to guide eviction decisions. RAC synthesizes two relation-aware signals: (1) Topical Prevalence, which aggregates access evidence at the topic level to capture long-horizon reuse; and (2) Structural Importance, which leverages local intra-topic dependency structure to discriminate entries by their future reuse value. Extensive evaluations show that RAC maintains high effectiveness across diverse workloads, consistently surpassing state-of-the-art baselines by 20%--30% in cache hit ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scaling of Large Language Model (LLM) services faces significant cost and latency challenges, making effective caching under tight capacity crucial. Existing cache replacement policies, from heuristics to learning-based methods, predominantly rely on limited-window statistics such as recency and frequency. We show these signals are not robust for real-world LLM workloads, which exhibit long reuse distances and sparse local recurrence.\n  To address these limitations, we propose Relation-Aware Cache (RAC), an online eviction strategy that leverages semantic relations among requests to guide eviction decisions. RAC synthesizes two relation-aware signals: (1) Topical Prevalence, which aggregates access evidence at the topic level to capture long-horizon reuse; and (2) Structural Importance, which leverages local intra-topic dependency structure to discriminate entries by their future reuse value. Extensive evaluations show that RAC maintains high effectiveness across diverse workloads, consistently surpassing state-of-the-art baselines by 20%--30% in cache hit ratio."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T04:10:27Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    4,
                    10,
                    27,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Yuchong Wu"
                    },
                    {
                        "name": "Zihuan Xu"
                    },
                    {
                        "name": "Wangze Ni"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren"
            },
            {
                "id": "http://arxiv.org/abs/2602.21477v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21477v1",
                "title": "Pancake: Hierarchical Memory System for Multi-Agent LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pancake: Hierarchical Memory System for Multi-Agent LLM Serving"
                },
                "updated": "2026-02-25T01:09:04Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    1,
                    9,
                    4,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21477v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this work, we identify and address the core challenges of agentic memory management in LLM serving, where large-scale storage, frequent updates, and multiple coexisting agents jointly introduce complex and high-cost approximate nearest neighbor (ANN) searching problems. We present Pancake, a multi-tier agentic memory system that unifies three key techniques: (i) multi-level index caching for single agents, (ii) coordinated index management across multiple agents, and (iii) collaborative GPU-CPU acceleration. Pancake exposes easy-to-use interface that can be integrated into memory-based agents like Mem-GPT, and is compatible with agentic frameworks such as LangChain and LlamaIndex. Experiments on realistic agent workloads show that Pancake substantially outperforms existing frameworks, achieving more than 4.29x end-to-end throughput improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we identify and address the core challenges of agentic memory management in LLM serving, where large-scale storage, frequent updates, and multiple coexisting agents jointly introduce complex and high-cost approximate nearest neighbor (ANN) searching problems. We present Pancake, a multi-tier agentic memory system that unifies three key techniques: (i) multi-level index caching for single agents, (ii) coordinated index management across multiple agents, and (iii) collaborative GPU-CPU acceleration. Pancake exposes easy-to-use interface that can be integrated into memory-based agents like Mem-GPT, and is compatible with agentic frameworks such as LangChain and LlamaIndex. Experiments on realistic agent workloads show that Pancake substantially outperforms existing frameworks, achieving more than 4.29x end-to-end throughput improvement."
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T01:09:04Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    1,
                    9,
                    4,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA"
                },
                "authors": [
                    {
                        "name": "Zhengding Hu"
                    },
                    {
                        "name": "Zaifeng Pan"
                    },
                    {
                        "name": "Prabhleen Kaur"
                    },
                    {
                        "name": "Vibha Murthy"
                    },
                    {
                        "name": "Zhongkai Yu"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Steven Swanson"
                    },
                    {
                        "name": "Yufei Ding"
                    }
                ],
                "author_detail": {
                    "name": "Yufei Ding"
                },
                "author": "Yufei Ding"
            },
            {
                "id": "http://arxiv.org/abs/2602.21460v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21460v1",
                "title": "Intrinsic Spin Filter Effect in a $d$-wave altermagnet KV$_2$Se$_2$O with Open Fermi Surface",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intrinsic Spin Filter Effect in a $d$-wave altermagnet KV$_2$Se$_2$O with Open Fermi Surface"
                },
                "updated": "2026-02-25T00:25:03Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    0,
                    25,
                    3,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21460v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Altermagnets offer a unique pathway to functional spintronics by combining vanishing magnetization with large spin splitting. Here, we demonstrate that the canonical d-wave altermagnet KV2Se2O can deliver giant tunneling magnetoresistance through orientation-dependent spin filtering. By analyzing the crystallographic spin segregation, we show that transport along specific crystallographic axes is nearly fully spin-polarized within the symmetry-protected ballistic channels. We implement this mechanism in a lattice-matched KV2Se2O/Bi2O2Se/KV2Se2O magnetic tunnel junction, which achieves a robust half-metallic transport regime. The symmetry-protected spectral gap in the parallel/anti-parallel configuration ensures a high tunneling magnetoresistance ratio, resulting in substantial tunneling magnetoresistance, robust thermally driven spin filtering, and spin Seebeck effect at room temperature. These findings provide a path of altermagnetic heterostructures as a high-performance platform for scalable, field-free, and thermally stable spin logic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnets offer a unique pathway to functional spintronics by combining vanishing magnetization with large spin splitting. Here, we demonstrate that the canonical d-wave altermagnet KV2Se2O can deliver giant tunneling magnetoresistance through orientation-dependent spin filtering. By analyzing the crystallographic spin segregation, we show that transport along specific crystallographic axes is nearly fully spin-polarized within the symmetry-protected ballistic channels. We implement this mechanism in a lattice-matched KV2Se2O/Bi2O2Se/KV2Se2O magnetic tunnel junction, which achieves a robust half-metallic transport regime. The symmetry-protected spectral gap in the parallel/anti-parallel configuration ensures a high tunneling magnetoresistance ratio, resulting in substantial tunneling magnetoresistance, robust thermally driven spin filtering, and spin Seebeck effect at room temperature. These findings provide a path of altermagnetic heterostructures as a high-performance platform for scalable, field-free, and thermally stable spin logic."
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T00:25:03Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    0,
                    25,
                    3,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "19 + 5 Pages; 4 + 3 Figures",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci"
                },
                "authors": [
                    {
                        "name": "Bin Liu"
                    },
                    {
                        "name": "Pei-Hao Fu"
                    },
                    {
                        "name": "Yu-Xuan Sun"
                    },
                    {
                        "name": "Xiao-Lin Zhang"
                    },
                    {
                        "name": "Si-Cong Zhu"
                    },
                    {
                        "name": "Xiang-Long Yu"
                    },
                    {
                        "name": "Hua Wu"
                    },
                    {
                        "name": "Yuan-Zhi Shao"
                    }
                ],
                "author_detail": {
                    "name": "Yuan-Zhi Shao"
                },
                "author": "Yuan-Zhi Shao"
            },
            {
                "id": "http://arxiv.org/abs/2602.21307v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21307v1",
                "title": "SymTorch: A Framework for Symbolic Distillation of Deep Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SymTorch: A Framework for Symbolic Distillation of Deep Neural Networks"
                },
                "updated": "2026-02-24T19:17:56Z",
                "updated_parsed": [
                    2026,
                    2,
                    24,
                    19,
                    17,
                    56,
                    1,
                    55,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21307v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Symbolic distillation replaces neural networks, or components thereof, with interpretable, closed-form mathematical expressions. This approach has shown promise in discovering physical laws and mathematical relationships directly from trained deep learning models, yet adoption remains limited due to the engineering barrier of integrating symbolic regression into deep learning workflows. We introduce SymTorch, a library that automates this distillation by wrapping neural network components, collecting their input-output behavior, and approximating them with human-readable equations via PySR. SymTorch handles the engineering challenges that have hindered adoption: GPU-CPU data transfer, input-output caching, model serialization, and seamless switching between neural and symbolic forward passes. We demonstrate SymTorch across diverse architectures including GNNs, PINNs and transformer models. Finally, we present a proof-of-concept for accelerating LLM inference by replacing MLP layers with symbolic surrogates, achieving an 8.3\\% throughput improvement with moderate performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic distillation replaces neural networks, or components thereof, with interpretable, closed-form mathematical expressions. This approach has shown promise in discovering physical laws and mathematical relationships directly from trained deep learning models, yet adoption remains limited due to the engineering barrier of integrating symbolic regression into deep learning workflows. We introduce SymTorch, a library that automates this distillation by wrapping neural network components, collecting their input-output behavior, and approximating them with human-readable equations via PySR. SymTorch handles the engineering challenges that have hindered adoption: GPU-CPU data transfer, input-output caching, model serialization, and seamless switching between neural and symbolic forward passes. We demonstrate SymTorch across diverse architectures including GNNs, PINNs and transformer models. Finally, we present a proof-of-concept for accelerating LLM inference by replacing MLP layers with symbolic surrogates, achieving an 8.3\\% throughput improvement with moderate performance degradation."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-24T19:17:56Z",
                "published_parsed": [
                    2026,
                    2,
                    24,
                    19,
                    17,
                    56,
                    1,
                    55,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Elizabeth S. Z. Tan"
                    },
                    {
                        "name": "Adil Soubki"
                    },
                    {
                        "name": "Miles Cranmer"
                    }
                ],
                "author_detail": {
                    "name": "Miles Cranmer"
                },
                "author": "Miles Cranmer"
            },
            {
                "id": "http://arxiv.org/abs/2602.21204v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21204v1",
                "title": "Test-Time Training with KV Binding Is Secretly Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Training with KV Binding Is Secretly Linear Attention"
                },
                "updated": "2026-02-24T18:59:30Z",
                "updated_parsed": [
                    2026,
                    2,
                    24,
                    18,
                    59,
                    30,
                    1,
                    55,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21204v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-24T18:59:30Z",
                "published_parsed": [
                    2026,
                    2,
                    24,
                    18,
                    59,
                    30,
                    1,
                    55,
                    0
                ],
                "arxiv_comment": "Webpage: https://research.nvidia.com/labs/sil/projects/tttla/",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Junchen Liu"
                    },
                    {
                        "name": "Sven Elflein"
                    },
                    {
                        "name": "Or Litany"
                    },
                    {
                        "name": "Zan Gojcic"
                    },
                    {
                        "name": "Ruilong Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruilong Li"
                },
                "author": "Ruilong Li"
            },
            {
                "id": "http://arxiv.org/abs/2602.21144v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21144v1",
                "title": "Scaling State-Space Models on Multiple GPUs with Tensor Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling State-Space Models on Multiple GPUs with Tensor Parallelism"
                },
                "updated": "2026-02-24T17:47:54Z",
                "updated_parsed": [
                    2026,
                    2,
                    24,
                    17,
                    47,
                    54,
                    1,
                    55,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21144v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Selective state space models (SSMs) have rapidly become a compelling backbone for large language models, especially for long-context workloads. Yet in deployment, their inference performance is often bounded by the memory capacity, bandwidth, and latency limits of a single GPU, making multi-GPU execution increasingly necessary. Although tensor parallelism (TP) is widely used to scale Transformer inference, applying it to selective SSM blocks is non-trivial because the SSM mixer couples large projections with a sequence-wise recurrent state update and local mixing whose efficiency depends on preserving locality and avoiding synchronization in the critical path.\n  This paper presents a communication-efficient TP design for selective SSM inference that addresses three practical engineering challenges: enabling TTFT improvements via an SSM state cache across prefill and decode, partitioning the mixer's packed parameter tensor so that recurrent updates remain local while minimizing communication, and reducing TP aggregation overhead with quantized AllReduce. We evaluate on three representative SSM-based LLMs spanning pure-SSM and hybrid architectures - Mamba, Falcon-Mamba, and Zamba - on NVIDIA A6000 and A100 clusters. Our experiments show substantial throughput gains from tensor-parallel SSM inference, improving batch-request throughput by ~1.6-2.1x on 2 GPUs and ~2.6-4.0x on 4 GPUs for Mamba, with the largest benefits at long context lengths, and achieving a further ~10-18% throughput improvement from quantized all-reduce by lowering synchronization bandwidth overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective state space models (SSMs) have rapidly become a compelling backbone for large language models, especially for long-context workloads. Yet in deployment, their inference performance is often bounded by the memory capacity, bandwidth, and latency limits of a single GPU, making multi-GPU execution increasingly necessary. Although tensor parallelism (TP) is widely used to scale Transformer inference, applying it to selective SSM blocks is non-trivial because the SSM mixer couples large projections with a sequence-wise recurrent state update and local mixing whose efficiency depends on preserving locality and avoiding synchronization in the critical path.\n  This paper presents a communication-efficient TP design for selective SSM inference that addresses three practical engineering challenges: enabling TTFT improvements via an SSM state cache across prefill and decode, partitioning the mixer's packed parameter tensor so that recurrent updates remain local while minimizing communication, and reducing TP aggregation overhead with quantized AllReduce. We evaluate on three representative SSM-based LLMs spanning pure-SSM and hybrid architectures - Mamba, Falcon-Mamba, and Zamba - on NVIDIA A6000 and A100 clusters. Our experiments show substantial throughput gains from tensor-parallel SSM inference, improving batch-request throughput by ~1.6-2.1x on 2 GPUs and ~2.6-4.0x on 4 GPUs for Mamba, with the largest benefits at long context lengths, and achieving a further ~10-18% throughput improvement from quantized all-reduce by lowering synchronization bandwidth overhead."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-24T17:47:54Z",
                "published_parsed": [
                    2026,
                    2,
                    24,
                    17,
                    47,
                    54,
                    1,
                    55,
                    0
                ],
                "arxiv_comment": "Submitted to 46th IEEE International Conference on Distributed Computing Systems (ICDCS 2026)",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Anurag Dutt"
                    },
                    {
                        "name": "Nimit Shah"
                    },
                    {
                        "name": "Hazem Masarani"
                    },
                    {
                        "name": "Anshul Gandhi"
                    }
                ],
                "author_detail": {
                    "name": "Anshul Gandhi"
                },
                "author": "Anshul Gandhi"
            },
            {
                "id": "http://arxiv.org/abs/2602.21273v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21273v1",
                "title": "StoryTailor:A Zero-Shot Pipeline for Action-Rich Multi-Subject Visual Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StoryTailor:A Zero-Shot Pipeline for Action-Rich Multi-Subject Visual Narratives"
                },
                "updated": "2026-02-24T16:07:02Z",
                "updated_parsed": [
                    2026,
                    2,
                    24,
                    16,
                    7,
                    2,
                    1,
                    55,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21273v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generating multi-frame, action-rich visual narratives without fine-tuning faces a threefold tension: action text faithfulness, subject identity fidelity, and cross-frame background continuity. We propose StoryTailor, a zero-shot pipeline that runs on a single RTX 4090 (24 GB) and produces temporally coherent, identity-preserving image sequences from a long narrative prompt, per-subject references, and grounding boxes. Three synergistic modules drive the system: Gaussian-Centered Attention (GCA) to dynamically focus on each subject core and ease grounding-box overlaps; Action-Boost Singular Value Reweighting (AB-SVR) to amplify action-related directions in the text embedding space; and Selective Forgetting Cache (SFC) that retains transferable background cues, forgets nonessential history, and selectively surfaces retained cues to build cross-scene semantic ties. Compared with baseline methods, experiments show that CLIP-T improves by up to 10-15%, with DreamSim lower than strong baselines, while CLIP-I stays in a visually acceptable, competitive range. With matched resolution and steps on a 24 GB GPU, inference is faster than FluxKontext. Qualitatively, StoryTailor delivers expressive interactions and evolving yet stable scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating multi-frame, action-rich visual narratives without fine-tuning faces a threefold tension: action text faithfulness, subject identity fidelity, and cross-frame background continuity. We propose StoryTailor, a zero-shot pipeline that runs on a single RTX 4090 (24 GB) and produces temporally coherent, identity-preserving image sequences from a long narrative prompt, per-subject references, and grounding boxes. Three synergistic modules drive the system: Gaussian-Centered Attention (GCA) to dynamically focus on each subject core and ease grounding-box overlaps; Action-Boost Singular Value Reweighting (AB-SVR) to amplify action-related directions in the text embedding space; and Selective Forgetting Cache (SFC) that retains transferable background cues, forgets nonessential history, and selectively surfaces retained cues to build cross-scene semantic ties. Compared with baseline methods, experiments show that CLIP-T improves by up to 10-15%, with DreamSim lower than strong baselines, while CLIP-I stays in a visually acceptable, competitive range. With matched resolution and steps on a 24 GB GPU, inference is faster than FluxKontext. Qualitatively, StoryTailor delivers expressive interactions and evolving yet stable scenes."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-24T16:07:02Z",
                "published_parsed": [
                    2026,
                    2,
                    24,
                    16,
                    7,
                    2,
                    1,
                    55,
                    0
                ],
                "arxiv_comment": "24 pages,19 figures,accepted by CVPR2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jinghao Hu"
                    },
                    {
                        "name": "Yuhe Zhang"
                    },
                    {
                        "name": "GuoHua Geng"
                    },
                    {
                        "name": "Kang Li"
                    },
                    {
                        "name": "Han Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Han Zhang"
                },
                "author": "Han Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2602.20870v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20870v1",
                "title": "FGFRFT: Fast Graph Fractional FourierTransform via Fourier Series Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FGFRFT: Fast Graph Fractional FourierTransform via Fourier Series Approximation"
                },
                "updated": "2026-02-24T13:14:25Z",
                "updated_parsed": [
                    2026,
                    2,
                    24,
                    13,
                    14,
                    25,
                    1,
                    55,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20870v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The graph fractional Fourier transform (GFRFT) generalizes the graph Fourier transform (GFT) but suffers from a significant computational bottleneck: determining the optimal transform order requires expensive eigendecomposition and matrix multiplication, leading to $O(N^3)$ complexity. To address this issue, we propose a fast GFRFT (FGFRFT) algorithm for unitary GFT matrices based on Fourier series approximation and an efficient caching strategy. FGFRFT reduces the complexity of generating transform matrices to $O(2LN^2)$ while preserving differentiability, thereby enabling adaptive order learning. We validate the algorithm through theoretical analysis, approximation accuracy tests, and order learning experiments. Furthermore, we demonstrate its practical efficacy for image and point cloud denoising and present the fractional specformer, which integrates the FGFRFT into the specformer architecture. This integration enables the model to overcome the limitations of a fixed GFT basis and learn optimal fractional orders for complex data. Experimental results confirm that the proposed algorithm significantly accelerates computation and achieves superior performance compared with the GFRFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The graph fractional Fourier transform (GFRFT) generalizes the graph Fourier transform (GFT) but suffers from a significant computational bottleneck: determining the optimal transform order requires expensive eigendecomposition and matrix multiplication, leading to $O(N^3)$ complexity. To address this issue, we propose a fast GFRFT (FGFRFT) algorithm for unitary GFT matrices based on Fourier series approximation and an efficient caching strategy. FGFRFT reduces the complexity of generating transform matrices to $O(2LN^2)$ while preserving differentiability, thereby enabling adaptive order learning. We validate the algorithm through theoretical analysis, approximation accuracy tests, and order learning experiments. Furthermore, we demonstrate its practical efficacy for image and point cloud denoising and present the fractional specformer, which integrates the FGFRFT into the specformer architecture. This integration enables the model to overcome the limitations of a fixed GFT basis and learn optimal fractional orders for complex data. Experimental results confirm that the proposed algorithm significantly accelerates computation and achieves superior performance compared with the GFRFT."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-24T13:14:25Z",
                "published_parsed": [
                    2026,
                    2,
                    24,
                    13,
                    14,
                    25,
                    1,
                    55,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "authors": [
                    {
                        "name": "Ziqi Yan"
                    },
                    {
                        "name": "Sen Shi"
                    },
                    {
                        "name": "Feiyue Zhao"
                    },
                    {
                        "name": "Manjun Cui"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Zhichao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Zhang"
                },
                "author": "Zhichao Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.18337v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.18337v2",
                "title": "Towards Efficient Agents: A Co-Design of Inference Architecture and System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Agents: A Co-Design of Inference Architecture and System"
                },
                "updated": "2026-02-24T12:33:49Z",
                "updated_parsed": [
                    2026,
                    2,
                    24,
                    12,
                    33,
                    49,
                    1,
                    55,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.18337v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.18337v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid development of large language model (LLM)-based agents has unlocked new possibilities for autonomous multi-turn reasoning and tool-augmented decision-making. However, their real-world deployment is hindered by severe inefficiencies that arise not from isolated model inference, but from the systemic latency accumulated across reasoning loops, context growth, and heterogeneous tool interactions. This paper presents AgentInfer, a unified framework for end-to-end agent acceleration that bridges inference optimization and architectural design. We decompose the problem into four synergistic components: AgentCollab, a hierarchical dual-model reasoning framework that balances large- and small-model usage through dynamic role assignment; AgentSched, a cache-aware hybrid scheduler that minimizes latency under heterogeneous request patterns; AgentSAM, a suffix-automaton-based speculative decoding method that reuses multi-session semantic memory to achieve low-overhead inference acceleration; and AgentCompress, a semantic compression mechanism that asynchronously distills and reorganizes agent memory without disrupting ongoing reasoning. Together, these modules form a Self-Evolution Engine capable of sustaining efficiency and cognitive stability throughout long-horizon reasoning tasks. Experiments on the BrowseComp-zh and DeepDiver benchmarks demonstrate that through the synergistic collaboration of these methods, AgentInfer reduces ineffective token consumption by over 50%, achieving an overall 1.8-2.5 times speedup with preserved accuracy. These results underscore that optimizing for agentic task completion-rather than merely per-token throughput-is the key to building scalable, efficient, and self-improving intelligent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language model (LLM)-based agents has unlocked new possibilities for autonomous multi-turn reasoning and tool-augmented decision-making. However, their real-world deployment is hindered by severe inefficiencies that arise not from isolated model inference, but from the systemic latency accumulated across reasoning loops, context growth, and heterogeneous tool interactions. This paper presents AgentInfer, a unified framework for end-to-end agent acceleration that bridges inference optimization and architectural design. We decompose the problem into four synergistic components: AgentCollab, a hierarchical dual-model reasoning framework that balances large- and small-model usage through dynamic role assignment; AgentSched, a cache-aware hybrid scheduler that minimizes latency under heterogeneous request patterns; AgentSAM, a suffix-automaton-based speculative decoding method that reuses multi-session semantic memory to achieve low-overhead inference acceleration; and AgentCompress, a semantic compression mechanism that asynchronously distills and reorganizes agent memory without disrupting ongoing reasoning. Together, these modules form a Self-Evolution Engine capable of sustaining efficiency and cognitive stability throughout long-horizon reasoning tasks. Experiments on the BrowseComp-zh and DeepDiver benchmarks demonstrate that through the synergistic collaboration of these methods, AgentInfer reduces ineffective token consumption by over 50%, achieving an overall 1.8-2.5 times speedup with preserved accuracy. These results underscore that optimizing for agentic task completion-rather than merely per-token throughput-is the key to building scalable, efficient, and self-improving intelligent systems."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-20T12:06:13Z",
                "published_parsed": [
                    2025,
                    12,
                    20,
                    12,
                    6,
                    13,
                    5,
                    354,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Weizhe Lin"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Xian Wang"
                    },
                    {
                        "name": "Renxi Liu"
                    },
                    {
                        "name": "Hanting Chen"
                    },
                    {
                        "name": "Wangze Zhang"
                    },
                    {
                        "name": "Chuansai Zhou"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhiyuan Yang"
                    },
                    {
                        "name": "Xiaosong Li"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Zhenhua Dong"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang"
            },
            {
                "id": "http://arxiv.org/abs/2602.20810v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20810v1",
                "title": "POMDPPlanners: Open-Source Package for POMDP Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POMDPPlanners: Open-Source Package for POMDP Planning"
                },
                "updated": "2026-02-24T11:50:04Z",
                "updated_parsed": [
                    2026,
                    2,
                    24,
                    11,
                    50,
                    4,
                    1,
                    55,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20810v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present POMDPPlanners, an open-source Python package for empirical evaluation of Partially Observable Markov Decision Process (POMDP) planning algorithms. The package integrates state-of-the-art planning algorithms, a suite of benchmark environments with safety-critical variants, automated hyperparameter optimization via Optuna, persistent caching with failure recovery, and configurable parallel simulation -- reducing the overhead of extensive simulation studies. POMDPPlanners is designed to enable scalable, reproducible research on decision-making under uncertainty, with particular emphasis on risk-sensitive settings where standard toolkits fall short.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present POMDPPlanners, an open-source Python package for empirical evaluation of Partially Observable Markov Decision Process (POMDP) planning algorithms. The package integrates state-of-the-art planning algorithms, a suite of benchmark environments with safety-critical variants, automated hyperparameter optimization via Optuna, persistent caching with failure recovery, and configurable parallel simulation -- reducing the overhead of extensive simulation studies. POMDPPlanners is designed to enable scalable, reproducible research on decision-making under uncertainty, with particular emphasis on risk-sensitive settings where standard toolkits fall short."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-24T11:50:04Z",
                "published_parsed": [
                    2026,
                    2,
                    24,
                    11,
                    50,
                    4,
                    1,
                    55,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Yaacov Pariente"
                    },
                    {
                        "name": "Vadim Indelman"
                    }
                ],
                "author_detail": {
                    "name": "Vadim Indelman"
                },
                "author": "Vadim Indelman"
            },
            {
                "id": "http://arxiv.org/abs/2602.19626v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19626v2",
                "title": "Nacrith: Neural Lossless Compression via Ensemble Context Modeling and High-Precision CDF Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nacrith: Neural Lossless Compression via Ensemble Context Modeling and High-Precision CDF Coding"
                },
                "updated": "2026-02-24T11:10:17Z",
                "updated_parsed": [
                    2026,
                    2,
                    24,
                    11,
                    10,
                    17,
                    1,
                    55,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19626v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present Nacrith, a lossless compression system that combines a 135M-parameter transformer language model (SmolLM2-135M) with an ensemble of lightweight online predictors and a 32-bit arithmetic coder, achieving the best compression results among the systems evaluated in this study on natural language text. Beyond the base LLM-plus-arithmetic-coding paradigm, Nacrith introduces several contributions: (1) a CDF precision upgrade from 2^16 to 2^24 that eliminates ~75% of quantization overhead caused by minimum-probability floors in large vocabularies; (2) a token-level N-gram model for fast local predictions; (3) an adaptive log-space bias head correcting per-document LLM errors via online gradient descent; (4) confidence-based LLM skip for accelerating highly predictable tokens; (5) a hybrid binary format (NC06) extending neural compression to arbitrary binary files--to our knowledge a first among LLM-based compressors; (6) a llama cpp inference backend achieving ~7x faster single-token decode than PyTorch; (7) parallel multi-GPU compression across up to 8 workers; and (8) native KV cache sliding window reducing per-slide cost by ~37x. The system requires only ~500 MB of GGUF weights and ~1.2 GB VRAM per worker, running on consumer GPUs.\n  On alice29 (Canterbury Corpus, 152 KB), Nacrith achieves 0.918 bits per byte (bpb)--outperforming gzip by 3.1x, bzip2 by 2.5x, CMIX v21 by 44%, and ts_zip by 20%, while compressing below the 0th-, 1st-, and 2nd-order byte-level Shannon entropy bounds. On enwik8 (100 MB), Nacrith achieves 0.9389 bpb (11.74%), surpassing ts_zip (~1.11 bpb) by 15% and FineZip (1.024 bpb) by 8% despite using a 60x smaller model with no fine-tuning. An out-of-distribution (OOD) evaluation on a document published after the model's training cutoff confirms these gains are not memorization artifacts, achieving 0.723 bpb on unseen text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Nacrith, a lossless compression system that combines a 135M-parameter transformer language model (SmolLM2-135M) with an ensemble of lightweight online predictors and a 32-bit arithmetic coder, achieving the best compression results among the systems evaluated in this study on natural language text. Beyond the base LLM-plus-arithmetic-coding paradigm, Nacrith introduces several contributions: (1) a CDF precision upgrade from 2^16 to 2^24 that eliminates ~75% of quantization overhead caused by minimum-probability floors in large vocabularies; (2) a token-level N-gram model for fast local predictions; (3) an adaptive log-space bias head correcting per-document LLM errors via online gradient descent; (4) confidence-based LLM skip for accelerating highly predictable tokens; (5) a hybrid binary format (NC06) extending neural compression to arbitrary binary files--to our knowledge a first among LLM-based compressors; (6) a llama cpp inference backend achieving ~7x faster single-token decode than PyTorch; (7) parallel multi-GPU compression across up to 8 workers; and (8) native KV cache sliding window reducing per-slide cost by ~37x. The system requires only ~500 MB of GGUF weights and ~1.2 GB VRAM per worker, running on consumer GPUs.\n  On alice29 (Canterbury Corpus, 152 KB), Nacrith achieves 0.918 bits per byte (bpb)--outperforming gzip by 3.1x, bzip2 by 2.5x, CMIX v21 by 44%, and ts_zip by 20%, while compressing below the 0th-, 1st-, and 2nd-order byte-level Shannon entropy bounds. On enwik8 (100 MB), Nacrith achieves 0.9389 bpb (11.74%), surpassing ts_zip (~1.11 bpb) by 15% and FineZip (1.024 bpb) by 8% despite using a 60x smaller model with no fine-tuning. An out-of-distribution (OOD) evaluation on a document published after the model's training cutoff confirms these gains are not memorization artifacts, achieving 0.723 bpb on unseen text."
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T09:14:05Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    9,
                    14,
                    5,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "10 pages",
                "arxiv_primary_category": {
                    "term": "cs.IT"
                },
                "authors": [
                    {
                        "name": "Roberto Tacconelli"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Tacconelli"
                },
                "author": "Roberto Tacconelli"
            },
            {
                "id": "http://arxiv.org/abs/2602.20732v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20732v1",
                "title": "CHESS: Context-aware Hierarchical Efficient Semantic Selection for Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHESS: Context-aware Hierarchical Efficient Semantic Selection for Long-Context LLM Inference"
                },
                "updated": "2026-02-24T09:54:59Z",
                "updated_parsed": [
                    2026,
                    2,
                    24,
                    9,
                    54,
                    59,
                    1,
                    55,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20732v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long-context LLMs demand accurate inference at low latency, yet decoding becomes primarily constrained by KV cache as context grows. Prior pruning methods are largely context-agnostic: their token selection ignores step-wise relevance and local semantics, which undermines quality. Moreover, their irregular accesses and selection overheads yield only limited wall-clock speedups. To address this, we propose \\textbf{CHESS}, an \\textit{algorithm-system co-design} KV-cache management system. Algorithmically, CHESS introduces a context-aware, hierarchical selection policy that dynamically reconstructs a coherent context for the current decoding. System-wise, coarse granularity selection eliminates expensive data movement, fully realizing practical acceleration from theoretical sparsity. Extensive evaluations demonstrate that CHESS surpasses Full-KV quality using only \\textbf{1\\%} of the KV cache, delivers low-latency stable inference with up to \\textbf{4.56$\\times$} higher throughput, and consistently outperforms other strong baselines. Code is available at \\href{https://anonymous.4open.science/r/CHESS-9958/}{https://anonymous.4open.science/r/CHESS/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs demand accurate inference at low latency, yet decoding becomes primarily constrained by KV cache as context grows. Prior pruning methods are largely context-agnostic: their token selection ignores step-wise relevance and local semantics, which undermines quality. Moreover, their irregular accesses and selection overheads yield only limited wall-clock speedups. To address this, we propose \\textbf{CHESS}, an \\textit{algorithm-system co-design} KV-cache management system. Algorithmically, CHESS introduces a context-aware, hierarchical selection policy that dynamically reconstructs a coherent context for the current decoding. System-wise, coarse granularity selection eliminates expensive data movement, fully realizing practical acceleration from theoretical sparsity. Extensive evaluations demonstrate that CHESS surpasses Full-KV quality using only \\textbf{1\\%} of the KV cache, delivers low-latency stable inference with up to \\textbf{4.56$\\times$} higher throughput, and consistently outperforms other strong baselines. Code is available at \\href{https://anonymous.4open.science/r/CHESS-9958/}{https://anonymous.4open.science/r/CHESS/}."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-24T09:54:59Z",
                "published_parsed": [
                    2026,
                    2,
                    24,
                    9,
                    54,
                    59,
                    1,
                    55,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Chao Fei"
                    },
                    {
                        "name": "Guozhong Li"
                    },
                    {
                        "name": "Chenxi Liu"
                    },
                    {
                        "name": "Panos Kalnis"
                    }
                ],
                "author_detail": {
                    "name": "Panos Kalnis"
                },
                "author": "Panos Kalnis"
            },
            {
                "id": "http://arxiv.org/abs/2602.20717v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20717v1",
                "title": "PackMonitor: Enabling Zero Package Hallucinations Through Decoding-Time Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PackMonitor: Enabling Zero Package Hallucinations Through Decoding-Time Monitoring"
                },
                "updated": "2026-02-24T09:26:11Z",
                "updated_parsed": [
                    2026,
                    2,
                    24,
                    9,
                    26,
                    11,
                    1,
                    55,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20717v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As Large Language Models (LLMs) are increasingly integrated into software development workflows, their trustworthiness has become a critical concern. However, in dependency recommendation scenarios, the reliability of LLMs is undermined by widespread package hallucinations, where models often recommend hallucinated packages. Recent studies have proposed a range of approaches to mitigate this issue. Nevertheless, existing approaches typically merely reduce hallucination rates rather than eliminate them, leaving persistent software security risks.\n  In this work, we argue that package hallucinations are theoretically preventable based on the key insight that package validity is decidable through finite and enumerable authoritative package lists. Building on this, we propose PackMonitor, the first approach capable of fundamentally eliminating package hallucinations by continuously monitoring the model's decoding process and intervening when necessary. To implement this in practice, PackMonitor addresses three key challenges: (1) determining when to trigger intervention via a Context-Aware Parser that continuously monitors model outputs and selectively activates intervening only during installation command generation; (2) resolving how to intervene by employing a Package-Name Intervenor that strictly limits the decoding space to an authoritative package list; and (3) ensuring monitoring efficiency through a DFA-Caching Mechanism that enables scalability to millions of packages with negligible overhead. Extensive experiments on five widely used LLMs demonstrate that PackMonitor is a training-free, plug-and-play solution that consistently reduces package hallucination rates to zero while maintaining low-latency inference and preserving original model capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) are increasingly integrated into software development workflows, their trustworthiness has become a critical concern. However, in dependency recommendation scenarios, the reliability of LLMs is undermined by widespread package hallucinations, where models often recommend hallucinated packages. Recent studies have proposed a range of approaches to mitigate this issue. Nevertheless, existing approaches typically merely reduce hallucination rates rather than eliminate them, leaving persistent software security risks.\n  In this work, we argue that package hallucinations are theoretically preventable based on the key insight that package validity is decidable through finite and enumerable authoritative package lists. Building on this, we propose PackMonitor, the first approach capable of fundamentally eliminating package hallucinations by continuously monitoring the model's decoding process and intervening when necessary. To implement this in practice, PackMonitor addresses three key challenges: (1) determining when to trigger intervention via a Context-Aware Parser that continuously monitors model outputs and selectively activates intervening only during installation command generation; (2) resolving how to intervene by employing a Package-Name Intervenor that strictly limits the decoding space to an authoritative package list; and (3) ensuring monitoring efficiency through a DFA-Caching Mechanism that enables scalability to millions of packages with negligible overhead. Extensive experiments on five widely used LLMs demonstrate that PackMonitor is a training-free, plug-and-play solution that consistently reduces package hallucination rates to zero while maintaining low-latency inference and preserving original model capabilities."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-24T09:26:11Z",
                "published_parsed": [
                    2026,
                    2,
                    24,
                    9,
                    26,
                    11,
                    1,
                    55,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Xiting Liu"
                    },
                    {
                        "name": "Yuetong Liu"
                    },
                    {
                        "name": "Yitong Zhang"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Shi-Min Hu"
                    }
                ],
                "author_detail": {
                    "name": "Shi-Min Hu"
                },
                "author": "Shi-Min Hu"
            },
            {
                "id": "http://arxiv.org/abs/2602.20706v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20706v1",
                "title": "Online Algorithms with Unreliable Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Algorithms with Unreliable Guidance"
                },
                "updated": "2026-02-24T09:11:56Z",
                "updated_parsed": [
                    2026,
                    2,
                    24,
                    9,
                    11,
                    56,
                    1,
                    55,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20706v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper introduces a new model for ML-augmented online decision making, called online algorithms with unreliable guidance (OAG). This model completely separates between the predictive and algorithmic components, thus offering a single well-defined analysis framework that relies solely on the considered problem. Formulated through the lens of request-answer games, an OAG algorithm receives, with each incoming request, a piece of guidance which is taken from the problem's answer space; ideally, this guidance is the optimal answer for the current request, however with probability $$, the guidance is adversarially corrupted. The goal is to develop OAG algorithms that admit good competitiveness when $= 0$ (a.k.a. consistency) as well as when $= 1$ (a.k.a. robustness); the appealing notion of smoothness, that in most prior work required a dedicated loss function, now arises naturally as $$ shifts from $0$ to $1$.\n  We then describe a systematic method, called the drop or trust blindly (DTB) compiler, which transforms any online algorithm into a learning-augmented online algorithm in the OAG model. Given a prediction-oblivious online algorithm, its learning-augmented counterpart produced by applying the DTB compiler either follows the incoming guidance blindly or ignores it altogether and proceeds as the initial algorithm would have; the choice between these two alternatives is based on the outcome of a (biased) coin toss. As our main technical contribution, we prove (rigorously) that although remarkably simple, the class of algorithms produced via the DTB compiler includes algorithms with attractive consistency-robustness guarantees for three classic online problems: for caching and uniform metrical task systems our algorithms are optimal, whereas for bipartite matching (with adversarial arrival order), our algorithm outperforms the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a new model for ML-augmented online decision making, called online algorithms with unreliable guidance (OAG). This model completely separates between the predictive and algorithmic components, thus offering a single well-defined analysis framework that relies solely on the considered problem. Formulated through the lens of request-answer games, an OAG algorithm receives, with each incoming request, a piece of guidance which is taken from the problem's answer space; ideally, this guidance is the optimal answer for the current request, however with probability $$, the guidance is adversarially corrupted. The goal is to develop OAG algorithms that admit good competitiveness when $= 0$ (a.k.a. consistency) as well as when $= 1$ (a.k.a. robustness); the appealing notion of smoothness, that in most prior work required a dedicated loss function, now arises naturally as $$ shifts from $0$ to $1$.\n  We then describe a systematic method, called the drop or trust blindly (DTB) compiler, which transforms any online algorithm into a learning-augmented online algorithm in the OAG model. Given a prediction-oblivious online algorithm, its learning-augmented counterpart produced by applying the DTB compiler either follows the incoming guidance blindly or ignores it altogether and proceeds as the initial algorithm would have; the choice between these two alternatives is based on the outcome of a (biased) coin toss. As our main technical contribution, we prove (rigorously) that although remarkably simple, the class of algorithms produced via the DTB compiler includes algorithms with attractive consistency-robustness guarantees for three classic online problems: for caching and uniform metrical task systems our algorithms are optimal, whereas for bipartite matching (with adversarial arrival order), our algorithm outperforms the state-of-the-art."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-24T09:11:56Z",
                "published_parsed": [
                    2026,
                    2,
                    24,
                    9,
                    11,
                    56,
                    1,
                    55,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Julien Dallot"
                    },
                    {
                        "name": "Yuval Emek"
                    },
                    {
                        "name": "Yuval Gil"
                    },
                    {
                        "name": "Maciej Pacut"
                    },
                    {
                        "name": "Stefan Schmid"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Schmid"
                },
                "author": "Stefan Schmid"
            },
            {
                "id": "http://arxiv.org/abs/2602.20705v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20705v1",
                "title": "The Careless Coupon Collector's Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Careless Coupon Collector's Problem"
                },
                "updated": "2026-02-24T09:11:05Z",
                "updated_parsed": [
                    2026,
                    2,
                    24,
                    9,
                    11,
                    5,
                    1,
                    55,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20705v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We initiate the study of the Careless Coupon Collector's Problem (CCCP), a novel variation of the classical coupon collector, that we envision as a model for information systems such as web crawlers, dynamic caches, and fault-resilient networks. In CCCP, a collector attempts to gather $n$ distinct coupon types by obtaining one coupon type uniformly at random in each discrete round, however the collector is \\textit{careless}: at the end of each round, each collected coupon type is independently lost with probability $p$. We analyze the number of rounds required to complete the collection as a function of $n$ and $p$. In particular, we show that it transitions from $(n \\ln n)$ when $p = o\\big(\\frac{\\ln n}{n^2}\\big)$ up to $\\big((\\frac{np}{1-p})^n\\big)$ when $p=\\big(\\frac{1}{n}\\big)$ in multiple distinct phases. Interestingly, when $p=\\frac{c}{n}$, the process remains in a metastable phase, where the fraction of collected coupon types is concentrated around $\\frac{1}{1+c}$ with probability $1-o(1)$, for a time window of length $e^{(n)}$. Finally, we give an algorithm that computes the expected completion time of CCCP in $O(n^2)$ time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We initiate the study of the Careless Coupon Collector's Problem (CCCP), a novel variation of the classical coupon collector, that we envision as a model for information systems such as web crawlers, dynamic caches, and fault-resilient networks. In CCCP, a collector attempts to gather $n$ distinct coupon types by obtaining one coupon type uniformly at random in each discrete round, however the collector is \\textit{careless}: at the end of each round, each collected coupon type is independently lost with probability $p$. We analyze the number of rounds required to complete the collection as a function of $n$ and $p$. In particular, we show that it transitions from $(n \\ln n)$ when $p = o\\big(\\frac{\\ln n}{n^2}\\big)$ up to $\\big((\\frac{np}{1-p})^n\\big)$ when $p=\\big(\\frac{1}{n}\\big)$ in multiple distinct phases. Interestingly, when $p=\\frac{c}{n}$, the process remains in a metastable phase, where the fraction of collected coupon types is concentrated around $\\frac{1}{1+c}$ with probability $1-o(1)$, for a time window of length $e^{(n)}$. Finally, we give an algorithm that computes the expected completion time of CCCP in $O(n^2)$ time."
                },
                "tags": [
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-24T09:11:05Z",
                "published_parsed": [
                    2026,
                    2,
                    24,
                    9,
                    11,
                    5,
                    1,
                    55,
                    0
                ],
                "arxiv_comment": "Published at FUN 2026",
                "arxiv_primary_category": {
                    "term": "cs.DM"
                },
                "authors": [
                    {
                        "name": "Emilio Cruciani"
                    },
                    {
                        "name": "Aditi Dudeja"
                    }
                ],
                "author_detail": {
                    "name": "Aditi Dudeja"
                },
                "author": "Aditi Dudeja"
            },
            {
                "id": "http://arxiv.org/abs/2602.20668v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20668v1",
                "title": "Development of a cost-effective X-ray imaging device based on Raspberry Pi Camera",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of a cost-effective X-ray imaging device based on Raspberry Pi Camera"
                },
                "updated": "2026-02-24T08:16:12Z",
                "updated_parsed": [
                    2026,
                    2,
                    24,
                    8,
                    16,
                    12,
                    1,
                    55,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20668v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This study reports the development and characterization of a cost-effective X-ray imaging device built from Raspberry Pi components, including a high-quality 12.3-megapixel camera configured for indirect detection with a Gd2O2S: Tb scintillation screen. The device was evaluated under both ambient light and X-ray exposure conditions. Initial characterization under ambient light ensured proper optical focusing; subsequently, camera settings (ISO and exposure time) were evaluated and optimized for X-ray imaging performance. Spatial resolution of the developed device was quantified using the Slanted-Edge method to derive the Modulation Transfer Function (MTF). The device achieves MTF20 values of 68 lp/mm under ambient light and 25 lp/mm under X-ray irradiation (50 and 70 kV) with Gd2O2S:Tb screen. Besides, the modularity of the developed device was confirmed by conducting the tests with LYSO:Ce and GAGG:Ce screens. Results demonstrate that this compact, cost-effective platform delivers spatial resolution comparable to clinical radiography systems, with potential applications in scientific, educational, and medical contexts where cost and portability are critical factors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study reports the development and characterization of a cost-effective X-ray imaging device built from Raspberry Pi components, including a high-quality 12.3-megapixel camera configured for indirect detection with a Gd2O2S: Tb scintillation screen. The device was evaluated under both ambient light and X-ray exposure conditions. Initial characterization under ambient light ensured proper optical focusing; subsequently, camera settings (ISO and exposure time) were evaluated and optimized for X-ray imaging performance. Spatial resolution of the developed device was quantified using the Slanted-Edge method to derive the Modulation Transfer Function (MTF). The device achieves MTF20 values of 68 lp/mm under ambient light and 25 lp/mm under X-ray irradiation (50 and 70 kV) with Gd2O2S:Tb screen. Besides, the modularity of the developed device was confirmed by conducting the tests with LYSO:Ce and GAGG:Ce screens. Results demonstrate that this compact, cost-effective platform delivers spatial resolution comparable to clinical radiography systems, with potential applications in scientific, educational, and medical contexts where cost and portability are critical factors."
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-24T08:16:12Z",
                "published_parsed": [
                    2026,
                    2,
                    24,
                    8,
                    16,
                    12,
                    1,
                    55,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det"
                },
                "authors": [
                    {
                        "name": "Nguyen Duc Ton"
                    },
                    {
                        "name": "Nguyen Thanh Luan"
                    },
                    {
                        "name": "Faizan Anjum"
                    },
                    {
                        "name": "D. Joseph Daniel"
                    },
                    {
                        "name": "Sunghwan Kim"
                    },
                    {
                        "name": "Suchart Kothan"
                    },
                    {
                        "name": "Jakrapong Kaewkhao"
                    },
                    {
                        "name": "Hong Joo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hong Joo Kim"
                },
                "author": "Hong Joo Kim"
            },
            {
                "id": "http://arxiv.org/abs/2602.20595v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20595v1",
                "title": "OptiLeak: Efficient Prompt Reconstruction via Reinforcement Learning in Multi-tenant LLM Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OptiLeak: Efficient Prompt Reconstruction via Reinforcement Learning in Multi-tenant LLM Services"
                },
                "updated": "2026-02-24T06:35:22Z",
                "updated_parsed": [
                    2026,
                    2,
                    24,
                    6,
                    35,
                    22,
                    1,
                    55,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20595v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multi-tenant LLM serving frameworks widely adopt shared Key-Value caches to enhance efficiency. However, this creates side-channel vulnerabilities enabling prompt leakage attacks. Prior studies identified these attack surfaces yet focused on expanding attack vectors rather than optimizing attack performance, reporting impractically high attack costs that underestimate the true privacy risk. We propose OptiLeak, a reinforcement learning-enhanced framework that maximizes prompt reconstruction efficiency through two-stage fine-tuning. Our key insight is that domain-specific ``hard tokens'' -- terms difficult to predict yet carrying sensitive information -- can be automatically identified via likelihood ranking and used to construct preference pairs for Direct Preference Optimization, eliminating manual annotation. This enables effective preference alignment while avoiding the overfitting issues of extended supervised fine-tuning. Evaluated on three benchmarks spanning medical and financial domains, OptiLeak achieves up to $12.48\\times$ reduction in average requests per token compared to baseline approaches, with consistent improvements across model scales from 3B to 14B parameters. Our findings demonstrate that cache-based prompt leakage poses a more severe threat than previously reported, underscoring the need for robust cache isolation in production deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-tenant LLM serving frameworks widely adopt shared Key-Value caches to enhance efficiency. However, this creates side-channel vulnerabilities enabling prompt leakage attacks. Prior studies identified these attack surfaces yet focused on expanding attack vectors rather than optimizing attack performance, reporting impractically high attack costs that underestimate the true privacy risk. We propose OptiLeak, a reinforcement learning-enhanced framework that maximizes prompt reconstruction efficiency through two-stage fine-tuning. Our key insight is that domain-specific ``hard tokens'' -- terms difficult to predict yet carrying sensitive information -- can be automatically identified via likelihood ranking and used to construct preference pairs for Direct Preference Optimization, eliminating manual annotation. This enables effective preference alignment while avoiding the overfitting issues of extended supervised fine-tuning. Evaluated on three benchmarks spanning medical and financial domains, OptiLeak achieves up to $12.48\\times$ reduction in average requests per token compared to baseline approaches, with consistent improvements across model scales from 3B to 14B parameters. Our findings demonstrate that cache-based prompt leakage poses a more severe threat than previously reported, underscoring the need for robust cache isolation in production deployments."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-24T06:35:22Z",
                "published_parsed": [
                    2026,
                    2,
                    24,
                    6,
                    35,
                    22,
                    1,
                    55,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Longxiang Wang"
                    },
                    {
                        "name": "Xiang Zheng"
                    },
                    {
                        "name": "Xuhao Zhang"
                    },
                    {
                        "name": "Yao Zhang"
                    },
                    {
                        "name": "Ye Wu"
                    },
                    {
                        "name": "Cong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Cong Wang"
                },
                "author": "Cong Wang"
            },
            {
                "id": "http://arxiv.org/abs/2602.20515v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20515v1",
                "title": "FAST-Prefill: FPGA Accelerated Sparse Attention for Long Context LLM Prefill",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAST-Prefill: FPGA Accelerated Sparse Attention for Long Context LLM Prefill"
                },
                "updated": "2026-02-24T03:36:25Z",
                "updated_parsed": [
                    2026,
                    2,
                    24,
                    3,
                    36,
                    25,
                    1,
                    55,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20515v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In long-context large language model (LLM) inference, the prefill stage dominates computation due to self-attention over the complete input context. Sparse attention significantly reduces self-attention computation by limiting each token's interactions to a subset of tokens. The attention sparsity pattern varies across input prompts, and within a prompt, each attention head can follow a distinct pattern. This makes attention sparsity dynamic. The requirement of generating the sparsity pattern, combined with limited data reuse in attention, shifts the prefill compute to being memory-bound. This, in addition to the huge energy requirements for long-context inference on GPU, motivates FPGAs as good candidates for accelerating dynamic long-context inference.\n  To tackle these challenges, we propose FAST-Prefill, the first FPGA accelerator for long-context prefill-stage inference with dynamic sparse attention. To efficiently generate sparse indices, we propose a \\textit{fused pipeline unit with a memory-aware execution order} to reduce large tensors and irregular memory accesses. To reduce off-chip memory traffic for accessing the KV cache, we utilize the memory hierarchy to design a \\textit{liveness-driven, dual-tier cache}. For high-throughput matrix multiplication, we design a \\textit{hybrid Matrix Processing Unit (MPU)} with DSPs and bit-plane decomposition using LUTs. We implement FAST-Prefill on Alveo U280 and evaluate it on the Llama and Qwen models (batch size = 1) for context lengths ranging from 4K to 128K tokens. We demonstrate an average speedup of up to 2.5$\\times$ in TTFT and 4.5$\\times$ improvement in energy efficiency over GPU implementation on Nvidia A5000 GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In long-context large language model (LLM) inference, the prefill stage dominates computation due to self-attention over the complete input context. Sparse attention significantly reduces self-attention computation by limiting each token's interactions to a subset of tokens. The attention sparsity pattern varies across input prompts, and within a prompt, each attention head can follow a distinct pattern. This makes attention sparsity dynamic. The requirement of generating the sparsity pattern, combined with limited data reuse in attention, shifts the prefill compute to being memory-bound. This, in addition to the huge energy requirements for long-context inference on GPU, motivates FPGAs as good candidates for accelerating dynamic long-context inference.\n  To tackle these challenges, we propose FAST-Prefill, the first FPGA accelerator for long-context prefill-stage inference with dynamic sparse attention. To efficiently generate sparse indices, we propose a \\textit{fused pipeline unit with a memory-aware execution order} to reduce large tensors and irregular memory accesses. To reduce off-chip memory traffic for accessing the KV cache, we utilize the memory hierarchy to design a \\textit{liveness-driven, dual-tier cache}. For high-throughput matrix multiplication, we design a \\textit{hybrid Matrix Processing Unit (MPU)} with DSPs and bit-plane decomposition using LUTs. We implement FAST-Prefill on Alveo U280 and evaluate it on the Llama and Qwen models (batch size = 1) for context lengths ranging from 4K to 128K tokens. We demonstrate an average speedup of up to 2.5$\\times$ in TTFT and 4.5$\\times$ improvement in energy efficiency over GPU implementation on Nvidia A5000 GPU."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-24T03:36:25Z",
                "published_parsed": [
                    2026,
                    2,
                    24,
                    3,
                    36,
                    25,
                    1,
                    55,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Rakshith Jayanth"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna"
            },
            {
                "id": "http://arxiv.org/abs/2602.20497v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20497v1",
                "title": "LESA: Learnable Stage-Aware Predictors for Diffusion Model Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LESA: Learnable Stage-Aware Predictors for Diffusion Model Acceleration"
                },
                "updated": "2026-02-24T02:53:28Z",
                "updated_parsed": [
                    2026,
                    2,
                    24,
                    2,
                    53,
                    28,
                    1,
                    55,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20497v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models have achieved remarkable success in image and video generation tasks. However, the high computational demands of Diffusion Transformers (DiTs) pose a significant challenge to their practical deployment. While feature caching is a promising acceleration strategy, existing methods based on simple reusing or training-free forecasting struggle to adapt to the complex, stage-dependent dynamics of the diffusion process, often resulting in quality degradation and failing to maintain consistency with the standard denoising process. To address this, we propose a LEarnable Stage-Aware (LESA) predictor framework based on two-stage training. Our approach leverages a Kolmogorov-Arnold Network (KAN) to accurately learn temporal feature mappings from data. We further introduce a multi-stage, multi-expert architecture that assigns specialized predictors to different noise-level stages, enabling more precise and robust feature forecasting. Extensive experiments show our method achieves significant acceleration while maintaining high-fidelity generation. Experiments demonstrate 5.00x acceleration on FLUX.1-dev with minimal quality degradation (1.0% drop), 6.25x speedup on Qwen-Image with a 20.2% quality improvement over the previous SOTA (TaylorSeer), and 5.00x acceleration on HunyuanVideo with a 24.7% PSNR improvement over TaylorSeer. State-of-the-art performance on both text-to-image and text-to-video synthesis validates the effectiveness and generalization capability of our training-based framework across different models. Our code is included in the supplementary materials and will be released on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have achieved remarkable success in image and video generation tasks. However, the high computational demands of Diffusion Transformers (DiTs) pose a significant challenge to their practical deployment. While feature caching is a promising acceleration strategy, existing methods based on simple reusing or training-free forecasting struggle to adapt to the complex, stage-dependent dynamics of the diffusion process, often resulting in quality degradation and failing to maintain consistency with the standard denoising process. To address this, we propose a LEarnable Stage-Aware (LESA) predictor framework based on two-stage training. Our approach leverages a Kolmogorov-Arnold Network (KAN) to accurately learn temporal feature mappings from data. We further introduce a multi-stage, multi-expert architecture that assigns specialized predictors to different noise-level stages, enabling more precise and robust feature forecasting. Extensive experiments show our method achieves significant acceleration while maintaining high-fidelity generation. Experiments demonstrate 5.00x acceleration on FLUX.1-dev with minimal quality degradation (1.0% drop), 6.25x speedup on Qwen-Image with a 20.2% quality improvement over the previous SOTA (TaylorSeer), and 5.00x acceleration on HunyuanVideo with a 24.7% PSNR improvement over TaylorSeer. State-of-the-art performance on both text-to-image and text-to-video synthesis validates the effectiveness and generalization capability of our training-based framework across different models. Our code is included in the supplementary materials and will be released on GitHub."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-24T02:53:28Z",
                "published_parsed": [
                    2026,
                    2,
                    24,
                    2,
                    53,
                    28,
                    1,
                    55,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Haowen Xu"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.03475v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.03475v3",
                "title": "ContextPilot: Fast Long-Context Inference via Context Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextPilot: Fast Long-Context Inference via Context Reuse"
                },
                "updated": "2026-02-23T20:26:15Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    20,
                    26,
                    15,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.03475v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.03475v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "AI applications increasingly depend on long-context inference, where LLMs consume substantial context to support stronger reasoning. Common examples include retrieval-augmented generation, agent memory layers, and multi-agent orchestration. As input contexts get longer, prefill latency becomes the main bottleneck. Yet today's prefill acceleration techniques face a trade-off: they either preserve reasoning quality but deliver little KV-cache reuse, or improve reuse at the cost of degraded reasoning quality.\n  We present ContextPilot, a system that accelerates prefill by introducing context reuse as a new mechanism for faster long-context inference. ContextPilot introduces a context index to identify overlapping context blocks across LLM interactions (e.g., across users and turns). It further proposes context ordering and de-duplication techniques to maximize KV-cache reuse. To preserve reasoning quality under reuse, it introduces succinct context annotations that prevent quality degradation. Finally, ContextPilot is built around a modular architecture with a clean interface that integrates with existing inference engines. Extensive evaluation shows that ContextPilot reduces LLM prefill latency by up to $3\\times{}$ compared to state-of-the-art methods while preserving reasoning quality. At longer context lengths, it can even improve reasoning quality. ContextPilot is open-sourced at: https://github.com/EfficientContext/ContextPilot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI applications increasingly depend on long-context inference, where LLMs consume substantial context to support stronger reasoning. Common examples include retrieval-augmented generation, agent memory layers, and multi-agent orchestration. As input contexts get longer, prefill latency becomes the main bottleneck. Yet today's prefill acceleration techniques face a trade-off: they either preserve reasoning quality but deliver little KV-cache reuse, or improve reuse at the cost of degraded reasoning quality.\n  We present ContextPilot, a system that accelerates prefill by introducing context reuse as a new mechanism for faster long-context inference. ContextPilot introduces a context index to identify overlapping context blocks across LLM interactions (e.g., across users and turns). It further proposes context ordering and de-duplication techniques to maximize KV-cache reuse. To preserve reasoning quality under reuse, it introduces succinct context annotations that prevent quality degradation. Finally, ContextPilot is built around a modular architecture with a clean interface that integrates with existing inference engines. Extensive evaluation shows that ContextPilot reduces LLM prefill latency by up to $3\\times{}$ compared to state-of-the-art methods while preserving reasoning quality. At longer context lengths, it can even improve reasoning quality. ContextPilot is open-sourced at: https://github.com/EfficientContext/ContextPilot."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-05T13:59:01Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    59,
                    1,
                    2,
                    309,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yinsicheng Jiang"
                    },
                    {
                        "name": "Yeqi Huang"
                    },
                    {
                        "name": "Liang Cheng"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Xuan Sun"
                    },
                    {
                        "name": "Luo Mai"
                    }
                ],
                "author_detail": {
                    "name": "Luo Mai"
                },
                "author": "Luo Mai"
            },
            {
                "id": "http://arxiv.org/abs/2602.19816v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19816v1",
                "title": "Depth-Structured Music Recurrence: Budgeted Recurrent Attention for Full-Piece Symbolic Music Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth-Structured Music Recurrence: Budgeted Recurrent Attention for Full-Piece Symbolic Music Modeling"
                },
                "updated": "2026-02-23T13:13:41Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    13,
                    41,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19816v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long-context modeling is essential for symbolic music generation, since motif repetition and developmental variation can span thousands of musical events. However, practical composition and performance workflows frequently rely on resource-limited devices (e.g., electronic instruments and portable computers), making heavy memory and attention computation difficult to deploy. We introduce Depth-Structured Music Recurrence (DSMR), a recurrent long-context Transformer for full-piece symbolic music modeling that extends context beyond fixed-length excerpts via segment-level recurrence with detached cross-segment states, featuring a layer-wise memory-horizon schedule that budgets recurrent KV states across depth. DSMR is trained in a single left-to-right pass over each complete composition, akin to how a musician experiences it from beginning to end, while carrying recurrent cross-segment states forward. Within this recurrent framework, we systematically study how depth-wise horizon allocations affect optimization, best-checkpoint perplexity, and efficiency. By allocating different history-window lengths across layers while keeping the total recurrent-state budget fixed, DSMR creates depth-dependent temporal receptive fields within a recurrent attention stack without reducing compute depth. Our main instantiation is a two-scale DSMR schedule that allocates long history windows to lower layers and a uniform short window to the remaining layers. Experiments on the piano performance dataset MAESTRO demonstrate that two-scale DSMR provides a practical quality--efficiency recipe for full-length long-context symbolic music modeling with recurrent attention under limited computational resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context modeling is essential for symbolic music generation, since motif repetition and developmental variation can span thousands of musical events. However, practical composition and performance workflows frequently rely on resource-limited devices (e.g., electronic instruments and portable computers), making heavy memory and attention computation difficult to deploy. We introduce Depth-Structured Music Recurrence (DSMR), a recurrent long-context Transformer for full-piece symbolic music modeling that extends context beyond fixed-length excerpts via segment-level recurrence with detached cross-segment states, featuring a layer-wise memory-horizon schedule that budgets recurrent KV states across depth. DSMR is trained in a single left-to-right pass over each complete composition, akin to how a musician experiences it from beginning to end, while carrying recurrent cross-segment states forward. Within this recurrent framework, we systematically study how depth-wise horizon allocations affect optimization, best-checkpoint perplexity, and efficiency. By allocating different history-window lengths across layers while keeping the total recurrent-state budget fixed, DSMR creates depth-dependent temporal receptive fields within a recurrent attention stack without reducing compute depth. Our main instantiation is a two-scale DSMR schedule that allocates long history windows to lower layers and a uniform short window to the remaining layers. Experiments on the piano performance dataset MAESTRO demonstrate that two-scale DSMR provides a practical quality--efficiency recipe for full-length long-context symbolic music modeling with recurrent attention under limited computational resources."
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T13:13:41Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    13,
                    41,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD"
                },
                "authors": [
                    {
                        "name": "Yungang Yi"
                    }
                ],
                "author_detail": {
                    "name": "Yungang Yi"
                },
                "author": "Yungang Yi"
            },
            {
                "id": "http://arxiv.org/abs/2602.19811v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19811v1",
                "title": "Semantic Caching for OLAP via LLM-Based Query Canonicalization (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching for OLAP via LLM-Based Query Canonicalization (Extended Version)"
                },
                "updated": "2026-02-23T13:12:05Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    12,
                    5,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19811v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Analytical workloads exhibit substantial semantic repetition, yet most production caches key entries by SQL surface form (text or AST), fragmenting reuse across BI tools, notebooks, and NL interfaces. We introduce a safety-first middleware cache for dashboard-style OLAP over star schemas that canonicalizes both SQL and NL into a unified key space -- the OLAP Intent Signature -- capturing measures, grouping levels, filters, and time windows. Reuse requires exact intent matches under strict schema validation and confidence-gated NL acceptance; two correctness-preserving derivations (roll-up, filter-down) extend coverage without approximate matching. Across TPC-DS, SSB, and NYC TLC (1,395 queries), we achieve 82% hit rate versus 28% (text) and 56% (AST) with zero false hits; derivations double hit rate on hierarchical queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analytical workloads exhibit substantial semantic repetition, yet most production caches key entries by SQL surface form (text or AST), fragmenting reuse across BI tools, notebooks, and NL interfaces. We introduce a safety-first middleware cache for dashboard-style OLAP over star schemas that canonicalizes both SQL and NL into a unified key space -- the OLAP Intent Signature -- capturing measures, grouping levels, filters, and time windows. Reuse requires exact intent matches under strict schema validation and confidence-gated NL acceptance; two correctness-preserving derivations (roll-up, filter-down) extend coverage without approximate matching. Across TPC-DS, SSB, and NYC TLC (1,395 queries), we achieve 82% hit rate versus 28% (text) and 56% (AST) with zero false hits; derivations double hit rate on hierarchical queries."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T13:12:05Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    13,
                    12,
                    5,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "12 pages, 2 figures, 5 tables. Extended version of the short paper published at DOLAP 2026 (co-located with EDBT/ICDT 2026)",
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Laurent Bindschaedler"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Bindschaedler"
                },
                "author": "Laurent Bindschaedler"
            },
            {
                "id": "http://arxiv.org/abs/2602.14934v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.14934v2",
                "title": "Activation-Space Uncertainty Quantification for Pretrained Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation-Space Uncertainty Quantification for Pretrained Networks"
                },
                "updated": "2026-02-23T10:54:32Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    10,
                    54,
                    32,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.14934v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.14934v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reliable uncertainty estimates are crucial for deploying pretrained models; yet, many strong methods for quantifying uncertainty require retraining, Monte Carlo sampling, or expensive second-order computations and may alter a frozen backbone's predictions. To address this, we introduce Gaussian Process Activations (GAPA), a post-hoc method that shifts Bayesian modeling from weights to activations. GAPA replaces standard nonlinearities with Gaussian-process activations whose posterior mean exactly matches the original activation, preserving the backbone's point predictions by construction while providing closed-form epistemic variances in activation space. To scale to modern architectures, we use a sparse variational inducing-point approximation over cached training activations, combined with local k-nearest-neighbor subset conditioning, enabling deterministic single-pass uncertainty propagation without sampling, backpropagation, or second-order information. Across regression, classification, image segmentation, and language modeling, GAPA matches or outperforms strong post-hoc baselines in calibration and out-of-distribution detection while remaining efficient at test time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable uncertainty estimates are crucial for deploying pretrained models; yet, many strong methods for quantifying uncertainty require retraining, Monte Carlo sampling, or expensive second-order computations and may alter a frozen backbone's predictions. To address this, we introduce Gaussian Process Activations (GAPA), a post-hoc method that shifts Bayesian modeling from weights to activations. GAPA replaces standard nonlinearities with Gaussian-process activations whose posterior mean exactly matches the original activation, preserving the backbone's point predictions by construction while providing closed-form epistemic variances in activation space. To scale to modern architectures, we use a sparse variational inducing-point approximation over cached training activations, combined with local k-nearest-neighbor subset conditioning, enabling deterministic single-pass uncertainty propagation without sampling, backpropagation, or second-order information. Across regression, classification, image segmentation, and language modeling, GAPA matches or outperforms strong post-hoc baselines in calibration and out-of-distribution detection while remaining efficient at test time."
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-16T17:17:08Z",
                "published_parsed": [
                    2026,
                    2,
                    16,
                    17,
                    17,
                    8,
                    0,
                    47,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML"
                },
                "authors": [
                    {
                        "name": "Richard Bergna"
                    },
                    {
                        "name": "Stefan Depeweg"
                    },
                    {
                        "name": "Sergio Calvo-Ordoez"
                    },
                    {
                        "name": "Jonathan Plenk"
                    },
                    {
                        "name": "Alvaro Cartea"
                    },
                    {
                        "name": "Jose Miguel Hernndez-Lobato"
                    }
                ],
                "author_detail": {
                    "name": "Jose Miguel Hernndez-Lobato"
                },
                "author": "Jose Miguel Hernndez-Lobato"
            },
            {
                "id": "http://arxiv.org/abs/2601.17354v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.17354v3",
                "title": "PocketGS: On-Device Training of 3D Gaussian Splatting for High Perceptual Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PocketGS: On-Device Training of 3D Gaussian Splatting for High Perceptual Modeling"
                },
                "updated": "2026-02-23T08:13:48Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    8,
                    13,
                    48,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.17354v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.17354v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Efficient and high-fidelity 3D scene modeling is a long-standing pursuit in computer graphics. While recent 3D Gaussian Splatting (3DGS) methods achieve impressive real-time modeling performance, they rely on resource-unconstrained training assumptions that fail on mobile devices, which are limited by minute-scale training budgets and hardware-available peak-memory. We present PocketGS, a mobile scene modeling paradigm that enables on-device 3DGS training under these tightly coupled constraints while preserving high perceptual fidelity. Our method resolves the fundamental contradictions of standard 3DGS through three co-designed operators: G builds geometry-faithful point-cloud priors; I injects local surface statistics to seed anisotropic Gaussians, thereby reducing early conditioning gaps; and T unrolls alpha compositing with cached intermediates and index-mapped gradient scattering for stable mobile backpropagation. Collectively, these operators satisfy the competing requirements of training efficiency, memory compactness, and modeling fidelity. Extensive experiments demonstrate that PocketGS is able to outperform the powerful mainstream workstation 3DGS baseline to deliver high-quality reconstructions, enabling a fully on-device, practical capture-to-rendering workflow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and high-fidelity 3D scene modeling is a long-standing pursuit in computer graphics. While recent 3D Gaussian Splatting (3DGS) methods achieve impressive real-time modeling performance, they rely on resource-unconstrained training assumptions that fail on mobile devices, which are limited by minute-scale training budgets and hardware-available peak-memory. We present PocketGS, a mobile scene modeling paradigm that enables on-device 3DGS training under these tightly coupled constraints while preserving high perceptual fidelity. Our method resolves the fundamental contradictions of standard 3DGS through three co-designed operators: G builds geometry-faithful point-cloud priors; I injects local surface statistics to seed anisotropic Gaussians, thereby reducing early conditioning gaps; and T unrolls alpha compositing with cached intermediates and index-mapped gradient scattering for stable mobile backpropagation. Collectively, these operators satisfy the competing requirements of training efficiency, memory compactness, and modeling fidelity. Extensive experiments demonstrate that PocketGS is able to outperform the powerful mainstream workstation 3DGS baseline to deliver high-quality reconstructions, enabling a fully on-device, practical capture-to-rendering workflow."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-24T07:58:53Z",
                "published_parsed": [
                    2026,
                    1,
                    24,
                    7,
                    58,
                    53,
                    5,
                    24,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Wenzhi Guo"
                    },
                    {
                        "name": "Guangchi Fang"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Bing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bing Wang"
                },
                "author": "Bing Wang"
            },
            {
                "id": "http://arxiv.org/abs/2602.19567v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19567v1",
                "title": "Spritz: Path-Aware Load Balancing in Low-Diameter Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spritz: Path-Aware Load Balancing in Low-Diameter Networks"
                },
                "updated": "2026-02-23T07:33:25Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    7,
                    33,
                    25,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19567v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19567v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Low-diameter topologies such as Dragonfly and Slim Fly are increasingly adopted in HPC and datacenter networks, yet existing load balancing techniques either rely on proprietary in-network mechanisms or fail to utilize the full path diversity of these topologies. We introduce Spritz, a flexible sender-based load balancing framework that shifts adaptive topology-aware routing to the endpoints using only standard Ethernet features. We propose two algorithms, Spritz-Scout and Spritz-Spray that, respectively, explore and adaptively cache efficient paths using ECN, packet trimming, and timeout feedback. Through simulation on Dragonfly and Slim Fly topologies with over 1000 endpoints, Spritz outperforms ECMP, UGAL-L, and prior sender-based approaches by up to 1.8x in flow completion time under AI training and datacenter workloads, while offering robust failover with performance improvements of up to 25.4x under link failures, all without additional hardware support. Spritz enables datacenter-scale, commodity Ethernet networks to efficiently leverage low-diameter topologies, offering unified routing and load balancing for the Ultra Ethernet era.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-diameter topologies such as Dragonfly and Slim Fly are increasingly adopted in HPC and datacenter networks, yet existing load balancing techniques either rely on proprietary in-network mechanisms or fail to utilize the full path diversity of these topologies. We introduce Spritz, a flexible sender-based load balancing framework that shifts adaptive topology-aware routing to the endpoints using only standard Ethernet features. We propose two algorithms, Spritz-Scout and Spritz-Spray that, respectively, explore and adaptively cache efficient paths using ECN, packet trimming, and timeout feedback. Through simulation on Dragonfly and Slim Fly topologies with over 1000 endpoints, Spritz outperforms ECMP, UGAL-L, and prior sender-based approaches by up to 1.8x in flow completion time under AI training and datacenter workloads, while offering robust failover with performance improvements of up to 25.4x under link failures, all without additional hardware support. Spritz enables datacenter-scale, commodity Ethernet networks to efficiently leverage low-diameter topologies, offering unified routing and load balancing for the Ultra Ethernet era."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T07:33:25Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    7,
                    33,
                    25,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "arxiv_journal_ref": "Proc. 40th IEEE International Parallel and Distributed Processing Symposium (IPDPS), 2026",
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler"
            },
            {
                "id": "http://arxiv.org/abs/2602.19506v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19506v1",
                "title": "Relational Feature Caching for Accelerating Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relational Feature Caching for Accelerating Diffusion Transformers"
                },
                "updated": "2026-02-23T04:45:38Z",
                "updated_parsed": [
                    2026,
                    2,
                    23,
                    4,
                    45,
                    38,
                    0,
                    54,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19506v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Feature caching approaches accelerate diffusion transformers (DiTs) by storing the output features of computationally expensive modules at certain timesteps, and exploiting them for subsequent steps to reduce redundant computations. Recent forecasting-based caching approaches employ temporal extrapolation techniques to approximate the output features with cached ones. Although effective, relying exclusively on temporal extrapolation still suffers from significant prediction errors, leading to performance degradation. Through a detailed analysis, we find that 1) these errors stem from the irregular magnitude of changes in the output features, and 2) an input feature of a module is strongly correlated with the corresponding output. Based on this, we propose relational feature caching (RFC), a novel framework that leverages the input-output relationship to enhance the accuracy of the feature prediction. Specifically, we introduce relational feature estimation (RFE) to estimate the magnitude of changes in the output features from the inputs, enabling more accurate feature predictions. We also present relational cache scheduling (RCS), which estimates the prediction errors using the input features and performs full computations only when the errors are expected to be substantial. Extensive experiments across various DiT models demonstrate that RFC consistently outperforms prior approaches significantly. Project page is available at https://cvlab.yonsei.ac.kr/projects/RFC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching approaches accelerate diffusion transformers (DiTs) by storing the output features of computationally expensive modules at certain timesteps, and exploiting them for subsequent steps to reduce redundant computations. Recent forecasting-based caching approaches employ temporal extrapolation techniques to approximate the output features with cached ones. Although effective, relying exclusively on temporal extrapolation still suffers from significant prediction errors, leading to performance degradation. Through a detailed analysis, we find that 1) these errors stem from the irregular magnitude of changes in the output features, and 2) an input feature of a module is strongly correlated with the corresponding output. Based on this, we propose relational feature caching (RFC), a novel framework that leverages the input-output relationship to enhance the accuracy of the feature prediction. Specifically, we introduce relational feature estimation (RFE) to estimate the magnitude of changes in the output features from the inputs, enabling more accurate feature predictions. We also present relational cache scheduling (RCS), which estimates the prediction errors using the input features and performs full computations only when the errors are expected to be substantial. Extensive experiments across various DiT models demonstrate that RFC consistently outperforms prior approaches significantly. Project page is available at https://cvlab.yonsei.ac.kr/projects/RFC"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T04:45:38Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    4,
                    45,
                    38,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "Accepted to ICLR 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Byunggwan Son"
                    },
                    {
                        "name": "Jeimin Jeon"
                    },
                    {
                        "name": "Jeongwoo Choi"
                    },
                    {
                        "name": "Bumsub Ham"
                    }
                ],
                "author_detail": {
                    "name": "Bumsub Ham"
                },
                "author": "Bumsub Ham"
            },
            {
                "id": "http://arxiv.org/abs/2510.03346v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.03346v3",
                "title": "KVComm: Enabling Efficient LLM Communication through Selective KV Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVComm: Enabling Efficient LLM Communication through Selective KV Sharing"
                },
                "updated": "2026-02-22T15:15:44Z",
                "updated_parsed": [
                    2026,
                    2,
                    22,
                    15,
                    15,
                    44,
                    6,
                    53,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.03346v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.03346v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are increasingly deployed in multi-agent systems, where effective inter-model communication is crucial. Existing communication protocols either rely on natural language, incurring high inference costs and information loss, or on hidden states, which suffer from information concentration bias and inefficiency. To address these limitations, we propose KVComm, a novel communication framework that enables efficient communication between LLMs through selective sharing of KV pairs. KVComm leverages the rich information encoded in the KV pairs while avoiding the pitfalls of hidden states. We introduce a KV layer-wise selection strategy based on attention importance scores with a Gaussian prior to identify the most informative KV pairs for communication. Extensive experiments across diverse tasks and model pairs demonstrate that KVComm achieves comparable performance to the upper-bound method, which directly merges inputs to one model without any communication, while transmitting as few as 30\\% of layers' KV pairs. Our study highlights the potential of KV pairs as an effective medium for inter-LLM communication, paving the way for scalable and efficient multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in multi-agent systems, where effective inter-model communication is crucial. Existing communication protocols either rely on natural language, incurring high inference costs and information loss, or on hidden states, which suffer from information concentration bias and inefficiency. To address these limitations, we propose KVComm, a novel communication framework that enables efficient communication between LLMs through selective sharing of KV pairs. KVComm leverages the rich information encoded in the KV pairs while avoiding the pitfalls of hidden states. We introduce a KV layer-wise selection strategy based on attention importance scores with a Gaussian prior to identify the most informative KV pairs for communication. Extensive experiments across diverse tasks and model pairs demonstrate that KVComm achieves comparable performance to the upper-bound method, which directly merges inputs to one model without any communication, while transmitting as few as 30\\% of layers' KV pairs. Our study highlights the potential of KV pairs as an effective medium for inter-LLM communication, paving the way for scalable and efficient multi-agent systems."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-02T16:01:54Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    1,
                    54,
                    3,
                    275,
                    0
                ],
                "arxiv_comment": "ICLR 2026",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Xiangyu Shi"
                    },
                    {
                        "name": "Marco Chiesa"
                    },
                    {
                        "name": "Gerald Q. Maguire"
                    },
                    {
                        "name": "Dejan Kostic"
                    }
                ],
                "author_detail": {
                    "name": "Dejan Kostic"
                },
                "author": "Dejan Kostic"
            },
            {
                "id": "http://arxiv.org/abs/2511.07399v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.07399v2",
                "title": "StreamDiffusionV2: A Streaming System for Dynamic and Interactive Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamDiffusionV2: A Streaming System for Dynamic and Interactive Video Generation"
                },
                "updated": "2026-02-22T13:10:27Z",
                "updated_parsed": [
                    2026,
                    2,
                    22,
                    13,
                    10,
                    27,
                    6,
                    53,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.07399v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.07399v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative models are reshaping the live-streaming industry by redefining how content is created, styled, and delivered. Previous image-based streaming diffusion models have powered efficient and creative live streaming products but have hit limits on temporal consistency due to the foundation of image-based designs. Recent advances in video diffusion have markedly improved temporal consistency and sampling efficiency for offline generation. However, offline generation systems primarily optimize throughput by batching large workloads. In contrast, live online streaming operates under strict service-level objectives (SLOs): time-to-first-frame must be minimal, and every frame must meet a per-frame deadline with low jitter. Besides, scalable multi-GPU serving for real-time streams remains largely unresolved so far. To address this, we present StreamDiffusionV2, a training-free pipeline for interactive live streaming with video diffusion models. StreamDiffusionV2 integrates an SLO-aware batching scheduler and a block scheduler, together with a sink-token--guided rolling KV cache, a motion-aware noise controller, and other system-level optimizations. Moreover, we introduce a scalable pipeline orchestration that parallelizes the diffusion process across denoising steps and network layers, achieving near-linear FPS scaling without violating latency guarantees. The system scales seamlessly across heterogeneous GPU environments and supports flexible denoising steps (e.g., 1--4), enabling both ultra-low-latency and higher-quality modes. Without TensorRT or quantization, StreamDiffusionV2 renders the first frame within 0.5s and attains 58.28 FPS with a 14B-parameter model and 64.52 FPS with a 1.3B-parameter model on four H100 GPUs, making state-of-the-art generative live streaming practical and accessible--from individual creators to enterprise-scale platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models are reshaping the live-streaming industry by redefining how content is created, styled, and delivered. Previous image-based streaming diffusion models have powered efficient and creative live streaming products but have hit limits on temporal consistency due to the foundation of image-based designs. Recent advances in video diffusion have markedly improved temporal consistency and sampling efficiency for offline generation. However, offline generation systems primarily optimize throughput by batching large workloads. In contrast, live online streaming operates under strict service-level objectives (SLOs): time-to-first-frame must be minimal, and every frame must meet a per-frame deadline with low jitter. Besides, scalable multi-GPU serving for real-time streams remains largely unresolved so far. To address this, we present StreamDiffusionV2, a training-free pipeline for interactive live streaming with video diffusion models. StreamDiffusionV2 integrates an SLO-aware batching scheduler and a block scheduler, together with a sink-token--guided rolling KV cache, a motion-aware noise controller, and other system-level optimizations. Moreover, we introduce a scalable pipeline orchestration that parallelizes the diffusion process across denoising steps and network layers, achieving near-linear FPS scaling without violating latency guarantees. The system scales seamlessly across heterogeneous GPU environments and supports flexible denoising steps (e.g., 1--4), enabling both ultra-low-latency and higher-quality modes. Without TensorRT or quantization, StreamDiffusionV2 renders the first frame within 0.5s and attains 58.28 FPS with a 14B-parameter model and 64.52 FPS with a 1.3B-parameter model on four H100 GPUs, making state-of-the-art generative live streaming practical and accessible--from individual creators to enterprise-scale platforms."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T18:51:28Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    18,
                    51,
                    28,
                    0,
                    314,
                    0
                ],
                "arxiv_comment": "Accepted by MLSys 2026. Project Page: http://streamdiffusionv2.github.io",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Tianrui Feng"
                    },
                    {
                        "name": "Zhi Li"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Haocheng Xi"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Lvmin Zhang"
                    },
                    {
                        "name": "Keting Yang"
                    },
                    {
                        "name": "Kelly Peng"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Maneesh Agrawala"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Akio Kodaira"
                    },
                    {
                        "name": "Chenfeng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenfeng Xu"
                },
                "author": "Chenfeng Xu"
            },
            {
                "id": "http://arxiv.org/abs/2602.19137v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19137v1",
                "title": "Derivation Depth as an Information Metric: Axioms, Coding Theorems, and Storage--Computation Tradeoffs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Derivation Depth as an Information Metric: Axioms, Coding Theorems, and Storage--Computation Tradeoffs"
                },
                "updated": "2026-02-22T11:44:34Z",
                "updated_parsed": [
                    2026,
                    2,
                    22,
                    11,
                    44,
                    34,
                    6,
                    53,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19137v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce derivation depth-a computable metric of the reasoning effort needed to answer a query based on a given set of premises. We model information as a two-layered structure linking abstract knowledge with physical carriers, and separate essential core facts from operational shortcuts. For any finite premise base, we define and prove the computability of derivation depth. By encoding reasoning traces and applying information-theoretic incompressibility arguments, we establish fundamental bounds linking depth to the descriptive complexity of queries. For frequently asked, information-rich queries, the minimal description length grows proportionally to depth times the logarithm of the knowledge base size. This leads to a practical storage-computation tradeoff: queries accessed beyond a critical threshold become cheaper to cache than recompute. We formulate optimal cache allocation as a mathematical optimization problem solvable with approximation guarantees and extend the framework to handle noisy or incomplete knowledge bases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce derivation depth-a computable metric of the reasoning effort needed to answer a query based on a given set of premises. We model information as a two-layered structure linking abstract knowledge with physical carriers, and separate essential core facts from operational shortcuts. For any finite premise base, we define and prove the computability of derivation depth. By encoding reasoning traces and applying information-theoretic incompressibility arguments, we establish fundamental bounds linking depth to the descriptive complexity of queries. For frequently asked, information-rich queries, the minimal description length grows proportionally to depth times the logarithm of the knowledge base size. This leads to a practical storage-computation tradeoff: queries accessed beyond a critical threshold become cheaper to cache than recompute. We formulate optimal cache allocation as a mathematical optimization problem solvable with approximation guarantees and extend the framework to handle noisy or incomplete knowledge bases."
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-22T11:44:34Z",
                "published_parsed": [
                    2026,
                    2,
                    22,
                    11,
                    44,
                    34,
                    6,
                    53,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT"
                },
                "authors": [
                    {
                        "name": "Jianfeng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Xu"
                },
                "author": "Jianfeng Xu"
            },
            {
                "id": "http://arxiv.org/abs/2512.22420v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.22420v3",
                "title": "Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving"
                },
                "updated": "2026-02-22T03:46:24Z",
                "updated_parsed": [
                    2026,
                    2,
                    22,
                    3,
                    46,
                    24,
                    6,
                    53,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.22420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.22420v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Speculative decoding (SD) accelerates LLM inference by verifying draft tokens in parallel. However, this method presents a critical trade-off: it improves throughput in low-load, memory-bound systems but degrades performance in high-load, compute-bound environments due to verification overhead. Existing Speculative Decoding strategies typically rely on static speculative lengths, failing to adapt to fluctuating request loads or identify the optimal moment to halt speculation. The cost of restarting speculative inference also remains unquantified. During traffic surges, the marginal utility of speculation diminishes; yet, the draft model's persistent memory footprint competes for available KV cache. This resource contention limits the maximum batch size, thereby degrading overall system throughput. To overcome this, we propose Nightjar, a resource-aware adaptive speculative framework. It first adjusts to the request load by dynamically selecting the optimal speculative length for different batch sizes. Crucially, upon detecting significant request queuing or KV cache shortages, it disables speculative decoding and offloads the draft model to the CPU. This reclaims memory for the KV cache, thereby facilitating larger batch sizes and maximizing overall system throughput. Experiments show that Nightjar achieves up to 27.29% higher throughput and 12.90% lower latency compared to standard speculative decoding under dynamic request arrival rates in real-time LLM serving scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD) accelerates LLM inference by verifying draft tokens in parallel. However, this method presents a critical trade-off: it improves throughput in low-load, memory-bound systems but degrades performance in high-load, compute-bound environments due to verification overhead. Existing Speculative Decoding strategies typically rely on static speculative lengths, failing to adapt to fluctuating request loads or identify the optimal moment to halt speculation. The cost of restarting speculative inference also remains unquantified. During traffic surges, the marginal utility of speculation diminishes; yet, the draft model's persistent memory footprint competes for available KV cache. This resource contention limits the maximum batch size, thereby degrading overall system throughput. To overcome this, we propose Nightjar, a resource-aware adaptive speculative framework. It first adjusts to the request load by dynamically selecting the optimal speculative length for different batch sizes. Crucially, upon detecting significant request queuing or KV cache shortages, it disables speculative decoding and offloads the draft model to the CPU. This reclaims memory for the KV cache, thereby facilitating larger batch sizes and maximizing overall system throughput. Experiments show that Nightjar achieves up to 27.29% higher throughput and 12.90% lower latency compared to standard speculative decoding under dynamic request arrival rates in real-time LLM serving scenarios."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-27T00:57:55Z",
                "published_parsed": [
                    2025,
                    12,
                    27,
                    0,
                    57,
                    55,
                    5,
                    361,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Zhaoning Zhang"
                    },
                    {
                        "name": "Libo Zhang"
                    },
                    {
                        "name": "Huaimin Wang"
                    },
                    {
                        "name": "Xiang Fu"
                    },
                    {
                        "name": "Zhiquan Lai"
                    }
                ],
                "author_detail": {
                    "name": "Zhiquan Lai"
                },
                "author": "Zhiquan Lai"
            },
            {
                "id": "http://arxiv.org/abs/2602.04595v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.04595v2",
                "title": "Harmonia: Algorithm-Hardware Co-Design for Memory- and Compute-Efficient BFP-based LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harmonia: Algorithm-Hardware Co-Design for Memory- and Compute-Efficient BFP-based LLM Inference"
                },
                "updated": "2026-02-22T03:10:11Z",
                "updated_parsed": [
                    2026,
                    2,
                    22,
                    3,
                    10,
                    11,
                    6,
                    53,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.04595v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.04595v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are powerful but incur high memory and computation costs. Quantization is an effective solution, with INT weights and FP activations being widely adopted to preserve accuracy. Prior works further reduce FP overhead by using block floating point (BFP) activations in linear layers, but fail to extend BFP to attention layers due to severe accuracy degradation, limiting overall efficiency. To address this challenge, we propose Harmonia, an algorithm-hardware co-design framework that enables all-layer BFP activations with a configurable hardware architecture. First, we systematically explore BFP configurations to achieve a better trade-off between accuracy and activation compression across all layers. Second, to reduce KV-cache storage and computation in attention layers, we introduce an asymmetric bit-allocation strategy and computations in attention layers,we introduce an asymmetric bit-allocation strategy combined with a hybrid offline-online outlier smoothing technique. This allow aggressive KV-cache compression from FP16 to 4-bit-mantissa BFP with only 0.3% average accuracy loss. Third, to fully exploit all-layer BFP activations, we design dedicated hardware components, including a reconfigurable PE supporting mixed data formats (BFP-INT and BPF-BFP), a real-time FP16-to-BFP converter, and a tiling-aware dataflow to reduce memory traffic. We evaluate Harmonia on GEMM operations in both linear and attention layers across eight widely used LLMs. Compared with prior works, Harmonia achieves 3.84x (up to 5.05x) higher area efficiency, 2.03x (up to 3.90x) better energy efficiency, and 3.08x (up to 4.62x) speedup on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are powerful but incur high memory and computation costs. Quantization is an effective solution, with INT weights and FP activations being widely adopted to preserve accuracy. Prior works further reduce FP overhead by using block floating point (BFP) activations in linear layers, but fail to extend BFP to attention layers due to severe accuracy degradation, limiting overall efficiency. To address this challenge, we propose Harmonia, an algorithm-hardware co-design framework that enables all-layer BFP activations with a configurable hardware architecture. First, we systematically explore BFP configurations to achieve a better trade-off between accuracy and activation compression across all layers. Second, to reduce KV-cache storage and computation in attention layers, we introduce an asymmetric bit-allocation strategy and computations in attention layers,we introduce an asymmetric bit-allocation strategy combined with a hybrid offline-online outlier smoothing technique. This allow aggressive KV-cache compression from FP16 to 4-bit-mantissa BFP with only 0.3% average accuracy loss. Third, to fully exploit all-layer BFP activations, we design dedicated hardware components, including a reconfigurable PE supporting mixed data formats (BFP-INT and BPF-BFP), a real-time FP16-to-BFP converter, and a tiling-aware dataflow to reduce memory traffic. We evaluate Harmonia on GEMM operations in both linear and attention layers across eight widely used LLMs. Compared with prior works, Harmonia achieves 3.84x (up to 5.05x) higher area efficiency, 2.03x (up to 3.90x) better energy efficiency, and 3.08x (up to 4.62x) speedup on average."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-04T14:22:08Z",
                "published_parsed": [
                    2026,
                    2,
                    4,
                    14,
                    22,
                    8,
                    2,
                    35,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Jieyu Li"
                    },
                    {
                        "name": "Yanan Sun"
                    },
                    {
                        "name": "Weifeng He"
                    }
                ],
                "author_detail": {
                    "name": "Weifeng He"
                },
                "author": "Weifeng He"
            },
            {
                "id": "http://arxiv.org/abs/2602.18993v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.18993v1",
                "title": "SeaCache: Spectral-Evolution-Aware Cache for Accelerating Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeaCache: Spectral-Evolution-Aware Cache for Accelerating Diffusion Models"
                },
                "updated": "2026-02-22T00:48:03Z",
                "updated_parsed": [
                    2026,
                    2,
                    22,
                    0,
                    48,
                    3,
                    6,
                    53,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.18993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.18993v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models are a strong backbone for visual generation, but their inherently sequential denoising process leads to slow inference. Previous methods accelerate sampling by caching and reusing intermediate outputs based on feature distances between adjacent timesteps. However, existing caching strategies typically rely on raw feature differences that entangle content and noise. This design overlooks spectral evolution, where low-frequency structure appears early and high-frequency detail is refined later. We introduce Spectral-Evolution-Aware Cache (SeaCache), a training-free cache schedule that bases reuse decisions on a spectrally aligned representation. Through theoretical and empirical analysis, we derive a Spectral-Evolution-Aware (SEA) filter that preserves content-relevant components while suppressing noise. Employing SEA-filtered input features to estimate redundancy leads to dynamic schedules that adapt to content while respecting the spectral priors underlying the diffusion model. Extensive experiments on diverse visual generative models and the baselines show that SeaCache achieves state-of-the-art latency-quality trade-offs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models are a strong backbone for visual generation, but their inherently sequential denoising process leads to slow inference. Previous methods accelerate sampling by caching and reusing intermediate outputs based on feature distances between adjacent timesteps. However, existing caching strategies typically rely on raw feature differences that entangle content and noise. This design overlooks spectral evolution, where low-frequency structure appears early and high-frequency detail is refined later. We introduce Spectral-Evolution-Aware Cache (SeaCache), a training-free cache schedule that bases reuse decisions on a spectrally aligned representation. Through theoretical and empirical analysis, we derive a Spectral-Evolution-Aware (SEA) filter that preserves content-relevant components while suppressing noise. Employing SEA-filtered input features to estimate redundancy leads to dynamic schedules that adapt to content while respecting the spectral priors underlying the diffusion model. Extensive experiments on diverse visual generative models and the baselines show that SeaCache achieves state-of-the-art latency-quality trade-offs."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-22T00:48:03Z",
                "published_parsed": [
                    2026,
                    2,
                    22,
                    0,
                    48,
                    3,
                    6,
                    53,
                    0
                ],
                "arxiv_comment": "Accepted to CVPR 2026 Main. Project page:https://jiwoogit.github.io/SeaCache",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jiwoo Chung"
                    },
                    {
                        "name": "Sangeek Hyun"
                    },
                    {
                        "name": "MinKyu Lee"
                    },
                    {
                        "name": "Byeongju Han"
                    },
                    {
                        "name": "Geonho Cha"
                    },
                    {
                        "name": "Dongyoon Wee"
                    },
                    {
                        "name": "Youngjun Hong"
                    },
                    {
                        "name": "Jae-Pil Heo"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Pil Heo"
                },
                "author": "Jae-Pil Heo"
            },
            {
                "id": "http://arxiv.org/abs/2602.18955v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.18955v1",
                "title": "Incremental Transformer Neural Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incremental Transformer Neural Processes"
                },
                "updated": "2026-02-21T20:30:04Z",
                "updated_parsed": [
                    2026,
                    2,
                    21,
                    20,
                    30,
                    4,
                    5,
                    52,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.18955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.18955v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Neural Processes (NPs), and specifically Transformer Neural Processes (TNPs), have demonstrated remarkable performance across tasks ranging from spatiotemporal forecasting to tabular data modelling. However, many of these applications are inherently sequential, involving continuous data streams such as real-time sensor readings or database updates. In such settings, models should support cheap, incremental updates rather than recomputing internal representations from scratch for every new observation -- a capability existing TNP variants lack. Drawing inspiration from Large Language Models, we introduce the Incremental TNP (incTNP). By leveraging causal masking, Key-Value (KV) caching, and a data-efficient autoregressive training strategy, incTNP matches the predictive performance of standard TNPs while reducing the computational cost of updates from quadratic to linear time complexity. We empirically evaluate our model on a range of synthetic and real-world tasks, including tabular regression and temperature prediction. Our results show that, surprisingly, incTNP delivers performance comparable to -- or better than -- non-causal TNPs while unlocking orders-of-magnitude speedups for sequential inference. Finally, we assess the consistency of the model's updates -- by adapting a metric of ``implicit Bayesianness\", we show that incTNP retains a prediction rule as implicitly Bayesian as standard non-causal TNPs, demonstrating that incTNP achieves the computational benefits of causal masking without sacrificing the consistency required for streaming inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Processes (NPs), and specifically Transformer Neural Processes (TNPs), have demonstrated remarkable performance across tasks ranging from spatiotemporal forecasting to tabular data modelling. However, many of these applications are inherently sequential, involving continuous data streams such as real-time sensor readings or database updates. In such settings, models should support cheap, incremental updates rather than recomputing internal representations from scratch for every new observation -- a capability existing TNP variants lack. Drawing inspiration from Large Language Models, we introduce the Incremental TNP (incTNP). By leveraging causal masking, Key-Value (KV) caching, and a data-efficient autoregressive training strategy, incTNP matches the predictive performance of standard TNPs while reducing the computational cost of updates from quadratic to linear time complexity. We empirically evaluate our model on a range of synthetic and real-world tasks, including tabular regression and temperature prediction. Our results show that, surprisingly, incTNP delivers performance comparable to -- or better than -- non-causal TNPs while unlocking orders-of-magnitude speedups for sequential inference. Finally, we assess the consistency of the model's updates -- by adapting a metric of ``implicit Bayesianness\", we show that incTNP retains a prediction rule as implicitly Bayesian as standard non-causal TNPs, demonstrating that incTNP achieves the computational benefits of causal masking without sacrificing the consistency required for streaming inference."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-21T20:30:04Z",
                "published_parsed": [
                    2026,
                    2,
                    21,
                    20,
                    30,
                    4,
                    5,
                    52,
                    0
                ],
                "arxiv_comment": "Code provided at https://github.com/philipmortimer/incTNP-code",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Philip Mortimer"
                    },
                    {
                        "name": "Cristiana Diaconu"
                    },
                    {
                        "name": "Tommy Rochussen"
                    },
                    {
                        "name": "Bruno Mlodozeniec"
                    },
                    {
                        "name": "Richard E. Turner"
                    }
                ],
                "author_detail": {
                    "name": "Richard E. Turner"
                },
                "author": "Richard E. Turner"
            },
            {
                "id": "http://arxiv.org/abs/2602.18922v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.18922v1",
                "title": "Why Agent Caching Fails and How to Fix It: Structured Intent Canonicalization with Few-Shot Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Agent Caching Fails and How to Fix It: Structured Intent Canonicalization with Few-Shot Learning"
                },
                "updated": "2026-02-21T18:25:18Z",
                "updated_parsed": [
                    2026,
                    2,
                    21,
                    18,
                    25,
                    18,
                    5,
                    52,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.18922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.18922v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Personal AI agents incur substantial cost via repeated LLM calls. We show existing caching methods fail: GPTCache achieves 37.9% accuracy on real benchmarks; APC achieves 0-12%. The root cause is optimizing for the wrong property -- cache effectiveness requires key consistency and precision,\n  not classification accuracy. We observe cache-key evaluation reduces to clustering evaluation and apply V-measure decomposition to separate these on n=8,682 points across MASSIVE, BANKING77, CLINC150, and NyayaBench v2, our new 8,514-entry multilingual agentic dataset (528 intents, 20 W5H2 classes, 63 languages). We introduce W5H2, a structured intent decomposition framework. Using SetFit with 8 examples per class, W5H2 achieves 91.1%+/-1.7% on MASSIVE in ~2ms -- vs 37.9% for\n  GPTCache and 68.8% for a 20B-parameter LLM at 3,447ms. On NyayaBench v2 (20 classes), SetFit achieves 55.3%, with cross-lingual transfer across 30 languages. Our five-tier cascade handles 85% of interactions locally, projecting 97.5% cost reduction. We provide risk-controlled selective prediction guarantees via RCPS with nine bound families.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personal AI agents incur substantial cost via repeated LLM calls. We show existing caching methods fail: GPTCache achieves 37.9% accuracy on real benchmarks; APC achieves 0-12%. The root cause is optimizing for the wrong property -- cache effectiveness requires key consistency and precision,\n  not classification accuracy. We observe cache-key evaluation reduces to clustering evaluation and apply V-measure decomposition to separate these on n=8,682 points across MASSIVE, BANKING77, CLINC150, and NyayaBench v2, our new 8,514-entry multilingual agentic dataset (528 intents, 20 W5H2 classes, 63 languages). We introduce W5H2, a structured intent decomposition framework. Using SetFit with 8 examples per class, W5H2 achieves 91.1%+/-1.7% on MASSIVE in ~2ms -- vs 37.9% for\n  GPTCache and 68.8% for a 20B-parameter LLM at 3,447ms. On NyayaBench v2 (20 classes), SetFit achieves 55.3%, with cross-lingual transfer across 30 languages. Our five-tier cascade handles 85% of interactions locally, projecting 97.5% cost reduction. We provide risk-controlled selective prediction guarantees via RCPS with nine bound families."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-21T18:25:18Z",
                "published_parsed": [
                    2026,
                    2,
                    21,
                    18,
                    25,
                    18,
                    5,
                    52,
                    0
                ],
                "arxiv_comment": "28 pages, 15 figures, 8 tables, 5 appendices",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Abhinaba Basu"
                    }
                ],
                "author_detail": {
                    "name": "Abhinaba Basu"
                },
                "author": "Abhinaba Basu"
            },
            {
                "id": "http://arxiv.org/abs/2506.01928v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.01928v3",
                "title": "Esoteric Language Models: Bridging Autoregressive and Masked Diffusion LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Esoteric Language Models: Bridging Autoregressive and Masked Diffusion LLMs"
                },
                "updated": "2026-02-21T18:05:37Z",
                "updated_parsed": [
                    2026,
                    2,
                    21,
                    18,
                    5,
                    37,
                    5,
                    52,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.01928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.01928v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion-based language models offer a compelling alternative to autoregressive (AR) models by enabling parallel and controllable generation. Within this family, Masked Diffusion Models (MDMs) currently perform best but still underperform AR models in perplexity and lack key inference-time efficiency features, most notably KV caching. We introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms, smoothly interpolating between their perplexities while overcoming their respective limitations. Unlike prior work, which uses transformers with bidirectional attention as MDM denoisers, we exploit the connection between MDMs and Any-Order autoregressive models and adopt causal attention. This design lets us compute the exact likelihood of MDMs for the first time and, crucially, enables us \\to introduce KV caching for MDMs while preserving parallel generation for the first time, significantly improving inference efficiency. Combined with an optimized sampling schedule, Eso-LMs achieves a new state of the art on the speed-quality Pareto frontier for unconditional generation. On long contexts, it yields $\\mathbf{14 - 65{}\\times}$ faster inference than standard MDMs and $\\mathbf{3 - 4{}\\times}$ faster inference than prior semi-autoregressive approaches. We provide code, model checkpoints, and a video tutorial on the project page: https://s-sahoo.com/Eso-LMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based language models offer a compelling alternative to autoregressive (AR) models by enabling parallel and controllable generation. Within this family, Masked Diffusion Models (MDMs) currently perform best but still underperform AR models in perplexity and lack key inference-time efficiency features, most notably KV caching. We introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms, smoothly interpolating between their perplexities while overcoming their respective limitations. Unlike prior work, which uses transformers with bidirectional attention as MDM denoisers, we exploit the connection between MDMs and Any-Order autoregressive models and adopt causal attention. This design lets us compute the exact likelihood of MDMs for the first time and, crucially, enables us \\to introduce KV caching for MDMs while preserving parallel generation for the first time, significantly improving inference efficiency. Combined with an optimized sampling schedule, Eso-LMs achieves a new state of the art on the speed-quality Pareto frontier for unconditional generation. On long contexts, it yields $\\mathbf{14 - 65{}\\times}$ faster inference than standard MDMs and $\\mathbf{3 - 4{}\\times}$ faster inference than prior semi-autoregressive approaches. We provide code, model checkpoints, and a video tutorial on the project page: https://s-sahoo.com/Eso-LMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-02T17:47:27Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    47,
                    27,
                    0,
                    153,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Johnna Liu"
                    },
                    {
                        "name": "Deepansha Singh"
                    },
                    {
                        "name": "Zhoujun Cheng"
                    },
                    {
                        "name": "Zhengzhong Liu"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "John Thickstun"
                    },
                    {
                        "name": "Arash Vahdat"
                    }
                ],
                "author_detail": {
                    "name": "Arash Vahdat"
                },
                "author": "Arash Vahdat"
            },
            {
                "id": "http://arxiv.org/abs/2602.18750v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.18750v1",
                "title": "HillInfer: Efficient Long-Context LLM Inference on the Edge with Hierarchical KV Eviction using SmartSSD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HillInfer: Efficient Long-Context LLM Inference on the Edge with Hierarchical KV Eviction using SmartSSD"
                },
                "updated": "2026-02-21T08:19:59Z",
                "updated_parsed": [
                    2026,
                    2,
                    21,
                    8,
                    19,
                    59,
                    5,
                    52,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.18750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.18750v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deploying Large Language Models (LLMs) on edge devices such as PCs enables low-latency inference with strong privacy guarantees, but long-context inference is fundamentally constrained by limited memory and compute resources. Beyond model parameters, the KV cache becomes the dominant bottleneck due to its linear growth with context length. Although prior work exploits contextual sparsity to evict unimportant KV data, these approaches are largely designed for memory-rich platforms and incur prohibitive data transfer overhead when applied to resource-constrained edge devices with external storage. In this paper, we propose HillInfer, an importance-aware long-context LLM inference framework on the edge that leverages SmartSSD-assisted hierarchical KV cache management. HillInfer jointly manages KV cache pools across the CPU and SmartSSD, and performs in-storage importance evaluation to reduce unnecessary data movement. Furthermore, we design an adaptive, prefetch-based pipeline that overlaps computation and KV data transfer across GPU, CPU, and SmartSSD, minimizing end-to-end inference latency without sacrificing accuracy. We implement HillInfer on a PC with a commodity GPU, and experiments across multiple models and benchmarks demonstrate up to 8.56 $\\times$ speedup over baselines while preserving model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Large Language Models (LLMs) on edge devices such as PCs enables low-latency inference with strong privacy guarantees, but long-context inference is fundamentally constrained by limited memory and compute resources. Beyond model parameters, the KV cache becomes the dominant bottleneck due to its linear growth with context length. Although prior work exploits contextual sparsity to evict unimportant KV data, these approaches are largely designed for memory-rich platforms and incur prohibitive data transfer overhead when applied to resource-constrained edge devices with external storage. In this paper, we propose HillInfer, an importance-aware long-context LLM inference framework on the edge that leverages SmartSSD-assisted hierarchical KV cache management. HillInfer jointly manages KV cache pools across the CPU and SmartSSD, and performs in-storage importance evaluation to reduce unnecessary data movement. Furthermore, we design an adaptive, prefetch-based pipeline that overlaps computation and KV data transfer across GPU, CPU, and SmartSSD, minimizing end-to-end inference latency without sacrificing accuracy. We implement HillInfer on a PC with a commodity GPU, and experiments across multiple models and benchmarks demonstrate up to 8.56 $\\times$ speedup over baselines while preserving model accuracy."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-21T08:19:59Z",
                "published_parsed": [
                    2026,
                    2,
                    21,
                    8,
                    19,
                    59,
                    5,
                    52,
                    0
                ],
                "arxiv_comment": "12 pages, 12 figures, under review",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "He Sun"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Mingjun Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Mingjun Xiao"
                },
                "author": "Mingjun Xiao"
            },
            {
                "id": "http://arxiv.org/abs/2508.08134v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.08134v4",
                "title": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control"
                },
                "updated": "2026-02-21T06:41:00Z",
                "updated_parsed": [
                    2026,
                    2,
                    21,
                    6,
                    41,
                    0,
                    5,
                    52,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.08134v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.08134v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While recent flow-based image editing models demonstrate general-purpose capabilities across diverse tasks, they often struggle to specialize in challenging scenarios -- particularly those involving large-scale shape transformations. When performing such structural edits, these methods either fail to achieve the intended shape change or inadvertently alter non-target regions, resulting in degraded background quality. We propose Follow-Your-Shape, a training-free and mask-free framework that supports precise and controllable editing of object shapes while strictly preserving non-target content. Motivated by the divergence between inversion and editing trajectories, we compute a Trajectory Divergence Map (TDM) by comparing token-wise velocity differences between the inversion and denoising paths. The TDM enables precise localization of editable regions and guides a Scheduled KV Injection mechanism that ensures stable and faithful editing. To facilitate a rigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120 new images and enriched prompt pairs specifically curated for shape-aware editing. Experiments demonstrate that our method achieves superior editability and visual fidelity, particularly in tasks requiring large-scale shape replacement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent flow-based image editing models demonstrate general-purpose capabilities across diverse tasks, they often struggle to specialize in challenging scenarios -- particularly those involving large-scale shape transformations. When performing such structural edits, these methods either fail to achieve the intended shape change or inadvertently alter non-target regions, resulting in degraded background quality. We propose Follow-Your-Shape, a training-free and mask-free framework that supports precise and controllable editing of object shapes while strictly preserving non-target content. Motivated by the divergence between inversion and editing trajectories, we compute a Trajectory Divergence Map (TDM) by comparing token-wise velocity differences between the inversion and denoising paths. The TDM enables precise localization of editable regions and guides a Scheduled KV Injection mechanism that ensures stable and faithful editing. To facilitate a rigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120 new images and enriched prompt pairs specifically curated for shape-aware editing. Experiments demonstrate that our method achieves superior editability and visual fidelity, particularly in tasks requiring large-scale shape replacement."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-11T16:10:00Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    10,
                    0,
                    0,
                    223,
                    0
                ],
                "arxiv_comment": "Accepted to ICLR 2026. Project webpage is available at https://follow-your-shape.github.io/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zeqian Long"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Kunyu Feng"
                    },
                    {
                        "name": "Xinhua Zhang"
                    },
                    {
                        "name": "Hongyu Liu"
                    },
                    {
                        "name": "Harry Yang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Yue Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yue Ma"
                },
                "author": "Yue Ma"
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.03771v5",
                "title": "vCache: Verified Semantic Prompt Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vCache: Verified Semantic Prompt Caching"
                },
                "updated": "2026-02-21T02:29:11Z",
                "updated_parsed": [
                    2026,
                    2,
                    21,
                    2,
                    29,
                    11,
                    5,
                    52,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.03771v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.03771v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Semantic caches return cached responses for semantically similar prompts to reduce LLM inference latency and cost. They embed cached prompts and store them alongside their response in a vector database. Embedding similarity metrics assign a numerical score to quantify the similarity between a request and its nearest neighbor prompt from the cache. Existing systems use the same static similarity threshold across all requests to determine whether two prompts can share similar responses. However, we observe that static thresholds do not give formal correctness guarantees, result in unexpected error rates, and lead to suboptimal cache hit rates. This paper proposes vCache, the first verified semantic cache with user-defined error rate guarantees for predictable performance. It employs an online learning algorithm to estimate an optimal threshold for each cached prompt, enabling reliable cache responses without additional training. Our experiments show that vCache consistently meets the specified error bounds while outperforming state-of-the-art static-threshold and fine-tuned embedding baselines with up to 12.5$\\times$ higher cache hit and 26$\\times$ lower error rates. We release the vCache implementation and four benchmarks to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caches return cached responses for semantically similar prompts to reduce LLM inference latency and cost. They embed cached prompts and store them alongside their response in a vector database. Embedding similarity metrics assign a numerical score to quantify the similarity between a request and its nearest neighbor prompt from the cache. Existing systems use the same static similarity threshold across all requests to determine whether two prompts can share similar responses. However, we observe that static thresholds do not give formal correctness guarantees, result in unexpected error rates, and lead to suboptimal cache hit rates. This paper proposes vCache, the first verified semantic cache with user-defined error rate guarantees for predictable performance. It employs an online learning algorithm to estimate an optimal threshold for each cached prompt, enabling reliable cache responses without additional training. Our experiments show that vCache consistently meets the specified error bounds while outperforming state-of-the-art static-threshold and fine-tuned embedding baselines with up to 12.5$\\times$ higher cache hit and 26$\\times$ lower error rates. We release the vCache implementation and four benchmarks to support future research."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "arxiv_comment": "ICLR 2026 (accepted)",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Aditya Desai"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Kyle Chu"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez"
            },
            {
                "id": "http://arxiv.org/abs/2602.18434v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.18434v1",
                "title": "Going Down Memory Lane: Scaling Tokens for Video Stream Understanding with Dynamic KV-Cache Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Going Down Memory Lane: Scaling Tokens for Video Stream Understanding with Dynamic KV-Cache Memory"
                },
                "updated": "2026-02-20T18:59:50Z",
                "updated_parsed": [
                    2026,
                    2,
                    20,
                    18,
                    59,
                    50,
                    4,
                    51,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.18434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.18434v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Streaming video understanding requires models to robustly encode, store, and retrieve information from a continuous video stream to support accurate video question answering (VQA). Existing state-of-the-art approaches rely on key-value caching to accumulate frame-level information over time, but use a limited number of tokens per frame, leading to the loss of fine-grained visual details. In this work, we propose scaling the token budget to enable more granular spatiotemporal understanding and reasoning. First, we find that current methods are ill-equipped to handle dense streams: their feature encoding causes query-frame similarity scores to increase over time, biasing retrieval toward later frames. To address this, we introduce an adaptive selection strategy that reduces token redundancy while preserving local spatiotemporal information. We further propose a training-free retrieval mixture-of-experts that leverages external models to better identify relevant frames. Our method, MemStream, achieves +8.0% on CG-Bench, +8.5% on LVBench, and +2.4% on VideoMME (Long) over ReKV with Qwen2.5-VL-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming video understanding requires models to robustly encode, store, and retrieve information from a continuous video stream to support accurate video question answering (VQA). Existing state-of-the-art approaches rely on key-value caching to accumulate frame-level information over time, but use a limited number of tokens per frame, leading to the loss of fine-grained visual details. In this work, we propose scaling the token budget to enable more granular spatiotemporal understanding and reasoning. First, we find that current methods are ill-equipped to handle dense streams: their feature encoding causes query-frame similarity scores to increase over time, biasing retrieval toward later frames. To address this, we introduce an adaptive selection strategy that reduces token redundancy while preserving local spatiotemporal information. We further propose a training-free retrieval mixture-of-experts that leverages external models to better identify relevant frames. Our method, MemStream, achieves +8.0% on CG-Bench, +8.5% on LVBench, and +2.4% on VideoMME (Long) over ReKV with Qwen2.5-VL-7B."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-20T18:59:50Z",
                "published_parsed": [
                    2026,
                    2,
                    20,
                    18,
                    59,
                    50,
                    4,
                    51,
                    0
                ],
                "arxiv_comment": "Project page: see https://vatsalag99.github.io/memstream/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Vatsal Agarwal"
                    },
                    {
                        "name": "Saksham Suri"
                    },
                    {
                        "name": "Matthew Gwilliam"
                    },
                    {
                        "name": "Pulkit Kumar"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Shrivastava"
                },
                "author": "Abhinav Shrivastava"
            },
            {
                "id": "http://arxiv.org/abs/2602.18304v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.18304v1",
                "title": "FeatureBleed: Inferring Private Enriched Attributes From Sparsity-Optimized AI Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FeatureBleed: Inferring Private Enriched Attributes From Sparsity-Optimized AI Accelerators"
                },
                "updated": "2026-02-20T16:01:16Z",
                "updated_parsed": [
                    2026,
                    2,
                    20,
                    16,
                    1,
                    16,
                    4,
                    51,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.18304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.18304v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Backend enrichment is now widely deployed in sensitive domains such as product recommendation pipelines, healthcare, and finance, where models are trained on confidential data and retrieve private features whose values influence inference behavior while remaining hidden from the API caller. This paper presents the first hardware-level backend retrieval data-stealing attack, showing that accelerator optimizations designed for performance can directly undermine data confidentiality and bypass state-of-the-art privacy defenses.\n  Our attack, FEATUREBLEED, exploits zero-skipping in AI accelerators to infer private backend-retrieved features solely through end-to-end timing, without relying on power analysis, DVFS manipulation, or shared-cache side channels. We evaluate FEATUREBLEED on three datasets spanning medical and non-medical domains: Texas-100X (clinical records), OrganAMNIST (medical imaging), and Census-19 (socioeconomic data). We further evaluate FEATUREBLEED across three hardware backends (Intel AVX, Intel AMX, and NVIDIA A100) and three model architectures (DNNs, CNNs, and hybrid CNN-MLP pipelines), demonstrating that the leakage generalizes across CPU and GPU accelerators, data modalities, and application domains, with an adversarial advantage of up to 98.87 percentage points.\n  Finally, we identify the root cause of the leakage as sparsity-driven zero-skipping in modern hardware. We quantify the privacy-performance-power trade-off: disabling zero-skipping increases Intel AMX per-operation energy by up to 25 percent and incurs 100 percent performance overhead. We propose a padding-based defense that masks timing leakage by equalizing responses to the worst-case execution time, achieving protection with only 7.24 percent average performance overhead and no additional power cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backend enrichment is now widely deployed in sensitive domains such as product recommendation pipelines, healthcare, and finance, where models are trained on confidential data and retrieve private features whose values influence inference behavior while remaining hidden from the API caller. This paper presents the first hardware-level backend retrieval data-stealing attack, showing that accelerator optimizations designed for performance can directly undermine data confidentiality and bypass state-of-the-art privacy defenses.\n  Our attack, FEATUREBLEED, exploits zero-skipping in AI accelerators to infer private backend-retrieved features solely through end-to-end timing, without relying on power analysis, DVFS manipulation, or shared-cache side channels. We evaluate FEATUREBLEED on three datasets spanning medical and non-medical domains: Texas-100X (clinical records), OrganAMNIST (medical imaging), and Census-19 (socioeconomic data). We further evaluate FEATUREBLEED across three hardware backends (Intel AVX, Intel AMX, and NVIDIA A100) and three model architectures (DNNs, CNNs, and hybrid CNN-MLP pipelines), demonstrating that the leakage generalizes across CPU and GPU accelerators, data modalities, and application domains, with an adversarial advantage of up to 98.87 percentage points.\n  Finally, we identify the root cause of the leakage as sparsity-driven zero-skipping in modern hardware. We quantify the privacy-performance-power trade-off: disabling zero-skipping increases Intel AMX per-operation energy by up to 25 percent and incurs 100 percent performance overhead. We propose a padding-based defense that masks timing leakage by equalizing responses to the worst-case execution time, achieving protection with only 7.24 percent average performance overhead and no additional power cost."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-20T16:01:16Z",
                "published_parsed": [
                    2026,
                    2,
                    20,
                    16,
                    1,
                    16,
                    4,
                    51,
                    0
                ],
                "arxiv_comment": "4 pages, 3 figures, 3 tables, Journal :- IEEE CAL",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Darsh Asher"
                    },
                    {
                        "name": "Farshad Dizani"
                    },
                    {
                        "name": "Joshua Kalyanapu"
                    },
                    {
                        "name": "Rosario Cammarota"
                    },
                    {
                        "name": "Aydin Aysu"
                    },
                    {
                        "name": "Samira Mirbagher Ajorpaz"
                    }
                ],
                "author_detail": {
                    "name": "Samira Mirbagher Ajorpaz"
                },
                "author": "Samira Mirbagher Ajorpaz"
            },
            {
                "id": "http://arxiv.org/abs/2602.18232v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.18232v1",
                "title": "Thinking by Subtraction: Confidence-Driven Contrastive Decoding for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking by Subtraction: Confidence-Driven Contrastive Decoding for LLM Reasoning"
                },
                "updated": "2026-02-20T14:13:22Z",
                "updated_parsed": [
                    2026,
                    2,
                    20,
                    14,
                    13,
                    22,
                    4,
                    51,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.18232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.18232v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent work on test-time scaling for large language model (LLM) reasoning typically assumes that allocating more inference-time computation uniformly improves correctness. However, prior studies show that reasoning uncertainty is highly localized: a small subset of low-confidence tokens disproportionately contributes to reasoning errors and unnecessary output expansion. Motivated by this observation, we propose Thinking by Subtraction, a confidence-driven contrastive decoding approach that improves reasoning reliability through targeted token-level intervention. Our method, Confidence-Driven Contrastive Decoding, detects low-confidence tokens during decoding and intervenes selectively at these positions. It constructs a contrastive reference by replacing high-confidence tokens with minimal placeholders, and refines predictions by subtracting this reference distribution at low-confidence locations. Experiments show that CCD significantly improves accuracy across mathematical reasoning benchmarks while substantially reducing output length, with minimal KV-cache overhead. As a training-free method, CCD enhances reasoning reliability through targeted low-confidence intervention without computational redundancy. Our code will be made available at: https://github.com/bolo-web/CCD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work on test-time scaling for large language model (LLM) reasoning typically assumes that allocating more inference-time computation uniformly improves correctness. However, prior studies show that reasoning uncertainty is highly localized: a small subset of low-confidence tokens disproportionately contributes to reasoning errors and unnecessary output expansion. Motivated by this observation, we propose Thinking by Subtraction, a confidence-driven contrastive decoding approach that improves reasoning reliability through targeted token-level intervention. Our method, Confidence-Driven Contrastive Decoding, detects low-confidence tokens during decoding and intervenes selectively at these positions. It constructs a contrastive reference by replacing high-confidence tokens with minimal placeholders, and refines predictions by subtracting this reference distribution at low-confidence locations. Experiments show that CCD significantly improves accuracy across mathematical reasoning benchmarks while substantially reducing output length, with minimal KV-cache overhead. As a training-free method, CCD enhances reasoning reliability through targeted low-confidence intervention without computational redundancy. Our code will be made available at: https://github.com/bolo-web/CCD."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-20T14:13:22Z",
                "published_parsed": [
                    2026,
                    2,
                    20,
                    14,
                    13,
                    22,
                    4,
                    51,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Lexiang Tang"
                    },
                    {
                        "name": "Weihao Gao"
                    },
                    {
                        "name": "Bingchen Zhao"
                    },
                    {
                        "name": "Lu Ma"
                    },
                    {
                        "name": "Qiao jin"
                    },
                    {
                        "name": "Bang Yang"
                    },
                    {
                        "name": "Yuexian Zou"
                    }
                ],
                "author_detail": {
                    "name": "Yuexian Zou"
                },
                "author": "Yuexian Zou"
            },
            {
                "id": "http://arxiv.org/abs/2602.18196v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.18196v1",
                "title": "RAT+: Train Dense, Infer Sparse -- Recurrence Augmented Attention for Dilated Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAT+: Train Dense, Infer Sparse -- Recurrence Augmented Attention for Dilated Inference"
                },
                "updated": "2026-02-20T13:09:49Z",
                "updated_parsed": [
                    2026,
                    2,
                    20,
                    13,
                    9,
                    49,
                    4,
                    51,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.18196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.18196v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Structured dilated attention has an appealing inference-time efficiency knob: it reduces the FLOPs of the attention and the KV cache size by a factor of the dilation size D, while preserving long-range connectivity. However, we find a persistent failure mode of them -- sparsifying a pretrained attention model to a dilated pattern leads to severe accuracy degradation. We introduce RAT+, a dense-pretraining architecture that augments attention with full-sequence recurrence and active recurrence learning. A single RAT+ model is pretrained densely once, then flexibly switched at inference time to dilated attention (optionally with local windows) or hybrid layer/head compositions, requiring only a short 1B-token resolution adaptation rather than retraining separate sparse models. At 1.5B parameters trained on 100B tokens, RAT+ closely matches dense accuracy at 16 and drops by about 2-3 points at 64 on commonsense reasoning and LongBench tasks, respectively. Moreover, RAT+ outperforms attention when sparsifying to the top-k block attention. We further scale to 2.6B parameters and 200B tokens and observe the same trend.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured dilated attention has an appealing inference-time efficiency knob: it reduces the FLOPs of the attention and the KV cache size by a factor of the dilation size D, while preserving long-range connectivity. However, we find a persistent failure mode of them -- sparsifying a pretrained attention model to a dilated pattern leads to severe accuracy degradation. We introduce RAT+, a dense-pretraining architecture that augments attention with full-sequence recurrence and active recurrence learning. A single RAT+ model is pretrained densely once, then flexibly switched at inference time to dilated attention (optionally with local windows) or hybrid layer/head compositions, requiring only a short 1B-token resolution adaptation rather than retraining separate sparse models. At 1.5B parameters trained on 100B tokens, RAT+ closely matches dense accuracy at 16 and drops by about 2-3 points at 64 on commonsense reasoning and LongBench tasks, respectively. Moreover, RAT+ outperforms attention when sparsifying to the top-k block attention. We further scale to 2.6B parameters and 200B tokens and observe the same trend."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-20T13:09:49Z",
                "published_parsed": [
                    2026,
                    2,
                    20,
                    13,
                    9,
                    49,
                    4,
                    51,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre"
            },
            {
                "id": "http://arxiv.org/abs/2601.09282v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.09282v2",
                "title": "Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing"
                },
                "updated": "2026-02-20T11:40:54Z",
                "updated_parsed": [
                    2026,
                    2,
                    20,
                    11,
                    40,
                    54,
                    4,
                    51,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.09282v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.09282v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/ACCESS.2026.3665989",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs a Large Language Model (LLM) integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences. The results validate using LLMs for accessible scheduling but highlight limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness. This work confirms the viability of semantic soft affinity for simplifying workload orchestration and presents a proof-of-concept design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs a Large Language Model (LLM) integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences. The results validate using LLMs for accessible scheduling but highlight limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness. This work confirms the viability of semantic soft affinity for simplifying workload orchestration and presents a proof-of-concept design."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-14T08:36:21Z",
                "published_parsed": [
                    2026,
                    1,
                    14,
                    8,
                    36,
                    21,
                    2,
                    14,
                    0
                ],
                "arxiv_comment": "This is the accepted version of the paper published in IEEE Access (2026). The final version is available at: https://doi.org/10.1109/ACCESS.2026.3665989",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Leszek Sliwko"
                    },
                    {
                        "name": "Jolanta Mizeria-Pietraszko"
                    }
                ],
                "author_detail": {
                    "name": "Jolanta Mizeria-Pietraszko"
                },
                "author": "Jolanta Mizeria-Pietraszko",
                "arxiv_doi": "10.1109/ACCESS.2026.3665989"
            },
            {
                "id": "http://arxiv.org/abs/2602.18093v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.18093v1",
                "title": "Predict to Skip: Linear Multistep Feature Forecasting for Efficient Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predict to Skip: Linear Multistep Feature Forecasting for Efficient Diffusion Transformers"
                },
                "updated": "2026-02-20T09:33:59Z",
                "updated_parsed": [
                    2026,
                    2,
                    20,
                    9,
                    33,
                    59,
                    4,
                    51,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.18093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.18093v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Transformers (DiT) have emerged as a widely adopted backbone for high-fidelity image and video generation, yet their iterative denoising process incurs high computational costs. Existing training-free acceleration methods rely on feature caching and reuse under the assumption of temporal stability. However, reusing features for multiple steps may lead to latent drift and visual degradation. We observe that model outputs evolve smoothly along much of the diffusion trajectory, enabling principled predictions rather than naive reuse. Based on this insight, we propose \\textbf{PrediT}, a training-free acceleration framework that formulates feature prediction as a linear multistep problem. We employ classical linear multistep methods to forecast future model outputs from historical information, combined with a corrector that activates in high-dynamics regions to prevent error accumulation. A dynamic step modulation mechanism adaptively adjusts the prediction horizon by monitoring the feature change rate. Together, these components enable substantial acceleration while preserving generation fidelity. Extensive experiments validate that our method achieves up to $5.54\\times$ latency reduction across various DiT-based image and video generation models, while incurring negligible quality degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as a widely adopted backbone for high-fidelity image and video generation, yet their iterative denoising process incurs high computational costs. Existing training-free acceleration methods rely on feature caching and reuse under the assumption of temporal stability. However, reusing features for multiple steps may lead to latent drift and visual degradation. We observe that model outputs evolve smoothly along much of the diffusion trajectory, enabling principled predictions rather than naive reuse. Based on this insight, we propose \\textbf{PrediT}, a training-free acceleration framework that formulates feature prediction as a linear multistep problem. We employ classical linear multistep methods to forecast future model outputs from historical information, combined with a corrector that activates in high-dynamics regions to prevent error accumulation. A dynamic step modulation mechanism adaptively adjusts the prediction horizon by monitoring the feature change rate. Together, these components enable substantial acceleration while preserving generation fidelity. Extensive experiments validate that our method achieves up to $5.54\\times$ latency reduction across various DiT-based image and video generation models, while incurring negligible quality degradation."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-20T09:33:59Z",
                "published_parsed": [
                    2026,
                    2,
                    20,
                    9,
                    33,
                    59,
                    4,
                    51,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Hanshuai Cui"
                    },
                    {
                        "name": "Zhiqing Tang"
                    },
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Zhi Yao"
                    },
                    {
                        "name": "Weijia Jia"
                    }
                ],
                "author_detail": {
                    "name": "Weijia Jia"
                },
                "author": "Weijia Jia"
            },
            {
                "id": "http://arxiv.org/abs/2511.19269v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.19269v2",
                "title": "CDLM: Consistency Diffusion Language Models For Faster Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDLM: Consistency Diffusion Language Models For Faster Sampling"
                },
                "updated": "2026-02-20T02:50:33Z",
                "updated_parsed": [
                    2026,
                    2,
                    20,
                    2,
                    50,
                    33,
                    4,
                    51,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.19269v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.19269v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Language Models (DLMs) offer a promising parallel generation paradigm but suffer from slow inference due to numerous refinement steps and the inability to use standard KV caching. We introduce CDLM (Consistency Diffusion Language Models), a training-based acceleration method that simultaneously tackles both bottlenecks. CDLM integrates consistency modeling to drastically reduce the number of required sampling steps by enabling multi-token finalization. Furthermore, we enforce a block-wise causal attention mask during fine-tuning, making the model fully compatible with KV caching. Experiments show CDLM achieves 3.6x-14.5x lower latency while maintaining competitive accuracy on math and coding tasks. The full training and evaluation code is available at https://github.com/SqueezeAILab/CDLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Language Models (DLMs) offer a promising parallel generation paradigm but suffer from slow inference due to numerous refinement steps and the inability to use standard KV caching. We introduce CDLM (Consistency Diffusion Language Models), a training-based acceleration method that simultaneously tackles both bottlenecks. CDLM integrates consistency modeling to drastically reduce the number of required sampling steps by enabling multi-token finalization. Furthermore, we enforce a block-wise causal attention mask during fine-tuning, making the model fully compatible with KV caching. Experiments show CDLM achieves 3.6x-14.5x lower latency while maintaining competitive accuracy on math and coding tasks. The full training and evaluation code is available at https://github.com/SqueezeAILab/CDLM."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-24T16:21:25Z",
                "published_parsed": [
                    2025,
                    11,
                    24,
                    16,
                    21,
                    25,
                    0,
                    328,
                    0
                ],
                "arxiv_comment": "Accepted to MLSys 2026",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Minseo Kim"
                    },
                    {
                        "name": "Chenfeng Xu"
                    },
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Harman Singh"
                    },
                    {
                        "name": "Ben Athiwaratkun"
                    },
                    {
                        "name": "Ce Zhang"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami"
            },
            {
                "id": "http://arxiv.org/abs/2508.04581v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.04581v2",
                "title": "Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning"
                },
                "updated": "2026-02-19T19:17:31Z",
                "updated_parsed": [
                    2026,
                    2,
                    19,
                    19,
                    17,
                    31,
                    3,
                    50,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.04581v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.04581v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models have revolutionized AI applications, yet their high computational and memory demands hinder their widespread deployment. Existing compression techniques focus on intra-block optimizations (e.g., low-rank approximation or attention pruning), while the repetitive layered structure of transformers implies significant inter-block redundancy - a dimension largely unexplored beyond key-value (KV) caching. Inspired by dictionary learning in convolutional networks, we propose a framework for structured weight sharing across transformer layers. Our approach decomposes attention projection matrices (Q, K, V, O) into shared dictionary atoms, reducing the attention module's parameters by 66.7\\% while achieving on-par performance. Unlike complex methods requiring distillation or architectural changes, MASA (Matrix Atom Sharing in Attention) operates as a drop-in replacement-trained with standard optimizers - and represents each layer's weights as linear combinations of shared matrix atoms. Experiments across scales (100M-700M parameters) show that MASA achieves better benchmark accuracy and perplexity than GQA, low-rank baselines and recent Repeat-all-over/Sequential sharing at comparable parameter budgets. Ablation studies confirm robustness to the dictionary size and the efficacy of shared representations in capturing cross-layer statistical regularities. Extending to Vision Transformers (ViT), MASA matches performance metrics on image classification tasks with 66.7\\% fewer attention parameters. By combining dictionary learning strategies with transformer efficiency, MASA offers a scalable blueprint for parameter-efficient models without sacrificing performance. Finally, we investigate the possibility of employing MASA on large pretrained models to reduce their number of parameters without experiencing any significant drop in their performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized AI applications, yet their high computational and memory demands hinder their widespread deployment. Existing compression techniques focus on intra-block optimizations (e.g., low-rank approximation or attention pruning), while the repetitive layered structure of transformers implies significant inter-block redundancy - a dimension largely unexplored beyond key-value (KV) caching. Inspired by dictionary learning in convolutional networks, we propose a framework for structured weight sharing across transformer layers. Our approach decomposes attention projection matrices (Q, K, V, O) into shared dictionary atoms, reducing the attention module's parameters by 66.7\\% while achieving on-par performance. Unlike complex methods requiring distillation or architectural changes, MASA (Matrix Atom Sharing in Attention) operates as a drop-in replacement-trained with standard optimizers - and represents each layer's weights as linear combinations of shared matrix atoms. Experiments across scales (100M-700M parameters) show that MASA achieves better benchmark accuracy and perplexity than GQA, low-rank baselines and recent Repeat-all-over/Sequential sharing at comparable parameter budgets. Ablation studies confirm robustness to the dictionary size and the efficacy of shared representations in capturing cross-layer statistical regularities. Extending to Vision Transformers (ViT), MASA matches performance metrics on image classification tasks with 66.7\\% fewer attention parameters. By combining dictionary learning strategies with transformer efficiency, MASA offers a scalable blueprint for parameter-efficient models without sacrificing performance. Finally, we investigate the possibility of employing MASA on large pretrained models to reduce their number of parameters without experiencing any significant drop in their performance."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-06T16:06:43Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    6,
                    43,
                    2,
                    218,
                    0
                ],
                "arxiv_comment": "This work has been accepted and presented at AAAI 2026 in Singapore",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Magauiya Zhussip"
                    },
                    {
                        "name": "Dmitriy Shopkhoev"
                    },
                    {
                        "name": "Ammar Ali"
                    },
                    {
                        "name": "Stamatios Lefkimmiatis"
                    }
                ],
                "author_detail": {
                    "name": "Stamatios Lefkimmiatis"
                },
                "author": "Stamatios Lefkimmiatis"
            },
            {
                "id": "http://arxiv.org/abs/2602.17518v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.17518v1",
                "title": "A Picture of Agentic Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Picture of Agentic Search"
                },
                "updated": "2026-02-19T16:32:34Z",
                "updated_parsed": [
                    2026,
                    2,
                    19,
                    16,
                    32,
                    34,
                    3,
                    50,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.17518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.17518v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With automated systems increasingly issuing search queries alongside humans, Information Retrieval (IR) faces a major shift. Yet IR remains human-centred, with systems, evaluation metrics, user models, and datasets designed around human queries and behaviours. Consequently, IR operates under assumptions that no longer hold in practice, with changes to workload volumes, predictability, and querying behaviours. This misalignment affects system performance and optimisation: caching may lose effectiveness, query pre-processing may add overhead without improving results, and standard metrics may mismeasure satisfaction. Without adaptation, retrieval models risk satisfying neither humans, nor the emerging user segment of agents. However, datasets capturing agent search behaviour are lacking, which is a critical gap given IR's historical reliance on data-driven evaluation and optimisation. We develop a methodology for collecting all the data produced and consumed by agentic retrieval-augmented systems when answering queries, and we release the Agentic Search Queryset (ASQ) dataset. ASQ contains reasoning-induced queries, retrieved documents, and thoughts for queries in HotpotQA, Researchy Questions, and MS MARCO, for 3 diverse agents and 2 retrieval pipelines. The accompanying toolkit enables ASQ to be extended to new agents, retrievers, and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With automated systems increasingly issuing search queries alongside humans, Information Retrieval (IR) faces a major shift. Yet IR remains human-centred, with systems, evaluation metrics, user models, and datasets designed around human queries and behaviours. Consequently, IR operates under assumptions that no longer hold in practice, with changes to workload volumes, predictability, and querying behaviours. This misalignment affects system performance and optimisation: caching may lose effectiveness, query pre-processing may add overhead without improving results, and standard metrics may mismeasure satisfaction. Without adaptation, retrieval models risk satisfying neither humans, nor the emerging user segment of agents. However, datasets capturing agent search behaviour are lacking, which is a critical gap given IR's historical reliance on data-driven evaluation and optimisation. We develop a methodology for collecting all the data produced and consumed by agentic retrieval-augmented systems when answering queries, and we release the Agentic Search Queryset (ASQ) dataset. ASQ contains reasoning-induced queries, retrieved documents, and thoughts for queries in HotpotQA, Researchy Questions, and MS MARCO, for 3 diverse agents and 2 retrieval pipelines. The accompanying toolkit enables ASQ to be extended to new agents, retrievers, and datasets."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-19T16:32:34Z",
                "published_parsed": [
                    2026,
                    2,
                    19,
                    16,
                    32,
                    34,
                    3,
                    50,
                    0
                ],
                "arxiv_comment": "7 pages, 2 figures",
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Francesca Pezzuti"
                    },
                    {
                        "name": "Ophir Frieder"
                    },
                    {
                        "name": "Fabrizio Silvestri"
                    },
                    {
                        "name": "Sean MacAvaney"
                    },
                    {
                        "name": "Nicola Tonellotto"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Tonellotto"
                },
                "author": "Nicola Tonellotto"
            },
            {
                "id": "http://arxiv.org/abs/2602.17414v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.17414v1",
                "title": "Nested Sampling with Slice-within-Gibbs: Efficient Evidence Calculation for Hierarchical Bayesian Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nested Sampling with Slice-within-Gibbs: Efficient Evidence Calculation for Hierarchical Bayesian Models"
                },
                "updated": "2026-02-19T14:43:59Z",
                "updated_parsed": [
                    2026,
                    2,
                    19,
                    14,
                    43,
                    59,
                    3,
                    50,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.17414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.17414v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present Nested Sampling with Slice-within-Gibbs (NS-SwiG), an algorithm for Bayesian inference and evidence estimation in high-dimensional models whose likelihood admits a factorization, such as hierarchical Bayesian models. We construct a procedure to sample from the likelihood-constrained prior using a Slice-within-Gibbs kernel: an outer update of hyperparameters followed by inner block updates over local parameters. A likelihood-budget decomposition caches per-block contributions so that each local update checks feasibility in constant time rather than recomputing the global constraint at linearly growing cost. This reduces the per-replacement cost from quadratic to linear in the number of groups, and the overall algorithmic complexity from cubic to quadratic under standard assumptions. The decomposition extends naturally beyond independent observations, and we demonstrate this on Markov-structured latent variables. We evaluate NS-SwiG on challenging benchmarks, demonstrating scalability to thousands of dimensions and accurate evidence estimates even on posterior geometries where state-of-the-art gradient-based samplers can struggle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Nested Sampling with Slice-within-Gibbs (NS-SwiG), an algorithm for Bayesian inference and evidence estimation in high-dimensional models whose likelihood admits a factorization, such as hierarchical Bayesian models. We construct a procedure to sample from the likelihood-constrained prior using a Slice-within-Gibbs kernel: an outer update of hyperparameters followed by inner block updates over local parameters. A likelihood-budget decomposition caches per-block contributions so that each local update checks feasibility in constant time rather than recomputing the global constraint at linearly growing cost. This reduces the per-replacement cost from quadratic to linear in the number of groups, and the overall algorithmic complexity from cubic to quadratic under standard assumptions. The decomposition extends naturally beyond independent observations, and we demonstrate this on Markov-structured latent variables. We evaluate NS-SwiG on challenging benchmarks, demonstrating scalability to thousands of dimensions and accurate evidence estimates even on posterior geometries where state-of-the-art gradient-based samplers can struggle."
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-19T14:43:59Z",
                "published_parsed": [
                    2026,
                    2,
                    19,
                    14,
                    43,
                    59,
                    3,
                    50,
                    0
                ],
                "arxiv_comment": "26 pages, 6 figures",
                "arxiv_primary_category": {
                    "term": "stat.CO"
                },
                "authors": [
                    {
                        "name": "David Yallup"
                    }
                ],
                "author_detail": {
                    "name": "David Yallup"
                },
                "author": "David Yallup"
            },
            {
                "id": "http://arxiv.org/abs/2602.17387v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.17387v1",
                "title": "DRetHTR: Linear-Time Decoder-Only Retentive Network for Handwritten Text Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRetHTR: Linear-Time Decoder-Only Retentive Network for Handwritten Text Recognition"
                },
                "updated": "2026-02-19T14:12:07Z",
                "updated_parsed": [
                    2026,
                    2,
                    19,
                    14,
                    12,
                    7,
                    3,
                    50,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.17387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.17387v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "State-of-the-art handwritten text recognition (HTR) systems commonly use Transformers, whose growing key-value (KV) cache makes decoding slow and memory-intensive. We introduce DRetHTR, a decoder-only model built on Retentive Networks (RetNet). Compared to an equally sized decoder-only Transformer baseline, DRetHTR delivers 1.6-1.9x faster inference with 38-42% less memory usage, without loss of accuracy. By replacing softmax attention with softmax-free retention and injecting multi-scale sequential priors, DRetHTR avoids a growing KV cache: decoding is linear in output length in both time and memory. To recover the local-to-global inductive bias of attention, we propose layer-wise gamma scaling, which progressively enlarges the effective retention horizon in deeper layers. This encourages early layers to model short-range dependencies and later layers to capture broader context, mitigating the flexibility gap introduced by removing softmax. Consequently, DRetHTR achieves best reported test character error rates of 2.26% (IAM-A, en), 1.81% (RIMES, fr), and 3.46% (Bentham, en), and is competitive on READ-2016 (de) with 4.21%. This demonstrates that decoder-only RetNet enables Transformer-level HTR accuracy with substantially improved decoding speed and memory efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art handwritten text recognition (HTR) systems commonly use Transformers, whose growing key-value (KV) cache makes decoding slow and memory-intensive. We introduce DRetHTR, a decoder-only model built on Retentive Networks (RetNet). Compared to an equally sized decoder-only Transformer baseline, DRetHTR delivers 1.6-1.9x faster inference with 38-42% less memory usage, without loss of accuracy. By replacing softmax attention with softmax-free retention and injecting multi-scale sequential priors, DRetHTR avoids a growing KV cache: decoding is linear in output length in both time and memory. To recover the local-to-global inductive bias of attention, we propose layer-wise gamma scaling, which progressively enlarges the effective retention horizon in deeper layers. This encourages early layers to model short-range dependencies and later layers to capture broader context, mitigating the flexibility gap introduced by removing softmax. Consequently, DRetHTR achieves best reported test character error rates of 2.26% (IAM-A, en), 1.81% (RIMES, fr), and 3.46% (Bentham, en), and is competitive on READ-2016 (de) with 4.21%. This demonstrates that decoder-only RetNet enables Transformer-level HTR accuracy with substantially improved decoding speed and memory efficiency."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-19T14:12:07Z",
                "published_parsed": [
                    2026,
                    2,
                    19,
                    14,
                    12,
                    7,
                    3,
                    50,
                    0
                ],
                "arxiv_comment": "Submitted to Pattern Recognition, 11 pages + 2-page appendix, 7 figures, 12 tables",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Changhun Kim"
                    },
                    {
                        "name": "Martin Mayr"
                    },
                    {
                        "name": "Thomas Gorges"
                    },
                    {
                        "name": "Fei Wu"
                    },
                    {
                        "name": "Mathias Seuret"
                    },
                    {
                        "name": "Andreas Maier"
                    },
                    {
                        "name": "Vincent Christlein"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Christlein"
                },
                "author": "Vincent Christlein"
            },
            {
                "id": "http://arxiv.org/abs/2512.03870v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.03870v3",
                "title": "Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers"
                },
                "updated": "2026-02-19T05:41:27Z",
                "updated_parsed": [
                    2026,
                    2,
                    19,
                    5,
                    41,
                    27,
                    3,
                    50,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.03870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.03870v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Transformer decoders have achieved strong results across tasks, but the memory required for the KV cache becomes prohibitive at long sequence lengths. Although Cross-layer KV Cache sharing (e.g., YOCO, CLA) offers a path to mitigate KV Cache bottleneck, it typically underperforms within-layer methods like GQA. To understand the root cause, we investigate the information flow of keys and values of the top-layers. Our preliminary reveals a clear distribution: values are predominantly derived from the bottom layer, while keys draw more information from both bottom and middle layers. Building upon this, we propose FusedKV, whose top-layer KV caches are a learnable fusion of the most informative ones from the bottom and middle layers. This fusion operates directly on post-RoPE keys, preserving relative positional information without the computational cost of re-applying rotary embeddings. To further improve efficiency, we propose FusedKV-Lite, an cross-layer sharing approach, where top-layer KV caches are directly derived from the bottom-layer values and the middle-layer keys. Compared to FusedKV, FusedKV-Lite reduces I/O overhead at the cost of a slight increase in perplexity. In experiments on LLMs ranging from 332M to 4B parameters, our proposed method reduce 50\\% cache memory while achieving lower validation perplexity than the standard Transformer decoder, establishing it as a memory-efficient, high-performance architectural alternative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer decoders have achieved strong results across tasks, but the memory required for the KV cache becomes prohibitive at long sequence lengths. Although Cross-layer KV Cache sharing (e.g., YOCO, CLA) offers a path to mitigate KV Cache bottleneck, it typically underperforms within-layer methods like GQA. To understand the root cause, we investigate the information flow of keys and values of the top-layers. Our preliminary reveals a clear distribution: values are predominantly derived from the bottom layer, while keys draw more information from both bottom and middle layers. Building upon this, we propose FusedKV, whose top-layer KV caches are a learnable fusion of the most informative ones from the bottom and middle layers. This fusion operates directly on post-RoPE keys, preserving relative positional information without the computational cost of re-applying rotary embeddings. To further improve efficiency, we propose FusedKV-Lite, an cross-layer sharing approach, where top-layer KV caches are directly derived from the bottom-layer values and the middle-layer keys. Compared to FusedKV, FusedKV-Lite reduces I/O overhead at the cost of a slight increase in perplexity. In experiments on LLMs ranging from 332M to 4B parameters, our proposed method reduce 50\\% cache memory while achieving lower validation perplexity than the standard Transformer decoder, establishing it as a memory-efficient, high-performance architectural alternative."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T15:22:00Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    15,
                    22,
                    0,
                    2,
                    337,
                    0
                ],
                "arxiv_comment": "Accepted by ICLR2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hongzhan Lin"
                    },
                    {
                        "name": "Zhiqi Bai"
                    },
                    {
                        "name": "Xinmiao Zhang"
                    },
                    {
                        "name": "Sen Yang"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Siran Yang"
                    },
                    {
                        "name": "Yunlong Xu"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yongchi Zhao"
                    },
                    {
                        "name": "Jiamang Wang"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2503.07474v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.07474v2",
                "title": "Revealing Rotational Symmetry Breaking Charge-density Wave Order in Kagome Superconductor (Rb, K)V$_3$Sb$_5$ by Ultrafast Pump-probe Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing Rotational Symmetry Breaking Charge-density Wave Order in Kagome Superconductor (Rb, K)V$_3$Sb$_5$ by Ultrafast Pump-probe Experiments"
                },
                "updated": "2026-02-18T21:45:28Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    21,
                    45,
                    28,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.07474v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.07474v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1103/PhysRevB.111.165134",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "The recently discovered Kagome superconductor AV$_3$Sb$_5$ (where A refers to K, Rb, Cs) has stimulated widespread research interest due to its interplay of non-trivial topology and unconventional correlated physics including charge-density waves (CDW) and superconductivity. The essential prerequisite to understanding the microscopic mechanisms of this complex electronic landscape is to unveil the configuration and symmetry of the charge-density wave order. As to now, little consensus has been made on what symmetry is broken. Herein, we clarify the microscopic structure and symmetry breaking of the CDW phase in RbV$_3$Sb$_5$ and KV$_3$Sb$_5$ by ultrafast time-resolved reflectivity. Our approach is based on extracting coherent phonon spectra induced by three-dimensional CDW and comparing them to calculated phonon frequencies via density-functional theory. The combination of these experimental results and calculations provides compelling evidence that the CDW structure of both compounds prevailing up to T$_{\\text{CDW}}$ is the 2 $\\times$ 2 $\\times$ 2 staggered inverse Star-of-David pattern with interlayer $$ phase shift, in which the six-fold rotational symmetry is broken. These observations thus corroborate six-fold rotational symmetry breaking throughout the CDW phase of RbV$_3$Sb$_5$ and KV$_3$Sb$_5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recently discovered Kagome superconductor AV$_3$Sb$_5$ (where A refers to K, Rb, Cs) has stimulated widespread research interest due to its interplay of non-trivial topology and unconventional correlated physics including charge-density waves (CDW) and superconductivity. The essential prerequisite to understanding the microscopic mechanisms of this complex electronic landscape is to unveil the configuration and symmetry of the charge-density wave order. As to now, little consensus has been made on what symmetry is broken. Herein, we clarify the microscopic structure and symmetry breaking of the CDW phase in RbV$_3$Sb$_5$ and KV$_3$Sb$_5$ by ultrafast time-resolved reflectivity. Our approach is based on extracting coherent phonon spectra induced by three-dimensional CDW and comparing them to calculated phonon frequencies via density-functional theory. The combination of these experimental results and calculations provides compelling evidence that the CDW structure of both compounds prevailing up to T$_{\\text{CDW}}$ is the 2 $\\times$ 2 $\\times$ 2 staggered inverse Star-of-David pattern with interlayer $$ phase shift, in which the six-fold rotational symmetry is broken. These observations thus corroborate six-fold rotational symmetry breaking throughout the CDW phase of RbV$_3$Sb$_5$ and KV$_3$Sb$_5$."
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-10T15:49:20Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    49,
                    20,
                    0,
                    69,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el"
                },
                "arxiv_journal_ref": "Phys. Rev. B 111, 165134 (2025)",
                "authors": [
                    {
                        "name": "Qinwen Deng"
                    },
                    {
                        "name": "Hengxin Tan"
                    },
                    {
                        "name": "Brenden R. Ortiz"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Binghai Yan"
                    },
                    {
                        "name": "Liang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wu"
                },
                "author": "Liang Wu",
                "arxiv_doi": "10.1103/PhysRevB.111.165134"
            },
            {
                "id": "http://arxiv.org/abs/2602.16839v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16839v1",
                "title": "Training Large Reasoning Models Efficiently via Progressive Thought Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Large Reasoning Models Efficiently via Progressive Thought Encoding"
                },
                "updated": "2026-02-18T20:03:38Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    20,
                    3,
                    38,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16839v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16839v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large reasoning models (LRMs) excel on complex problems but face a critical barrier to efficiency: reinforcement learning (RL) training requires long rollouts for outcome-based rewards, where autoregressive decoding dominates time and memory usage. While sliding-window cache strategies can bound memory, they disrupt long-context reasoning and degrade performance. We introduce Progressive Thought Encoding, a parameter-efficient fine-tuning method that enables LRMs to reason effectively under fixed-size caches. By progressively encoding intermediate reasoning into fixed-size vector representations, our approach eliminates the need to backpropagate through full-cache rollouts, thereby reducing memory usage, while maintaining constant memory during inference. Experiments on three models, including Qwen2.5-3B-Instruct, Qwen2.5-7B-Instruct, and DeepSeek-R1-Distill-Llama-8B, on six widely used challenging mathematical benchmarks show consistent gains: our method achieves +19.3% improvement over LoRA-based fine-tuning and +29.9% over LRMs without fine-tuning on average, with up to +23.4 accuracy improvement on AIME2024/2025 under the same tight cache budgets. These results demonstrate that Progressive Thought Encoding not only improves reasoning accuracy but also makes RL training of LRMs substantially more efficient and scalable under real-world memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) excel on complex problems but face a critical barrier to efficiency: reinforcement learning (RL) training requires long rollouts for outcome-based rewards, where autoregressive decoding dominates time and memory usage. While sliding-window cache strategies can bound memory, they disrupt long-context reasoning and degrade performance. We introduce Progressive Thought Encoding, a parameter-efficient fine-tuning method that enables LRMs to reason effectively under fixed-size caches. By progressively encoding intermediate reasoning into fixed-size vector representations, our approach eliminates the need to backpropagate through full-cache rollouts, thereby reducing memory usage, while maintaining constant memory during inference. Experiments on three models, including Qwen2.5-3B-Instruct, Qwen2.5-7B-Instruct, and DeepSeek-R1-Distill-Llama-8B, on six widely used challenging mathematical benchmarks show consistent gains: our method achieves +19.3% improvement over LoRA-based fine-tuning and +29.9% over LRMs without fine-tuning on average, with up to +23.4 accuracy improvement on AIME2024/2025 under the same tight cache budgets. These results demonstrate that Progressive Thought Encoding not only improves reasoning accuracy but also makes RL training of LRMs substantially more efficient and scalable under real-world memory constraints."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T20:03:38Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    20,
                    3,
                    38,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "ICLR 2026, 15 pages",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Chenliang Xu"
                    },
                    {
                        "name": "Jianfeng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Gao"
                },
                "author": "Jianfeng Gao"
            },
            {
                "id": "http://arxiv.org/abs/2602.16564v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16564v1",
                "title": "A Scalable Approach to Solving Simulation-Based Network Security Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable Approach to Solving Simulation-Based Network Security Games"
                },
                "updated": "2026-02-18T16:07:01Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    7,
                    1,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16564v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce MetaDOAR, a lightweight meta-controller that augments the Double Oracle / PSRO paradigm with a learned, partition-aware filtering layer and Q-value caching to enable scalable multi-agent reinforcement learning on very large cyber-network environments. MetaDOAR learns a compact state projection from per node structural embeddings to rapidly score and select a small subset of devices (a top-k partition) on which a conventional low-level actor performs focused beam search utilizing a critic agent. Selected candidate actions are evaluated with batched critic forwards and stored in an LRU cache keyed by a quantized state projection and local action identifiers, dramatically reducing redundant critic computation while preserving decision quality via conservative k-hop cache invalidation. Empirically, MetaDOAR attains higher player payoffs than SOTA baselines on large network topologies, without significant scaling issues in terms of memory usage or training time. This contribution provide a practical, theoretically motivated path to efficient hierarchical policy learning for large-scale networked decision problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MetaDOAR, a lightweight meta-controller that augments the Double Oracle / PSRO paradigm with a learned, partition-aware filtering layer and Q-value caching to enable scalable multi-agent reinforcement learning on very large cyber-network environments. MetaDOAR learns a compact state projection from per node structural embeddings to rapidly score and select a small subset of devices (a top-k partition) on which a conventional low-level actor performs focused beam search utilizing a critic agent. Selected candidate actions are evaluated with batched critic forwards and stored in an LRU cache keyed by a quantized state projection and local action identifiers, dramatically reducing redundant critic computation while preserving decision quality via conservative k-hop cache invalidation. Empirically, MetaDOAR attains higher player payoffs than SOTA baselines on large network topologies, without significant scaling issues in terms of memory usage or training time. This contribution provide a practical, theoretically motivated path to efficient hierarchical policy learning for large-scale networked decision problems."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T16:07:01Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    16,
                    7,
                    1,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Michael Lanier"
                    },
                    {
                        "name": "Yevgeniy Vorobeychik"
                    }
                ],
                "author_detail": {
                    "name": "Yevgeniy Vorobeychik"
                },
                "author": "Yevgeniy Vorobeychik"
            },
            {
                "id": "http://arxiv.org/abs/2602.16512v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16512v1",
                "title": "Framework of Thoughts: A Foundation Framework for Dynamic and Optimized Reasoning based on Chains, Trees, and Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Framework of Thoughts: A Foundation Framework for Dynamic and Optimized Reasoning based on Chains, Trees, and Graphs"
                },
                "updated": "2026-02-18T14:58:25Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    58,
                    25,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16512v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Prompting schemes such as Chain of Thought, Tree of Thoughts, and Graph of Thoughts can significantly enhance the reasoning capabilities of large language models. However, most existing schemes require users to define static, problem-specific reasoning structures that lack adaptability to dynamic or unseen problem types. Additionally, these schemes are often under-optimized in terms of hyperparameters, prompts, runtime, and prompting cost. To address these limitations, we introduce Framework of Thoughts (FoT)--a general-purpose foundation framework for building and optimizing dynamic reasoning schemes. FoT comes with built-in features for hyperparameter tuning, prompt optimization, parallel execution, and intelligent caching, unlocking the latent performance potential of reasoning schemes. We demonstrate FoT's capabilities by implementing three popular schemes--Tree of Thoughts, Graph of Thoughts, and ProbTree--within FoT. We empirically show that FoT enables significantly faster execution, reduces costs, and achieves better task scores through optimization. We release our codebase to facilitate the development of future dynamic and efficient reasoning schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting schemes such as Chain of Thought, Tree of Thoughts, and Graph of Thoughts can significantly enhance the reasoning capabilities of large language models. However, most existing schemes require users to define static, problem-specific reasoning structures that lack adaptability to dynamic or unseen problem types. Additionally, these schemes are often under-optimized in terms of hyperparameters, prompts, runtime, and prompting cost. To address these limitations, we introduce Framework of Thoughts (FoT)--a general-purpose foundation framework for building and optimizing dynamic reasoning schemes. FoT comes with built-in features for hyperparameter tuning, prompt optimization, parallel execution, and intelligent caching, unlocking the latent performance potential of reasoning schemes. We demonstrate FoT's capabilities by implementing three popular schemes--Tree of Thoughts, Graph of Thoughts, and ProbTree--within FoT. We empirically show that FoT enables significantly faster execution, reduces costs, and achieves better task scores through optimization. We release our codebase to facilitate the development of future dynamic and efficient reasoning schemes."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T14:58:25Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    14,
                    58,
                    25,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Felix Fricke"
                    },
                    {
                        "name": "Simon Malberg"
                    },
                    {
                        "name": "Georg Groh"
                    }
                ],
                "author_detail": {
                    "name": "Georg Groh"
                },
                "author": "Georg Groh"
            },
            {
                "id": "http://arxiv.org/abs/2602.16284v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16284v1",
                "title": "Fast KV Compaction via Attention Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast KV Compaction via Attention Matching"
                },
                "updated": "2026-02-18T09:06:53Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    9,
                    6,
                    53,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16284v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Scaling language models to long contexts is often bottlenecked by the size of the key-value (KV) cache. In deployed settings, long contexts are typically managed through compaction in token space via summarization. However, summarization can be highly lossy, substantially harming downstream performance. Recent work on Cartridges has shown that it is possible to train highly compact KV caches in latent space that closely match full-context performance, but at the cost of slow and expensive end-to-end optimization. This work describes an approach for fast context compaction in latent space through Attention Matching, which constructs compact keys and values to reproduce attention outputs and preserve attention mass at a per-KV-head level. We show that this formulation naturally decomposes into simple subproblems, some of which admit efficient closed-form solutions. Within this framework, we develop a family of methods that significantly push the Pareto frontier of compaction time versus quality, achieving up to 50x compaction in seconds on some datasets with little quality loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to long contexts is often bottlenecked by the size of the key-value (KV) cache. In deployed settings, long contexts are typically managed through compaction in token space via summarization. However, summarization can be highly lossy, substantially harming downstream performance. Recent work on Cartridges has shown that it is possible to train highly compact KV caches in latent space that closely match full-context performance, but at the cost of slow and expensive end-to-end optimization. This work describes an approach for fast context compaction in latent space through Attention Matching, which constructs compact keys and values to reproduce attention outputs and preserve attention mass at a per-KV-head level. We show that this formulation naturally decomposes into simple subproblems, some of which admit efficient closed-form solutions. Within this framework, we develop a family of methods that significantly push the Pareto frontier of compaction time versus quality, achieving up to 50x compaction in seconds on some datasets with little quality loss."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T09:06:53Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    9,
                    6,
                    53,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Adam Zweiger"
                    },
                    {
                        "name": "Xinghong Fu"
                    },
                    {
                        "name": "Han Guo"
                    },
                    {
                        "name": "Yoon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Yoon Kim"
                },
                "author": "Yoon Kim"
            },
            {
                "id": "http://arxiv.org/abs/2502.01258v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.01258v4",
                "title": "Magnetizing altermagnets by ultrafast asymmetric spin dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetizing altermagnets by ultrafast asymmetric spin dynamics"
                },
                "updated": "2026-02-18T07:41:23Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    7,
                    41,
                    23,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.01258v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.01258v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Laser pulses are known to induce symmetric demagnetization: equal loss of magnetic moments in the identical sublattices of antiferromagnets and ferromagnets at ultrashort timescales. Using time-dependent density functional theory, we show that linearly polarized laser pulses can drive asymmetric demagnetization between otherwise identical sublattices in the $d$-wave compensated altermagnet (AM) RuO$_2$, resulting in a \\textit{photo-induced ferrimagnetic state} with a strong net magnetization of $\\sim$0.2 $_B$ per unit cell. The sign and magnitude of this metastable magnetization are highly controllable by laser polarization. We identify polarization-selective asymmetric optical intersite spin transfer (a-OISTR) as the primary mechanism generating the net moment, followed by asymmetric spin flips (a-SF) that further amplifies it. Both effects originate from the characteristic nodal spin band topology of \\textit{d}-wave AMs. Moreover, we demonstrate that this laser-induced magnetization is universal across various $d$-wave AMs, including experimentally confirmed KV$_2$Se$_2$O and RbV$_2$Te$_2$O. We uncover a robust route to light-controlled magnetization in AMs on ultrafast timescales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laser pulses are known to induce symmetric demagnetization: equal loss of magnetic moments in the identical sublattices of antiferromagnets and ferromagnets at ultrashort timescales. Using time-dependent density functional theory, we show that linearly polarized laser pulses can drive asymmetric demagnetization between otherwise identical sublattices in the $d$-wave compensated altermagnet (AM) RuO$_2$, resulting in a \\textit{photo-induced ferrimagnetic state} with a strong net magnetization of $\\sim$0.2 $_B$ per unit cell. The sign and magnitude of this metastable magnetization are highly controllable by laser polarization. We identify polarization-selective asymmetric optical intersite spin transfer (a-OISTR) as the primary mechanism generating the net moment, followed by asymmetric spin flips (a-SF) that further amplifies it. Both effects originate from the characteristic nodal spin band topology of \\textit{d}-wave AMs. Moreover, we demonstrate that this laser-induced magnetization is universal across various $d$-wave AMs, including experimentally confirmed KV$_2$Se$_2$O and RbV$_2$Te$_2$O. We uncover a robust route to light-controlled magnetization in AMs on ultrafast timescales."
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-03T11:31:12Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    11,
                    31,
                    12,
                    0,
                    34,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci"
                },
                "authors": [
                    {
                        "name": "Zhaobo Zhou"
                    },
                    {
                        "name": "Sangeeta Sharma"
                    },
                    {
                        "name": "John Kay Dewhurst"
                    },
                    {
                        "name": "Junjie He"
                    }
                ],
                "author_detail": {
                    "name": "Junjie He"
                },
                "author": "Junjie He"
            },
            {
                "id": "http://arxiv.org/abs/2602.17726v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.17726v1",
                "title": "Closing Africa's Early Warning Gap: AI Weather Forecasting for Disaster Prevention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Closing Africa's Early Warning Gap: AI Weather Forecasting for Disaster Prevention"
                },
                "updated": "2026-02-18T06:33:03Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    6,
                    33,
                    3,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.17726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.17726v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In January 2026, torrential rains killed 200-300 people across Southern Africa, exposing a critical reality: 60% of the continent lacks effective early warning systems due to infrastructure costs. Traditional radar stations exceed USD 1 million each, leaving Africa with an 18x coverage deficit compared to the US and EU. We present a production-grade architecture for deploying NVIDIA Earth-2 AI weather models at USD 1,430-1,730/month for national-scale deployment - enabling coverage at 2,000-4,545x lower cost than radar. The system generates 15-day global atmospheric forecasts, cached in PostgreSQL to enable user queries under 200 milliseconds without real-time inference.\n  Deployed in South Africa in February 2026, our system demonstrates three technical contributions: (1) a ProcessPoolExecutor-based event loop isolation pattern that resolves aiobotocore session lifecycle conflicts in async Python applications; (2) a database-backed serving architecture where the GPU writes global forecasts directly to PostgreSQL, eliminating HTTP transfer bottlenecks for high-resolution tensors; and (3) an automated coordinate management pattern for multi-step inference across 61 timesteps. Forecasts are delivered via WhatsApp, leveraging 80%+ market penetration. This architecture makes continent-scale early warning systems economically viable, supporting UNDRR findings that such systems reduce disaster death rates by 6x. All architectural details are documented inline for full reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In January 2026, torrential rains killed 200-300 people across Southern Africa, exposing a critical reality: 60% of the continent lacks effective early warning systems due to infrastructure costs. Traditional radar stations exceed USD 1 million each, leaving Africa with an 18x coverage deficit compared to the US and EU. We present a production-grade architecture for deploying NVIDIA Earth-2 AI weather models at USD 1,430-1,730/month for national-scale deployment - enabling coverage at 2,000-4,545x lower cost than radar. The system generates 15-day global atmospheric forecasts, cached in PostgreSQL to enable user queries under 200 milliseconds without real-time inference.\n  Deployed in South Africa in February 2026, our system demonstrates three technical contributions: (1) a ProcessPoolExecutor-based event loop isolation pattern that resolves aiobotocore session lifecycle conflicts in async Python applications; (2) a database-backed serving architecture where the GPU writes global forecasts directly to PostgreSQL, eliminating HTTP transfer bottlenecks for high-resolution tensors; and (3) an automated coordinate management pattern for multi-step inference across 61 timesteps. Forecasts are delivered via WhatsApp, leveraging 80%+ market penetration. This architecture makes continent-scale early warning systems economically viable, supporting UNDRR findings that such systems reduce disaster death rates by 6x. All architectural details are documented inline for full reproducibility."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T06:33:03Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    6,
                    33,
                    3,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "23 pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Qness Ndlovu"
                    }
                ],
                "author_detail": {
                    "name": "Qness Ndlovu"
                },
                "author": "Qness Ndlovu"
            },
            {
                "id": "http://arxiv.org/abs/2602.08242v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.08242v4",
                "title": "Software Testing at the Network Layer: Automated HTTP API Quality Assessment and Security Analysis of Production Web Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software Testing at the Network Layer: Automated HTTP API Quality Assessment and Security Analysis of Production Web Applications"
                },
                "updated": "2026-02-18T04:00:37Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    4,
                    0,
                    37,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.08242v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.08242v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern web applications rely heavily on client-side API calls to fetch data, render content, and communicate with backend services. However, the quality of these network interactions (redundant requests, missing cache headers, oversized payloads, and excessive third-party dependencies) is rarely tested in a systematic way. Moreover, many of these quality deficiencies carry security implications: missing cache headers enable cache poisoning, excessive third-party dependencies expand the supply-chain attack surface, and error responses risk leaking server internals. In this study, we present an automated software testing framework that captures and analyzes the complete HTTP traffic of 18 production websites spanning 11 categories (e-commerce, news, government, developer tools, travel, and more). Using automated browser instrumentation via Playwright, we record 108 HAR (HTTP Archive) files across 3 independent runs per page, then apply 8 heuristic-based anti-pattern detectors to produce a composite quality score (0-100) for each site. Our results reveal a wide quality spectrum: minimalist server-rendered sites achieve perfect scores of 100, while content-heavy commercial sites score as low as 56.8. We identify redundant API calls and missing cache headers as the two most pervasive anti-patterns, each affecting 67% of sites, while third-party overhead exceeds 20% on 72% of sites. One utility site makes 2,684 requests per page load, which is 447x more than the most minimal site. To protect site reputations, all identities are anonymized using category-based pseudonyms. We provide all analysis scripts, anonymized results, and reproducibility instructions as an open artifact. This work establishes an empirical baseline for HTTP API call quality across the modern web and offers a reproducible testing framework that researchers and practitioners can apply to their own applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern web applications rely heavily on client-side API calls to fetch data, render content, and communicate with backend services. However, the quality of these network interactions (redundant requests, missing cache headers, oversized payloads, and excessive third-party dependencies) is rarely tested in a systematic way. Moreover, many of these quality deficiencies carry security implications: missing cache headers enable cache poisoning, excessive third-party dependencies expand the supply-chain attack surface, and error responses risk leaking server internals. In this study, we present an automated software testing framework that captures and analyzes the complete HTTP traffic of 18 production websites spanning 11 categories (e-commerce, news, government, developer tools, travel, and more). Using automated browser instrumentation via Playwright, we record 108 HAR (HTTP Archive) files across 3 independent runs per page, then apply 8 heuristic-based anti-pattern detectors to produce a composite quality score (0-100) for each site. Our results reveal a wide quality spectrum: minimalist server-rendered sites achieve perfect scores of 100, while content-heavy commercial sites score as low as 56.8. We identify redundant API calls and missing cache headers as the two most pervasive anti-patterns, each affecting 67% of sites, while third-party overhead exceeds 20% on 72% of sites. One utility site makes 2,684 requests per page load, which is 447x more than the most minimal site. To protect site reputations, all identities are anonymized using category-based pseudonyms. We provide all analysis scripts, anonymized results, and reproducibility instructions as an open artifact. This work establishes an empirical baseline for HTTP API call quality across the modern web and offers a reproducible testing framework that researchers and practitioners can apply to their own applications."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-09T03:39:45Z",
                "published_parsed": [
                    2026,
                    2,
                    9,
                    3,
                    39,
                    45,
                    0,
                    40,
                    0
                ],
                "arxiv_comment": "18+ pages, 5 figures, 3 tables. Code and data: https://github.com/amughalbscs16/network-layer-quality-testing",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Ali Hassaan Mughal"
                    },
                    {
                        "name": "Muhammad Bilal"
                    },
                    {
                        "name": "Noor Fatima"
                    }
                ],
                "author_detail": {
                    "name": "Noor Fatima"
                },
                "author": "Noor Fatima"
            },
            {
                "id": "http://arxiv.org/abs/2601.11837v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.11837v2",
                "title": "High-Voltage Performance Testing in LAr of the PMMA Cathode Connection for the DarkSide-20k Experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Voltage Performance Testing in LAr of the PMMA Cathode Connection for the DarkSide-20k Experiment"
                },
                "updated": "2026-02-18T02:02:11Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    2,
                    2,
                    11,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.11837v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.11837v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "DarkSide-20k (DS-20k) is a next-generation dual-phase liquid argon (LAr) time projection chamber (TPC) devoted to the direct-detection of dark matter. The detector is currently under construction in Hall-C at the Laboratori Nazionali del Gran Sasso, Italy, at a depth of approximately 3500 m water equivalent. The detector will instrument 49.7 t of low-radioactivity underground LAr contained within an acrylic TPC and is designed to reach a WIMP-nucleon spin-independent cross-section sensitivity down to $10^{-48}\\,\\mathrm{cm}^{2}$ for a WIMP mass of $0.1\\,\\mathrm{TeV}/c^{2}$ in a 200 tonne-year run. In DS-20k a uniform electric drift field is established in the active volume to transport ionization electrons toward the electroluminescence region, with the required high voltage delivered to the TPC cathode through a custom cable and stress-cone assembly. At the University of California, Davis, a dedicated test setup was developed to reproduce the DS-20k cathode high-voltage connection in LAr, matching the local electric-field conditions. This work summarizes the results of a comprehensive test campaign validating the operation of the DS-20k cathode HV system in LAr up to $-100$ kV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DarkSide-20k (DS-20k) is a next-generation dual-phase liquid argon (LAr) time projection chamber (TPC) devoted to the direct-detection of dark matter. The detector is currently under construction in Hall-C at the Laboratori Nazionali del Gran Sasso, Italy, at a depth of approximately 3500 m water equivalent. The detector will instrument 49.7 t of low-radioactivity underground LAr contained within an acrylic TPC and is designed to reach a WIMP-nucleon spin-independent cross-section sensitivity down to $10^{-48}\\,\\mathrm{cm}^{2}$ for a WIMP mass of $0.1\\,\\mathrm{TeV}/c^{2}$ in a 200 tonne-year run. In DS-20k a uniform electric drift field is established in the active volume to transport ionization electrons toward the electroluminescence region, with the required high voltage delivered to the TPC cathode through a custom cable and stress-cone assembly. At the University of California, Davis, a dedicated test setup was developed to reproduce the DS-20k cathode high-voltage connection in LAr, matching the local electric-field conditions. This work summarizes the results of a comprehensive test campaign validating the operation of the DS-20k cathode HV system in LAr up to $-100$ kV."
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-17T00:03:03Z",
                "published_parsed": [
                    2026,
                    1,
                    17,
                    0,
                    3,
                    3,
                    5,
                    17,
                    0
                ],
                "arxiv_comment": "Proceedings of LIght Detection In Noble Elements - LIDINE 2025",
                "arxiv_primary_category": {
                    "term": "physics.ins-det"
                },
                "authors": [
                    {
                        "name": "Ludovico Luzzi"
                    }
                ],
                "author_detail": {
                    "name": "Ludovico Luzzi"
                },
                "author": "Ludovico Luzzi"
            },
            {
                "id": "http://arxiv.org/abs/2602.16132v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16132v1",
                "title": "CHAI: CacHe Attention Inference for text2video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHAI: CacHe Attention Inference for text2video"
                },
                "updated": "2026-02-18T01:53:29Z",
                "updated_parsed": [
                    2026,
                    2,
                    18,
                    1,
                    53,
                    29,
                    2,
                    49,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16132v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Text-to-video diffusion models deliver impressive results but remain slow because of the sequential denoising of 3D latents. Existing approaches to speed up inference either require expensive model retraining or use heuristic-based step skipping, which struggles to maintain video quality as the number of denoising steps decreases. Our work, CHAI, aims to use cross-inference caching to reduce latency while maintaining video quality. We introduce Cache Attention as an effective method for attending to shared objects/scenes across cross-inference latents. This selective attention mechanism enables effective reuse of cached latents across semantically related prompts, yielding high cache hit rates. We show that it is possible to generate high-quality videos using Cache Attention with as few as 8 denoising steps. When integrated into the overall system, CHAI is 1.65x - 3.35x faster than baseline OpenSora 1.2 while maintaining video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-video diffusion models deliver impressive results but remain slow because of the sequential denoising of 3D latents. Existing approaches to speed up inference either require expensive model retraining or use heuristic-based step skipping, which struggles to maintain video quality as the number of denoising steps decreases. Our work, CHAI, aims to use cross-inference caching to reduce latency while maintaining video quality. We introduce Cache Attention as an effective method for attending to shared objects/scenes across cross-inference latents. This selective attention mechanism enables effective reuse of cached latents across semantically related prompts, yielding high cache hit rates. We show that it is possible to generate high-quality videos using Cache Attention with as few as 8 denoising steps. When integrated into the overall system, CHAI is 1.65x - 3.35x faster than baseline OpenSora 1.2 while maintaining video quality."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T01:53:29Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    1,
                    53,
                    29,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Joel Mathew Cherian"
                    },
                    {
                        "name": "Ashutosh Muralidhara Bharadwaj"
                    },
                    {
                        "name": "Vima Gupta"
                    },
                    {
                        "name": "Anand Padmanabha Iyer"
                    }
                ],
                "author_detail": {
                    "name": "Anand Padmanabha Iyer"
                },
                "author": "Anand Padmanabha Iyer"
            },
            {
                "id": "http://arxiv.org/abs/2602.02958v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.02958v2",
                "title": "Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization"
                },
                "updated": "2026-02-17T23:49:23Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    23,
                    49,
                    23,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.02958v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.02958v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite rapid progress in autoregressive video diffusion, an emerging system algorithm bottleneck limits both deployability and generation capability: KV cache memory. In autoregressive video generation models, the KV cache grows with generation history and quickly dominates GPU memory, often exceeding 30 GB, preventing deployment on widely available hardware. More critically, constrained KV cache budgets restrict the effective working memory, directly degrading long horizon consistency in identity, layout, and motion. To address this challenge, we present Quant VideoGen (QVG), a training free KV cache quantization framework for autoregressive video diffusion models. QVG leverages video spatiotemporal redundancy through Semantic Aware Smoothing, producing low magnitude, quantization friendly residuals. It further introduces Progressive Residual Quantization, a coarse to fine multi stage scheme that reduces quantization error while enabling a smooth quality memory trade off. Across LongCat Video, HY WorldPlay, and Self Forcing benchmarks, QVG establishes a new Pareto frontier between quality and memory efficiency, reducing KV cache memory by up to 7.0 times with less than 4% end to end latency overhead while consistently outperforming existing baselines in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite rapid progress in autoregressive video diffusion, an emerging system algorithm bottleneck limits both deployability and generation capability: KV cache memory. In autoregressive video generation models, the KV cache grows with generation history and quickly dominates GPU memory, often exceeding 30 GB, preventing deployment on widely available hardware. More critically, constrained KV cache budgets restrict the effective working memory, directly degrading long horizon consistency in identity, layout, and motion. To address this challenge, we present Quant VideoGen (QVG), a training free KV cache quantization framework for autoregressive video diffusion models. QVG leverages video spatiotemporal redundancy through Semantic Aware Smoothing, producing low magnitude, quantization friendly residuals. It further introduces Progressive Residual Quantization, a coarse to fine multi stage scheme that reduces quantization error while enabling a smooth quality memory trade off. Across LongCat Video, HY WorldPlay, and Self Forcing benchmarks, QVG establishes a new Pareto frontier between quality and memory efficiency, reducing KV cache memory by up to 7.0 times with less than 4% end to end latency overhead while consistently outperforming existing baselines in generation quality."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-03T00:54:32Z",
                "published_parsed": [
                    2026,
                    2,
                    3,
                    0,
                    54,
                    32,
                    1,
                    34,
                    0
                ],
                "arxiv_comment": "11 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Haocheng Xi"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Xingyang Li"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhuoyang Zhang"
                    },
                    {
                        "name": "Jintao Zhang"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Zhiying Xu"
                    },
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Chenfeng Xu"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Kurt Keutzer"
                    }
                ],
                "author_detail": {
                    "name": "Kurt Keutzer"
                },
                "author": "Kurt Keutzer"
            },
            {
                "id": "http://arxiv.org/abs/2602.16092v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16092v1",
                "title": "Why Any-Order Autoregressive Models Need Two-Stream Attention: A Structural-Semantic Tradeoff",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Any-Order Autoregressive Models Need Two-Stream Attention: A Structural-Semantic Tradeoff"
                },
                "updated": "2026-02-17T23:39:39Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    23,
                    39,
                    39,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16092v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Any-order autoregressive models (AO-ARMs) offer a promising path toward efficient masked diffusion by enabling native key-value caching, but competitive performance has so far required two-stream attention, typically motivated as a means of decoupling token content from position. In this work, we argue that two-stream attention may be serving a more subtle role. We identify a structural-semantic tradeoff in any-order generation: the hidden representation at each step must simultaneously attend to semantically informative tokens for prediction and structurally recent tokens for summarization, objectives that compete for attention capacity in a single stream but can specialize across two streams. To isolate this tradeoff from position-content separation, we propose Decoupled RoPE, a modification to rotary position embeddings that provides target position information without revealing target content. Decoupled RoPE performs competitively at short sequence lengths--where semantic and structural proximity coincide--but degrades as sequence length increases and the two orderings diverge. These results suggest that the success of two-stream attention stems not merely from separating position from content, but from circumventing the deeper structural-semantic tradeoff inherent to any-order generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Any-order autoregressive models (AO-ARMs) offer a promising path toward efficient masked diffusion by enabling native key-value caching, but competitive performance has so far required two-stream attention, typically motivated as a means of decoupling token content from position. In this work, we argue that two-stream attention may be serving a more subtle role. We identify a structural-semantic tradeoff in any-order generation: the hidden representation at each step must simultaneously attend to semantically informative tokens for prediction and structurally recent tokens for summarization, objectives that compete for attention capacity in a single stream but can specialize across two streams. To isolate this tradeoff from position-content separation, we propose Decoupled RoPE, a modification to rotary position embeddings that provides target position information without revealing target content. Decoupled RoPE performs competitively at short sequence lengths--where semantic and structural proximity coincide--but degrades as sequence length increases and the two orderings diverge. These results suggest that the success of two-stream attention stems not merely from separating position from content, but from circumventing the deeper structural-semantic tradeoff inherent to any-order generation."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-17T23:39:39Z",
                "published_parsed": [
                    2026,
                    2,
                    17,
                    23,
                    39,
                    39,
                    1,
                    48,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Patrick Pynadath"
                    },
                    {
                        "name": "Ruqi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruqi Zhang"
                },
                "author": "Ruqi Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2602.16054v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16054v1",
                "title": "CLAA: Cross-Layer Attention Aggregation for Accelerating LLM Prefill",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLAA: Cross-Layer Attention Aggregation for Accelerating LLM Prefill"
                },
                "updated": "2026-02-17T22:08:16Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    22,
                    8,
                    16,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16054v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The prefill stage in long-context LLM inference remains a computational bottleneck. Recent token-ranking heuristics accelerate inference by selectively processing a subset of semantically relevant tokens. However, existing methods suffer from unstable token importance estimation, often varying between layers. Evaluating token-ranking quality independently from heuristic-specific architectures is challenging. To address this, we introduce an Answer-Informed Oracle, which defines ground-truth token importance by measuring attention from generated answers back to the prompt. This oracle reveals that existing heuristics exhibit high variance across layers: rankings can degrade sharply at specific layers, a failure mode invisible to end-to-end benchmarks. The diagnosis suggests a simple fix: aggregate scores across layers rather than relying on any single one. We implement this as Cross-Layer Attention Aggregation (CLAA), which closes the gap to the oracle upper bound and reduces Time-to-First-Token (TTFT) by up to 39\\% compared to the Full KV Cache baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The prefill stage in long-context LLM inference remains a computational bottleneck. Recent token-ranking heuristics accelerate inference by selectively processing a subset of semantically relevant tokens. However, existing methods suffer from unstable token importance estimation, often varying between layers. Evaluating token-ranking quality independently from heuristic-specific architectures is challenging. To address this, we introduce an Answer-Informed Oracle, which defines ground-truth token importance by measuring attention from generated answers back to the prompt. This oracle reveals that existing heuristics exhibit high variance across layers: rankings can degrade sharply at specific layers, a failure mode invisible to end-to-end benchmarks. The diagnosis suggests a simple fix: aggregate scores across layers rather than relying on any single one. We implement this as Cross-Layer Attention Aggregation (CLAA), which closes the gap to the oracle upper bound and reduces Time-to-First-Token (TTFT) by up to 39\\% compared to the Full KV Cache baseline."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-17T22:08:16Z",
                "published_parsed": [
                    2026,
                    2,
                    17,
                    22,
                    8,
                    16,
                    1,
                    48,
                    0
                ],
                "arxiv_comment": "15 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Bradley McDanel"
                    },
                    {
                        "name": "Steven Li"
                    },
                    {
                        "name": "Harshit Khaitan"
                    }
                ],
                "author_detail": {
                    "name": "Harshit Khaitan"
                },
                "author": "Harshit Khaitan"
            },
            {
                "id": "http://arxiv.org/abs/2602.16727v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16727v1",
                "title": "Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation"
                },
                "updated": "2026-02-17T15:39:51Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    15,
                    39,
                    51,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16727v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost limits scalability. To address this, we design a mobility-aware cache framework named MobCache that leverages reconstructible caches to enable efficient large-scale human mobility simulations. It consists of: (1) a reasoning component that encodes each reasoning step as a latent-space embedding and uses a latent-space evaluator to enable the reuse and recombination of reasoning steps; and (2) a decoding component that employs a lightweight decoder trained with mobility law-constrained distillation to translate latent-space reasoning chains into natural language, thereby improving simulation efficiency while maintaining fidelity. Experiments show that MobCache significantly improves efficiency across multiple dimensions while maintaining performance comparable to state-of-the-art LLM-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost limits scalability. To address this, we design a mobility-aware cache framework named MobCache that leverages reconstructible caches to enable efficient large-scale human mobility simulations. It consists of: (1) a reasoning component that encodes each reasoning step as a latent-space embedding and uses a latent-space evaluator to enable the reuse and recombination of reasoning steps; and (2) a decoding component that employs a lightweight decoder trained with mobility law-constrained distillation to translate latent-space reasoning chains into natural language, thereby improving simulation efficiency while maintaining fidelity. Experiments show that MobCache significantly improves efficiency across multiple dimensions while maintaining performance comparable to state-of-the-art LLM-based methods."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-17T15:39:51Z",
                "published_parsed": [
                    2026,
                    2,
                    17,
                    15,
                    39,
                    51,
                    1,
                    48,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Hua Yan"
                    },
                    {
                        "name": "Heng Tan"
                    },
                    {
                        "name": "Yingxue Zhang"
                    },
                    {
                        "name": "Yu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Yang"
                },
                "author": "Yu Yang"
            },
            {
                "id": "http://arxiv.org/abs/2601.15311v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.15311v3",
                "title": "Aeon: High-Performance Neuro-Symbolic Memory Management for Long-Horizon LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aeon: High-Performance Neuro-Symbolic Memory Management for Long-Horizon LLM Agents"
                },
                "updated": "2026-02-17T15:21:45Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    15,
                    21,
                    45,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.15311v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.15311v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are fundamentally constrained by the quadratic computational cost of self-attention and the \"Lost in the Middle\" phenomenon, where reasoning capabilities degrade as context windows expand. Existing solutions, primarily \"Flat RAG\" architectures relying on vector databases, treat memory as an unstructured bag of embeddings, failing to capture the hierarchical and temporal structure of long-horizon interactions. This paper presents Aeon, a Neuro-Symbolic Cognitive Operating System that redefines memory as a managed OS resource. Aeon structures memory into a Memory Palace (a spatial index implemented via Atlas, a SIMD-accelerated Page-Clustered Vector Index) and a Trace (a neuro-symbolic episodic graph). This architecture introduces three advances: (1) Symmetric INT8 Scalar Quantization, achieving 3.1x spatial compression and 5.6x math acceleration via NEON SDOT intrinsics; (2) a decoupled Write-Ahead Log (WAL) ensuring crash-recoverability with statistically negligible overhead (<1%); and (3) a Sidecar Blob Arena eliminating the prior 440-character text ceiling via an append-only mmap-backed blob file with generational garbage collection. The Semantic Lookaside Buffer (SLB) exploits conversational locality to achieve sub-5us retrieval latencies, with INT8 vectors dequantized to FP32 on cache insertion to preserve L1-resident lookup performance. Benchmarks on Apple M4 Max demonstrate that the combined architecture achieves 4.70ns INT8 dot product latency, 3.09us tree traversal at 100K nodes (3.4x over FP32), and P99 read latency of 750ns under hostile 16-thread contention via epoch-based reclamation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are fundamentally constrained by the quadratic computational cost of self-attention and the \"Lost in the Middle\" phenomenon, where reasoning capabilities degrade as context windows expand. Existing solutions, primarily \"Flat RAG\" architectures relying on vector databases, treat memory as an unstructured bag of embeddings, failing to capture the hierarchical and temporal structure of long-horizon interactions. This paper presents Aeon, a Neuro-Symbolic Cognitive Operating System that redefines memory as a managed OS resource. Aeon structures memory into a Memory Palace (a spatial index implemented via Atlas, a SIMD-accelerated Page-Clustered Vector Index) and a Trace (a neuro-symbolic episodic graph). This architecture introduces three advances: (1) Symmetric INT8 Scalar Quantization, achieving 3.1x spatial compression and 5.6x math acceleration via NEON SDOT intrinsics; (2) a decoupled Write-Ahead Log (WAL) ensuring crash-recoverability with statistically negligible overhead (<1%); and (3) a Sidecar Blob Arena eliminating the prior 440-character text ceiling via an append-only mmap-backed blob file with generational garbage collection. The Semantic Lookaside Buffer (SLB) exploits conversational locality to achieve sub-5us retrieval latencies, with INT8 vectors dequantized to FP32 on cache insertion to preserve L1-resident lookup performance. Benchmarks on Apple M4 Max demonstrate that the combined architecture achieves 4.70ns INT8 dot product latency, 3.09us tree traversal at 100K nodes (3.4x over FP32), and P99 read latency of 750ns under hostile 16-thread contention via epoch-based reclamation."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-14T15:23:22Z",
                "published_parsed": [
                    2026,
                    1,
                    14,
                    15,
                    23,
                    22,
                    2,
                    14,
                    0
                ],
                "arxiv_comment": "v3: Production hardening. Added INT8 quantization (5.6x dot product speedup, 3.1x compression), crash recovery via decoupled WAL (<1% overhead), unlimited text storage via sidecar blob arena with generational GC, and epoch-based reclamation for lock-free reads (P99 750ns under 16-thread contention). Revised for systems engineering clarity",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Mustafa Arslan"
                    }
                ],
                "author_detail": {
                    "name": "Mustafa Arslan"
                },
                "author": "Mustafa Arslan"
            },
            {
                "id": "http://arxiv.org/abs/2408.00539v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2408.00539v2",
                "title": "Intermittent Semi-Working Mask: A New Masking Paradigm for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermittent Semi-Working Mask: A New Masking Paradigm for LLMs"
                },
                "updated": "2026-02-17T13:11:05Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    13,
                    11,
                    5,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2408.00539v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2408.00539v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multi-turn dialogues and context-intensive tasks challenge Large Language Models (LLMs) to integrate long histories without sacrificing generation quality. Although prefix LLMs can better exploit historical context via bidirectional attention on prefix tokens, they are rarely used in practice because multi-turn training requires many duplicated triplets, and its bidirectional prefix prevents KV-cache reuse at inference time, driving up high cost and latency. To retain the contextual understanding of prefix mask while preserving the inference-time efficiency of causal mask, we introduce Intermittent Semi-working Mask (ISM), a masking scheme that injects sparse bidirectional attention into the causal backbone. ISM alternates bidirectional attention over query segments with unidirectional attention over answer segments, enabling the synthesis of in-context while preserving global causality. This design eliminates triplet expansion during training and maintains KV-cache reuse during inference, yielding latency comparable to standard causal LLMs. ISM is architecture-agnostic and parameter-free, adding only minimal latency. Across extensive evaluations, ISM outperforms causal baselines not only on multi-turn dialogue, but also on context-intensive tasks like mathematical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues and context-intensive tasks challenge Large Language Models (LLMs) to integrate long histories without sacrificing generation quality. Although prefix LLMs can better exploit historical context via bidirectional attention on prefix tokens, they are rarely used in practice because multi-turn training requires many duplicated triplets, and its bidirectional prefix prevents KV-cache reuse at inference time, driving up high cost and latency. To retain the contextual understanding of prefix mask while preserving the inference-time efficiency of causal mask, we introduce Intermittent Semi-working Mask (ISM), a masking scheme that injects sparse bidirectional attention into the causal backbone. ISM alternates bidirectional attention over query segments with unidirectional attention over answer segments, enabling the synthesis of in-context while preserving global causality. This design eliminates triplet expansion during training and maintains KV-cache reuse during inference, yielding latency comparable to standard causal LLMs. ISM is architecture-agnostic and parameter-free, adding only minimal latency. Across extensive evaluations, ISM outperforms causal baselines not only on multi-turn dialogue, but also on context-intensive tasks like mathematical reasoning."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-08-01T13:22:01Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "HaoYuan Hu"
                    },
                    {
                        "name": "Mingcong Lu"
                    },
                    {
                        "name": "Di Luo"
                    },
                    {
                        "name": "XinYa Wu"
                    },
                    {
                        "name": "Jiangcai Zhu"
                    },
                    {
                        "name": "Taoye Yin"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Shusheng Zhang"
                    },
                    {
                        "name": "KeZun Zhang"
                    },
                    {
                        "name": "KaiLai Shao"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Feng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wang"
                },
                "author": "Feng Wang"
            },
            {
                "id": "http://arxiv.org/abs/2507.01110v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.01110v4",
                "title": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory"
                },
                "updated": "2026-02-17T12:40:07Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    12,
                    40,
                    7,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.01110v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.01110v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Gaussian Splatting has emerged as a high-performance technique for novel view synthesis, enabling real-time rendering and high-quality reconstruction of small scenes. However, scaling to larger environments has so far relied on partitioning the scene into chunks -- a strategy that introduces artifacts at chunk boundaries, complicates training across varying scales, and is poorly suited to unstructured scenarios such as city-scale flyovers combined with street-level views. Moreover, rendering remains fundamentally limited by GPU memory, as all visible chunks must reside in VRAM simultaneously. We introduce A LoD of Gaussians, a framework for training and rendering ultra-large-scale Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our method stores the full scene out-of-core (e.g., in CPU memory) and trains a Level-of-Detail (LoD) representation directly, dynamically streaming only the relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with Sequential Point Trees enables efficient, view-dependent LoD selection, while a lightweight caching and view scheduling system exploits temporal coherence to support real-time streaming and rendering. Together, these innovations enable seamless multi-scale reconstruction and interactive visualization of complex scenes -- from broad aerial views to fine-grained ground-level details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Splatting has emerged as a high-performance technique for novel view synthesis, enabling real-time rendering and high-quality reconstruction of small scenes. However, scaling to larger environments has so far relied on partitioning the scene into chunks -- a strategy that introduces artifacts at chunk boundaries, complicates training across varying scales, and is poorly suited to unstructured scenarios such as city-scale flyovers combined with street-level views. Moreover, rendering remains fundamentally limited by GPU memory, as all visible chunks must reside in VRAM simultaneously. We introduce A LoD of Gaussians, a framework for training and rendering ultra-large-scale Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our method stores the full scene out-of-core (e.g., in CPU memory) and trains a Level-of-Detail (LoD) representation directly, dynamically streaming only the relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with Sequential Point Trees enables efficient, view-dependent LoD selection, while a lightweight caching and view scheduling system exploits temporal coherence to support real-time streaming and rendering. Together, these innovations enable seamless multi-scale reconstruction and interactive visualization of complex scenes -- from broad aerial views to fine-grained ground-level details."
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-01T18:12:43Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    18,
                    12,
                    43,
                    1,
                    182,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR"
                },
                "authors": [
                    {
                        "name": "Felix Windisch"
                    },
                    {
                        "name": "Thomas Khler"
                    },
                    {
                        "name": "Lukas Radl"
                    },
                    {
                        "name": "Mattia D'Urso"
                    },
                    {
                        "name": "Michael Steiner"
                    },
                    {
                        "name": "Dieter Schmalstieg"
                    },
                    {
                        "name": "Markus Steinberger"
                    }
                ],
                "author_detail": {
                    "name": "Markus Steinberger"
                },
                "author": "Markus Steinberger"
            },
            {
                "id": "http://arxiv.org/abs/2602.02108v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.02108v3",
                "title": "Out of the Memory Barrier: A Highly Memory Efficient Training System for LLMs with Million-Token Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out of the Memory Barrier: A Highly Memory Efficient Training System for LLMs with Million-Token Contexts"
                },
                "updated": "2026-02-17T11:41:00Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    11,
                    41,
                    0,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.02108v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.02108v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Training Large Language Models (LLMs) on long contexts is severely constrained by prohibitive GPU memory overhead, not training time. The primary culprits are the activations, whose memory footprints scale linearly with sequence length. We introduce OOMB, a highly memory-efficient training system that directly confronts this barrier. Our approach employs a chunk-recurrent training framework with on-the-fly activation recomputation, which maintains a constant activation memory footprint (O(1)) and shifts the primary bottleneck to the growing KV cache. To manage the KV cache, OOMB integrates a suite of synergistic optimizations: a paged memory manager for both the KV cache and its gradients to eliminate fragmentation, asynchronous CPU offloading to hide data transfer latency, and page-level sparse attention to reduce both computational complexity and communication overhead. The synergy of these techniques yields exceptional efficiency. Our empirical results show that for every additional 10K tokens of context, the end-to-end training memory overhead increases by a mere 10MB for Qwen2.5-7B. This allows training Qwen2.5-7B with a 4M-token context on a single H200 GPU, a feat that would otherwise require a large cluster using context parallelism. This work represents a substantial advance in resource efficiency for long-context LLM training. The source code is available at https://github.com/wenhaoli-xmu/OOMB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Large Language Models (LLMs) on long contexts is severely constrained by prohibitive GPU memory overhead, not training time. The primary culprits are the activations, whose memory footprints scale linearly with sequence length. We introduce OOMB, a highly memory-efficient training system that directly confronts this barrier. Our approach employs a chunk-recurrent training framework with on-the-fly activation recomputation, which maintains a constant activation memory footprint (O(1)) and shifts the primary bottleneck to the growing KV cache. To manage the KV cache, OOMB integrates a suite of synergistic optimizations: a paged memory manager for both the KV cache and its gradients to eliminate fragmentation, asynchronous CPU offloading to hide data transfer latency, and page-level sparse attention to reduce both computational complexity and communication overhead. The synergy of these techniques yields exceptional efficiency. Our empirical results show that for every additional 10K tokens of context, the end-to-end training memory overhead increases by a mere 10MB for Qwen2.5-7B. This allows training Qwen2.5-7B with a 4M-token context on a single H200 GPU, a feat that would otherwise require a large cluster using context parallelism. This work represents a substantial advance in resource efficiency for long-context LLM training. The source code is available at https://github.com/wenhaoli-xmu/OOMB."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-02T13:52:40Z",
                "published_parsed": [
                    2026,
                    2,
                    2,
                    13,
                    52,
                    40,
                    0,
                    33,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Daohai Yu"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    },
                    {
                        "name": "Yifan Wu"
                    },
                    {
                        "name": "Jiaxin Liu"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Zimu Liao"
                    }
                ],
                "author_detail": {
                    "name": "Zimu Liao"
                },
                "author": "Zimu Liao"
            },
            {
                "id": "http://arxiv.org/abs/2505.11695v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.11695v3",
                "title": "Qronos: Correcting the Past by Shaping the Future... in Post-Training Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qronos: Correcting the Past by Shaping the Future... in Post-Training Quantization"
                },
                "updated": "2026-02-17T05:04:13Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    5,
                    4,
                    13,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.11695v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.11695v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce Qronos -- a new state-of-the-art post-training quantization algorithm that sequentially rounds and updates neural network weights. Qronos not only explicitly corrects errors due to both weight and activation quantization, but also errors resulting from quantizing previous layers. Our iterative algorithm is based on an interpretable and disciplined optimization framework that subsumes and surpasses existing data-driven approaches. At each step, Qronos alternates between error correction and diffusion via optimal update rules. Importantly, we prove that Qronos admits an efficient implementation that uses the Cholesky decomposition for solving least-squares problems. We also demonstrate that Qronos is compatible with existing transformation techniques such as Hadamard-based incoherence processing and weight-activation scaling equalization, among others. We evaluate Qronos using recent autoregressive language generation models in the Llama3 family; Qronos consistently outperforms previous state-of-the-art adaptive rounding methods when quantizing the weights, activations, and/or KV caches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Qronos -- a new state-of-the-art post-training quantization algorithm that sequentially rounds and updates neural network weights. Qronos not only explicitly corrects errors due to both weight and activation quantization, but also errors resulting from quantizing previous layers. Our iterative algorithm is based on an interpretable and disciplined optimization framework that subsumes and surpasses existing data-driven approaches. At each step, Qronos alternates between error correction and diffusion via optimal update rules. Importantly, we prove that Qronos admits an efficient implementation that uses the Cholesky decomposition for solving least-squares problems. We also demonstrate that Qronos is compatible with existing transformation techniques such as Hadamard-based incoherence processing and weight-activation scaling equalization, among others. We evaluate Qronos using recent autoregressive language generation models in the Llama3 family; Qronos consistently outperforms previous state-of-the-art adaptive rounding methods when quantizing the weights, activations, and/or KV caches."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-16T21:04:25Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    21,
                    4,
                    25,
                    4,
                    136,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Shihao Zhang"
                    },
                    {
                        "name": "Haoyu Zhang"
                    },
                    {
                        "name": "Ian Colbert"
                    },
                    {
                        "name": "Rayan Saab"
                    }
                ],
                "author_detail": {
                    "name": "Rayan Saab"
                },
                "author": "Rayan Saab"
            },
            {
                "id": "http://arxiv.org/abs/2602.15318v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.15318v1",
                "title": "Sparrow: Text-Anchored Window Attention with Visual-Semantic Glimpsing for Speculative Decoding in Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparrow: Text-Anchored Window Attention with Visual-Semantic Glimpsing for Speculative Decoding in Video LLMs"
                },
                "updated": "2026-02-17T02:51:36Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    2,
                    51,
                    36,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.15318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.15318v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Although speculative decoding is widely used to accelerate Vision-Language Models (VLMs) inference, it faces severe performance collapse when applied to Video Large Language Models (Vid-LLMs). The draft model typically falls into the trap of attention dilution and negative visual gain due to key-value cache explosion and context window mismatches. We observe a visual semantic internalization phenomenon in Vid-LLMs, indicating that critical visual semantics are implicitly encoded into text hidden states during deep-layer interactions, which renders raw visual inputs structurally redundant during deep inference. To address this, we propose the Sparrow framework, which first utilizes visually-aware text-anchored window attention via hidden state reuse to fully offload visual computation to the target model, and leverages intermediate-layer visual state bridging to train the draft model with semantic-rich intermediate states, thereby filtering out low-level visual noise. Additionally, a multi-token prediction strategy is introduced to bridge the training-inference distribution shift. Experiments show that Sparrow achieves an average speedup of 2.82x even with 25k visual tokens, effectively resolving the performance degradation in long sequences and offering a practical solution for real-time long video tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although speculative decoding is widely used to accelerate Vision-Language Models (VLMs) inference, it faces severe performance collapse when applied to Video Large Language Models (Vid-LLMs). The draft model typically falls into the trap of attention dilution and negative visual gain due to key-value cache explosion and context window mismatches. We observe a visual semantic internalization phenomenon in Vid-LLMs, indicating that critical visual semantics are implicitly encoded into text hidden states during deep-layer interactions, which renders raw visual inputs structurally redundant during deep inference. To address this, we propose the Sparrow framework, which first utilizes visually-aware text-anchored window attention via hidden state reuse to fully offload visual computation to the target model, and leverages intermediate-layer visual state bridging to train the draft model with semantic-rich intermediate states, thereby filtering out low-level visual noise. Additionally, a multi-token prediction strategy is introduced to bridge the training-inference distribution shift. Experiments show that Sparrow achieves an average speedup of 2.82x even with 25k visual tokens, effectively resolving the performance degradation in long sequences and offering a practical solution for real-time long video tasks."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-17T02:51:36Z",
                "published_parsed": [
                    2026,
                    2,
                    17,
                    2,
                    51,
                    36,
                    1,
                    48,
                    0
                ],
                "arxiv_comment": "15 pages , 6 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Libo Zhang"
                    },
                    {
                        "name": "Zhaoning Zhang"
                    },
                    {
                        "name": "Wangyang Hong"
                    },
                    {
                        "name": "Peng Qiao"
                    },
                    {
                        "name": "Dongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Dongsheng Li"
                },
                "author": "Dongsheng Li"
            },
            {
                "id": "http://arxiv.org/abs/2602.14492v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.14492v2",
                "title": "Query as Anchor: Scenario-Adaptive User Representation via Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query as Anchor: Scenario-Adaptive User Representation via Large Language Model"
                },
                "updated": "2026-02-17T02:44:08Z",
                "updated_parsed": [
                    2026,
                    2,
                    17,
                    2,
                    44,
                    8,
                    1,
                    48,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.14492v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.14492v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Industrial-scale user representation learning requires balancing robust universality with acute task-sensitivity. However, existing paradigms primarily yield static, task-agnostic embeddings that struggle to reconcile the divergent requirements of downstream scenarios within unified vector spaces. Furthermore, heterogeneous multi-source data introduces inherent noise and modality conflicts, degrading representation. We propose Query-as-Anchor, a framework shifting user modeling from static encoding to dynamic, query-aware synthesis. To empower Large Language Models (LLMs) with deep user understanding, we first construct UserU, an industrial-scale pre-training dataset that aligns multi-modal behavioral sequences with user understanding semantics, and our Q-Anchor Embedding architecture integrates hierarchical coarse-to-fine encoders into dual-tower LLMs via joint contrastive-autoregressive optimization for query-aware user representation. To bridge the gap between general pre-training and specialized business logic, we further introduce Cluster-based Soft Prompt Tuning to enforce discriminative latent structures, effectively aligning model attention with scenario-specific modalities. For deployment, anchoring queries at sequence termini enables KV-cache-accelerated inference with negligible incremental latency. Evaluations on 10 Alipay industrial benchmarks show consistent SOTA performance, strong scalability, and efficient deployment. Large-scale online A/B testing in Alipay's production system across two real-world scenarios further validates its practical effectiveness. Our code is prepared for public release and will be available at: https://github.com/JhCircle/Q-Anchor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial-scale user representation learning requires balancing robust universality with acute task-sensitivity. However, existing paradigms primarily yield static, task-agnostic embeddings that struggle to reconcile the divergent requirements of downstream scenarios within unified vector spaces. Furthermore, heterogeneous multi-source data introduces inherent noise and modality conflicts, degrading representation. We propose Query-as-Anchor, a framework shifting user modeling from static encoding to dynamic, query-aware synthesis. To empower Large Language Models (LLMs) with deep user understanding, we first construct UserU, an industrial-scale pre-training dataset that aligns multi-modal behavioral sequences with user understanding semantics, and our Q-Anchor Embedding architecture integrates hierarchical coarse-to-fine encoders into dual-tower LLMs via joint contrastive-autoregressive optimization for query-aware user representation. To bridge the gap between general pre-training and specialized business logic, we further introduce Cluster-based Soft Prompt Tuning to enforce discriminative latent structures, effectively aligning model attention with scenario-specific modalities. For deployment, anchoring queries at sequence termini enables KV-cache-accelerated inference with negligible incremental latency. Evaluations on 10 Alipay industrial benchmarks show consistent SOTA performance, strong scalability, and efficient deployment. Large-scale online A/B testing in Alipay's production system across two real-world scenarios further validates its practical effectiveness. Our code is prepared for public release and will be available at: https://github.com/JhCircle/Q-Anchor."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-16T06:09:31Z",
                "published_parsed": [
                    2026,
                    2,
                    16,
                    6,
                    9,
                    31,
                    0,
                    47,
                    0
                ],
                "arxiv_comment": "15 pages, 12 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jiahao Yuan"
                    },
                    {
                        "name": "Yike Xu"
                    },
                    {
                        "name": "Jinyong Wen"
                    },
                    {
                        "name": "Baokun Wang"
                    },
                    {
                        "name": "Ziyi Gao"
                    },
                    {
                        "name": "Xiaotong Lin"
                    },
                    {
                        "name": "Yun Liu"
                    },
                    {
                        "name": "Xing Fu"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Yongchao Liu"
                    },
                    {
                        "name": "Weiqiang Wang"
                    },
                    {
                        "name": "Zhongle Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zhongle Xie"
                },
                "author": "Zhongle Xie"
            },
            {
                "id": "http://arxiv.org/abs/2602.01872v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.01872v2",
                "title": "Grappa: Gradient-Only Communication for Scalable Graph Neural Network Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grappa: Gradient-Only Communication for Scalable Graph Neural Network Training"
                },
                "updated": "2026-02-16T21:24:43Z",
                "updated_parsed": [
                    2026,
                    2,
                    16,
                    21,
                    24,
                    43,
                    0,
                    47,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.01872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.01872v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Cross-partition edges dominate the cost of distributed GNN training: fetching remote features and activations per iteration overwhelms the network as graphs deepen and partition counts grow. Grappa is a distributed GNN training framework that enforces gradient-only communication: during each iteration, partitions train in isolation and exchange only gradients for the global update. To recover accuracy lost to isolation, Grappa (i) periodically repartitions to expose new neighborhoods and (ii) applies a lightweight coverage-corrected gradient aggregation inspired by importance sampling. We present an asymptotically unbiased estimator for gradient correction, which we use to develop a minimum-distance batch-level variant that is compatible with common deep-learning packages. We also introduce a shrinkage version that improves stability in practice. Empirical results on real and synthetic graphs show that Grappa trains GNNs 4x faster on average (up to 13x) than state-of-the-art systems, achieves better accuracy especially for deeper models, and sustains training at the trillion-edge scale on commodity hardware. Grappa is model-agnostic, supports full-graph and mini-batch training, and does not rely on high-bandwidth interconnects or caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-partition edges dominate the cost of distributed GNN training: fetching remote features and activations per iteration overwhelms the network as graphs deepen and partition counts grow. Grappa is a distributed GNN training framework that enforces gradient-only communication: during each iteration, partitions train in isolation and exchange only gradients for the global update. To recover accuracy lost to isolation, Grappa (i) periodically repartitions to expose new neighborhoods and (ii) applies a lightweight coverage-corrected gradient aggregation inspired by importance sampling. We present an asymptotically unbiased estimator for gradient correction, which we use to develop a minimum-distance batch-level variant that is compatible with common deep-learning packages. We also introduce a shrinkage version that improves stability in practice. Empirical results on real and synthetic graphs show that Grappa trains GNNs 4x faster on average (up to 13x) than state-of-the-art systems, achieves better accuracy especially for deeper models, and sustains training at the trillion-edge scale on commodity hardware. Grappa is model-agnostic, supports full-graph and mini-batch training, and does not rely on high-bandwidth interconnects or caching."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-02T09:44:12Z",
                "published_parsed": [
                    2026,
                    2,
                    2,
                    9,
                    44,
                    12,
                    0,
                    33,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Chongyang Xu"
                    },
                    {
                        "name": "Christoph Siebenbrunner"
                    },
                    {
                        "name": "Laurent Bindschaedler"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Bindschaedler"
                },
                "author": "Laurent Bindschaedler"
            },
            {
                "id": "http://arxiv.org/abs/2602.14683v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.14683v1",
                "title": "Joint Majorization-Minimization for Nonnegative CP and Tucker Decompositions under $$-Divergences: Unfolding-Free Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Majorization-Minimization for Nonnegative CP and Tucker Decompositions under $$-Divergences: Unfolding-Free Updates"
                },
                "updated": "2026-02-16T12:16:01Z",
                "updated_parsed": [
                    2026,
                    2,
                    16,
                    12,
                    16,
                    1,
                    0,
                    47,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.14683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.14683v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We study majorization-minimization methods for nonnegative tensor decompositions under the $$-divergence family, focusing on nonnegative CP and Tucker models. Our aim is to avoid explicit mode unfoldings and large auxiliary matrices by deriving separable surrogates whose multiplicative updates can be implemented using only tensor contractions (einsum-style operations). We present both classical block-MM updates in contraction-only form and a joint majorization strategy, inspired by joint MM for matrix $$-NMF, that reuses cached reference quantities across inexpensive inner updates. We prove tightness of the proposed majorizers, establish monotonic decrease of the objective, and show convergence of the sequence of objective values; we also discuss how BSUM theory applies to the block-MM scheme for analyzing limit points. Finally, experiments on synthetic tensors and the Uber spatiotemporal count tensor demonstrate substantial speedups over unfolding-based baselines and a recent einsum-factorization framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study majorization-minimization methods for nonnegative tensor decompositions under the $$-divergence family, focusing on nonnegative CP and Tucker models. Our aim is to avoid explicit mode unfoldings and large auxiliary matrices by deriving separable surrogates whose multiplicative updates can be implemented using only tensor contractions (einsum-style operations). We present both classical block-MM updates in contraction-only form and a joint majorization strategy, inspired by joint MM for matrix $$-NMF, that reuses cached reference quantities across inexpensive inner updates. We prove tightness of the proposed majorizers, establish monotonic decrease of the objective, and show convergence of the sequence of objective values; we also discuss how BSUM theory applies to the block-MM scheme for analyzing limit points. Finally, experiments on synthetic tensors and the Uber spatiotemporal count tensor demonstrate substantial speedups over unfolding-based baselines and a recent einsum-factorization framework."
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-16T12:16:01Z",
                "published_parsed": [
                    2026,
                    2,
                    16,
                    12,
                    16,
                    1,
                    0,
                    47,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "math.OC"
                },
                "authors": [
                    {
                        "name": "Valentin Leplat"
                    }
                ],
                "author_detail": {
                    "name": "Valentin Leplat"
                },
                "author": "Valentin Leplat"
            },
            {
                "id": "http://arxiv.org/abs/2601.01112v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01112v2",
                "title": "EmoLoom-2B: Fast Base-Model Screening for Emotion Classification and VAD with Lexicon-Weak Supervision and KV-Off Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmoLoom-2B: Fast Base-Model Screening for Emotion Classification and VAD with Lexicon-Weak Supervision and KV-Off Evaluation"
                },
                "updated": "2026-02-16T10:23:22Z",
                "updated_parsed": [
                    2026,
                    2,
                    16,
                    10,
                    23,
                    22,
                    0,
                    47,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01112v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01112v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce EmoLoom-2B, a lightweight and reproducible pipeline that turns small language models under 2B parameters into fast screening candidates for joint emotion classification and Valence-Arousal-Dominance prediction. To ensure protocol-faithful and fair evaluation, we unify data loading, training, and inference under a single JSON input-output contract and remove avoidable variance by adopting KV-off decoding as the default setting. We incorporate two orthogonal semantic regularizers: a VAD-preserving constraint that aligns generated text with target VAD triples, and a lightweight external appraisal classifier that provides training-time guidance on goal attainment, controllability, certainty, and fairness without injecting long rationales. To improve polarity sensitivity, we introduce Valence Flip augmentation based on mirrored emotional pairs. During supervised fine-tuning, we apply A/B mixture sampling with entropy-aware temperature scheduling to balance coverage and convergence. Using Qwen-1.8B-Chat as the base model, EmoLoom-2B achieves strong performance on GoEmotions and EmpatheticDialogues, and demonstrates robust cross-corpus generalization on DailyDialog. The proposed recipe is budget-aware, auditable, and re-entrant, serving as a dependable screening pass before heavier training or multimodal fusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce EmoLoom-2B, a lightweight and reproducible pipeline that turns small language models under 2B parameters into fast screening candidates for joint emotion classification and Valence-Arousal-Dominance prediction. To ensure protocol-faithful and fair evaluation, we unify data loading, training, and inference under a single JSON input-output contract and remove avoidable variance by adopting KV-off decoding as the default setting. We incorporate two orthogonal semantic regularizers: a VAD-preserving constraint that aligns generated text with target VAD triples, and a lightweight external appraisal classifier that provides training-time guidance on goal attainment, controllability, certainty, and fairness without injecting long rationales. To improve polarity sensitivity, we introduce Valence Flip augmentation based on mirrored emotional pairs. During supervised fine-tuning, we apply A/B mixture sampling with entropy-aware temperature scheduling to balance coverage and convergence. Using Qwen-1.8B-Chat as the base model, EmoLoom-2B achieves strong performance on GoEmotions and EmpatheticDialogues, and demonstrates robust cross-corpus generalization on DailyDialog. The proposed recipe is budget-aware, auditable, and re-entrant, serving as a dependable screening pass before heavier training or multimodal fusion."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-03T08:25:58Z",
                "published_parsed": [
                    2026,
                    1,
                    3,
                    8,
                    25,
                    58,
                    5,
                    3,
                    0
                ],
                "arxiv_comment": "This paper presents an initial and self-contained study of a lightweight screening pipeline for emotion-aware language modeling, intended as a reproducible baseline and system-level design reference. This latest version corrects and updates certain personal information",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zilin Li"
                    },
                    {
                        "name": "Weiwei Xu"
                    },
                    {
                        "name": "Xuanbo Lu"
                    },
                    {
                        "name": "Zheda Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheda Liu"
                },
                "author": "Zheda Liu"
            },
            {
                "id": "http://arxiv.org/abs/2501.16909v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2501.16909v3",
                "title": "Understanding GPU Resource Interference One Level Deeper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding GPU Resource Interference One Level Deeper"
                },
                "updated": "2026-02-16T09:26:53Z",
                "updated_parsed": [
                    2026,
                    2,
                    16,
                    9,
                    26,
                    53,
                    0,
                    47,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2501.16909v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2501.16909v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "GPUs are vastly underutilized, even when running resource-intensive AI applications, as GPU kernels within each job have diverse resource profiles that may saturate some parts of a device while often leaving other parts idle. Colocating applications is known to improve GPU utilization, but is not common practice as it becomes difficult to provide predictable performance due to workload interference. Providing predictable performance guarantees requires a deep understanding of how applications contend for shared GPU resources such as block schedulers, compute units, L1/L2 caches, and memory bandwidth. We study the key types of GPU resource interference and develop a methodology to quantify the sensitivity of a workload to each type. We discuss how this methodology can serve as the foundation for GPU schedulers that enforce strict performance guarantees and how application developers can design GPU kernels with colocation in mind to improve efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPUs are vastly underutilized, even when running resource-intensive AI applications, as GPU kernels within each job have diverse resource profiles that may saturate some parts of a device while often leaving other parts idle. Colocating applications is known to improve GPU utilization, but is not common practice as it becomes difficult to provide predictable performance due to workload interference. Providing predictable performance guarantees requires a deep understanding of how applications contend for shared GPU resources such as block schedulers, compute units, L1/L2 caches, and memory bandwidth. We study the key types of GPU resource interference and develop a methodology to quantify the sensitivity of a workload to each type. We discuss how this methodology can serve as the foundation for GPU schedulers that enforce strict performance guarantees and how application developers can design GPU kernels with colocation in mind to improve efficiency."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-01-28T12:57:53Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Paul Elvinger"
                    },
                    {
                        "name": "Foteini Strati"
                    },
                    {
                        "name": "Natalie Enright Jerger"
                    },
                    {
                        "name": "Ana Klimovic"
                    }
                ],
                "author_detail": {
                    "name": "Ana Klimovic"
                },
                "author": "Ana Klimovic"
            },
            {
                "id": "http://arxiv.org/abs/2509.23094v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.23094v2",
                "title": "d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching"
                },
                "updated": "2026-02-16T06:46:00Z",
                "updated_parsed": [
                    2026,
                    2,
                    16,
                    6,
                    46,
                    0,
                    0,
                    47,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.23094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.23094v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion-based large language models (dLLMs), despite their promising performance, still suffer from inferior inference efficiency. This is because dLLMs rely on bidirectional attention and cannot directly benefit from the standard key-value (KV) cache as autoregressive models (ARMs) do. To tackle this issue, we introduce \\textit{Dual aDaptive Cache} (d$^2$Cache), which is a training-free approximate KV cache framework for accelerating dLLM inference. d$^2$Cache features a two-stage fine-grained selection strategy to identify tokens and adaptively update their KV states at each decoding step, while caching the KV states of the remaining tokens for reuse. Furthermore, d$^2$Cache naturally offers a more reliable decoding alternative, which can enable quasi left-to-right generation and mitigate premature overconfidence in tokens at the end of the sequence. Extensive experimental results on two representative dLLMs (\\ie, LLaDA and Dream) demonstrate that d$^2$Cache not only achieves substantial inference speedups, but also yields consistent improvements in generation quality. The code is available at https://github.com/Kamichanw/d2Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs), despite their promising performance, still suffer from inferior inference efficiency. This is because dLLMs rely on bidirectional attention and cannot directly benefit from the standard key-value (KV) cache as autoregressive models (ARMs) do. To tackle this issue, we introduce \\textit{Dual aDaptive Cache} (d$^2$Cache), which is a training-free approximate KV cache framework for accelerating dLLM inference. d$^2$Cache features a two-stage fine-grained selection strategy to identify tokens and adaptively update their KV states at each decoding step, while caching the KV states of the remaining tokens for reuse. Furthermore, d$^2$Cache naturally offers a more reliable decoding alternative, which can enable quasi left-to-right generation and mitigate premature overconfidence in tokens at the end of the sequence. Extensive experimental results on two representative dLLMs (\\ie, LLaDA and Dream) demonstrate that d$^2$Cache not only achieves substantial inference speedups, but also yields consistent improvements in generation quality. The code is available at https://github.com/Kamichanw/d2Cache."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-27T04:07:23Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    4,
                    7,
                    23,
                    5,
                    270,
                    0
                ],
                "arxiv_comment": "Accepted by ICLR 2026, 21 pages, 9 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yuchu Jiang"
                    },
                    {
                        "name": "Yue Cai"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Jiale Fu"
                    },
                    {
                        "name": "Jiarui Wang"
                    },
                    {
                        "name": "Chonghan Liu"
                    },
                    {
                        "name": "Xu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xu Yang"
                },
                "author": "Xu Yang"
            },
            {
                "id": "http://arxiv.org/abs/2502.01068v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.01068v6",
                "title": "FastKV: Decoupling of Context Reduction and KV Cache Compression for Prefill-Decoding Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastKV: Decoupling of Context Reduction and KV Cache Compression for Prefill-Decoding Acceleration"
                },
                "updated": "2026-02-16T06:37:44Z",
                "updated_parsed": [
                    2026,
                    2,
                    16,
                    6,
                    37,
                    44,
                    0,
                    47,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.01068v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.01068v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While large language models (LLMs) excel at handling long-context sequences, they require substantial prefill computation and key-value (KV) cache, which can heavily burden computational efficiency and memory usage in both prefill and decoding stages. Recent works that compress KV caches with prefill acceleration reduce this cost but inadvertently tie the prefill compute reduction to the decoding KV budget. This coupling arises from overlooking the layer-dependent variation of critical context, often leading to accuracy degradation. To address this issue, we introduce FastKV, a KV cache compression framework designed to reduce latency in both prefill and decoding by leveraging the stabilization of token importance in later layers. FastKV performs full-context computation until a Token-Selective Propagation (TSP) layer, which forwards only the most informative tokens to subsequent layers. From these propagated tokens, FastKV independently selects salient KV entries for caching, thereby decoupling KV budget from the prefill compute reduction based on the TSP decision. This independent control of the TSP rate and KV retention rate enables flexible optimization of efficiency and accuracy. Experimental results show that FastKV achieves speedups of up to 1.82$\\times$ in prefill and 2.87$\\times$ in decoding compared to the full-context baseline, while matching the accuracy of the baselines that only accelerate the decoding stage. Our code is available at https://github.com/dongwonjo/FastKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) excel at handling long-context sequences, they require substantial prefill computation and key-value (KV) cache, which can heavily burden computational efficiency and memory usage in both prefill and decoding stages. Recent works that compress KV caches with prefill acceleration reduce this cost but inadvertently tie the prefill compute reduction to the decoding KV budget. This coupling arises from overlooking the layer-dependent variation of critical context, often leading to accuracy degradation. To address this issue, we introduce FastKV, a KV cache compression framework designed to reduce latency in both prefill and decoding by leveraging the stabilization of token importance in later layers. FastKV performs full-context computation until a Token-Selective Propagation (TSP) layer, which forwards only the most informative tokens to subsequent layers. From these propagated tokens, FastKV independently selects salient KV entries for caching, thereby decoupling KV budget from the prefill compute reduction based on the TSP decision. This independent control of the TSP rate and KV retention rate enables flexible optimization of efficiency and accuracy. Experimental results show that FastKV achieves speedups of up to 1.82$\\times$ in prefill and 2.87$\\times$ in decoding compared to the full-context baseline, while matching the accuracy of the baselines that only accelerate the decoding stage. Our code is available at https://github.com/dongwonjo/FastKV."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-03T05:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim"
            },
            {
                "id": "http://arxiv.org/abs/2602.14381v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.14381v1",
                "title": "Adapting VACE for Real-Time Autoregressive Video Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting VACE for Real-Time Autoregressive Video Diffusion"
                },
                "updated": "2026-02-16T01:13:33Z",
                "updated_parsed": [
                    2026,
                    2,
                    16,
                    1,
                    13,
                    33,
                    0,
                    47,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.14381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.14381v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We describe an adaptation of VACE (Video All-in-one Creation and Editing) for real-time autoregressive video generation. VACE provides unified video control (reference guidance, structural conditioning, inpainting, and temporal extension) but assumes bidirectional attention over full sequences, making it incompatible with streaming pipelines that require fixed chunk sizes and causal attention. The key modification moves reference frames from the diffusion latent space into a parallel conditioning pathway, preserving the fixed chunk sizes and KV caching that autoregressive models require. This adaptation reuses existing pretrained VACE weights without additional training. Across 1.3B and 14B model scales, VACE adds 20-30% latency overhead for structural control and inpainting, with negligible VRAM cost relative to the base model. Reference-to-video fidelity is severely degraded compared to batch VACE due to causal attention constraints. A reference implementation is available at https://github.com/daydreamlive/scope.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe an adaptation of VACE (Video All-in-one Creation and Editing) for real-time autoregressive video generation. VACE provides unified video control (reference guidance, structural conditioning, inpainting, and temporal extension) but assumes bidirectional attention over full sequences, making it incompatible with streaming pipelines that require fixed chunk sizes and causal attention. The key modification moves reference frames from the diffusion latent space into a parallel conditioning pathway, preserving the fixed chunk sizes and KV caching that autoregressive models require. This adaptation reuses existing pretrained VACE weights without additional training. Across 1.3B and 14B model scales, VACE adds 20-30% latency overhead for structural control and inpainting, with negligible VRAM cost relative to the base model. Reference-to-video fidelity is severely degraded compared to batch VACE due to causal attention constraints. A reference implementation is available at https://github.com/daydreamlive/scope."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-16T01:13:33Z",
                "published_parsed": [
                    2026,
                    2,
                    16,
                    1,
                    13,
                    33,
                    0,
                    47,
                    0
                ],
                "arxiv_comment": "10 pages, 4 figures, 7 tables",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Ryan Fosdick"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Fosdick"
                },
                "arxiv_affiliation": "Daydream",
                "author": "Ryan Fosdick"
            },
            {
                "id": "http://arxiv.org/abs/2510.22876v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.22876v3",
                "title": "Batch Speculative Decoding Done Right",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch Speculative Decoding Done Right"
                },
                "updated": "2026-02-15T22:53:25Z",
                "updated_parsed": [
                    2026,
                    2,
                    15,
                    22,
                    53,
                    25,
                    6,
                    46,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.22876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.22876v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Speculative decoding must produce outputs distribution identical to standard autoregressive generation-this output equivalence is not an optimization target but the defining criterion of valid speculative decoding. We demonstrate that all existing batch speculative decoding implementations violate this fundamental requirement, producing corrupted outputs ranging from repetitive tokens to gibberish. These failures stem from the ragged tensor problem: sequences in the same batch accept different numbers of draft tokens, desynchronizing position IDs, attention masks, and KV-cache state. We present the first authentic batch speculative decoding framework. We (1) formalize the synchronization invariants that valid batch speculative decoding must satisfy, (2) present EQSPEC, the first algorithm that guarantees output equivalence, and analyze its cost structure to show that alignment overhead grows superlinearly and consumes up to 40\\% of computation, and (3) introduce EXSPEC, which reduces this overhead through cross-batch scheduling that dynamically groups same-length sequences. On SpecBench across Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B pairs, our methods achieve up to 3x throughput improvement at batch size 8 while maintaining algorithmic correctness. Our methods achieve 95\\% decoding-equivalence, with residual divergence attributable to floating-point non-determinism in GPU inference, not the synchronization failures that cause near-zero equivalence of prior methods. Our code is available at https://github.com/eBay/spec_dec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding must produce outputs distribution identical to standard autoregressive generation-this output equivalence is not an optimization target but the defining criterion of valid speculative decoding. We demonstrate that all existing batch speculative decoding implementations violate this fundamental requirement, producing corrupted outputs ranging from repetitive tokens to gibberish. These failures stem from the ragged tensor problem: sequences in the same batch accept different numbers of draft tokens, desynchronizing position IDs, attention masks, and KV-cache state. We present the first authentic batch speculative decoding framework. We (1) formalize the synchronization invariants that valid batch speculative decoding must satisfy, (2) present EQSPEC, the first algorithm that guarantees output equivalence, and analyze its cost structure to show that alignment overhead grows superlinearly and consumes up to 40\\% of computation, and (3) introduce EXSPEC, which reduces this overhead through cross-batch scheduling that dynamically groups same-length sequences. On SpecBench across Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B pairs, our methods achieve up to 3x throughput improvement at batch size 8 while maintaining algorithmic correctness. Our methods achieve 95\\% decoding-equivalence, with residual divergence attributable to floating-point non-determinism in GPU inference, not the synchronization failures that cause near-zero equivalence of prior methods. Our code is available at https://github.com/eBay/spec_dec."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-26T23:59:23Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    23,
                    59,
                    23,
                    6,
                    299,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ranran Haoran Zhang"
                    },
                    {
                        "name": "Soumik Dey"
                    },
                    {
                        "name": "Ashirbad Mishra"
                    },
                    {
                        "name": "Hansi Wu"
                    },
                    {
                        "name": "Binbin Li"
                    },
                    {
                        "name": "Rui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhang"
                },
                "author": "Rui Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2602.14262v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.14262v1",
                "title": "ABI: A tightly integrated, unified, sparsity-aware, reconfigurable, compute near-register file/cache GPU architecture with light-weight softmax for deep learning, linear algebra, and Ising compute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ABI: A tightly integrated, unified, sparsity-aware, reconfigurable, compute near-register file/cache GPU architecture with light-weight softmax for deep learning, linear algebra, and Ising compute"
                },
                "updated": "2026-02-15T18:19:06Z",
                "updated_parsed": [
                    2026,
                    2,
                    15,
                    18,
                    19,
                    6,
                    6,
                    46,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.14262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.14262v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present a tightly integrated and unified near-memory GPU architecture that delivers 6 to 16 times speedup and 6 to 13 times energy savings across Convolutional Neural Networks, Graph Convolutional Networks, Linear Programming, Large Language Models, and Ising workloads compared to MIAOW GPU. The design includes a custom sparsity-aware near-memory circuit providing about 1.5 times energy savings, and a lightweight softmax circuit providing about 1.6 times energy savings. The architecture supports reconfigurable compute up to INT16 with dynamic resolution updates and scales efficiently across problem sizes. ABI-enabled MI300 and Blackwell systems achieve about 4.5 times speedup over baseline MI300 and Blackwell.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a tightly integrated and unified near-memory GPU architecture that delivers 6 to 16 times speedup and 6 to 13 times energy savings across Convolutional Neural Networks, Graph Convolutional Networks, Linear Programming, Large Language Models, and Ising workloads compared to MIAOW GPU. The design includes a custom sparsity-aware near-memory circuit providing about 1.5 times energy savings, and a lightweight softmax circuit providing about 1.6 times energy savings. The architecture supports reconfigurable compute up to INT16 with dynamic resolution updates and scales efficiently across problem sizes. ABI-enabled MI300 and Blackwell systems achieve about 4.5 times speedup over baseline MI300 and Blackwell."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-15T18:19:06Z",
                "published_parsed": [
                    2026,
                    2,
                    15,
                    18,
                    19,
                    6,
                    6,
                    46,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Siddhartha Raman Sundara Raman"
                    },
                    {
                        "name": "Jaydeep P. Kulkarni"
                    }
                ],
                "author_detail": {
                    "name": "Jaydeep P. Kulkarni"
                },
                "author": "Jaydeep P. Kulkarni"
            },
            {
                "id": "http://arxiv.org/abs/2602.14236v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.14236v1",
                "title": "Dual-Signal Adaptive KV-Cache Optimization for Long-Form Video Understanding in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual-Signal Adaptive KV-Cache Optimization for Long-Form Video Understanding in Vision-Language Models"
                },
                "updated": "2026-02-15T17:06:02Z",
                "updated_parsed": [
                    2026,
                    2,
                    15,
                    17,
                    6,
                    2,
                    6,
                    46,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.14236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.14236v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-Language Models (VLMs) face a critical memory bottleneck when processing long-form video content due to the linear growth of the Key-Value (KV) cache with sequence length. Existing solutions predominantly employ reactive eviction strategies that compute full attention matrices before discarding tokens, resulting in substantial computational waste. We propose Sali-Cache, a novel a priori optimization framework that implements dual-signal adaptive caching through proactive memory management. By integrating a temporal filter based on optical flow analysis for detecting inter-frame redundancy and a spatial filter leveraging saliency detection for identifying visually significant regions, Sali-Cache intelligently manages memory allocation before entering computationally expensive attention operations. Experimental evaluation on the LLaVA 1.6 architecture demonstrates that our method achieves a 2.20x compression ratio in effective memory usage while maintaining 100% accuracy across BLEU, ROUGE-L, and Exact Match metrics. Furthermore, under identical memory budget constraints, Sali-Cache preserves context-rich features over extended temporal durations without degrading model performance, enabling efficient processing of long-form video content on consumer-grade hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) face a critical memory bottleneck when processing long-form video content due to the linear growth of the Key-Value (KV) cache with sequence length. Existing solutions predominantly employ reactive eviction strategies that compute full attention matrices before discarding tokens, resulting in substantial computational waste. We propose Sali-Cache, a novel a priori optimization framework that implements dual-signal adaptive caching through proactive memory management. By integrating a temporal filter based on optical flow analysis for detecting inter-frame redundancy and a spatial filter leveraging saliency detection for identifying visually significant regions, Sali-Cache intelligently manages memory allocation before entering computationally expensive attention operations. Experimental evaluation on the LLaVA 1.6 architecture demonstrates that our method achieves a 2.20x compression ratio in effective memory usage while maintaining 100% accuracy across BLEU, ROUGE-L, and Exact Match metrics. Furthermore, under identical memory budget constraints, Sali-Cache preserves context-rich features over extended temporal durations without degrading model performance, enabling efficient processing of long-form video content on consumer-grade hardware."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-15T17:06:02Z",
                "published_parsed": [
                    2026,
                    2,
                    15,
                    17,
                    6,
                    2,
                    6,
                    46,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Vishnu Sai"
                    },
                    {
                        "name": "Dheeraj Sai"
                    },
                    {
                        "name": "Srinath B"
                    },
                    {
                        "name": "Girish Varma"
                    },
                    {
                        "name": "Priyesh Shukla"
                    }
                ],
                "author_detail": {
                    "name": "Priyesh Shukla"
                },
                "author": "Priyesh Shukla"
            },
            {
                "id": "http://arxiv.org/abs/2602.14209v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.14209v1",
                "title": "MAGE: All-[MASK] Block Already Knows Where to Look in Diffusion LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGE: All-[MASK] Block Already Knows Where to Look in Diffusion LLM"
                },
                "updated": "2026-02-15T16:07:51Z",
                "updated_parsed": [
                    2026,
                    2,
                    15,
                    16,
                    7,
                    51,
                    6,
                    46,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.14209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.14209v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Block diffusion LLMs are emerging as a promising next paradigm for language generation, but their use of KV caching makes memory access a dominant bottleneck in long-context settings. While dynamic sparse attention has been actively explored, existing methods designed for autoregressive LLMs rely on approximate importance estimation and perform poorly when adapted to block diffusion. This work identifies a key opportunity unique to block diffusion: attention at the first All-[MASK] denoising step reliably predicts important KV entries and budget requirements, enabling MAGE to perform a single exact attention pass per block and reuse it for training-free sparse denoising. Across long-context benchmarks including LongBench and Needle-in-a-Haystack, MAGE achieves near-lossless accuracy with a fraction of the KV budget while delivering up to 3-4x end-to-end speedup, consistently outperforming AR-oriented sparse attention baselines. A lightweight fine-tuning strategy further strengthens [MASK]-guided patterns with minimal cost, requiring only a few hours of training on a single NVIDIA H100 GPU for both 1.5B and 7B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block diffusion LLMs are emerging as a promising next paradigm for language generation, but their use of KV caching makes memory access a dominant bottleneck in long-context settings. While dynamic sparse attention has been actively explored, existing methods designed for autoregressive LLMs rely on approximate importance estimation and perform poorly when adapted to block diffusion. This work identifies a key opportunity unique to block diffusion: attention at the first All-[MASK] denoising step reliably predicts important KV entries and budget requirements, enabling MAGE to perform a single exact attention pass per block and reuse it for training-free sparse denoising. Across long-context benchmarks including LongBench and Needle-in-a-Haystack, MAGE achieves near-lossless accuracy with a fraction of the KV budget while delivering up to 3-4x end-to-end speedup, consistently outperforming AR-oriented sparse attention baselines. A lightweight fine-tuning strategy further strengthens [MASK]-guided patterns with minimal cost, requiring only a few hours of training on a single NVIDIA H100 GPU for both 1.5B and 7B models."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-15T16:07:51Z",
                "published_parsed": [
                    2026,
                    2,
                    15,
                    16,
                    7,
                    51,
                    6,
                    46,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Omin Kwon"
                    },
                    {
                        "name": "Yeonjae Kim"
                    },
                    {
                        "name": "Doyeon Kim"
                    },
                    {
                        "name": "Minseo Kim"
                    },
                    {
                        "name": "Yeonhong Park"
                    },
                    {
                        "name": "Jae W. Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jae W. Lee"
                },
                "author": "Jae W. Lee"
            },
            {
                "id": "http://arxiv.org/abs/2602.14162v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.14162v1",
                "title": "Index Light, Reason Deep: Deferred Visual Ingestion for Visual-Dense Document Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Index Light, Reason Deep: Deferred Visual Ingestion for Visual-Dense Document Question Answering"
                },
                "updated": "2026-02-15T14:23:50Z",
                "updated_parsed": [
                    2026,
                    2,
                    15,
                    14,
                    23,
                    50,
                    6,
                    46,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.14162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.14162v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Existing multimodal document question answering methods universally adopt a supply-side ingestion strategy: running a Vision-Language Model (VLM) on every page during indexing to generate comprehensive descriptions, then answering questions through text retrieval. However, this \"pre-ingestion\" approach is costly (a 113-page engineering drawing package requires approximately 80,000 VLM tokens), end-to-end unreliable (VLM outputs may fail to be correctly retrieved due to format mismatches in the retrieval infrastructure), and irrecoverable once it fails. This paper proposes the Deferred Visual Ingestion (DVI) framework, adopting a demand-side ingestion strategy: the indexing phase performs only lightweight metadata extraction, deferring visual understanding to the moment users pose specific questions. DVI's core principle is \"Index for locating, not understanding\"--achieving page localization through structured metadata indexes and BM25 full-text search, then sending original images along with specific questions to a VLM for targeted analysis. Experiments on two real industrial engineering drawings (113 pages + 7 pages) demonstrate that DVI achieves comparable overall accuracy at zero ingestion VLM cost (46.7% vs. 48.9%), an effectiveness rate of 50% on visually necessary queries (vs. 0% for pre-ingestion), and 100% page localization (98% search space compression). DVI also supports interactive refinement and progressive caching, transforming the \"QA accuracy\" problem into a \"page localization\" problem--once the correct drawing page is found, obtaining the answer becomes a matter of interaction rounds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing multimodal document question answering methods universally adopt a supply-side ingestion strategy: running a Vision-Language Model (VLM) on every page during indexing to generate comprehensive descriptions, then answering questions through text retrieval. However, this \"pre-ingestion\" approach is costly (a 113-page engineering drawing package requires approximately 80,000 VLM tokens), end-to-end unreliable (VLM outputs may fail to be correctly retrieved due to format mismatches in the retrieval infrastructure), and irrecoverable once it fails. This paper proposes the Deferred Visual Ingestion (DVI) framework, adopting a demand-side ingestion strategy: the indexing phase performs only lightweight metadata extraction, deferring visual understanding to the moment users pose specific questions. DVI's core principle is \"Index for locating, not understanding\"--achieving page localization through structured metadata indexes and BM25 full-text search, then sending original images along with specific questions to a VLM for targeted analysis. Experiments on two real industrial engineering drawings (113 pages + 7 pages) demonstrate that DVI achieves comparable overall accuracy at zero ingestion VLM cost (46.7% vs. 48.9%), an effectiveness rate of 50% on visually necessary queries (vs. 0% for pre-ingestion), and 100% page localization (98% search space compression). DVI also supports interactive refinement and progressive caching, transforming the \"QA accuracy\" problem into a \"page localization\" problem--once the correct drawing page is found, obtaining the answer becomes a matter of interaction rounds."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-15T14:23:50Z",
                "published_parsed": [
                    2026,
                    2,
                    15,
                    14,
                    23,
                    50,
                    6,
                    46,
                    0
                ],
                "arxiv_comment": "24 pages, 9 figures, 9 tables",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Tao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xu"
                },
                "author": "Tao Xu"
            },
            {
                "id": "http://arxiv.org/abs/2602.13993v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.13993v1",
                "title": "Elastic Diffusion Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elastic Diffusion Transformer"
                },
                "updated": "2026-02-15T05:19:17Z",
                "updated_parsed": [
                    2026,
                    2,
                    15,
                    5,
                    19,
                    17,
                    6,
                    46,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.13993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.13993v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Transformers (DiT) have demonstrated remarkable generative capabilities but remain highly computationally expensive. Previous acceleration methods, such as pruning and distillation, typically rely on a fixed computational capacity, leading to insufficient acceleration and degraded generation quality. To address this limitation, we propose \\textbf{Elastic Diffusion Transformer (E-DiT)}, an adaptive acceleration framework for DiT that effectively improves efficiency while maintaining generation quality. Specifically, we observe that the generative process of DiT exhibits substantial sparsity (i.e., some computations can be skipped with minimal impact on quality), and this sparsity varies significantly across samples. Motivated by this observation, E-DiT equips each DiT block with a lightweight router that dynamically identifies sample-dependent sparsity from the input latent. Each router adaptively determines whether the corresponding block can be skipped. If the block is not skipped, the router then predicts the optimal MLP width reduction ratio within the block. During inference, we further introduce a block-level feature caching mechanism that leverages router predictions to eliminate redundant computations in a training-free manner. Extensive experiments across 2D image (Qwen-Image and FLUX) and 3D asset (Hunyuan3D-3.0) demonstrate the effectiveness of E-DiT, achieving up to $\\sim$2$\\times$ speedup with negligible loss in generation quality. Code will be available at https://github.com/wangjiangshan0725/Elastic-DiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have demonstrated remarkable generative capabilities but remain highly computationally expensive. Previous acceleration methods, such as pruning and distillation, typically rely on a fixed computational capacity, leading to insufficient acceleration and degraded generation quality. To address this limitation, we propose \\textbf{Elastic Diffusion Transformer (E-DiT)}, an adaptive acceleration framework for DiT that effectively improves efficiency while maintaining generation quality. Specifically, we observe that the generative process of DiT exhibits substantial sparsity (i.e., some computations can be skipped with minimal impact on quality), and this sparsity varies significantly across samples. Motivated by this observation, E-DiT equips each DiT block with a lightweight router that dynamically identifies sample-dependent sparsity from the input latent. Each router adaptively determines whether the corresponding block can be skipped. If the block is not skipped, the router then predicts the optimal MLP width reduction ratio within the block. During inference, we further introduce a block-level feature caching mechanism that leverages router predictions to eliminate redundant computations in a training-free manner. Extensive experiments across 2D image (Qwen-Image and FLUX) and 3D asset (Hunyuan3D-3.0) demonstrate the effectiveness of E-DiT, achieving up to $\\sim$2$\\times$ speedup with negligible loss in generation quality. Code will be available at https://github.com/wangjiangshan0725/Elastic-DiT."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-15T05:19:17Z",
                "published_parsed": [
                    2026,
                    2,
                    15,
                    5,
                    19,
                    17,
                    6,
                    46,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Zeqiang Lai"
                    },
                    {
                        "name": "Jiarui Chen"
                    },
                    {
                        "name": "Jiayi Guo"
                    },
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Xiu Li"
                    },
                    {
                        "name": "Xiangyu Yue"
                    },
                    {
                        "name": "Chunchao Guo"
                    }
                ],
                "author_detail": {
                    "name": "Chunchao Guo"
                },
                "author": "Chunchao Guo"
            },
            {
                "id": "http://arxiv.org/abs/2602.13692v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.13692v1",
                "title": "ThunderAgent: A Simple, Fast and Program-Aware Agentic Inference System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThunderAgent: A Simple, Fast and Program-Aware Agentic Inference System"
                },
                "updated": "2026-02-14T09:26:41Z",
                "updated_parsed": [
                    2026,
                    2,
                    14,
                    9,
                    26,
                    41,
                    5,
                    45,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.13692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.13692v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models(LLMs) are now used to power complex multi-turn agentic workflows. Existing systems run agentic inference by loosely assembling isolated components: an LLM inference engine (e.g., vLLM) and a tool orchestrator (e.g., Kubernetes). Although agentic workflows involve multiple LLM and tool requests, these systems schedule and allocate resources separately on a per-request basis, without end-to-end knowledge of the workflow. This leads to sub-optimal management of KV cache and tool execution environments. To address the challenges, we propose ThunderAgent, a fast, simple, and program-aware agentic inference system. We first abstract agentic workflows as LLM Programs, enabling a unified view of heterogeneous resources, including KV caches, system states, and external tool assets such as disk memory and network ports. Built upon this abstraction, ThunderAgent introduces a program-aware scheduler and a tool resource manager designed to maximize KV cache hit rates, mitigate memory imbalances, and enable asynchronous environment preparation. Evaluations across coding, routing, and scientific discovery agents demonstrate that ThunderAgent achieves 1.5-3.6x throughput improvements in serving, 1.8-3.9x in RL rollout, and up to 4.2x disk memory savings compared to state-of-the-art inference systems. To facilitate reproducibility and support future development, we open-source the system implementations of the whole ThunderAgent at: https://github.com/Agentic-Kinetics/ThunderAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models(LLMs) are now used to power complex multi-turn agentic workflows. Existing systems run agentic inference by loosely assembling isolated components: an LLM inference engine (e.g., vLLM) and a tool orchestrator (e.g., Kubernetes). Although agentic workflows involve multiple LLM and tool requests, these systems schedule and allocate resources separately on a per-request basis, without end-to-end knowledge of the workflow. This leads to sub-optimal management of KV cache and tool execution environments. To address the challenges, we propose ThunderAgent, a fast, simple, and program-aware agentic inference system. We first abstract agentic workflows as LLM Programs, enabling a unified view of heterogeneous resources, including KV caches, system states, and external tool assets such as disk memory and network ports. Built upon this abstraction, ThunderAgent introduces a program-aware scheduler and a tool resource manager designed to maximize KV cache hit rates, mitigate memory imbalances, and enable asynchronous environment preparation. Evaluations across coding, routing, and scientific discovery agents demonstrate that ThunderAgent achieves 1.5-3.6x throughput improvements in serving, 1.8-3.9x in RL rollout, and up to 4.2x disk memory savings compared to state-of-the-art inference systems. To facilitate reproducibility and support future development, we open-source the system implementations of the whole ThunderAgent at: https://github.com/Agentic-Kinetics/ThunderAgent."
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-14T09:26:41Z",
                "published_parsed": [
                    2026,
                    2,
                    14,
                    9,
                    26,
                    41,
                    5,
                    45,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS"
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Ziyang Li"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Weili Xu"
                    },
                    {
                        "name": "Yinfang Chen"
                    },
                    {
                        "name": "Junxiong Wang"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Chenfeng Xu"
                    },
                    {
                        "name": "Simran Arora"
                    }
                ],
                "author_detail": {
                    "name": "Simran Arora"
                },
                "author": "Simran Arora"
            },
            {
                "id": "http://arxiv.org/abs/2408.10746v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2408.10746v2",
                "title": "Resource-Efficient Personal Large Language Models Fine-Tuning with Collaborative Edge Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Efficient Personal Large Language Models Fine-Tuning with Collaborative Edge Computing"
                },
                "updated": "2026-02-14T07:14:40Z",
                "updated_parsed": [
                    2026,
                    2,
                    14,
                    7,
                    14,
                    40,
                    5,
                    45,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2408.10746v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2408.10746v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have unlocked a plethora of powerful applications at the network edge, such as intelligent personal assistants. Data privacy and security concerns have prompted a shift towards edge-based fine-tuning of personal LLMs, away from cloud reliance. However, this raises issues of computational intensity and resource scarcity, hindering training efficiency and feasibility. While current studies investigate parameter-efficient fine-tuning (PEFT) techniques to mitigate resource constraints, our analysis indicates that these techniques are not sufficiently resource-efficient for edge devices. To tackle these challenges, we propose Pluto and Charon (PAC), a time and memory efficient collaborative edge AI framework for personal LLMs fine-tuning. PAC breaks the resource wall of personal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1) Algorithmically, PAC implements a personal LLMs fine-tuning technique that is efficient in terms of parameters, time, and memory. It utilizes Parallel Adapters to circumvent the need for a full backward pass through the LLM backbone. Additionally, an activation cache mechanism further streamlining the process by negating the necessity for repeated forward passes across multiple epochs. (2) Systematically, PAC leverages edge devices in close proximity, pooling them as a collective resource for in-situ personal LLMs fine-tuning, utilizing a hybrid data and pipeline parallelism to orchestrate distributed training. The use of the activation cache eliminates the need for forward pass through the LLM backbone,enabling exclusive fine-tuning of the Parallel Adapters using data parallelism. Extensive evaluation based on prototype implementation demonstrates that PAC remarkably outperforms state-of-the-art approaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction in memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unlocked a plethora of powerful applications at the network edge, such as intelligent personal assistants. Data privacy and security concerns have prompted a shift towards edge-based fine-tuning of personal LLMs, away from cloud reliance. However, this raises issues of computational intensity and resource scarcity, hindering training efficiency and feasibility. While current studies investigate parameter-efficient fine-tuning (PEFT) techniques to mitigate resource constraints, our analysis indicates that these techniques are not sufficiently resource-efficient for edge devices. To tackle these challenges, we propose Pluto and Charon (PAC), a time and memory efficient collaborative edge AI framework for personal LLMs fine-tuning. PAC breaks the resource wall of personal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1) Algorithmically, PAC implements a personal LLMs fine-tuning technique that is efficient in terms of parameters, time, and memory. It utilizes Parallel Adapters to circumvent the need for a full backward pass through the LLM backbone. Additionally, an activation cache mechanism further streamlining the process by negating the necessity for repeated forward passes across multiple epochs. (2) Systematically, PAC leverages edge devices in close proximity, pooling them as a collective resource for in-situ personal LLMs fine-tuning, utilizing a hybrid data and pipeline parallelism to orchestrate distributed training. The use of the activation cache eliminates the need for forward pass through the LLM backbone,enabling exclusive fine-tuning of the Parallel Adapters using data parallelism. Extensive evaluation based on prototype implementation demonstrates that PAC remarkably outperforms state-of-the-art approaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction in memory footprint."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-08-20T11:30:12Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Shengyuan Ye"
                    },
                    {
                        "name": "Bei Ouyang"
                    },
                    {
                        "name": "Tianyi Qian"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Jiangsu Du"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen"
            },
            {
                "id": "http://arxiv.org/abs/2506.02634v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.02634v5",
                "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache at a Large Cloud Provider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache at a Large Cloud Provider"
                },
                "updated": "2026-02-14T05:32:46Z",
                "updated_parsed": [
                    2026,
                    2,
                    14,
                    5,
                    32,
                    46,
                    5,
                    45,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.02634v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.02634v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Serving large language models (LLMs) is important for cloud providers, and caching intermediate results (KV\\$) after processing each request substantially improves serving throughput and latency. However, there is limited understanding of how LLM serving benefits from KV\\$ caching, where system design decisions like cache eviction policies are highly workload-dependent. In this paper, we present the first systematic characterization of the KV\\$ workload patterns from one of the leading LLM service providers. We draw observations that were not covered by previous studies focusing on synthetic workloads, including: KV\\$ reuses are skewed across requests, where reuses between single-turn requests are equally important as multi-turn requests; the reuse time and probability are diverse considering all requests, but for a specific request category, the pattern tends to be predictable; and the overall cache size required for an ideal cache hit ratio is moderate. Based on the characterization, we further propose a workload-aware cache eviction policy that improves the serving performance under real-world traces, especially with limited cache capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is important for cloud providers, and caching intermediate results (KV\\$) after processing each request substantially improves serving throughput and latency. However, there is limited understanding of how LLM serving benefits from KV\\$ caching, where system design decisions like cache eviction policies are highly workload-dependent. In this paper, we present the first systematic characterization of the KV\\$ workload patterns from one of the leading LLM service providers. We draw observations that were not covered by previous studies focusing on synthetic workloads, including: KV\\$ reuses are skewed across requests, where reuses between single-turn requests are equally important as multi-turn requests; the reuse time and probability are diverse considering all requests, but for a specific request category, the pattern tends to be predictable; and the overall cache size required for an ideal cache hit ratio is moderate. Based on the characterization, we further propose a workload-aware cache eviction policy that improves the serving performance under real-world traces, especially with limited cache capacity."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-03T08:51:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "arxiv_comment": "Accepted by USENIX ATC'25",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Jinbo Han"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Chenguang Fang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen"
            },
            {
                "id": "http://arxiv.org/abs/2602.03983v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.03983v2",
                "title": "Efficient Long-Horizon Vision-Language-Action Models via Static-Dynamic Disentanglement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Long-Horizon Vision-Language-Action Models via Static-Dynamic Disentanglement"
                },
                "updated": "2026-02-14T03:09:51Z",
                "updated_parsed": [
                    2026,
                    2,
                    14,
                    3,
                    9,
                    51,
                    5,
                    45,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.03983v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.03983v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-Language-Action (VLA) models have recently emerged as a promising paradigm for generalist robotic control. Built upon vision-language model (VLM) architectures, VLAs predict actions conditioned on visual observations and language instructions, achieving strong performance and generalization across tasks. However, VLAs face two major challenges: limited long-horizon context and inefficient inference due to the quadratic attention complexity and large parameter counts. Our work is motivated by the observation that much of the visual information in a trajectory remains static across timesteps (e.g., the background). Leveraging this property, we propose SD-VLA, a framework that disentangles visual inputs into multi-level static and dynamic tokens, which enables (1) retaining a single copy of static tokens across frames to significantly reduce context length, and (2) reusing the key-value (KV) cache of static tokens through a lightweight recache gate that updates only when necessary. This design enables efficient multi-frame integration and efficient inference. In addition, we introduce a new benchmark that more effectively evaluates the long-horizon temporal dependency modeling ability of VLAs. Experimental results show that our approach outperforms baselines on this benchmark by 39.8% absolute improvement in success rate, and achieves a 3.9% gain on the SimplerEnv benchmark. Moreover, SD-VLA delivers a 2.26x inference speedup over the base VLA model on the same benchmark, enabling faster and more practical real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have recently emerged as a promising paradigm for generalist robotic control. Built upon vision-language model (VLM) architectures, VLAs predict actions conditioned on visual observations and language instructions, achieving strong performance and generalization across tasks. However, VLAs face two major challenges: limited long-horizon context and inefficient inference due to the quadratic attention complexity and large parameter counts. Our work is motivated by the observation that much of the visual information in a trajectory remains static across timesteps (e.g., the background). Leveraging this property, we propose SD-VLA, a framework that disentangles visual inputs into multi-level static and dynamic tokens, which enables (1) retaining a single copy of static tokens across frames to significantly reduce context length, and (2) reusing the key-value (KV) cache of static tokens through a lightweight recache gate that updates only when necessary. This design enables efficient multi-frame integration and efficient inference. In addition, we introduce a new benchmark that more effectively evaluates the long-horizon temporal dependency modeling ability of VLAs. Experimental results show that our approach outperforms baselines on this benchmark by 39.8% absolute improvement in success rate, and achieves a 3.9% gain on the SimplerEnv benchmark. Moreover, SD-VLA delivers a 2.26x inference speedup over the base VLA model on the same benchmark, enabling faster and more practical real-world deployment."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-03T20:17:47Z",
                "published_parsed": [
                    2026,
                    2,
                    3,
                    20,
                    17,
                    47,
                    1,
                    34,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Weikang Qiu"
                    },
                    {
                        "name": "Tinglin Huang"
                    },
                    {
                        "name": "Rex Ying"
                    }
                ],
                "author_detail": {
                    "name": "Rex Ying"
                },
                "author": "Rex Ying"
            },
            {
                "id": "http://arxiv.org/abs/2503.10568v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.10568v3",
                "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Randomized Parallel Decoding"
                },
                "updated": "2026-02-14T02:23:24Z",
                "updated_parsed": [
                    2026,
                    2,
                    14,
                    2,
                    23,
                    24,
                    5,
                    45,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.10568v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.10568v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce ARPG, a novel visual autoregressive model that enables randomized parallel generation, addressing the inherent limitations of conventional raster-order approaches, which hinder inference efficiency and zero-shot generalization due to their sequential, predefined token generation order. Our key insight is that effective random-order modeling necessitates explicit guidance for determining the position of the next predicted token. To this end, we propose a novel decoupled decoding framework that decouples positional guidance from content representation, encoding them separately as queries and key-value pairs. By directly incorporating this guidance into the causal attention mechanism, our approach enables fully random-order training and generation, eliminating the need for bidirectional attention. Consequently, ARPG readily generalizes to zero-shot inference tasks such as image inpainting, outpainting, and resolution expansion. Furthermore, it supports parallel inference by concurrently processing multiple queries using a shared KV cache. On the ImageNet-1K 256 benchmark, our approach attains an FID of 1.83 with only 32 sampling steps, achieving over a 30 times speedup in inference and a 75 percent reduction in memory consumption compared to representative recent autoregressive models at a similar scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ARPG, a novel visual autoregressive model that enables randomized parallel generation, addressing the inherent limitations of conventional raster-order approaches, which hinder inference efficiency and zero-shot generalization due to their sequential, predefined token generation order. Our key insight is that effective random-order modeling necessitates explicit guidance for determining the position of the next predicted token. To this end, we propose a novel decoupled decoding framework that decouples positional guidance from content representation, encoding them separately as queries and key-value pairs. By directly incorporating this guidance into the causal attention mechanism, our approach enables fully random-order training and generation, eliminating the need for bidirectional attention. Consequently, ARPG readily generalizes to zero-shot inference tasks such as image inpainting, outpainting, and resolution expansion. Furthermore, it supports parallel inference by concurrently processing multiple queries using a shared KV cache. On the ImageNet-1K 256 benchmark, our approach attains an FID of 1.83 with only 32 sampling steps, achieving over a 30 times speedup in inference and a 75 percent reduction in memory consumption compared to representative recent autoregressive models at a similar scale."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-13T17:19:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Haopeng Li"
                    },
                    {
                        "name": "Jinyue Yang"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang"
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2602.22212v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22212v1",
                "title": "Neu-PiG: Neural Preconditioned Grids for Fast Dynamic Surface Reconstruction on Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neu-PiG: Neural Preconditioned Grids for Fast Dynamic Surface Reconstruction on Long Sequences"
                },
                "updated": "2026-02-25T18:59:53Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    59,
                    53,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22212v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Temporally consistent surface reconstruction of dynamic 3D objects from unstructured point cloud data remains challenging, especially for very long sequences. Existing methods either optimize deformations incrementally, risking drift and requiring long runtimes, or rely on complex learned models that demand category-specific training. We present Neu-PiG, a fast deformation optimization method based on a novel preconditioned latent-grid encoding that distributes spatial features parameterized on the position and normal direction of a keyframe surface. Our method encodes entire deformations across all time steps at various spatial scales into a multi-resolution latent grid, parameterized by the position and normal direction of a reference surface from a single keyframe. This latent representation is then augmented for time modulation and decoded into per-frame 6-DoF deformations via a lightweight multilayer perceptron (MLP). To achieve high-fidelity, drift-free surface reconstructions in seconds, we employ Sobolev preconditioning during gradient-based training of the latent space, completely avoiding the need for any explicit correspondences or further priors. Experiments across diverse human and animal datasets demonstrate that Neu-PiG outperforms state-the-art approaches, offering both superior accuracy and scalability to long sequences while running at least 60x faster than existing training-free methods and achieving inference speeds on the same order as heavy pretrained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporally consistent surface reconstruction of dynamic 3D objects from unstructured point cloud data remains challenging, especially for very long sequences. Existing methods either optimize deformations incrementally, risking drift and requiring long runtimes, or rely on complex learned models that demand category-specific training. We present Neu-PiG, a fast deformation optimization method based on a novel preconditioned latent-grid encoding that distributes spatial features parameterized on the position and normal direction of a keyframe surface. Our method encodes entire deformations across all time steps at various spatial scales into a multi-resolution latent grid, parameterized by the position and normal direction of a reference surface from a single keyframe. This latent representation is then augmented for time modulation and decoded into per-frame 6-DoF deformations via a lightweight multilayer perceptron (MLP). To achieve high-fidelity, drift-free surface reconstructions in seconds, we employ Sobolev preconditioning during gradient-based training of the latent space, completely avoiding the need for any explicit correspondences or further priors. Experiments across diverse human and animal datasets demonstrate that Neu-PiG outperforms state-the-art approaches, offering both superior accuracy and scalability to long sequences while running at least 60x faster than existing training-free methods and achieving inference speeds on the same order as heavy pretrained models."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T18:59:53Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    59,
                    53,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "CVPR 2026, Code: https://github.com/vc-bonn/neu-pig",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Julian Kaltheuner"
                    },
                    {
                        "name": "Hannah Drge"
                    },
                    {
                        "name": "Markus Plack"
                    },
                    {
                        "name": "Patrick Stotko"
                    },
                    {
                        "name": "Reinhard Klein"
                    }
                ],
                "author_detail": {
                    "name": "Reinhard Klein"
                },
                "author": "Reinhard Klein"
            },
            {
                "id": "http://arxiv.org/abs/2602.22207v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22207v1",
                "title": "Recovered in Translation: Efficient Pipeline for Automated Translation of Benchmarks and Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recovered in Translation: Efficient Pipeline for Automated Translation of Benchmarks and Datasets"
                },
                "updated": "2026-02-25T18:58:25Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    58,
                    25,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22207v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The reliability of multilingual Large Language Model (LLM) evaluation is currently compromised by the inconsistent quality of translated benchmarks. Existing resources often suffer from semantic drift and context loss, which can lead to misleading performance metrics. In this work, we present a fully automated framework designed to address these challenges by enabling scalable, high-quality translation of datasets and benchmarks. We demonstrate that adapting test-time compute scaling strategies, specifically Universal Self-Improvement (USI) and our proposed multi-round ranking method, T-RANK, allows for significantly higher quality outputs compared to traditional pipelines. Our framework ensures that benchmarks preserve their original task structure and linguistic nuances during localization. We apply this approach to translate popular benchmarks and datasets into eight Eastern and Southern European languages (Ukrainian, Bulgarian, Slovak, Romanian, Lithuanian, Estonian, Turkish, Greek). Evaluations using both reference-based metrics and LLM-as-a-judge show that our translations surpass existing resources, resulting in more accurate downstream model assessment. We release both the framework and the improved benchmarks to facilitate robust and reproducible multilingual AI development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reliability of multilingual Large Language Model (LLM) evaluation is currently compromised by the inconsistent quality of translated benchmarks. Existing resources often suffer from semantic drift and context loss, which can lead to misleading performance metrics. In this work, we present a fully automated framework designed to address these challenges by enabling scalable, high-quality translation of datasets and benchmarks. We demonstrate that adapting test-time compute scaling strategies, specifically Universal Self-Improvement (USI) and our proposed multi-round ranking method, T-RANK, allows for significantly higher quality outputs compared to traditional pipelines. Our framework ensures that benchmarks preserve their original task structure and linguistic nuances during localization. We apply this approach to translate popular benchmarks and datasets into eight Eastern and Southern European languages (Ukrainian, Bulgarian, Slovak, Romanian, Lithuanian, Estonian, Turkish, Greek). Evaluations using both reference-based metrics and LLM-as-a-judge show that our translations surpass existing resources, resulting in more accurate downstream model assessment. We release both the framework and the improved benchmarks to facilitate robust and reproducible multilingual AI development."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T18:58:25Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    58,
                    25,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hanna Yukhymenko"
                    },
                    {
                        "name": "Anton Alexandrov"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev"
            },
            {
                "id": "http://arxiv.org/abs/2602.00288v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.00288v3",
                "title": "TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs"
                },
                "updated": "2026-02-25T18:57:52Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    57,
                    52,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.00288v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.00288v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Fine-grained spatio-temporal understanding is essential for video reasoning and embodied AI. Yet, while Multimodal Large Language Models (MLLMs) master static semantics, their grasp of temporal dynamics remains brittle. We present TimeBlind, a diagnostic benchmark for compositional spatio-temporal understanding. Inspired by cognitive science, TimeBlind categorizes fine-grained temporal understanding into three levels: recognizing atomic events, characterizing event properties, and reasoning about event interdependencies. Unlike benchmarks that conflate recognition with temporal reasoning, TimeBlind leverages a minimal-pairs paradigm: video pairs share identical static visual content but differ solely in temporal structure, utilizing complementary questions to neutralize language priors. Evaluating over 20 state-of-the-art MLLMs (e.g., GPT-5, Gemini 3 Pro) on 600 curated instances (2400 video-question pairs), reveals that the Instance Accuracy (correctly distinguishing both videos in a pair) of the best performing MLLM is only 48.2%, far below the human performance (98.2%). These results demonstrate that even frontier models rely heavily on static visual shortcuts rather than genuine temporal logic, positioning TimeBlind as a vital diagnostic tool for next-generation video understanding. Dataset and code are available at https://baiqi-li.github.io/timeblind_project/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained spatio-temporal understanding is essential for video reasoning and embodied AI. Yet, while Multimodal Large Language Models (MLLMs) master static semantics, their grasp of temporal dynamics remains brittle. We present TimeBlind, a diagnostic benchmark for compositional spatio-temporal understanding. Inspired by cognitive science, TimeBlind categorizes fine-grained temporal understanding into three levels: recognizing atomic events, characterizing event properties, and reasoning about event interdependencies. Unlike benchmarks that conflate recognition with temporal reasoning, TimeBlind leverages a minimal-pairs paradigm: video pairs share identical static visual content but differ solely in temporal structure, utilizing complementary questions to neutralize language priors. Evaluating over 20 state-of-the-art MLLMs (e.g., GPT-5, Gemini 3 Pro) on 600 curated instances (2400 video-question pairs), reveals that the Instance Accuracy (correctly distinguishing both videos in a pair) of the best performing MLLM is only 48.2%, far below the human performance (98.2%). These results demonstrate that even frontier models rely heavily on static visual shortcuts rather than genuine temporal logic, positioning TimeBlind as a vital diagnostic tool for next-generation video understanding. Dataset and code are available at https://baiqi-li.github.io/timeblind_project/ ."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-30T20:21:46Z",
                "published_parsed": [
                    2026,
                    1,
                    30,
                    20,
                    21,
                    46,
                    4,
                    30,
                    0
                ],
                "arxiv_comment": "For code and data, see https://baiqi-li.github.io/timeblind_project/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Baiqi Li"
                    },
                    {
                        "name": "Kangyi Zhao"
                    },
                    {
                        "name": "Ce Zhang"
                    },
                    {
                        "name": "Chancharik Mitra"
                    },
                    {
                        "name": "Jean de Dieu Nyandwi"
                    },
                    {
                        "name": "Gedas Bertasius"
                    }
                ],
                "author_detail": {
                    "name": "Gedas Bertasius"
                },
                "author": "Gedas Bertasius"
            },
            {
                "id": "http://arxiv.org/abs/2504.17203v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.17203v4",
                "title": "High-Fidelity And Complex Test Data Generation For Google SQL Code Generation Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Fidelity And Complex Test Data Generation For Google SQL Code Generation Services"
                },
                "updated": "2026-02-25T18:55:05Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    55,
                    5,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.17203v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.17203v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The demand for high-fidelity test data is paramount in industrial settings where access to production data is largely restricted. Traditional data generation methods often fall short, struggling with low-fidelity and the ability to model complex data structures and semantic relationships that are critical for testing complex SQL code generation services like Natural Language to SQL (NL2SQL). In this paper, we address the critical need for generating syntactically correct and semantically relevant high-fidelity mock data for complex data structures that includes columns with nested structures that we frequently encounter in Google workloads. We highlight the limitations of existing approaches used in production, particularly their inability to handle large and complex data structures, as well as the lack of semantically coherent test data that lead to limited test coverage. We demonstrate that by leveraging Large Language Models (LLMs) and incorporating strategic pre- and post-processing steps, we can generate syntactically correct and semantically relevant high-fidelity test data that adheres to complex structural constraints and maintains semantic integrity to the SQL test targets (queries/functions). This approach supports comprehensive testing of complex SQL queries involving joins, aggregations, and even deeply nested subqueries, ensuring robust evaluation of SQL code generation services, like NL2SQL and SQL Code Assistant. Our results demonstrate the practical utility of an LLM (\\textit{Gemini}) based test data generation for industrial SQL code generation services where generating high-fidelity test data is essential due to the frequent unavailability and inaccessibility of production datasets for testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The demand for high-fidelity test data is paramount in industrial settings where access to production data is largely restricted. Traditional data generation methods often fall short, struggling with low-fidelity and the ability to model complex data structures and semantic relationships that are critical for testing complex SQL code generation services like Natural Language to SQL (NL2SQL). In this paper, we address the critical need for generating syntactically correct and semantically relevant high-fidelity mock data for complex data structures that includes columns with nested structures that we frequently encounter in Google workloads. We highlight the limitations of existing approaches used in production, particularly their inability to handle large and complex data structures, as well as the lack of semantically coherent test data that lead to limited test coverage. We demonstrate that by leveraging Large Language Models (LLMs) and incorporating strategic pre- and post-processing steps, we can generate syntactically correct and semantically relevant high-fidelity test data that adheres to complex structural constraints and maintains semantic integrity to the SQL test targets (queries/functions). This approach supports comprehensive testing of complex SQL queries involving joins, aggregations, and even deeply nested subqueries, ensuring robust evaluation of SQL code generation services, like NL2SQL and SQL Code Assistant. Our results demonstrate the practical utility of an LLM (\\textit{Gemini}) based test data generation for industrial SQL code generation services where generating high-fidelity test data is essential due to the frequent unavailability and inaccessibility of production datasets for testing."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-24T02:27:17Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    2,
                    27,
                    17,
                    3,
                    114,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Shivasankari Kannan"
                    },
                    {
                        "name": "Yeounoh Chung"
                    },
                    {
                        "name": "Amita Gondi"
                    },
                    {
                        "name": "Tristan Swadell"
                    },
                    {
                        "name": "Fatma Ozcan"
                    }
                ],
                "author_detail": {
                    "name": "Fatma Ozcan"
                },
                "author": "Fatma Ozcan"
            },
            {
                "id": "http://arxiv.org/abs/2602.16893v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16893v2",
                "title": "CalmReminder: A Design Probe for Parental Engagement with Children with Hyperactivity, Augmented by Real-Time Motion Sensing with a Watch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CalmReminder: A Design Probe for Parental Engagement with Children with Hyperactivity, Augmented by Real-Time Motion Sensing with a Watch"
                },
                "updated": "2026-02-25T18:46:14Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    46,
                    14,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16893v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16893v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3772318.3791871",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Families raising children with ADHD often experience heightened stress and reactive parenting. While digital interventions promise personalization, many remain one-size-fits-all and fail to reflect parents' lived practices. We present CalmReminder, a watch-based system that detects children's calm moments and delivers just-in-time prompts to parents. Through a four-week deployment with 16 families (twelve completed) of children with ADHD, we compared notification strategies ranging from hourly to random to only when the child was inferred to be calm. Our sensing-based notifications were frequently perceived as arriving during calm moments. More importantly, parents adopted the system in diverse ways: using notifications for praise, mindfulness, activity planning, or conversation. These findings show that parents are not passive recipients but active designers, reshaping interventions to fit their parenting styles. We contribute a calm detection pipeline, empirical insights into families' flexible appropriation of notifications, and design implications for intervention systems that foster agency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Families raising children with ADHD often experience heightened stress and reactive parenting. While digital interventions promise personalization, many remain one-size-fits-all and fail to reflect parents' lived practices. We present CalmReminder, a watch-based system that detects children's calm moments and delivers just-in-time prompts to parents. Through a four-week deployment with 16 families (twelve completed) of children with ADHD, we compared notification strategies ranging from hourly to random to only when the child was inferred to be calm. Our sensing-based notifications were frequently perceived as arriving during calm moments. More importantly, parents adopted the system in diverse ways: using notifications for praise, mindfulness, activity planning, or conversation. These findings show that parents are not passive recipients but active designers, reshaping interventions to fit their parenting styles. We contribute a calm detection pipeline, empirical insights into families' flexible appropriation of notifications, and design implications for intervention systems that foster agency."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T21:20:24Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    21,
                    20,
                    24,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "Accepted by ACM CHI Conference on Human Factors in Computing Systems(CHI'26)",
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Riku Arakawa"
                    },
                    {
                        "name": "Shreya Bali"
                    },
                    {
                        "name": "Anupama Sitaraman"
                    },
                    {
                        "name": "Woosuk Seo"
                    },
                    {
                        "name": "Sam Shaaban"
                    },
                    {
                        "name": "Oliver Lindheim"
                    },
                    {
                        "name": "Traci M. Kennedy"
                    },
                    {
                        "name": "Mayank Goel"
                    }
                ],
                "author_detail": {
                    "name": "Mayank Goel"
                },
                "author": "Mayank Goel",
                "arxiv_doi": "10.1145/3772318.3791871"
            },
            {
                "id": "http://arxiv.org/abs/2507.08017v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.08017v5",
                "title": "Mechanistic Indicators of Understanding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mechanistic Indicators of Understanding in Large Language Models"
                },
                "updated": "2026-02-25T18:34:16Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    34,
                    16,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.08017v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.08017v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) are often portrayed as merely imitating linguistic patterns without genuine understanding. We argue that recent findings in mechanistic interpretability (MI), the emerging field probing the inner workings of LLMs, render this picture increasingly untenable--but only once those findings are integrated within a theoretical account of understanding. We propose a tiered framework for thinking about understanding in LLMs and use it to synthesize the most relevant findings to date. The framework distinguishes three hierarchical varieties of understanding, each tied to a corresponding level of computational organization: conceptual understanding emerges when a model forms \"features\" as directions in latent space, learning connections between diverse manifestations of a single entity or property; state-of-the-world understanding emerges when a model learns contingent factual connections between features and dynamically tracks changes in the world; principled understanding emerges when a model ceases to rely on memorized facts and discovers a compact \"circuit\" connecting these facts. Across these tiers, MI uncovers internal organizations that can underwrite understanding-like unification. However, these also diverge from human cognition in their parallel exploitation of heterogeneous mechanisms. Fusing philosophical theory with mechanistic evidence thus allows us to transcend binary debates over whether AI understands, paving the way for a comparative, mechanistically grounded epistemology that explores how AI understanding aligns with--and diverges from--our own.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are often portrayed as merely imitating linguistic patterns without genuine understanding. We argue that recent findings in mechanistic interpretability (MI), the emerging field probing the inner workings of LLMs, render this picture increasingly untenable--but only once those findings are integrated within a theoretical account of understanding. We propose a tiered framework for thinking about understanding in LLMs and use it to synthesize the most relevant findings to date. The framework distinguishes three hierarchical varieties of understanding, each tied to a corresponding level of computational organization: conceptual understanding emerges when a model forms \"features\" as directions in latent space, learning connections between diverse manifestations of a single entity or property; state-of-the-world understanding emerges when a model learns contingent factual connections between features and dynamically tracks changes in the world; principled understanding emerges when a model ceases to rely on memorized facts and discovers a compact \"circuit\" connecting these facts. Across these tiers, MI uncovers internal organizations that can underwrite understanding-like unification. However, these also diverge from human cognition in their parallel exploitation of heterogeneous mechanisms. Fusing philosophical theory with mechanistic evidence thus allows us to transcend binary debates over whether AI understands, paving the way for a comparative, mechanistically grounded epistemology that explores how AI understanding aligns with--and diverges from--our own."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-07T20:26:31Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    20,
                    26,
                    31,
                    0,
                    188,
                    0
                ],
                "arxiv_comment": "38 pages",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Pierre Beckmann"
                    },
                    {
                        "name": "Matthieu Queloz"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Queloz"
                },
                "author": "Matthieu Queloz"
            },
            {
                "id": "http://arxiv.org/abs/2602.22188v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22188v1",
                "title": "Surrogate models for Rock-Fluid Interaction: A Grid-Size-Invariant Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models for Rock-Fluid Interaction: A Grid-Size-Invariant Approach"
                },
                "updated": "2026-02-25T18:34:03Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    34,
                    3,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22188v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modelling rock-fluid interaction requires solving a set of partial differential equations (PDEs) to predict the flow behaviour and the reactions of the fluid with the rock on the interfaces. Conventional high-fidelity numerical models require a high resolution to obtain reliable results, resulting in huge computational expense. This restricts the applicability of these models for multi-query problems, such as uncertainty quantification and optimisation, which require running numerous scenarios. As a cheaper alternative to high-fidelity models, this work develops eight surrogate models for predicting the fluid flow in porous media. Four of these are reduced-order models (ROM) based on one neural network for compression and another for prediction. The other four are single neural networks with the property of grid-size invariance; a term which we use to refer to image-to-image models that are capable of inferring on computational domains that are larger than those used during training. In addition to the novel grid-size-invariant framework for surrogate models, we compare the predictive performance of UNet and UNet++ architectures, and demonstrate that UNet++ outperforms UNet for surrogate models. Furthermore, we show that the grid-size-invariant approach is a reliable way to reduce memory consumption during training, resulting in good correlation between predicted and ground-truth values and outperforming the ROMs analysed. The application analysed is particularly challenging because fluid-induced rock dissolution results in a non-static solid field and, consequently, it cannot be used to help in adjustments of the future prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling rock-fluid interaction requires solving a set of partial differential equations (PDEs) to predict the flow behaviour and the reactions of the fluid with the rock on the interfaces. Conventional high-fidelity numerical models require a high resolution to obtain reliable results, resulting in huge computational expense. This restricts the applicability of these models for multi-query problems, such as uncertainty quantification and optimisation, which require running numerous scenarios. As a cheaper alternative to high-fidelity models, this work develops eight surrogate models for predicting the fluid flow in porous media. Four of these are reduced-order models (ROM) based on one neural network for compression and another for prediction. The other four are single neural networks with the property of grid-size invariance; a term which we use to refer to image-to-image models that are capable of inferring on computational domains that are larger than those used during training. In addition to the novel grid-size-invariant framework for surrogate models, we compare the predictive performance of UNet and UNet++ architectures, and demonstrate that UNet++ outperforms UNet for surrogate models. Furthermore, we show that the grid-size-invariant approach is a reliable way to reduce memory consumption during training, resulting in good correlation between predicted and ground-truth values and outperforming the ROMs analysed. The application analysed is particularly challenging because fluid-induced rock dissolution results in a non-static solid field and, consequently, it cannot be used to help in adjustments of the future prediction."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T18:34:03Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    34,
                    3,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Nathalie C. Pinheiro"
                    },
                    {
                        "name": "Donghu Guo"
                    },
                    {
                        "name": "Hannah P. Menke"
                    },
                    {
                        "name": "Aniket C. Joshi"
                    },
                    {
                        "name": "Claire E. Heaney"
                    },
                    {
                        "name": "Ahmed H. ElSheikh"
                    },
                    {
                        "name": "Christopher C. Pain"
                    }
                ],
                "author_detail": {
                    "name": "Christopher C. Pain"
                },
                "author": "Christopher C. Pain"
            },
            {
                "id": "http://arxiv.org/abs/2602.22186v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22186v1",
                "title": "Codesigning Ripplet: an LLM-Assisted Assessment Authoring System Grounded in a Conceptual Model of Teachers' Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Codesigning Ripplet: an LLM-Assisted Assessment Authoring System Grounded in a Conceptual Model of Teachers' Workflows"
                },
                "updated": "2026-02-25T18:30:44Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    30,
                    44,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22186v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3772318.3790418",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Assessments are critical in education, but creating them can be difficult. To address this challenge in a grounded way, we partnered with 13 teachers in a seven-month codesign process. We developed a conceptual model that characterizes the iterative dual process where teachers develop assessments while simultaneously refining requirements. To enact this model in practice, we built Ripplet, a web-based tool with multilevel reusable interactions to support assessment authoring. The extended codesign revealed that Ripplet enabled teachers to create formative assessments they would not have otherwise made, shifted their practices from generation to curation, and helped them reflect more on assessment quality. In a user study with 15 additional teachers, compared to their current practices, teachers felt the results were more worth their effort and that assessment quality improved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessments are critical in education, but creating them can be difficult. To address this challenge in a grounded way, we partnered with 13 teachers in a seven-month codesign process. We developed a conceptual model that characterizes the iterative dual process where teachers develop assessments while simultaneously refining requirements. To enact this model in practice, we built Ripplet, a web-based tool with multilevel reusable interactions to support assessment authoring. The extended codesign revealed that Ripplet enabled teachers to create formative assessments they would not have otherwise made, shifted their practices from generation to curation, and helped them reflect more on assessment quality. In a user study with 15 additional teachers, compared to their current practices, teachers felt the results were more worth their effort and that assessment quality improved."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T18:30:44Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    30,
                    44,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "Proceedings of the 2026 CHI Conference on Human Factors in Computing Systems",
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Yuan Cui"
                    },
                    {
                        "name": "Annabel Goldman"
                    },
                    {
                        "name": "Jovy Zhou"
                    },
                    {
                        "name": "Xiaolin Liu"
                    },
                    {
                        "name": "Clarissa Shieh"
                    },
                    {
                        "name": "Joshua Yao"
                    },
                    {
                        "name": "Mia Cheng"
                    },
                    {
                        "name": "Matthew Kay"
                    },
                    {
                        "name": "Fumeng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Fumeng Yang"
                },
                "author": "Fumeng Yang",
                "arxiv_doi": "10.1145/3772318.3790418"
            },
            {
                "id": "http://arxiv.org/abs/2512.16762v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.16762v2",
                "title": "NRGPT: An Energy-based Alternative for GPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NRGPT: An Energy-based Alternative for GPT"
                },
                "updated": "2026-02-25T18:23:01Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    23,
                    1,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.16762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.16762v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative Pre-trained Transformer (GPT) architectures are the most popular design for language modeling. Energy-based modeling is a different paradigm that views inference as a dynamical process operating on an energy landscape. We propose a minimal modification of the GPT setting to unify it with the EBM framework. The inference step of our model, which we call eNeRgy-GPT (NRGPT), is conceptualized as an exploration of the tokens on the energy landscape. We prove, and verify empirically, that under certain circumstances this exploration becomes gradient descent, although they don't necessarily lead to the best performing models. We demonstrate that our model performs well for simple language (Shakespeare dataset), algebraic ListOPS tasks, and richer settings such as OpenWebText language modeling. We also observe that our models may be more resistant to overfitting, doing so only during very long training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Pre-trained Transformer (GPT) architectures are the most popular design for language modeling. Energy-based modeling is a different paradigm that views inference as a dynamical process operating on an energy landscape. We propose a minimal modification of the GPT setting to unify it with the EBM framework. The inference step of our model, which we call eNeRgy-GPT (NRGPT), is conceptualized as an exploration of the tokens on the energy landscape. We prove, and verify empirically, that under certain circumstances this exploration becomes gradient descent, although they don't necessarily lead to the best performing models. We demonstrate that our model performs well for simple language (Shakespeare dataset), algebraic ListOPS tasks, and richer settings such as OpenWebText language modeling. We also observe that our models may be more resistant to overfitting, doing so only during very long training."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-18T16:59:10Z",
                "published_parsed": [
                    2025,
                    12,
                    18,
                    16,
                    59,
                    10,
                    3,
                    352,
                    0
                ],
                "arxiv_comment": "Accepted to ICLR 2026 main conference",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Nima Dehmamy"
                    },
                    {
                        "name": "Benjamin Hoover"
                    },
                    {
                        "name": "Bishwajit Saha"
                    },
                    {
                        "name": "Leo Kozachkov"
                    },
                    {
                        "name": "Jean-Jacques Slotine"
                    },
                    {
                        "name": "Dmitry Krotov"
                    }
                ],
                "author_detail": {
                    "name": "Dmitry Krotov"
                },
                "author": "Dmitry Krotov"
            },
            {
                "id": "http://arxiv.org/abs/2505.23725v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.23725v2",
                "title": "MuLoCo: Muon is a practical inner optimizer for DiLoCo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MuLoCo: Muon is a practical inner optimizer for DiLoCo"
                },
                "updated": "2026-02-25T18:22:58Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    22,
                    58,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.23725v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.23725v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "DiLoCo is a powerful framework for training large language models (LLMs), enabling larger optimal batch sizes and increased accelerator utilization under networking constraints. However, DiLoCo's performance has been shown to degrade as the number of workers (K) increases (Charles et al., 2025). In this work, we posit that a related but often overlooked factor in DiLoCo's behavior is the choice of inner optimizer, which shapes the pseudogradient used by the outer optimizer. Given the recent success of Muon relative to AdamW for data parallel (DP) training, we examine how Muon's normalized optimizer steps can affect the pseudogradient's quality. We find that, relative to AdamW, Muon yields more directionally correct pseudogradients as the number of workers (K) increases. In our experiments pre-training language models, we conduct extensive hyperparameter tuning across 150M, 416M, 914M, 1.76B, and 3.1B models for DiLoCo, MuLoCo, AdamW DP, and Muon DP. Consistently across all scales, we find that with K>=1 workers, MuLoCo (Muon inner optimizer DiLoCo) achieves superior performance to DiLoCo in absolute terms and for K>2 it outperforms DiLoCo relative to their data parallel baselines, while being compatible with quantization, streaming, and long synchronization intervals. At K=1, we find that MuLoCo can even outperform the data-parallel gold standard while having larger critical batch sizes. Finally, we extrapolate optimal hyperparameters to 15B scale and train a model with each method (six in total) using K=1 and K=16 workers. We find that K=16 MuLoCo nearly matches single-worker performance at this scale, while MuLoCo K=1 matches the best performing baseline while using a much larger 16M token batch size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiLoCo is a powerful framework for training large language models (LLMs), enabling larger optimal batch sizes and increased accelerator utilization under networking constraints. However, DiLoCo's performance has been shown to degrade as the number of workers (K) increases (Charles et al., 2025). In this work, we posit that a related but often overlooked factor in DiLoCo's behavior is the choice of inner optimizer, which shapes the pseudogradient used by the outer optimizer. Given the recent success of Muon relative to AdamW for data parallel (DP) training, we examine how Muon's normalized optimizer steps can affect the pseudogradient's quality. We find that, relative to AdamW, Muon yields more directionally correct pseudogradients as the number of workers (K) increases. In our experiments pre-training language models, we conduct extensive hyperparameter tuning across 150M, 416M, 914M, 1.76B, and 3.1B models for DiLoCo, MuLoCo, AdamW DP, and Muon DP. Consistently across all scales, we find that with K>=1 workers, MuLoCo (Muon inner optimizer DiLoCo) achieves superior performance to DiLoCo in absolute terms and for K>2 it outperforms DiLoCo relative to their data parallel baselines, while being compatible with quantization, streaming, and long synchronization intervals. At K=1, we find that MuLoCo can even outperform the data-parallel gold standard while having larger critical batch sizes. Finally, we extrapolate optimal hyperparameters to 15B scale and train a model with each method (six in total) using K=1 and K=16 workers. We find that K=16 MuLoCo nearly matches single-worker performance at this scale, while MuLoCo K=1 matches the best performing baseline while using a much larger 16M token batch size."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-29T17:55:37Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    55,
                    37,
                    3,
                    149,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Benjamin Thrien"
                    },
                    {
                        "name": "Xiaolong Huang"
                    },
                    {
                        "name": "Aaron Defazio"
                    },
                    {
                        "name": "Irina Rish"
                    },
                    {
                        "name": "Eugene Belilovsky"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Belilovsky"
                },
                "author": "Eugene Belilovsky"
            },
            {
                "id": "http://arxiv.org/abs/2508.01617v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.01617v2",
                "title": "LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical Image Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical Image Understanding"
                },
                "updated": "2026-02-25T18:15:23Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    15,
                    23,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.01617v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.01617v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Autoregressive models (ARMs) have long dominated the landscape of biomedical vision-language models (VLMs). Recently, masked diffusion models such as LLaDA have emerged as promising alternatives, yet their application in the biomedical domain remains largely underexplored. To bridge this gap, we introduce LLaDA-MedV, the first large language diffusion model tailored for biomedical image understanding through vision instruction tuning. LLaDA-MedV achieves relative performance gains of 7.855% over LLaVA-Med and 1.867% over LLaDA-V in the open-ended biomedical visual conversation task, and sets new state-of-the-art accuracy on the closed-form subset of three VQA benchmarks: 84.93% on VQA-RAD, 92.31% on SLAKE, and 95.15% on PathVQA. Furthermore, a detailed comparison with LLaVA-Med suggests that LLaDA-MedV is capable of generating reasonably longer responses by explicitly controlling response length, which can lead to more informative outputs. We also conduct an in-depth analysis of both the training and inference stages, highlighting the critical roles of initialization weight selection, fine-tuning strategies, and the interplay between sampling steps and response repetition. The code and model weight is released at https://github.com/LLM-VLM-GSL/LLaDA-MedV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive models (ARMs) have long dominated the landscape of biomedical vision-language models (VLMs). Recently, masked diffusion models such as LLaDA have emerged as promising alternatives, yet their application in the biomedical domain remains largely underexplored. To bridge this gap, we introduce LLaDA-MedV, the first large language diffusion model tailored for biomedical image understanding through vision instruction tuning. LLaDA-MedV achieves relative performance gains of 7.855% over LLaVA-Med and 1.867% over LLaDA-V in the open-ended biomedical visual conversation task, and sets new state-of-the-art accuracy on the closed-form subset of three VQA benchmarks: 84.93% on VQA-RAD, 92.31% on SLAKE, and 95.15% on PathVQA. Furthermore, a detailed comparison with LLaVA-Med suggests that LLaDA-MedV is capable of generating reasonably longer responses by explicitly controlling response length, which can lead to more informative outputs. We also conduct an in-depth analysis of both the training and inference stages, highlighting the critical roles of initialization weight selection, fine-tuning strategies, and the interplay between sampling steps and response repetition. The code and model weight is released at https://github.com/LLM-VLM-GSL/LLaDA-MedV."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-03T06:46:46Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    6,
                    46,
                    46,
                    6,
                    215,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Xuanzhao Dong"
                    },
                    {
                        "name": "Wenhui Zhu"
                    },
                    {
                        "name": "Xiwen Chen"
                    },
                    {
                        "name": "Zhipeng Wang"
                    },
                    {
                        "name": "Peijie Qiu"
                    },
                    {
                        "name": "Shao Tang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Yalin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yalin Wang"
                },
                "author": "Yalin Wang"
            },
            {
                "id": "http://arxiv.org/abs/2602.20156v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20156v3",
                "title": "Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks"
                },
                "updated": "2026-02-25T18:14:01Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    14,
                    1,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20156v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20156v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature. Skills allow users to extend LLM applications with specialized third-party code, knowledge, and instructions. Although this can extend agent capabilities to new domains, it creates an increasingly complex agent supply chain, offering new surfaces for prompt injection attacks. We identify skill-based prompt injection as a significant threat and introduce SkillInject, a benchmark evaluating the susceptibility of widely-used LLM agents to injections through skill files. SkillInject contains 202 injection-task pairs with attacks ranging from obviously malicious injections to subtle, context-dependent attacks hidden in otherwise legitimate instructions. We evaluate frontier LLMs on SkillInject, measuring both security in terms of harmful instruction avoidance and utility in terms of legitimate instruction compliance. Our results show that today's agents are highly vulnerable with up to 80% attack success rate with frontier models, often executing extremely harmful instructions including data exfiltration, destructive action, and ransomware-like behavior. They furthermore suggest that this problem will not be solved through model scaling or simple input filtering, but that robust agent security will require context-aware authorization frameworks. Our benchmark is available at https://www.skill-inject.com/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature. Skills allow users to extend LLM applications with specialized third-party code, knowledge, and instructions. Although this can extend agent capabilities to new domains, it creates an increasingly complex agent supply chain, offering new surfaces for prompt injection attacks. We identify skill-based prompt injection as a significant threat and introduce SkillInject, a benchmark evaluating the susceptibility of widely-used LLM agents to injections through skill files. SkillInject contains 202 injection-task pairs with attacks ranging from obviously malicious injections to subtle, context-dependent attacks hidden in otherwise legitimate instructions. We evaluate frontier LLMs on SkillInject, measuring both security in terms of harmful instruction avoidance and utility in terms of legitimate instruction compliance. Our results show that today's agents are highly vulnerable with up to 80% attack success rate with frontier models, often executing extremely harmful instructions including data exfiltration, destructive action, and ransomware-like behavior. They furthermore suggest that this problem will not be solved through model scaling or simple input filtering, but that robust agent security will require context-aware authorization frameworks. Our benchmark is available at https://www.skill-inject.com/."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:59:27Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    59,
                    27,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "David Schmotz"
                    },
                    {
                        "name": "Luca Beurer-Kellner"
                    },
                    {
                        "name": "Sahar Abdelnabi"
                    },
                    {
                        "name": "Maksym Andriushchenko"
                    }
                ],
                "author_detail": {
                    "name": "Maksym Andriushchenko"
                },
                "author": "Maksym Andriushchenko"
            },
            {
                "id": "http://arxiv.org/abs/2602.18182v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.18182v2",
                "title": "Capabilities Ain't All You Need: Measuring Propensities in AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capabilities Ain't All You Need: Measuring Propensities in AI"
                },
                "updated": "2026-02-25T18:12:06Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    12,
                    6,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.18182v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.18182v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "AI evaluation has primarily focused on measuring capabilities, with formal approaches inspired from Item Response Theory (IRT) being increasingly applied. Yet propensities - the tendencies of models to exhibit particular behaviours - play a central role in determining both performance and safety outcomes. However, traditional IRT describes a model's success on a task as a monotonic function of model capabilities and task demands, an approach unsuited to propensities, where both excess and deficiency can be problematic. Here, we introduce the first formal framework for measuring AI propensities by using a bilogistic formulation for model success, which attributes high success probability when the model's propensity is within an \"ideal band\". Further, we estimate the limits of the ideal band using LLMs equipped with newly developed task-agnostic rubrics. Applying our framework to six families of LLM models whose propensities are incited in either direction, we find that we can measure how much the propensity is shifted and what effect this has on the tasks. Critically, propensities estimated using one benchmark successfully predict behaviour on held-out tasks. Moreover, we obtain stronger predictive power when combining propensities and capabilities than either separately. More broadly, our framework showcases how rigorous propensity measurements can be conducted and how it yields gains over solely using capability evaluations to predict AI behaviour.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI evaluation has primarily focused on measuring capabilities, with formal approaches inspired from Item Response Theory (IRT) being increasingly applied. Yet propensities - the tendencies of models to exhibit particular behaviours - play a central role in determining both performance and safety outcomes. However, traditional IRT describes a model's success on a task as a monotonic function of model capabilities and task demands, an approach unsuited to propensities, where both excess and deficiency can be problematic. Here, we introduce the first formal framework for measuring AI propensities by using a bilogistic formulation for model success, which attributes high success probability when the model's propensity is within an \"ideal band\". Further, we estimate the limits of the ideal band using LLMs equipped with newly developed task-agnostic rubrics. Applying our framework to six families of LLM models whose propensities are incited in either direction, we find that we can measure how much the propensity is shifted and what effect this has on the tasks. Critically, propensities estimated using one benchmark successfully predict behaviour on held-out tasks. Moreover, we obtain stronger predictive power when combining propensities and capabilities than either separately. More broadly, our framework showcases how rigorous propensity measurements can be conducted and how it yields gains over solely using capability evaluations to predict AI behaviour."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-20T12:40:18Z",
                "published_parsed": [
                    2026,
                    2,
                    20,
                    12,
                    40,
                    18,
                    4,
                    51,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Daniel Romero-Alvarado"
                    },
                    {
                        "name": "Fernando Martnez-Plumed"
                    },
                    {
                        "name": "Lorenzo Pacchiardi"
                    },
                    {
                        "name": "Hugo Save"
                    },
                    {
                        "name": "Siddhesh Milind Pawar"
                    },
                    {
                        "name": "Behzad Mehrbakhsh"
                    },
                    {
                        "name": "Pablo Antonio Moreno Casares"
                    },
                    {
                        "name": "Ben Slater"
                    },
                    {
                        "name": "Paolo Bova"
                    },
                    {
                        "name": "Peter Romero"
                    },
                    {
                        "name": "Zachary R. Tyler"
                    },
                    {
                        "name": "Jonathan Prunty"
                    },
                    {
                        "name": "Luning Sun"
                    },
                    {
                        "name": "Jose Hernandez-Orallo"
                    }
                ],
                "author_detail": {
                    "name": "Jose Hernandez-Orallo"
                },
                "author": "Jose Hernandez-Orallo"
            },
            {
                "id": "http://arxiv.org/abs/2602.18671v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.18671v2",
                "title": "Spilled Energy in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spilled Energy in Large Language Models"
                },
                "updated": "2026-02-25T18:09:08Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    9,
                    8,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.18671v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.18671v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We reinterpret the final Large Language Model (LLM) softmax classifier as an Energy-Based Model (EBM), decomposing the sequence-to-sequence probability chain into multiple interacting EBMs at inference. This principled approach allows us to track \"energy spills\" during decoding, which we empirically show correlate with factual errors, biases, and failures. Similar to Orgad et al. (2025), our method localizes the exact answer token and subsequently tests for hallucinations. Crucially, however, we achieve this without requiring trained probe classifiers or activation ablations. Instead, we introduce two completely training-free metrics derived directly from output logits: spilled energy, which captures the discrepancy between energy values across consecutive generation steps that should theoretically match, and marginalized energy, which is measurable at a single step. Evaluated on nine benchmarks across state-of-the-art LLMs (including LLaMA, Mistral, and Gemma) and on synthetic algebraic operations (Qwen3), our approach demonstrates robust, competitive hallucination detection and cross-task generalization. Notably, these results hold for both pretrained and instruction-tuned variants without introducing any training overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We reinterpret the final Large Language Model (LLM) softmax classifier as an Energy-Based Model (EBM), decomposing the sequence-to-sequence probability chain into multiple interacting EBMs at inference. This principled approach allows us to track \"energy spills\" during decoding, which we empirically show correlate with factual errors, biases, and failures. Similar to Orgad et al. (2025), our method localizes the exact answer token and subsequently tests for hallucinations. Crucially, however, we achieve this without requiring trained probe classifiers or activation ablations. Instead, we introduce two completely training-free metrics derived directly from output logits: spilled energy, which captures the discrepancy between energy values across consecutive generation steps that should theoretically match, and marginalized energy, which is measurable at a single step. Evaluated on nine benchmarks across state-of-the-art LLMs (including LLaMA, Mistral, and Gemma) and on synthetic algebraic operations (Qwen3), our approach demonstrates robust, competitive hallucination detection and cross-task generalization. Notably, these results hold for both pretrained and instruction-tuned variants without introducing any training overhead."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-21T00:38:47Z",
                "published_parsed": [
                    2026,
                    2,
                    21,
                    0,
                    38,
                    47,
                    5,
                    52,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Adrian Robert Minut"
                    },
                    {
                        "name": "Hazem Dewidar"
                    },
                    {
                        "name": "Iacopo Masi"
                    }
                ],
                "author_detail": {
                    "name": "Iacopo Masi"
                },
                "author": "Iacopo Masi"
            },
            {
                "id": "http://arxiv.org/abs/2602.22159v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22159v1",
                "title": "CASR: A Robust Cyclic Framework for Arbitrary Large-Scale Super-Resolution with Distribution Alignment and Self-Similarity Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CASR: A Robust Cyclic Framework for Arbitrary Large-Scale Super-Resolution with Distribution Alignment and Self-Similarity Awareness"
                },
                "updated": "2026-02-25T18:05:51Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    5,
                    51,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22159v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Arbitrary-Scale SR (ASISR) remains fundamentally limited by cross-scale distribution shift: once the inference scale leaves the training range, noise, blur, and artifacts accumulate sharply. We revisit this challenge from a cross-scale distribution transition perspective and propose CASR, a simple yet highly efficient cyclic SR framework that reformulates ultra-magnification as a sequence of in-distribution scale transitions. This design ensures stable inference at arbitrary scales while requiring only a single model. CASR tackles two major bottlenecks: distribution drift across iterations and patch-wise diffusion inconsistencies. The proposed SDAM module aligns structural distributions via superpixel aggregation, preventing error accumulation, while SARM module restores high-frequency textures by enforcing autocorrelation and embedding LR self-similarity priors. Despite using only a single model, our approach significantly reduces distribution drift, preserves long-range texture consistency, and achieves superior generalization even at extreme magnification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arbitrary-Scale SR (ASISR) remains fundamentally limited by cross-scale distribution shift: once the inference scale leaves the training range, noise, blur, and artifacts accumulate sharply. We revisit this challenge from a cross-scale distribution transition perspective and propose CASR, a simple yet highly efficient cyclic SR framework that reformulates ultra-magnification as a sequence of in-distribution scale transitions. This design ensures stable inference at arbitrary scales while requiring only a single model. CASR tackles two major bottlenecks: distribution drift across iterations and patch-wise diffusion inconsistencies. The proposed SDAM module aligns structural distributions via superpixel aggregation, preventing error accumulation, while SARM module restores high-frequency textures by enforcing autocorrelation and embedding LR self-similarity priors. Despite using only a single model, our approach significantly reduces distribution drift, preserves long-range texture consistency, and achieves superior generalization even at extreme magnification."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T18:05:51Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    5,
                    51,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Wenhao Guo"
                    },
                    {
                        "name": "Zhaoran Zhao"
                    },
                    {
                        "name": "Peng Lu"
                    },
                    {
                        "name": "Sheng Li"
                    },
                    {
                        "name": "Qian Qiao"
                    },
                    {
                        "name": "RuiDe Li"
                    }
                ],
                "author_detail": {
                    "name": "RuiDe Li"
                },
                "author": "RuiDe Li"
            },
            {
                "id": "http://arxiv.org/abs/2602.22158v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22158v1",
                "title": "LLMTailor: A Layer-wise Tailoring Tool for Efficient Checkpointing of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMTailor: A Layer-wise Tailoring Tool for Efficient Checkpointing of Large Language Models"
                },
                "updated": "2026-02-25T18:05:17Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    5,
                    17,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22158v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3731599.3767515",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Checkpointing is essential for fault tolerance in training large language models (LLMs). However, existing methods, regardless of their I/O strategies, periodically store the entire model and optimizer states, incurring substantial storage overhead and resource contention. Recent studies reveal that updates across LLM layers are highly non-uniform. Across training steps, some layers may undergo more significant changes, while others remain relatively stable or even unchanged. This suggests that selectively checkpointing only layers with significant updates could reduce overhead without harming training. Implementing such selective strategies requires fine-grained control over both weights and optimizer states, which no current tool provides. To address this gap, we propose \\texttt{LLMTailor}, a checkpoint-merging framework that filters and assembles layers from different checkpoints to form a composite checkpoint. Our evaluation indicates that LLMTailor can work with different selective checkpointing strategies and effectively reduce checkpoint size (e.g., 4.3 times smaller for Llama3.1-8B) and checkpoint time (e.g., 2.8 times faster for Qwen2.5-7B) while maintaining model quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Checkpointing is essential for fault tolerance in training large language models (LLMs). However, existing methods, regardless of their I/O strategies, periodically store the entire model and optimizer states, incurring substantial storage overhead and resource contention. Recent studies reveal that updates across LLM layers are highly non-uniform. Across training steps, some layers may undergo more significant changes, while others remain relatively stable or even unchanged. This suggests that selectively checkpointing only layers with significant updates could reduce overhead without harming training. Implementing such selective strategies requires fine-grained control over both weights and optimizer states, which no current tool provides. To address this gap, we propose \\texttt{LLMTailor}, a checkpoint-merging framework that filters and assembles layers from different checkpoints to form a composite checkpoint. Our evaluation indicates that LLMTailor can work with different selective checkpointing strategies and effectively reduce checkpoint size (e.g., 4.3 times smaller for Llama3.1-8B) and checkpoint time (e.g., 2.8 times faster for Qwen2.5-7B) while maintaining model quality."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T18:05:17Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    5,
                    17,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "9 pages, 3 figures, accepted at PDSW'25",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Minqiu Sun"
                    },
                    {
                        "name": "Xin Huang"
                    },
                    {
                        "name": "Luanzheng Guo"
                    },
                    {
                        "name": "Nathan R. Tallent"
                    },
                    {
                        "name": "Kento Sato"
                    },
                    {
                        "name": "Dong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Dong Dai"
                },
                "author": "Dong Dai",
                "arxiv_doi": "10.1145/3731599.3767515"
            },
            {
                "id": "http://arxiv.org/abs/2602.22157v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22157v1",
                "title": "Dynamic Personality Adaptation in Large Language Models via State Machines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Personality Adaptation in Large Language Models via State Machines"
                },
                "updated": "2026-02-25T18:05:11Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    5,
                    11,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22157v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The inability of Large Language Models (LLMs) to modulate their personality expression in response to evolving dialogue dynamics hinders their performance in complex, interactive contexts. We propose a model-agnostic framework for dynamic personality simulation that employs state machines to represent latent personality states, where transition probabilities are dynamically adapted to the conversational context. Part of our architecture is a modular pipeline for continuous personality scoring that evaluates dialogues along latent axes while remaining agnostic to the specific personality models, their dimensions, transition mechanisms, or LLMs used. These scores function as dynamic state variables that systematically reconfigure the system prompt, steering behavioral alignment throughout the interaction.We evaluate this framework by operationalizing the Interpersonal Circumplex (IPC) in a medical education setting. Results demonstrate that the system successfully adapts its personality state to user inputs, but also influences user behavior, thereby facilitating de-escalation training. Notably, the scoring pipeline maintains comparable precision even when utilizing lightweight, fine-tuned classifiers instead of large-scale LLMs. This work demonstrates the feasibility of modular, personality-adaptive architectures for education, customer support, and broader human-computer interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inability of Large Language Models (LLMs) to modulate their personality expression in response to evolving dialogue dynamics hinders their performance in complex, interactive contexts. We propose a model-agnostic framework for dynamic personality simulation that employs state machines to represent latent personality states, where transition probabilities are dynamically adapted to the conversational context. Part of our architecture is a modular pipeline for continuous personality scoring that evaluates dialogues along latent axes while remaining agnostic to the specific personality models, their dimensions, transition mechanisms, or LLMs used. These scores function as dynamic state variables that systematically reconfigure the system prompt, steering behavioral alignment throughout the interaction.We evaluate this framework by operationalizing the Interpersonal Circumplex (IPC) in a medical education setting. Results demonstrate that the system successfully adapts its personality state to user inputs, but also influences user behavior, thereby facilitating de-escalation training. Notably, the scoring pipeline maintains comparable precision even when utilizing lightweight, fine-tuned classifiers instead of large-scale LLMs. This work demonstrates the feasibility of modular, personality-adaptive architectures for education, customer support, and broader human-computer interaction."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T18:05:11Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    5,
                    11,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "22 pages, 5 figures, submitted to ICPR 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Leon Pielage"
                    },
                    {
                        "name": "Ole Htscher"
                    },
                    {
                        "name": "Mitja Back"
                    },
                    {
                        "name": "Bernhard Marschall"
                    },
                    {
                        "name": "Benjamin Risse"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Risse"
                },
                "author": "Benjamin Risse"
            },
            {
                "id": "http://arxiv.org/abs/2602.17849v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.17849v2",
                "title": "Quad Length Codes for Lossless Compression of e4m3",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quad Length Codes for Lossless Compression of e4m3"
                },
                "updated": "2026-02-25T17:58:32Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    58,
                    32,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.17849v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.17849v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Training and serving Large Language Models (LLMs) relies heavily on parallelization and collective operations, which are frequently bottlenecked by network bandwidth. Lossless compression using e.g., Huffman codes can alleviate the issue, however, Huffman codes suffer from slow, bit-sequential decoding and high hardware complexity due to deep tree traversals. Universal codes e.g., Exponential-Golomb codes are faster to decode but do not exploit the symbol frequency distributions. To address these limitations, this paper introduces Quad Length Codes, a hybrid approach designed to balance compression efficiency with decoding speed. The coding scheme uses 3 prefix bits to divide the 256 symbols into 8 areas. Each area has a different code length and encodes a different number of symbols. The scheme uses a Look Up Table with 256 entries, significantly simplifying the hardware implementation compared to Huffman trees. The coding scheme can be adapted for different distributions. For the e4m3 data type, the scheme achieves a compressibility of 13.9% in comparison to 15.9% achieved by Huffman codes, but it significantly speeds up the decoding and simplifies the hardware complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and serving Large Language Models (LLMs) relies heavily on parallelization and collective operations, which are frequently bottlenecked by network bandwidth. Lossless compression using e.g., Huffman codes can alleviate the issue, however, Huffman codes suffer from slow, bit-sequential decoding and high hardware complexity due to deep tree traversals. Universal codes e.g., Exponential-Golomb codes are faster to decode but do not exploit the symbol frequency distributions. To address these limitations, this paper introduces Quad Length Codes, a hybrid approach designed to balance compression efficiency with decoding speed. The coding scheme uses 3 prefix bits to divide the 256 symbols into 8 areas. Each area has a different code length and encodes a different number of symbols. The scheme uses a Look Up Table with 256 entries, significantly simplifying the hardware implementation compared to Huffman trees. The coding scheme can be adapted for different distributions. For the e4m3 data type, the scheme achieves a compressibility of 13.9% in comparison to 15.9% achieved by Huffman codes, but it significantly speeds up the decoding and simplifies the hardware complexity."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-19T21:31:33Z",
                "published_parsed": [
                    2026,
                    2,
                    19,
                    21,
                    31,
                    33,
                    3,
                    50,
                    0
                ],
                "arxiv_comment": "The first version proposed lossless compression of BFloat16 using dual length codes. This version proposes lossless compression of e4m3 using quad length codes. The versions will be merged later",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Aditya Agrawal"
                    },
                    {
                        "name": "Albert Magyar"
                    },
                    {
                        "name": "Hiteshwar Eswaraiah"
                    },
                    {
                        "name": "Patrick Sheridan"
                    },
                    {
                        "name": "Pradeep Janedula"
                    },
                    {
                        "name": "Ravi Krishnan Venkatesan"
                    },
                    {
                        "name": "Krishna Nair"
                    },
                    {
                        "name": "Ravi Iyer"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Iyer"
                },
                "author": "Ravi Iyer"
            },
            {
                "id": "http://arxiv.org/abs/2602.22146v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22146v1",
                "title": "Provable Last-Iterate Convergence for Multi-Objective Safe LLM Alignment via Optimistic Primal-Dual",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provable Last-Iterate Convergence for Multi-Objective Safe LLM Alignment via Optimistic Primal-Dual"
                },
                "updated": "2026-02-25T17:54:52Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    54,
                    52,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22146v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reinforcement Learning from Human Feedback (RLHF) plays a significant role in aligning Large Language Models (LLMs) with human preferences. While RLHF with expected reward constraints can be formulated as a primal-dual optimization problem, standard primal-dual methods only guarantee convergence with a distributional policy where the saddle-point problem is in convex-concave form. Moreover, standard primal-dual methods may exhibit instability or divergence in the last iterate under policy parameterization in practical applications. In this work, we propose a universal primal-dual framework for safe RLHF that unifies a broad class of existing alignment algorithms, including safe-RLHF, one-shot, and multi-shot based methods. Building on this framework, we introduce an optimistic primal-dual (OPD) algorithm that incorporates predictive updates for both primal and dual variables to stabilize saddle-point dynamics. We establish last-iterate convergence guarantees for the proposed method, covering both exact policy optimization in the distributional space and convergence to a neighborhood of the optimal solution whose gap is related to approximation error and bias under parameterized policies. Our analysis reveals that optimism plays a crucial role in mitigating oscillations inherent to constrained alignment objectives, thereby closing a key theoretical gap between constrained RL and practical RLHF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) plays a significant role in aligning Large Language Models (LLMs) with human preferences. While RLHF with expected reward constraints can be formulated as a primal-dual optimization problem, standard primal-dual methods only guarantee convergence with a distributional policy where the saddle-point problem is in convex-concave form. Moreover, standard primal-dual methods may exhibit instability or divergence in the last iterate under policy parameterization in practical applications. In this work, we propose a universal primal-dual framework for safe RLHF that unifies a broad class of existing alignment algorithms, including safe-RLHF, one-shot, and multi-shot based methods. Building on this framework, we introduce an optimistic primal-dual (OPD) algorithm that incorporates predictive updates for both primal and dual variables to stabilize saddle-point dynamics. We establish last-iterate convergence guarantees for the proposed method, covering both exact policy optimization in the distributional space and convergence to a neighborhood of the optimal solution whose gap is related to approximation error and bias under parameterized policies. Our analysis reveals that optimism plays a crucial role in mitigating oscillations inherent to constrained alignment objectives, thereby closing a key theoretical gap between constrained RL and practical RLHF."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T17:54:52Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    54,
                    52,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yining Li"
                    },
                    {
                        "name": "Peizhong Ju"
                    },
                    {
                        "name": "Ness Shroff"
                    }
                ],
                "author_detail": {
                    "name": "Ness Shroff"
                },
                "author": "Ness Shroff"
            },
            {
                "id": "http://arxiv.org/abs/2602.22145v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22145v1",
                "title": "When AI Writes, Whose Voice Remains? Quantifying Cultural Marker Erasure Across World English Varieties in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When AI Writes, Whose Voice Remains? Quantifying Cultural Marker Erasure Across World English Varieties in Large Language Models"
                },
                "updated": "2026-02-25T17:54:42Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    54,
                    42,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22145v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3772363.3799085",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Large Language Models (LLMs) are increasingly used to ``professionalize'' workplace communication, often at the cost of linguistic identity. We introduce \"Cultural Ghosting\", the systematic erasure of linguistic markers unique to non-native English varieties during text processing. Through analysis of 22,350 LLM outputs generated from 1,490 culturally marked texts (Indian, Singaporean,& Nigerian English) processed by five models under three prompt conditions, we quantify this phenomenon using two novel metrics: Identity Erasure Rate (IER) & Semantic Preservation Score (SPS). Across all prompts, we find an overall IER of 10.26%, with model-level variation from 3.5% to 20.5% (5.9x range). Crucially, we identify a Semantic Preservation Paradox: models maintain high semantic similarity (mean SPS = 0.748) while systematically erasing cultural markers. Pragmatic markers (politeness conventions) are 1.9x more vulnerable than lexical markers (71.5% vs. 37.1% erasure). Our experiments demonstrate that explicit cultural-preservation prompts reduce erasure by 29% without sacrificing semantic quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used to ``professionalize'' workplace communication, often at the cost of linguistic identity. We introduce \"Cultural Ghosting\", the systematic erasure of linguistic markers unique to non-native English varieties during text processing. Through analysis of 22,350 LLM outputs generated from 1,490 culturally marked texts (Indian, Singaporean,& Nigerian English) processed by five models under three prompt conditions, we quantify this phenomenon using two novel metrics: Identity Erasure Rate (IER) & Semantic Preservation Score (SPS). Across all prompts, we find an overall IER of 10.26%, with model-level variation from 3.5% to 20.5% (5.9x range). Crucially, we identify a Semantic Preservation Paradox: models maintain high semantic similarity (mean SPS = 0.748) while systematically erasing cultural markers. Pragmatic markers (politeness conventions) are 1.9x more vulnerable than lexical markers (71.5% vs. 37.1% erasure). Our experiments demonstrate that explicit cultural-preservation prompts reduce erasure by 29% without sacrificing semantic quality."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T17:54:42Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    54,
                    42,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Satyam Kumar Navneet"
                    },
                    {
                        "name": "Joydeep Chandra"
                    },
                    {
                        "name": "Yong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yong Zhang"
                },
                "author": "Yong Zhang",
                "arxiv_doi": "10.1145/3772363.3799085"
            },
            {
                "id": "http://arxiv.org/abs/2510.26656v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.26656v3",
                "title": "Heuristic Adaptation of Potentially Misspecified Domain Support for Likelihood-Free Inference in Stochastic Dynamical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heuristic Adaptation of Potentially Misspecified Domain Support for Likelihood-Free Inference in Stochastic Dynamical Systems"
                },
                "updated": "2026-02-25T17:52:16Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    52,
                    16,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.26656v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.26656v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In robotics, likelihood-free inference (LFI) can provide the domain distribution that adapts a learnt agent in a parametric set of deployment conditions. LFI assumes an arbitrary support for sampling, which remains constant as the initial generic prior is iteratively refined to more descriptive posteriors. However, a potentially misspecified support can lead to suboptimal, yet falsely certain, posteriors. To address this issue, we propose three heuristic LFI variants: EDGE, MODE, and CENTRE. Each interprets the posterior mode shift over inference steps in its own way and, when integrated into an LFI step, adapts the support alongside posterior inference. We first expose the support misspecification issue and evaluate our heuristics using stochastic dynamical benchmarks. We then evaluate the impact of heuristic support adaptation on parameter inference and policy learning for a dynamic deformable linear object (DLO) manipulation task. Inference results in a finer length and stiffness classification for a parametric set of DLOs. When the resulting posteriors are used as domain distributions for sim-based policy learning, they lead to more robust object-centric agent performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In robotics, likelihood-free inference (LFI) can provide the domain distribution that adapts a learnt agent in a parametric set of deployment conditions. LFI assumes an arbitrary support for sampling, which remains constant as the initial generic prior is iteratively refined to more descriptive posteriors. However, a potentially misspecified support can lead to suboptimal, yet falsely certain, posteriors. To address this issue, we propose three heuristic LFI variants: EDGE, MODE, and CENTRE. Each interprets the posterior mode shift over inference steps in its own way and, when integrated into an LFI step, adapts the support alongside posterior inference. We first expose the support misspecification issue and evaluate our heuristics using stochastic dynamical benchmarks. We then evaluate the impact of heuristic support adaptation on parameter inference and policy learning for a dynamic deformable linear object (DLO) manipulation task. Inference results in a finer length and stiffness classification for a parametric set of DLOs. When the resulting posteriors are used as domain distributions for sim-based policy learning, they lead to more robust object-centric agent performance."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-30T16:23:46Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    23,
                    46,
                    3,
                    303,
                    0
                ],
                "arxiv_comment": "20 pages, 18 figures",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Georgios Kamaras"
                    },
                    {
                        "name": "Craig Innes"
                    },
                    {
                        "name": "Subramanian Ramamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Subramanian Ramamoorthy"
                },
                "author": "Subramanian Ramamoorthy"
            },
            {
                "id": "http://arxiv.org/abs/2512.07793v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.07793v2",
                "title": "Hot, Photoionized X-ray Gas in Two Luminous Type 2 Quasars: Chandra-HST Evidence for a Wind-Driven Sequence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hot, Photoionized X-ray Gas in Two Luminous Type 2 Quasars: Chandra-HST Evidence for a Wind-Driven Sequence"
                },
                "updated": "2026-02-25T17:47:05Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    47,
                    5,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.07793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.07793v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present new Chandra/ACIS-S imaging spectroscopy of two luminous type 2 quasars, FIRST J120041.4+314745 ($z$=0.116) and 2MASX J13003807+5454367 ($z$=0.088), and compare their X-ray emission with Hubble Space Telescope [O III]$$5007 morphologies and kinematics. Both systems show kiloparsec-scale soft X-ray emission. In FIRST J120041, the X-ray morphology is clumpy and closely follows the [O III] structures, with surface-brightness peaks co-spatial with the highest [O III] velocities (600-750 km s$^{-1}$) and broadest line widths ($\\sim$1700 km s$^{-1}$). In 2MASX J130038, the X-ray emission is centrally concentrated and weakly correlated with rotational [O III] kinematics. Spectral modeling indicates that photoionization dominates the soft X-rays in both quasars. The inferred hot-gas reservoirs are substantial, $M_{\\rm x-ray}\\sim4.5\\times10^{8}M_\\odot$ (FIRST J120041) and $M_{\\rm x-ray}\\sim1.8\\times10^{8}M_\\odot$ (2MASX J130038), exceeding the outflowing [O III] masses by factors of $\\sim$4 and $\\sim$16. In 2MASX J130038, we identify a tentative blueshifted Fe XXVI Ly$$ line at $E_{\\rm rest}=7.14\\pm0.06$ keV ($v\\sim7600$ km s$^{-1}$), consistent with a hot wind confined to the inner few hundred parsecs. Combining these results with a broader sample of twelve type 2 quasars, we argue that luminous quasars evolve along a continuous feedback sequence regulated by progressive clearing of circumnuclear gas. As AGN radiation and winds pierce the surrounding medium, systems transition from heavily enshrouded, compact configurations to phases where the X-ray and [O III] components strongly couple and, eventually, to energetically dominant outflows. FIRST J120041 and 2MASX J130038 represent two points along this sequence, tracing the emergence and growth of hot winds as primary drivers of quasar-scale feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present new Chandra/ACIS-S imaging spectroscopy of two luminous type 2 quasars, FIRST J120041.4+314745 ($z$=0.116) and 2MASX J13003807+5454367 ($z$=0.088), and compare their X-ray emission with Hubble Space Telescope [O III]$$5007 morphologies and kinematics. Both systems show kiloparsec-scale soft X-ray emission. In FIRST J120041, the X-ray morphology is clumpy and closely follows the [O III] structures, with surface-brightness peaks co-spatial with the highest [O III] velocities (600-750 km s$^{-1}$) and broadest line widths ($\\sim$1700 km s$^{-1}$). In 2MASX J130038, the X-ray emission is centrally concentrated and weakly correlated with rotational [O III] kinematics. Spectral modeling indicates that photoionization dominates the soft X-rays in both quasars. The inferred hot-gas reservoirs are substantial, $M_{\\rm x-ray}\\sim4.5\\times10^{8}M_\\odot$ (FIRST J120041) and $M_{\\rm x-ray}\\sim1.8\\times10^{8}M_\\odot$ (2MASX J130038), exceeding the outflowing [O III] masses by factors of $\\sim$4 and $\\sim$16. In 2MASX J130038, we identify a tentative blueshifted Fe XXVI Ly$$ line at $E_{\\rm rest}=7.14\\pm0.06$ keV ($v\\sim7600$ km s$^{-1}$), consistent with a hot wind confined to the inner few hundred parsecs. Combining these results with a broader sample of twelve type 2 quasars, we argue that luminous quasars evolve along a continuous feedback sequence regulated by progressive clearing of circumnuclear gas. As AGN radiation and winds pierce the surrounding medium, systems transition from heavily enshrouded, compact configurations to phases where the X-ray and [O III] components strongly couple and, eventually, to energetically dominant outflows. FIRST J120041 and 2MASX J130038 represent two points along this sequence, tracing the emergence and growth of hot winds as primary drivers of quasar-scale feedback."
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-08T18:24:01Z",
                "published_parsed": [
                    2025,
                    12,
                    8,
                    18,
                    24,
                    1,
                    0,
                    342,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA"
                },
                "authors": [
                    {
                        "name": "Anna Trindade Falco"
                    },
                    {
                        "name": "S. Kraemer"
                    },
                    {
                        "name": "L. Feuillet"
                    },
                    {
                        "name": "R. Middei"
                    },
                    {
                        "name": "T. J. Turner"
                    },
                    {
                        "name": "J. Reeves"
                    },
                    {
                        "name": "V. Braito"
                    },
                    {
                        "name": "A. Ptak"
                    },
                    {
                        "name": "H. R. Schmitt"
                    },
                    {
                        "name": "T. C. Fischer"
                    },
                    {
                        "name": "D. M. Crenshaw"
                    },
                    {
                        "name": "Luis C. Ho"
                    },
                    {
                        "name": "M. Revalski"
                    },
                    {
                        "name": "T. Storchi-Bergmann"
                    },
                    {
                        "name": "M. Vestergaard"
                    },
                    {
                        "name": "C. M. Gaskell"
                    },
                    {
                        "name": "W. P. Maksym"
                    },
                    {
                        "name": "M. Elvis"
                    },
                    {
                        "name": "M. J. Ward"
                    },
                    {
                        "name": "H. Netzer"
                    }
                ],
                "author_detail": {
                    "name": "H. Netzer"
                },
                "author": "H. Netzer"
            },
            {
                "id": "http://arxiv.org/abs/2602.22142v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22142v1",
                "title": "WeaveTime: Stream from Earlier Frames into Emergent Memory in VideoLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeaveTime: Stream from Earlier Frames into Emergent Memory in VideoLLMs"
                },
                "updated": "2026-02-25T17:45:45Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    45,
                    45,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22142v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in Multimodal Large Language Models have greatly improved visual understanding and reasoning, yet their quadratic attention and offline training protocols make them ill-suited for streaming settings where frames arrive sequentially and future observations are inaccessible. We diagnose a core limitation of current Video-LLMs, namely Time-Agnosticism, in which videos are treated as an unordered bag of evidence rather than a causally ordered sequence, yielding two failures in streams: temporal order ambiguity, in which the model cannot follow or reason over the correct chronological order, and past-current focus blindness where it fails to distinguish present observations from accumulated history. We present WeaveTime, a simple, efficient, and model agnostic framework that first teaches order and then uses order. We introduce a lightweight Temporal Reconstruction objective-our Streaming Order Perception enhancement-that instills order aware representations with minimal finetuning and no specialized streaming data. At inference, a Past-Current Dynamic Focus Cache performs uncertainty triggered, coarse-to-fine retrieval, expanding history only when needed. Plugged into exsiting Video-LLM without architectural changes, WeaveTime delivers consistent gains on representative streaming benchmarks, improving accuracy while reducing latency. These results establish WeaveTime as a practical path toward time aware stream Video-LLMs under strict online, time causal constraints. Code and weights will be made publicly available. Project Page: https://zhangyl4.github.io/publications/weavetime/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Multimodal Large Language Models have greatly improved visual understanding and reasoning, yet their quadratic attention and offline training protocols make them ill-suited for streaming settings where frames arrive sequentially and future observations are inaccessible. We diagnose a core limitation of current Video-LLMs, namely Time-Agnosticism, in which videos are treated as an unordered bag of evidence rather than a causally ordered sequence, yielding two failures in streams: temporal order ambiguity, in which the model cannot follow or reason over the correct chronological order, and past-current focus blindness where it fails to distinguish present observations from accumulated history. We present WeaveTime, a simple, efficient, and model agnostic framework that first teaches order and then uses order. We introduce a lightweight Temporal Reconstruction objective-our Streaming Order Perception enhancement-that instills order aware representations with minimal finetuning and no specialized streaming data. At inference, a Past-Current Dynamic Focus Cache performs uncertainty triggered, coarse-to-fine retrieval, expanding history only when needed. Plugged into exsiting Video-LLM without architectural changes, WeaveTime delivers consistent gains on representative streaming benchmarks, improving accuracy while reducing latency. These results establish WeaveTime as a practical path toward time aware stream Video-LLMs under strict online, time causal constraints. Code and weights will be made publicly available. Project Page: https://zhangyl4.github.io/publications/weavetime/"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T17:45:45Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    45,
                    45,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "Accepted at CVPR 2026 (preview; camera-ready in preparation)",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yulin Zhang"
                    },
                    {
                        "name": "Cheng Shi"
                    },
                    {
                        "name": "Sibei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Sibei Yang"
                },
                "author": "Sibei Yang"
            },
            {
                "id": "http://arxiv.org/abs/2311.11216v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2311.11216v5",
                "title": "Reconciling Overt Bias and Hidden Bias in Sensitivity Analysis for Matched Observational Studies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconciling Overt Bias and Hidden Bias in Sensitivity Analysis for Matched Observational Studies"
                },
                "updated": "2026-02-25T17:42:17Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    42,
                    17,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2311.11216v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2311.11216v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Matching is one of the most widely used causal inference designs in observational studies, but post-matching confounding bias remains a critical concern. This bias includes overt bias from inexact matching on measured confounders and hidden bias from unmeasured confounders. Researchers routinely apply the famous Rosenbaum-type sensitivity analysis after matching to assess the impact of these biases on causal conclusions. In this work, we show that this approach is often conservative and may overstate sensitivity to confounding bias because the classical solution to the Rosenbaum sensitivity model may allocate hypothetical hidden bias in ways that contradict the overt bias observed in the matched dataset. To address this problem, we propose a new approach to Rosenbaum-type sensitivity analysis by ensuring compatibility between hidden and overt biases. Our approach does not need to add any additional assumptions (beyond mild regularity conditions) to Rosenbaum-type sensitivity analysis, and can produce uniformly more informative sensitivity analysis results than the conventional Rosenbaum-type sensitivity analysis. Computationally, our approach can be solved efficiently via iterative convex programming. Extensive simulations and a real data application demonstrate substantial gains in statistical power of sensitivity analysis. Importantly, our approach can also be applied to many other sensitivity analysis frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matching is one of the most widely used causal inference designs in observational studies, but post-matching confounding bias remains a critical concern. This bias includes overt bias from inexact matching on measured confounders and hidden bias from unmeasured confounders. Researchers routinely apply the famous Rosenbaum-type sensitivity analysis after matching to assess the impact of these biases on causal conclusions. In this work, we show that this approach is often conservative and may overstate sensitivity to confounding bias because the classical solution to the Rosenbaum sensitivity model may allocate hypothetical hidden bias in ways that contradict the overt bias observed in the matched dataset. To address this problem, we propose a new approach to Rosenbaum-type sensitivity analysis by ensuring compatibility between hidden and overt biases. Our approach does not need to add any additional assumptions (beyond mild regularity conditions) to Rosenbaum-type sensitivity analysis, and can produce uniformly more informative sensitivity analysis results than the conventional Rosenbaum-type sensitivity analysis. Computationally, our approach can be solved efficiently via iterative convex programming. Extensive simulations and a real data application demonstrate substantial gains in statistical power of sensitivity analysis. Importantly, our approach can also be applied to many other sensitivity analysis frameworks."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2023-11-19T03:53:03Z",
                "published_parsed": [
                    2023,
                    11,
                    19,
                    3,
                    53,
                    3,
                    6,
                    323,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Siyu Heng"
                    },
                    {
                        "name": "Yanxin Shen"
                    },
                    {
                        "name": "Pengyun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Pengyun Wang"
                },
                "author": "Pengyun Wang"
            },
            {
                "id": "http://arxiv.org/abs/2602.20440v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20440v2",
                "title": "Intelligence Without Integrity: Why Capable LLMs May Undermine Reliability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligence Without Integrity: Why Capable LLMs May Undermine Reliability"
                },
                "updated": "2026-02-25T17:41:10Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    41,
                    10,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20440v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As LLMs become embedded in research workflows and organizational decision processes, their effect on analytical reliability remains uncertain. We distinguish two dimensions of analytical reliability -- intelligence (the capacity to reach correct conclusions) and integrity (the stability of conclusions when analytically irrelevant cues about desired outcomes are introduced) -- and ask whether frontier LLMs possess both. Whether these dimensions trade off is theoretically ambiguous: the sophistication enabling accurate analysis may also enable responsiveness to non-evidential cues, or alternatively, greater capability may confer protection through better calibration and discernment. Using synthetically generated data with embedded ground truth, we evaluate fourteen models on a task simulating empirical analysis of hospital merger effects. We find that intelligence and integrity trade off: frontier models most likely to reach correct conclusions under neutral conditions are often most susceptible to shifting conclusions under motivated framing. We extend work on sycophancy by introducing goal-conditioned analytical sycophancy: sensitivity of inference to cues about desired outcomes, even when no belief is asserted and evidence is held constant. Unlike simple prompt sensitivity, models shift conclusions away from objective evidence in response to analytically irrelevant framing. This finding has important implications for empirical research and organizations. Selecting tools based on capability benchmarks may inadvertently select against the stability needed for reliable and replicable analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs become embedded in research workflows and organizational decision processes, their effect on analytical reliability remains uncertain. We distinguish two dimensions of analytical reliability -- intelligence (the capacity to reach correct conclusions) and integrity (the stability of conclusions when analytically irrelevant cues about desired outcomes are introduced) -- and ask whether frontier LLMs possess both. Whether these dimensions trade off is theoretically ambiguous: the sophistication enabling accurate analysis may also enable responsiveness to non-evidential cues, or alternatively, greater capability may confer protection through better calibration and discernment. Using synthetically generated data with embedded ground truth, we evaluate fourteen models on a task simulating empirical analysis of hospital merger effects. We find that intelligence and integrity trade off: frontier models most likely to reach correct conclusions under neutral conditions are often most susceptible to shifting conclusions under motivated framing. We extend work on sycophancy by introducing goal-conditioned analytical sycophancy: sensitivity of inference to cues about desired outcomes, even when no belief is asserted and evidence is held constant. Unlike simple prompt sensitivity, models shift conclusions away from objective evidence in response to analytically irrelevant framing. This finding has important implications for empirical research and organizations. Selecting tools based on capability benchmarks may inadvertently select against the stability needed for reliable and replicable analysis."
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-24T00:50:21Z",
                "published_parsed": [
                    2026,
                    2,
                    24,
                    0,
                    50,
                    21,
                    1,
                    55,
                    0
                ],
                "arxiv_comment": "45 pages, 9 figures",
                "arxiv_primary_category": {
                    "term": "econ.GN"
                },
                "authors": [
                    {
                        "name": "Ryan Allen"
                    },
                    {
                        "name": "Aticus Peterson"
                    }
                ],
                "author_detail": {
                    "name": "Aticus Peterson"
                },
                "author": "Aticus Peterson"
            },
            {
                "id": "http://arxiv.org/abs/2602.20659v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20659v2",
                "title": "Recursive Belief Vision Language Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recursive Belief Vision Language Action Models"
                },
                "updated": "2026-02-25T17:38:24Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    38,
                    24,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20659v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-language-action models must enable agents to execute long-horizon tasks under partial observability. However, most existing approaches remain observation-driven, relying on short context windows or repeated queries to vision-language models (VLMs). This leads to loss of task progress, action repetition under perceptual aliasing, and high inference latency. While semantic grounding is important, long-horizon manipulation fundamentally requires persistent, action-conditioned state representations. Current VLAs lack such representations and exhibit limited temporal and physical reasoning, making them ill-suited for multi-stage control. This paper introduces RB-VLA, a belief-centric architecture trained with self-supervised world-model objectives that maintains a compact latent state encoding task-relevant history, dynamics, and object interactions. Queried once per task, the VLM provides high-level intent, while the belief tracks task progress and enables phase-aware, causally grounded control under partial observability without storing raw observations or scaling memory with time. The belief and intent jointly condition a diffusion policy for robust closed-loop execution. RB-VLA outperforms prior VLAs on long-horizon benchmarks, achieving 52.5 percent and 37.5 percent higher success rates on multi-stage pick-and-place and stacking tasks, respectively, compared to pi_0. It also reduces inference latency by up to five times relative to baselines and eliminates memory growth across timesteps observed in existing VLAs. Ablations show the belief module is the primary driver of performance, increasing success rates from 32.5 percent without belief to 77.5 percent with belief.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language-action models must enable agents to execute long-horizon tasks under partial observability. However, most existing approaches remain observation-driven, relying on short context windows or repeated queries to vision-language models (VLMs). This leads to loss of task progress, action repetition under perceptual aliasing, and high inference latency. While semantic grounding is important, long-horizon manipulation fundamentally requires persistent, action-conditioned state representations. Current VLAs lack such representations and exhibit limited temporal and physical reasoning, making them ill-suited for multi-stage control. This paper introduces RB-VLA, a belief-centric architecture trained with self-supervised world-model objectives that maintains a compact latent state encoding task-relevant history, dynamics, and object interactions. Queried once per task, the VLM provides high-level intent, while the belief tracks task progress and enables phase-aware, causally grounded control under partial observability without storing raw observations or scaling memory with time. The belief and intent jointly condition a diffusion policy for robust closed-loop execution. RB-VLA outperforms prior VLAs on long-horizon benchmarks, achieving 52.5 percent and 37.5 percent higher success rates on multi-stage pick-and-place and stacking tasks, respectively, compared to pi_0. It also reduces inference latency by up to five times relative to baselines and eliminates memory growth across timesteps observed in existing VLAs. Ablations show the belief module is the primary driver of performance, increasing success rates from 32.5 percent without belief to 77.5 percent with belief."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-24T08:02:16Z",
                "published_parsed": [
                    2026,
                    2,
                    24,
                    8,
                    2,
                    16,
                    1,
                    55,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Vaidehi Bagaria"
                    },
                    {
                        "name": "Bijo Sebastian"
                    },
                    {
                        "name": "Nirav Kumar Patel"
                    }
                ],
                "author_detail": {
                    "name": "Nirav Kumar Patel"
                },
                "author": "Nirav Kumar Patel"
            },
            {
                "id": "http://arxiv.org/abs/2602.22136v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22136v1",
                "title": "SigmaQuant: Hardware-Aware Heterogeneous Quantization Method for Edge DNN Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SigmaQuant: Hardware-Aware Heterogeneous Quantization Method for Edge DNN Inference"
                },
                "updated": "2026-02-25T17:34:14Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    34,
                    14,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22136v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deep neural networks (DNNs) are essential for performing advanced tasks on edge or mobile devices, yet their deployment is often hindered by severe resource constraints, including limited memory, energy, and computational power. While uniform quantization provides a straightforward approach to compress model and reduce hardware requirement, it fails to fully leverage the varying robustness across layers, and often lead to accuracy degradation or suboptimal resource usage, particularly at low bitwidths. In contrast, heterogeneous quantization, which allocates different bitwidths to individual layers, can mitigate these drawbacks. Nonetheless, current heterogeneous quantization methods either needs huge brute-force design space search or lacks the adaptability to meet different hardware conditions, such as memory size, energy budget, and latency requirement. Filling these gaps, this work introduces \\textbf{\\textit{SigmaQuant}}, an adaptive layer-wise heterogeneous quantization framework designed to efficiently balance accuracy and resource usage for varied edge environments without exhaustive search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks (DNNs) are essential for performing advanced tasks on edge or mobile devices, yet their deployment is often hindered by severe resource constraints, including limited memory, energy, and computational power. While uniform quantization provides a straightforward approach to compress model and reduce hardware requirement, it fails to fully leverage the varying robustness across layers, and often lead to accuracy degradation or suboptimal resource usage, particularly at low bitwidths. In contrast, heterogeneous quantization, which allocates different bitwidths to individual layers, can mitigate these drawbacks. Nonetheless, current heterogeneous quantization methods either needs huge brute-force design space search or lacks the adaptability to meet different hardware conditions, such as memory size, energy budget, and latency requirement. Filling these gaps, this work introduces \\textbf{\\textit{SigmaQuant}}, an adaptive layer-wise heterogeneous quantization framework designed to efficiently balance accuracy and resource usage for varied edge environments without exhaustive search."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T17:34:14Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    34,
                    14,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Qunyou Liu"
                    },
                    {
                        "name": "Pengbo Yu"
                    },
                    {
                        "name": "Marina Zapater"
                    },
                    {
                        "name": "David Atienza"
                    }
                ],
                "author_detail": {
                    "name": "David Atienza"
                },
                "author": "David Atienza"
            },
            {
                "id": "http://arxiv.org/abs/2602.22134v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22134v1",
                "title": "Secure Semantic Communications via AI Defenses: Fundamentals, Solutions, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure Semantic Communications via AI Defenses: Fundamentals, Solutions, and Future Directions"
                },
                "updated": "2026-02-25T17:28:07Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    28,
                    7,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22134v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Semantic communication (SemCom) redefines wireless communication from reproducing symbols to transmitting task-relevant semantics. However, this AI-native architecture also introduces new vulnerabilities, as semantic failures may arise from adversarial perturbations to models, corrupted training data, desynchronized priors, or misaligned inference even when lower-layer transmission reliability and cryptographic protection remain intact. This survey provides a defense-centered and system-oriented synthesis of security in SemCom via AI defense. We analyze AI-centric threat models by consolidating existing studies and organizing attack surfaces across model-level, channel-realizable, knowledge-based, and networked inference vectors. Building on this foundation, we present a structured taxonomy of defense strategies organized by where semantic integrity can be compromised in SemCom systems despite correct symbol delivery, spanning semantic encoding, wireless transmission, knowledge integrity, and coordination among multiple agents. These categories correspond to distinct security failure modes, including representation fragility, channel-realizable manipulation, semantic prior poisoning or desynchronization, and adversarial propagation through distributed inference. We also examine security utility operating envelopes that capture tradeoffs among semantic fidelity, robustness, latency, and energy under realistic constraints, survey evaluation frameworks and representative applications, and identify open challenges in cross-layer composition and deployment-time certification. Overall, this survey offers a unified system-level perspective that enables readers to understand major threat and defense mechanisms in AI-native SemCom systems and to leverage emerging security techniques in the design and deployment of robust SemCom architectures for next-generation intelligent networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic communication (SemCom) redefines wireless communication from reproducing symbols to transmitting task-relevant semantics. However, this AI-native architecture also introduces new vulnerabilities, as semantic failures may arise from adversarial perturbations to models, corrupted training data, desynchronized priors, or misaligned inference even when lower-layer transmission reliability and cryptographic protection remain intact. This survey provides a defense-centered and system-oriented synthesis of security in SemCom via AI defense. We analyze AI-centric threat models by consolidating existing studies and organizing attack surfaces across model-level, channel-realizable, knowledge-based, and networked inference vectors. Building on this foundation, we present a structured taxonomy of defense strategies organized by where semantic integrity can be compromised in SemCom systems despite correct symbol delivery, spanning semantic encoding, wireless transmission, knowledge integrity, and coordination among multiple agents. These categories correspond to distinct security failure modes, including representation fragility, channel-realizable manipulation, semantic prior poisoning or desynchronization, and adversarial propagation through distributed inference. We also examine security utility operating envelopes that capture tradeoffs among semantic fidelity, robustness, latency, and energy under realistic constraints, survey evaluation frameworks and representative applications, and identify open challenges in cross-layer composition and deployment-time certification. Overall, this survey offers a unified system-level perspective that enables readers to understand major threat and defense mechanisms in AI-native SemCom systems and to leverage emerging security techniques in the design and deployment of robust SemCom architectures for next-generation intelligent networks."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T17:28:07Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    28,
                    7,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Lan Zhang"
                    },
                    {
                        "name": "Chengsi Liang"
                    },
                    {
                        "name": "Zeming Zhuang"
                    },
                    {
                        "name": "Yao Sun"
                    },
                    {
                        "name": "Fang Fang"
                    },
                    {
                        "name": "Xiaoyong Yuan"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato"
            },
            {
                "id": "http://arxiv.org/abs/2503.03178v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.03178v4",
                "title": "Active operator learning with predictive uncertainty quantification for partial differential equations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active operator learning with predictive uncertainty quantification for partial differential equations"
                },
                "updated": "2026-02-25T17:27:35Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    27,
                    35,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.03178v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.03178v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the increased prevalence of neural operators being used to provide rapid solutions to partial differential equations (PDEs), understanding the accuracy of model predictions and the associated error levels is necessary for deploying reliable surrogate models in scientific applications. Existing uncertainty quantification (UQ) frameworks employ ensembles or Bayesian methods, which can incur substantial computational costs during both training and inference. We propose a lightweight predictive UQ method tailored for Deep operator networks (DeepONets) that also generalizes to other operator networks. Numerical experiments on linear and nonlinear PDEs demonstrate that the framework's uncertainty estimates are unbiased and provide accurate out-of-distribution uncertainty predictions with a sufficiently large training dataset. Our framework provides fast inference and uncertainty estimates that can efficiently drive outer-loop analyses that would be prohibitively expensive with conventional solvers. We demonstrate how predictive uncertainties can be used in the context of Bayesian optimization and active learning problems to yield improvements in accuracy and data-efficiency for outer-loop optimization procedures. In the active learning setup, we extend the framework to Fourier Neural Operators (FNO) and describe a generalized method for other operator networks. To enable real-time deployment, we introduce an inference strategy based on precomputed trunk outputs and a sparse placement matrix, reducing evaluation time by more than a factor of five. Our method provides a practical route to uncertainty-aware operator learning in time-sensitive settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increased prevalence of neural operators being used to provide rapid solutions to partial differential equations (PDEs), understanding the accuracy of model predictions and the associated error levels is necessary for deploying reliable surrogate models in scientific applications. Existing uncertainty quantification (UQ) frameworks employ ensembles or Bayesian methods, which can incur substantial computational costs during both training and inference. We propose a lightweight predictive UQ method tailored for Deep operator networks (DeepONets) that also generalizes to other operator networks. Numerical experiments on linear and nonlinear PDEs demonstrate that the framework's uncertainty estimates are unbiased and provide accurate out-of-distribution uncertainty predictions with a sufficiently large training dataset. Our framework provides fast inference and uncertainty estimates that can efficiently drive outer-loop analyses that would be prohibitively expensive with conventional solvers. We demonstrate how predictive uncertainties can be used in the context of Bayesian optimization and active learning problems to yield improvements in accuracy and data-efficiency for outer-loop optimization procedures. In the active learning setup, we extend the framework to Fourier Neural Operators (FNO) and describe a generalized method for other operator networks. To enable real-time deployment, we introduce an inference strategy based on precomputed trunk outputs and a sparse placement matrix, reducing evaluation time by more than a factor of five. Our method provides a practical route to uncertainty-aware operator learning in time-sensitive settings."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-05T04:48:14Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    48,
                    14,
                    2,
                    64,
                    0
                ],
                "arxiv_comment": "Submitted to the Journal of Computational Physics",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Nick Winovich"
                    },
                    {
                        "name": "Mitchell Daneker"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Guang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Guang Lin"
                },
                "author": "Guang Lin"
            },
            {
                "id": "http://arxiv.org/abs/2602.22125v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22125v1",
                "title": "IndicIFEval: A Benchmark for Verifiable Instruction-Following Evaluation in 14 Indic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IndicIFEval: A Benchmark for Verifiable Instruction-Following Evaluation in 14 Indic Languages"
                },
                "updated": "2026-02-25T17:12:37Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    12,
                    37,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22125v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Instruction-following benchmarks remain predominantly English-centric, leaving a critical evaluation gap for the hundreds of millions of Indic language speakers. We introduce IndicIFEval, a benchmark evaluating constrained generation of LLMs across 14 Indic languages using automatically verifiable, rule-based instructions. It comprises around 800 human-verified examples per language spread across two complementary subsets: IndicIFEval-Ground, translated prompts from IFEval (Zhou et al., 2023) carefully localized for Indic contexts, and IndicIFEval-Ground, synthetically generated instructions grounded in native Indic content. We conduct a comprehensive evaluation of major open-weight and proprietary models spanning both reasoning and non-reasoning models. While models maintain strong adherence to formatting constraints, they struggle significantly with lexical and cross-lingual tasks -- and despite progress in high-resource languages, instruction-following across the broader Indic family lags significantly behind English. We release IndicIFEval and its evaluation scripts to support progress on multilingual constrained generation (http://github.com/ai4bharat/IndicIFEval).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-following benchmarks remain predominantly English-centric, leaving a critical evaluation gap for the hundreds of millions of Indic language speakers. We introduce IndicIFEval, a benchmark evaluating constrained generation of LLMs across 14 Indic languages using automatically verifiable, rule-based instructions. It comprises around 800 human-verified examples per language spread across two complementary subsets: IndicIFEval-Ground, translated prompts from IFEval (Zhou et al., 2023) carefully localized for Indic contexts, and IndicIFEval-Ground, synthetically generated instructions grounded in native Indic content. We conduct a comprehensive evaluation of major open-weight and proprietary models spanning both reasoning and non-reasoning models. While models maintain strong adherence to formatting constraints, they struggle significantly with lexical and cross-lingual tasks -- and despite progress in high-resource languages, instruction-following across the broader Indic family lags significantly behind English. We release IndicIFEval and its evaluation scripts to support progress on multilingual constrained generation (http://github.com/ai4bharat/IndicIFEval)."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T17:12:37Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    12,
                    37,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "8 pages + Appendix",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Thanmay Jayakumar"
                    },
                    {
                        "name": "Mohammed Safi Ur Rahman Khan"
                    },
                    {
                        "name": "Raj Dabre"
                    },
                    {
                        "name": "Ratish Puduppully"
                    },
                    {
                        "name": "Anoop Kunchukuttan"
                    }
                ],
                "author_detail": {
                    "name": "Anoop Kunchukuttan"
                },
                "author": "Anoop Kunchukuttan"
            },
            {
                "id": "http://arxiv.org/abs/2602.20309v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20309v2",
                "title": "QuantVLA: Scale-Calibrated Post-Training Quantization for Vision-Language-Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuantVLA: Scale-Calibrated Post-Training Quantization for Vision-Language-Action Models"
                },
                "updated": "2026-02-25T17:11:08Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    11,
                    8,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20309v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20309v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-language-action (VLA) models unify perception, language, and control for embodied agents but face significant challenges in practical deployment due to rapidly increasing compute and memory demands, especially as models scale to longer horizons and larger backbones. To address these bottlenecks, we introduce QuantVLA, a training-free post-training quantization (PTQ) framework that, to our knowledge, is the first PTQ approach for VLA systems and the first to successfully quantize a diffusion transformer (DiT) action head. QuantVLA incorporates three scale-calibrated components: (1) a selective quantization layout that integerizes all linear layers in both the language backbone and the DiT while keeping attention projections in floating point to preserve the original operator schedule; (2) attention temperature matching, a lightweight per-head scaling mechanism that stabilizes attention logits and is folded into the dequantization scales at inference; and (3) output head balancing, a per-layer residual interface calibration that mitigates post-projection energy drift. The framework requires no additional training, uses only a small unlabeled calibration buffer, and supports integer kernels for low-bit weights and activations while leaving the architecture unchanged. Across representative VLA models on LIBERO, QuantVLA exceeds the task success rates of full-precision baselines, achieves about 70% relative memory savings on the quantized components, and delivers a 1.22x speedup in end-to-end inference latency, providing a practical pathway toward scalable low-bit embodied intelligence under strict compute, memory, and power constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language-action (VLA) models unify perception, language, and control for embodied agents but face significant challenges in practical deployment due to rapidly increasing compute and memory demands, especially as models scale to longer horizons and larger backbones. To address these bottlenecks, we introduce QuantVLA, a training-free post-training quantization (PTQ) framework that, to our knowledge, is the first PTQ approach for VLA systems and the first to successfully quantize a diffusion transformer (DiT) action head. QuantVLA incorporates three scale-calibrated components: (1) a selective quantization layout that integerizes all linear layers in both the language backbone and the DiT while keeping attention projections in floating point to preserve the original operator schedule; (2) attention temperature matching, a lightweight per-head scaling mechanism that stabilizes attention logits and is folded into the dequantization scales at inference; and (3) output head balancing, a per-layer residual interface calibration that mitigates post-projection energy drift. The framework requires no additional training, uses only a small unlabeled calibration buffer, and supports integer kernels for low-bit weights and activations while leaving the architecture unchanged. Across representative VLA models on LIBERO, QuantVLA exceeds the task success rates of full-precision baselines, achieves about 70% relative memory savings on the quantized components, and delivers a 1.22x speedup in end-to-end inference latency, providing a practical pathway toward scalable low-bit embodied intelligence under strict compute, memory, and power constraints."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T19:55:54Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    19,
                    55,
                    54,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "CVPR2026",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Jingxuan Zhang"
                    },
                    {
                        "name": "Yunta Hsieh"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Haokun Lin"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Ziqi Wang"
                    },
                    {
                        "name": "Yingtie Lei"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2502.18615v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.18615v3",
                "title": "A Distributional Treatment of Real2Sim2Real for Object-Centric Agent Adaptation in Vision-Driven Deformable Linear Object Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Distributional Treatment of Real2Sim2Real for Object-Centric Agent Adaptation in Vision-Driven Deformable Linear Object Manipulation"
                },
                "updated": "2026-02-25T17:09:15Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    9,
                    15,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.18615v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.18615v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/LRA.2025.3581744",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "We present an integrated (or end-to-end) framework for the Real2Sim2Real problem of manipulating deformable linear objects (DLOs) based on visual perception. Working with a parameterised set of DLOs, we use likelihood-free inference (LFI) to compute the posterior distributions for the physical parameters using which we can approximately simulate the behaviour of each specific DLO. We use these posteriors for domain randomisation while training, in simulation, object-specific visuomotor policies (i.e. assuming only visual and proprioceptive sensory) for a DLO reaching task, using model-free reinforcement learning. We demonstrate the utility of this approach by deploying sim-trained DLO manipulation policies in the real world in a zero-shot manner, i.e. without any further fine-tuning. In this context, we evaluate the capacity of a prominent LFI method to perform fine classification over the parametric set of DLOs, using only visual and proprioceptive data obtained in a dynamic manipulation trajectory. We then study the implications of the resulting domain distributions in sim-based policy learning and real-world performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an integrated (or end-to-end) framework for the Real2Sim2Real problem of manipulating deformable linear objects (DLOs) based on visual perception. Working with a parameterised set of DLOs, we use likelihood-free inference (LFI) to compute the posterior distributions for the physical parameters using which we can approximately simulate the behaviour of each specific DLO. We use these posteriors for domain randomisation while training, in simulation, object-specific visuomotor policies (i.e. assuming only visual and proprioceptive sensory) for a DLO reaching task, using model-free reinforcement learning. We demonstrate the utility of this approach by deploying sim-trained DLO manipulation policies in the real world in a zero-shot manner, i.e. without any further fine-tuning. In this context, we evaluate the capacity of a prominent LFI method to perform fine classification over the parametric set of DLOs, using only visual and proprioceptive data obtained in a dynamic manipulation trajectory. We then study the implications of the resulting domain distributions in sim-based policy learning and real-world performance."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-25T20:01:06Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    20,
                    1,
                    6,
                    1,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "arxiv_journal_ref": "In IEEE Robotics and Automation Letters, Volume 10, Issue 8, August 2025, Pages 8075-8082",
                "authors": [
                    {
                        "name": "Georgios Kamaras"
                    },
                    {
                        "name": "Subramanian Ramamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Subramanian Ramamoorthy"
                },
                "author": "Subramanian Ramamoorthy",
                "arxiv_doi": "10.1109/LRA.2025.3581744"
            },
            {
                "id": "http://arxiv.org/abs/2502.18424v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.18424v2",
                "title": "Compressing Language Models for Specialized Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing Language Models for Specialized Domains"
                },
                "updated": "2026-02-25T17:00:00Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    0,
                    0,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.18424v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.18424v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Language models (LMs) excel at tasks across diverse domains, yet require substantial computational resources during inference. Compression techniques such as pruning and quantization offer a practical path towards efficient LM deployment, exemplified by their ability to preserve performance on general-purpose benchmarks. However, general-purpose LM compression methods can negatively affect performance in specialized domains (e.g. biomedical or legal). Recent work has sought to address this issue, but requires a computationally expensive full-parameter fine-tuning pipeline. To this end, we propose MixCal, a novel calibration method designed to improve the in-domain performance of compressed LMs in a post-training setting. Through extensive experimentation, we demonstrate that MixCal substantially outperforms existing approaches on domain-specific tasks and preserves general performance. Notably, these performance gains are achieved while also reducing the computational cost of LM compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models (LMs) excel at tasks across diverse domains, yet require substantial computational resources during inference. Compression techniques such as pruning and quantization offer a practical path towards efficient LM deployment, exemplified by their ability to preserve performance on general-purpose benchmarks. However, general-purpose LM compression methods can negatively affect performance in specialized domains (e.g. biomedical or legal). Recent work has sought to address this issue, but requires a computationally expensive full-parameter fine-tuning pipeline. To this end, we propose MixCal, a novel calibration method designed to improve the in-domain performance of compressed LMs in a post-training setting. Through extensive experimentation, we demonstrate that MixCal substantially outperforms existing approaches on domain-specific tasks and preserves general performance. Notably, these performance gains are achieved while also reducing the computational cost of LM compression."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-25T18:20:00Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    20,
                    0,
                    1,
                    56,
                    0
                ],
                "arxiv_comment": "EACL 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Miles Williams"
                    },
                    {
                        "name": "George Chrysostomou"
                    },
                    {
                        "name": "Vitor Jeronymo"
                    },
                    {
                        "name": "Nikolaos Aletras"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Aletras"
                },
                "author": "Nikolaos Aletras"
            },
            {
                "id": "http://arxiv.org/abs/2509.04625v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.04625v2",
                "title": "Nexus: Efficient and Scalable Multi-Cell mmWave Baseband Processing with Heterogeneous Compute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nexus: Efficient and Scalable Multi-Cell mmWave Baseband Processing with Heterogeneous Compute"
                },
                "updated": "2026-02-25T16:57:22Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    57,
                    22,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.04625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.04625v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid adoption of 5G New Radio (NR), particularly in the millimeter-wave (mmWave) spectrum, imposes stringent demands on the flexibility, scalability, and efficiency of baseband processing. While virtualized Radio Access Networks (vRANs) enable dynamic spectrum sharing across cells, compute resource allocation for baseband processing, especially in multi-cell deployments with heterogeneous workloads, remains underexplored. In this paper, we present NEXUS, the first system to realize real-time, virtualized multi-cell mmWave baseband processing on a single server with heterogeneous compute resources. NEXUS integrates software-based digital signal processing pipelines with hardware-accelerated LDPC decoding, and introduces a novel framework for sharing Intel's ACC100 eASIC across multiple CPU cores via virtual functions (VFs). For single-cell operation, NEXUS employs a random forest (RAF)-based model that predicts the most energy-efficient resource allocation for the given cell configuration with microsecond-level inference latency and high accuracy. For multi-cell scenarios, NEXUS introduces a power-aware scheduler that incorporates a lightweight contention model to adjust resource allocation strategies under concurrent execution. Through extensive evaluation across various Frequency Range 2 (FR2) cell configurations, we show that NEXUS supports up to 16 concurrent cells under full load, achieving 5.37Gbps aggregate throughput, while reducing the multi-cell scheduling search space by orders of magnitude. These results demonstrate that virtualized, resource-aware baseband processing is both practical and efficient for next-generation vRAN systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adoption of 5G New Radio (NR), particularly in the millimeter-wave (mmWave) spectrum, imposes stringent demands on the flexibility, scalability, and efficiency of baseband processing. While virtualized Radio Access Networks (vRANs) enable dynamic spectrum sharing across cells, compute resource allocation for baseband processing, especially in multi-cell deployments with heterogeneous workloads, remains underexplored. In this paper, we present NEXUS, the first system to realize real-time, virtualized multi-cell mmWave baseband processing on a single server with heterogeneous compute resources. NEXUS integrates software-based digital signal processing pipelines with hardware-accelerated LDPC decoding, and introduces a novel framework for sharing Intel's ACC100 eASIC across multiple CPU cores via virtual functions (VFs). For single-cell operation, NEXUS employs a random forest (RAF)-based model that predicts the most energy-efficient resource allocation for the given cell configuration with microsecond-level inference latency and high accuracy. For multi-cell scenarios, NEXUS introduces a power-aware scheduler that incorporates a lightweight contention model to adjust resource allocation strategies under concurrent execution. Through extensive evaluation across various Frequency Range 2 (FR2) cell configurations, we show that NEXUS supports up to 16 concurrent cells under full load, achieving 5.37Gbps aggregate throughput, while reducing the multi-cell scheduling search space by orders of magnitude. These results demonstrate that virtualized, resource-aware baseband processing is both practical and efficient for next-generation vRAN systems."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-04T19:20:35Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    19,
                    20,
                    35,
                    3,
                    247,
                    0
                ],
                "arxiv_comment": "Accepted to ACM MobiCom 2026",
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Zhenzhou Qi"
                    },
                    {
                        "name": "Chung-Hsuan Tung"
                    },
                    {
                        "name": "Zhihui Gao"
                    },
                    {
                        "name": "Tingjun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tingjun Chen"
                },
                "author": "Tingjun Chen"
            },
            {
                "id": "http://arxiv.org/abs/2602.16852v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16852v2",
                "title": "Meenz bleibt Meenz, but Large Language Models Do Not Speak Its Dialect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meenz bleibt Meenz, but Large Language Models Do Not Speak Its Dialect"
                },
                "updated": "2026-02-25T16:49:29Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    49,
                    29,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16852v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16852v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Meenzerisch, the dialect spoken in the German city of Mainz, is also the traditional language of the Mainz carnival, a yearly celebration well known throughout Germany. However, Meenzerisch is on the verge of dying out-a fate it shares with many other German dialects. Natural language processing (NLP) has the potential to help with the preservation and revival efforts of languages and dialects. However, so far no NLP research has looked at Meenzerisch. This work presents the first research in the field of NLP that is explicitly focused on the dialect of Mainz. We introduce a digital dictionary-an NLP-ready dataset derived from an existing resource (Schramm, 1966)-to support researchers in modeling and benchmarking the language. It contains 2,351 words in the dialect paired with their meanings described in Standard German. We then use this dataset to answer the following research questions: (1) Can state-of-the-art large language models (LLMs) generate definitions for dialect words? (2) Can LLMs generate words in Meenzerisch, given their definitions? Our experiments show that LLMs can do neither: the best model for definitions reaches only 6.27% accuracy and the best word generation model's accuracy is 1.51%. We then conduct two additional experiments in order to see if accuracy is improved by few-shot learning and by extracting rules from the training set, which are then passed to the LLM. While those approaches are able to improve the results, accuracy remains below 10%. This highlights that additional resources and an intensification of research efforts focused on German dialects are desperately needed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meenzerisch, the dialect spoken in the German city of Mainz, is also the traditional language of the Mainz carnival, a yearly celebration well known throughout Germany. However, Meenzerisch is on the verge of dying out-a fate it shares with many other German dialects. Natural language processing (NLP) has the potential to help with the preservation and revival efforts of languages and dialects. However, so far no NLP research has looked at Meenzerisch. This work presents the first research in the field of NLP that is explicitly focused on the dialect of Mainz. We introduce a digital dictionary-an NLP-ready dataset derived from an existing resource (Schramm, 1966)-to support researchers in modeling and benchmarking the language. It contains 2,351 words in the dialect paired with their meanings described in Standard German. We then use this dataset to answer the following research questions: (1) Can state-of-the-art large language models (LLMs) generate definitions for dialect words? (2) Can LLMs generate words in Meenzerisch, given their definitions? Our experiments show that LLMs can do neither: the best model for definitions reaches only 6.27% accuracy and the best word generation model's accuracy is 1.51%. We then conduct two additional experiments in order to see if accuracy is improved by few-shot learning and by extracting rules from the training set, which are then passed to the LLM. While those approaches are able to improve the results, accuracy remains below 10%. This highlights that additional resources and an intensification of research efforts focused on German dialects are desperately needed."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T20:29:02Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    20,
                    29,
                    2,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "Accepted at LREC 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Minh Duc Bui"
                    },
                    {
                        "name": "Manuel Mager"
                    },
                    {
                        "name": "Peter Herbert Kann"
                    },
                    {
                        "name": "Katharina von der Wense"
                    }
                ],
                "author_detail": {
                    "name": "Katharina von der Wense"
                },
                "author": "Katharina von der Wense"
            },
            {
                "id": "http://arxiv.org/abs/2511.21104v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.21104v2",
                "title": "BRIDGE: Building Representations In Domain Guided Program Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BRIDGE: Building Representations In Domain Guided Program Synthesis"
                },
                "updated": "2026-02-25T16:45:04Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    45,
                    4,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.21104v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.21104v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) are good at generating code, but remain brittle for formal verification in systems like Lean4. A core scalability challenge is that verified synthesis requires consistent outputs across multiple artifacts: executable code, precise specifications, theorem statements, and ultimately proofs. Existing approaches rarely treat these as a unified pipeline. We present BRIDGE, a structured prompting framework that decomposes verification into three interconnected domains: Code (implementations), Specifications (formal intent), and Theorem Statements (constructive correctness claims), and elicits domain-specific intermediate reasoning to connect them. In Lean4, BRIDGE often adopts a code-first workflow, using the generated implementation as a semantic anchor for downstream specification and theorem statement generation. Across 178 algorithmic problems and five LLMs, BRIDGE improves Lean executable correctness by nearly 1.5x (pass at 5) over direct baselines and can be 2x more sample-efficient at inference time, requiring fewer samples per verified solution at comparable generation lengths. We further find that specification-driven prompting improves Python pass rates by up to 17.5 percent. Beyond inference-time prompting, supervised fine-tuning on BRIDGE-style reasoning traces yields nearly 1.5x higher Lean pass success than code-only SFT, indicating that these intermediate representations are learnable. BRIDGE provides a practical foundation for scaling verified synthesis and motivates future work on expert iteration and full proof generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are good at generating code, but remain brittle for formal verification in systems like Lean4. A core scalability challenge is that verified synthesis requires consistent outputs across multiple artifacts: executable code, precise specifications, theorem statements, and ultimately proofs. Existing approaches rarely treat these as a unified pipeline. We present BRIDGE, a structured prompting framework that decomposes verification into three interconnected domains: Code (implementations), Specifications (formal intent), and Theorem Statements (constructive correctness claims), and elicits domain-specific intermediate reasoning to connect them. In Lean4, BRIDGE often adopts a code-first workflow, using the generated implementation as a semantic anchor for downstream specification and theorem statement generation. Across 178 algorithmic problems and five LLMs, BRIDGE improves Lean executable correctness by nearly 1.5x (pass at 5) over direct baselines and can be 2x more sample-efficient at inference time, requiring fewer samples per verified solution at comparable generation lengths. We further find that specification-driven prompting improves Python pass rates by up to 17.5 percent. Beyond inference-time prompting, supervised fine-tuning on BRIDGE-style reasoning traces yields nearly 1.5x higher Lean pass success than code-only SFT, indicating that these intermediate representations are learnable. BRIDGE provides a practical foundation for scaling verified synthesis and motivates future work on expert iteration and full proof generation."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-26T06:39:19Z",
                "published_parsed": [
                    2025,
                    11,
                    26,
                    6,
                    39,
                    19,
                    2,
                    330,
                    0
                ],
                "arxiv_comment": "Approx. 23 pages including appendices, 10 figures, 3 tables. Empirical study of LLM-based verified program synthesis in Lean4 (code, specs, and proofs)",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Robert Joseph George"
                    },
                    {
                        "name": "Carson Eisenach"
                    },
                    {
                        "name": "Udaya Ghai"
                    },
                    {
                        "name": "Dominique Perrault-Joncas"
                    },
                    {
                        "name": "Anima Anandkumar"
                    },
                    {
                        "name": "Dean Foster"
                    }
                ],
                "author_detail": {
                    "name": "Dean Foster"
                },
                "author": "Dean Foster"
            },
            {
                "id": "http://arxiv.org/abs/2602.13477v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.13477v2",
                "title": "OMNI-LEAK: Orchestrator Multi-Agent Network Induced Data Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OMNI-LEAK: Orchestrator Multi-Agent Network Induced Data Leakage"
                },
                "updated": "2026-02-25T16:43:53Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    43,
                    53,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.13477v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.13477v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As Large Language Model (LLM) agents become more capable, their coordinated use in the form of multi-agent systems is anticipated to emerge as a practical paradigm. Prior work has examined the safety and misuse risks associated with agents. However, much of this has focused on the single-agent case and/or setups missing basic engineering safeguards such as access control, revealing a scarcity of threat modeling in multi-agent systems. We investigate the security vulnerabilities of a popular multi-agent pattern known as the orchestrator setup, in which a central agent decomposes and delegates tasks to specialized agents. Through red-teaming a concrete setup representative of a likely future use case, we demonstrate a novel attack vector, OMNI-LEAK, that compromises several agents to leak sensitive data through a single indirect prompt injection, even in the presence of data access control. We report the susceptibility of frontier models to different categories of attacks, finding that both reasoning and non-reasoning models are vulnerable, even when the attacker lacks insider knowledge of the implementation details. Our work highlights the importance of safety research to generalize from single-agent to multi-agent settings, in order to reduce the serious risks of real-world privacy breaches and financial losses and overall public trust in AI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Model (LLM) agents become more capable, their coordinated use in the form of multi-agent systems is anticipated to emerge as a practical paradigm. Prior work has examined the safety and misuse risks associated with agents. However, much of this has focused on the single-agent case and/or setups missing basic engineering safeguards such as access control, revealing a scarcity of threat modeling in multi-agent systems. We investigate the security vulnerabilities of a popular multi-agent pattern known as the orchestrator setup, in which a central agent decomposes and delegates tasks to specialized agents. Through red-teaming a concrete setup representative of a likely future use case, we demonstrate a novel attack vector, OMNI-LEAK, that compromises several agents to leak sensitive data through a single indirect prompt injection, even in the presence of data access control. We report the susceptibility of frontier models to different categories of attacks, finding that both reasoning and non-reasoning models are vulnerable, even when the attacker lacks insider knowledge of the implementation details. Our work highlights the importance of safety research to generalize from single-agent to multi-agent settings, in order to reduce the serious risks of real-world privacy breaches and financial losses and overall public trust in AI agents."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-13T21:32:32Z",
                "published_parsed": [
                    2026,
                    2,
                    13,
                    21,
                    32,
                    32,
                    4,
                    44,
                    0
                ],
                "arxiv_comment": "Preprint; corrected typos",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Akshat Naik"
                    },
                    {
                        "name": "Jay Culligan"
                    },
                    {
                        "name": "Yarin Gal"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Rahaf Aljundi"
                    },
                    {
                        "name": "Alasdair Paren"
                    },
                    {
                        "name": "Adel Bibi"
                    }
                ],
                "author_detail": {
                    "name": "Adel Bibi"
                },
                "author": "Adel Bibi"
            },
            {
                "id": "http://arxiv.org/abs/2602.22091v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22091v1",
                "title": "Learning to Drive is a Free Gift: Large-Scale Label-Free Autonomy Pretraining from Unposed In-The-Wild Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Drive is a Free Gift: Large-Scale Label-Free Autonomy Pretraining from Unposed In-The-Wild Videos"
                },
                "updated": "2026-02-25T16:38:53Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    38,
                    53,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22091v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Ego-centric driving videos available online provide an abundant source of visual data for autonomous driving, yet their lack of annotations makes it difficult to learn representations that capture both semantic structure and 3D geometry. Recent advances in large feedforward spatial models demonstrate that point maps and ego-motion can be inferred in a single forward pass, suggesting a promising direction for scalable driving perception. We therefore propose a label-free, teacher-guided framework for learning autonomous driving representations directly from unposed videos. Unlike prior self-supervised approaches that focus primarily on frame-to-frame consistency, we posit that safe and reactive driving depends critically on temporal context. To this end, we leverage a feedforward architecture equipped with a lightweight autoregressive module, trained using multi-modal supervisory signals that guide the model to jointly predict current and future point maps, camera poses, semantic segmentation, and motion masks. Multi-modal teachers provide sequence-level pseudo-supervision, enabling LFG to learn a unified pseudo-4D representation from raw YouTube videos without poses, labels, or LiDAR. The resulting encoder not only transfers effectively to downstream autonomous driving planning on the NAVSIM benchmark, surpassing multi-camera and LiDAR baselines with only a single monocular camera, but also yields strong performance when evaluated on a range of semantic, geometric, and qualitative motion prediction tasks. These geometry and motion-aware features position LFG as a compelling video-centric foundation model for autonomous driving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ego-centric driving videos available online provide an abundant source of visual data for autonomous driving, yet their lack of annotations makes it difficult to learn representations that capture both semantic structure and 3D geometry. Recent advances in large feedforward spatial models demonstrate that point maps and ego-motion can be inferred in a single forward pass, suggesting a promising direction for scalable driving perception. We therefore propose a label-free, teacher-guided framework for learning autonomous driving representations directly from unposed videos. Unlike prior self-supervised approaches that focus primarily on frame-to-frame consistency, we posit that safe and reactive driving depends critically on temporal context. To this end, we leverage a feedforward architecture equipped with a lightweight autoregressive module, trained using multi-modal supervisory signals that guide the model to jointly predict current and future point maps, camera poses, semantic segmentation, and motion masks. Multi-modal teachers provide sequence-level pseudo-supervision, enabling LFG to learn a unified pseudo-4D representation from raw YouTube videos without poses, labels, or LiDAR. The resulting encoder not only transfers effectively to downstream autonomous driving planning on the NAVSIM benchmark, surpassing multi-camera and LiDAR baselines with only a single monocular camera, but also yields strong performance when evaluated on a range of semantic, geometric, and qualitative motion prediction tasks. These geometry and motion-aware features position LFG as a compelling video-centric foundation model for autonomous driving."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T16:38:53Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    38,
                    53,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "Accepted at CVPR 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Matthew Strong"
                    },
                    {
                        "name": "Wei-Jer Chang"
                    },
                    {
                        "name": "Quentin Herau"
                    },
                    {
                        "name": "Jiezhi Yang"
                    },
                    {
                        "name": "Yihan Hu"
                    },
                    {
                        "name": "Chensheng Peng"
                    },
                    {
                        "name": "Wei Zhan"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhan"
                },
                "author": "Wei Zhan"
            },
            {
                "id": "http://arxiv.org/abs/2602.22090v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22090v1",
                "title": "Confidence-Driven Multi-Scale Model Selection for Cost-Efficient Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence-Driven Multi-Scale Model Selection for Cost-Efficient Inference"
                },
                "updated": "2026-02-25T16:38:03Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    38,
                    3,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22090v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have revolutionized inference across diverse natural language tasks, with larger models performing better but at higher computational costs. We propose a confidence-driven strategy that dynamically selects the most suitable model based on confidence estimates. By assessing a model's confidence in handling the task and response accuracy, tasks that are likely to be solved correctly are retained, while more uncertain or complex cases are delegated to a larger model, ensuring reliability while minimizing computation. Specifically, we evaluate a model's likelihood of knowing the correct answer and the probability that its response is accurate. Experiments on the Massive Multitask Language Understanding (MMLU) benchmark show that our approach achieves accuracy comparable to the largest model while reducing computational costs by 20\\% to 40\\%. When applied to GPT-4o API calls, it reduces token usage by approximately 60\\%, further improving cost efficiency. These findings indicate the potential of confidence-based model selection to enhance real-world LLM deployment, particularly in resource-constrained settings such as edge devices and commercial API applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized inference across diverse natural language tasks, with larger models performing better but at higher computational costs. We propose a confidence-driven strategy that dynamically selects the most suitable model based on confidence estimates. By assessing a model's confidence in handling the task and response accuracy, tasks that are likely to be solved correctly are retained, while more uncertain or complex cases are delegated to a larger model, ensuring reliability while minimizing computation. Specifically, we evaluate a model's likelihood of knowing the correct answer and the probability that its response is accurate. Experiments on the Massive Multitask Language Understanding (MMLU) benchmark show that our approach achieves accuracy comparable to the largest model while reducing computational costs by 20\\% to 40\\%. When applied to GPT-4o API calls, it reduces token usage by approximately 60\\%, further improving cost efficiency. These findings indicate the potential of confidence-based model selection to enhance real-world LLM deployment, particularly in resource-constrained settings such as edge devices and commercial API applications."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T16:38:03Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    38,
                    3,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "Accepted by EACL 2026 Findings",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Bo-Wei Chen"
                    },
                    {
                        "name": "Chung-Chi Chen"
                    },
                    {
                        "name": "An-Zi Yen"
                    }
                ],
                "author_detail": {
                    "name": "An-Zi Yen"
                },
                "author": "An-Zi Yen"
            },
            {
                "id": "http://arxiv.org/abs/2602.22079v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22079v1",
                "title": "Overlap Zoo Beta: A Catalogue of ~800 Occulting Pairs in the DESI Legacy Survey using Citizen Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overlap Zoo Beta: A Catalogue of ~800 Occulting Pairs in the DESI Legacy Survey using Citizen Science"
                },
                "updated": "2026-02-25T16:28:28Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    28,
                    28,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22079v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Overlapping galaxies, in which a foreground galaxy partially overlaps a background galaxy, offer a unique opportunity to measure dust attenuation, a key nuisance parameter in galaxy studies, empirically and in great detail by modelling the light of both the foreground and background galaxy and inferring the missing light in the overlapping region. However, the current catalogue of overlapping pairs is relatively limited in number compared to catalogues dedicated to individual galaxies. Expanding this catalogue is not only a necessity to facilitate further detailed dust studies beyond the few limited studies conducted thus far, but also to improve pair-to-pair variance and support automated identification through machine learning techniques. To achieve this, we utilise galaxies classified as \"overlapping\" from Galaxy Zoo DECaLS (GZD-1, -2, and -5), along with images from Data Release 10 (DR10) of the DESI Legacy Survey, in our individual citizen science project to classify these pairs directly using volunteers. This new catalogue will not only provide a wealth of targets for future dust studies but will also contribute to a deeper understanding of these pairs and dust as a whole.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overlapping galaxies, in which a foreground galaxy partially overlaps a background galaxy, offer a unique opportunity to measure dust attenuation, a key nuisance parameter in galaxy studies, empirically and in great detail by modelling the light of both the foreground and background galaxy and inferring the missing light in the overlapping region. However, the current catalogue of overlapping pairs is relatively limited in number compared to catalogues dedicated to individual galaxies. Expanding this catalogue is not only a necessity to facilitate further detailed dust studies beyond the few limited studies conducted thus far, but also to improve pair-to-pair variance and support automated identification through machine learning techniques. To achieve this, we utilise galaxies classified as \"overlapping\" from Galaxy Zoo DECaLS (GZD-1, -2, and -5), along with images from Data Release 10 (DR10) of the DESI Legacy Survey, in our individual citizen science project to classify these pairs directly using volunteers. This new catalogue will not only provide a wealth of targets for future dust studies but will also contribute to a deeper understanding of these pairs and dust as a whole."
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T16:28:28Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    28,
                    28,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "15 pages, 7 figures, 8 tables, and a link to the catalogue included. Soon to be published in PASA",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA"
                },
                "authors": [
                    {
                        "name": "T. Butrum"
                    },
                    {
                        "name": "B. Holwerda"
                    },
                    {
                        "name": "W. C. Keel"
                    },
                    {
                        "name": "C. Robertson"
                    },
                    {
                        "name": "I. Castellano"
                    },
                    {
                        "name": "S. Pandey"
                    },
                    {
                        "name": "S. M. R. Adnan"
                    },
                    {
                        "name": "L. C. Bills"
                    },
                    {
                        "name": "D. Patel"
                    },
                    {
                        "name": "K. Cook"
                    },
                    {
                        "name": "T. Hardin"
                    },
                    {
                        "name": "A. Palao"
                    },
                    {
                        "name": "B. Connelly"
                    },
                    {
                        "name": "M. Morton"
                    }
                ],
                "author_detail": {
                    "name": "M. Morton"
                },
                "author": "M. Morton"
            },
            {
                "id": "http://arxiv.org/abs/2602.22077v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22077v1",
                "title": "ViSTAR: Virtual Skill Training with Augmented Reality with 3D Avatars and LLM coaching agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViSTAR: Virtual Skill Training with Augmented Reality with 3D Avatars and LLM coaching agent"
                },
                "updated": "2026-02-25T16:26:00Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    26,
                    0,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22077v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present ViSTAR, a Virtual Skill Training system in AR that supports self-guided basketball skill practice, with feedback on balance, posture, and timing. From a formative study with basketball players and coaches, the system addresses three challenges: understanding skills, identifying errors, and correcting mistakes. ViSTAR follows the Behavioral Skills Training (BST) framework-instruction, modeling, rehearsal, and feedback. It provides feedback through visual overlays, rhythm and timing cues, and an AI-powered coaching agent using 3D motion reconstruction. We generate verbal feedback by analyzing spatio-temporal joint data and mapping features to natural-language coaching cues via a Large Language Model (LLM). A key novelty is this feedback generation: motion features become concise coaching insights. In two studies (N=16), participants generally preferred our AI-generated feedback to coach feedback and reported that ViSTAR helped them notice posture and balance issues and refine movements beyond self-observation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ViSTAR, a Virtual Skill Training system in AR that supports self-guided basketball skill practice, with feedback on balance, posture, and timing. From a formative study with basketball players and coaches, the system addresses three challenges: understanding skills, identifying errors, and correcting mistakes. ViSTAR follows the Behavioral Skills Training (BST) framework-instruction, modeling, rehearsal, and feedback. It provides feedback through visual overlays, rhythm and timing cues, and an AI-powered coaching agent using 3D motion reconstruction. We generate verbal feedback by analyzing spatio-temporal joint data and mapping features to natural-language coaching cues via a Large Language Model (LLM). A key novelty is this feedback generation: motion features become concise coaching insights. In two studies (N=16), participants generally preferred our AI-generated feedback to coach feedback and reported that ViSTAR helped them notice posture and balance issues and refine movements beyond self-observation."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T16:26:00Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    26,
                    0,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Chunggi Lee"
                    },
                    {
                        "name": "Hayato Saiki"
                    },
                    {
                        "name": "Tica Lin"
                    },
                    {
                        "name": "Eiji Ikeda"
                    },
                    {
                        "name": "Kenji Suzuki"
                    },
                    {
                        "name": "Chen Zhu-Tian"
                    },
                    {
                        "name": "Hanspeter Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Hanspeter Pfister"
                },
                "author": "Hanspeter Pfister"
            },
            {
                "id": "http://arxiv.org/abs/2412.15042v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2412.15042v2",
                "title": "Scylla: Translating an Applicative Subset of C to Safe Rust",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scylla: Translating an Applicative Subset of C to Safe Rust"
                },
                "updated": "2026-02-25T16:25:13Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    25,
                    13,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2412.15042v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2412.15042v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The popularity of the Rust language continues to explode; yet, many critical codebases remain authored in C. Automatically translating C to Rust is thus an appealing course of action. Several works have gone down this path, handling an ever-increasing subset of C through a variety of Rust features, such as unsafe. While the prospect of automation is appealing, producing code that relies on unsafe negates the memory safety guarantees offered by Rust, and therefore the main advantages of porting existing codebases to memory-safe languages. We instead advocate for a different approach, where the programmer iterates on the original C, gradually making the code more structured until it becomes eligible for compilation to safe Rust. This means that redesigns and rewrites can be evaluated incrementally for performance and correctness against existing test suites and production environments.\n  Compiling structured C to safe Rust relies on the following contributions: a type-directed translation from (a subset of) C to safe Rust; a novel static analysis based on \"split trees\" which allows expressing C's pointer arithmetic using Rust's slices and splitting operations; an analysis that infers which borrows need to be mutable; and a compilation strategy for C pointer types that is compatible with Rust's distinction between non-owned and owned allocations. We evaluate our approach on real-world cryptographic libraries, binary parsers and serializers, and a file compression library. We show that these can be rewritten to Rust with small refactors of the original C code, and that the resulting Rust code exhibits similar performance characteristics as the original C code. As part of our translation process, we also identify and report undefined behaviors in the bzip2 compression library and in Microsoft's implementation of the FrodoKEM cryptographic primitive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The popularity of the Rust language continues to explode; yet, many critical codebases remain authored in C. Automatically translating C to Rust is thus an appealing course of action. Several works have gone down this path, handling an ever-increasing subset of C through a variety of Rust features, such as unsafe. While the prospect of automation is appealing, producing code that relies on unsafe negates the memory safety guarantees offered by Rust, and therefore the main advantages of porting existing codebases to memory-safe languages. We instead advocate for a different approach, where the programmer iterates on the original C, gradually making the code more structured until it becomes eligible for compilation to safe Rust. This means that redesigns and rewrites can be evaluated incrementally for performance and correctness against existing test suites and production environments.\n  Compiling structured C to safe Rust relies on the following contributions: a type-directed translation from (a subset of) C to safe Rust; a novel static analysis based on \"split trees\" which allows expressing C's pointer arithmetic using Rust's slices and splitting operations; an analysis that infers which borrows need to be mutable; and a compilation strategy for C pointer types that is compatible with Rust's distinction between non-owned and owned allocations. We evaluate our approach on real-world cryptographic libraries, binary parsers and serializers, and a file compression library. We show that these can be rewritten to Rust with small refactors of the original C code, and that the resulting Rust code exhibits similar performance characteristics as the original C code. As part of our translation process, we also identify and report undefined behaviors in the bzip2 compression library and in Microsoft's implementation of the FrodoKEM cryptographic primitive."
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-12-19T16:51:29Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    51,
                    29,
                    3,
                    354,
                    0
                ],
                "arxiv_comment": "OOPSLA 2026 camera-ready version",
                "arxiv_primary_category": {
                    "term": "cs.PL"
                },
                "authors": [
                    {
                        "name": "Aymeric Fromherz"
                    },
                    {
                        "name": "Jonathan Protzenko"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Protzenko"
                },
                "author": "Jonathan Protzenko"
            },
            {
                "id": "http://arxiv.org/abs/2602.22072v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22072v1",
                "title": "Understanding Artificial Theory of Mind: Perturbed Tasks and Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Artificial Theory of Mind: Perturbed Tasks and Reasoning in Large Language Models"
                },
                "updated": "2026-02-25T16:24:35Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    24,
                    35,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22072v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Theory of Mind (ToM) refers to an agent's ability to model the internal states of others. Contributing to the debate whether large language models (LLMs) exhibit genuine ToM capabilities, our study investigates their ToM robustness using perturbations on false-belief tasks and examines the potential of Chain-of-Thought prompting (CoT) to enhance performance and explain the LLM's decision. We introduce a handcrafted, richly annotated ToM dataset, including classic and perturbed false belief tasks, the corresponding spaces of valid reasoning chains for correct task completion, subsequent reasoning faithfulness, task solutions, and propose metrics to evaluate reasoning chain correctness and to what extent final answers are faithful to reasoning traces of the generated CoT. We show a steep drop in ToM capabilities under task perturbation for all evaluated LLMs, questioning the notion of any robust form of ToM being present. While CoT prompting improves the ToM performance overall in a faithful manner, it surprisingly degrades accuracy for some perturbation classes, indicating that selective application is necessary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory of Mind (ToM) refers to an agent's ability to model the internal states of others. Contributing to the debate whether large language models (LLMs) exhibit genuine ToM capabilities, our study investigates their ToM robustness using perturbations on false-belief tasks and examines the potential of Chain-of-Thought prompting (CoT) to enhance performance and explain the LLM's decision. We introduce a handcrafted, richly annotated ToM dataset, including classic and perturbed false belief tasks, the corresponding spaces of valid reasoning chains for correct task completion, subsequent reasoning faithfulness, task solutions, and propose metrics to evaluate reasoning chain correctness and to what extent final answers are faithful to reasoning traces of the generated CoT. We show a steep drop in ToM capabilities under task perturbation for all evaluated LLMs, questioning the notion of any robust form of ToM being present. While CoT prompting improves the ToM performance overall in a faithful manner, it surprisingly degrades accuracy for some perturbation classes, indicating that selective application is necessary."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T16:24:35Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    24,
                    35,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Christian Nickel"
                    },
                    {
                        "name": "Laura Schrewe"
                    },
                    {
                        "name": "Florian Mai"
                    },
                    {
                        "name": "Lucie Flek"
                    }
                ],
                "author_detail": {
                    "name": "Lucie Flek"
                },
                "author": "Lucie Flek"
            },
            {
                "id": "http://arxiv.org/abs/2602.22070v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22070v1",
                "title": "Language Models Exhibit Inconsistent Biases Towards Algorithmic Agents and Human Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Exhibit Inconsistent Biases Towards Algorithmic Agents and Human Experts"
                },
                "updated": "2026-02-25T16:18:28Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    18,
                    28,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22070v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models are increasingly used in decision-making tasks that require them to process information from a variety of sources, including both human experts and other algorithmic agents. How do LLMs weigh the information provided by these different sources? We consider the well-studied phenomenon of algorithm aversion, in which human decision-makers exhibit bias against predictions from algorithms. Drawing upon experimental paradigms from behavioural economics, we evaluate how eightdifferent LLMs delegate decision-making tasks when the delegatee is framed as a human expert or an algorithmic agent. To be inclusive of different evaluation formats, we conduct our study with two task presentations: stated preferences, modeled through direct queries about trust towards either agent, and revealed preferences, modeled through providing in-context examples of the performance of both agents. When prompted to rate the trustworthiness of human experts and algorithms across diverse tasks, LLMs give higher ratings to the human expert, which correlates with prior results from human respondents. However, when shown the performance of a human expert and an algorithm and asked to place an incentivized bet between the two, LLMs disproportionately choose the algorithm, even when it performs demonstrably worse. These discrepant results suggest that LLMs may encode inconsistent biases towards humans and algorithms, which need to be carefully considered when they are deployed in high-stakes scenarios. Furthermore, we discuss the sensitivity of LLMs to task presentation formats that should be broadly scrutinized in evaluation robustness for AI safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are increasingly used in decision-making tasks that require them to process information from a variety of sources, including both human experts and other algorithmic agents. How do LLMs weigh the information provided by these different sources? We consider the well-studied phenomenon of algorithm aversion, in which human decision-makers exhibit bias against predictions from algorithms. Drawing upon experimental paradigms from behavioural economics, we evaluate how eightdifferent LLMs delegate decision-making tasks when the delegatee is framed as a human expert or an algorithmic agent. To be inclusive of different evaluation formats, we conduct our study with two task presentations: stated preferences, modeled through direct queries about trust towards either agent, and revealed preferences, modeled through providing in-context examples of the performance of both agents. When prompted to rate the trustworthiness of human experts and algorithms across diverse tasks, LLMs give higher ratings to the human expert, which correlates with prior results from human respondents. However, when shown the performance of a human expert and an algorithm and asked to place an incentivized bet between the two, LLMs disproportionately choose the algorithm, even when it performs demonstrably worse. These discrepant results suggest that LLMs may encode inconsistent biases towards humans and algorithms, which need to be carefully considered when they are deployed in high-stakes scenarios. Furthermore, we discuss the sensitivity of LLMs to task presentation formats that should be broadly scrutinized in evaluation robustness for AI safety."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T16:18:28Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    18,
                    28,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "Second Conference of the International Association for Safe and Ethical Artificial Intelligence (IASEAI 2026)",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jessica Y. Bo"
                    },
                    {
                        "name": "Lillio Mok"
                    },
                    {
                        "name": "Ashton Anderson"
                    }
                ],
                "author_detail": {
                    "name": "Ashton Anderson"
                },
                "author": "Ashton Anderson"
            },
            {
                "id": "http://arxiv.org/abs/2602.22067v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22067v1",
                "title": "Semantic Partial Grounding via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Partial Grounding via LLMs"
                },
                "updated": "2026-02-25T16:13:26Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    13,
                    26,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22067v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Grounding is a critical step in classical planning, yet it often becomes a computational bottleneck due to the exponential growth in grounded actions and atoms as task size increases. Recent advances in partial grounding have addressed this challenge by incrementally grounding only the most promising operators, guided by predictive models. However, these approaches primarily rely on relational features or learned embeddings and do not leverage the textual and structural cues present in PDDL descriptions. We propose SPG-LLM, which uses LLMs to analyze the domain and problem files to heuristically identify potentially irrelevant objects, actions, and predicates prior to grounding, significantly reducing the size of the grounded task. Across seven hard-to-ground benchmarks, SPG-LLM achieves faster grounding-often by orders of magnitude-while delivering comparable or better plan costs in some domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounding is a critical step in classical planning, yet it often becomes a computational bottleneck due to the exponential growth in grounded actions and atoms as task size increases. Recent advances in partial grounding have addressed this challenge by incrementally grounding only the most promising operators, guided by predictive models. However, these approaches primarily rely on relational features or learned embeddings and do not leverage the textual and structural cues present in PDDL descriptions. We propose SPG-LLM, which uses LLMs to analyze the domain and problem files to heuristically identify potentially irrelevant objects, actions, and predicates prior to grounding, significantly reducing the size of the grounded task. Across seven hard-to-ground benchmarks, SPG-LLM achieves faster grounding-often by orders of magnitude-while delivering comparable or better plan costs in some domains."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T16:13:26Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    13,
                    26,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Giuseppe Canonaco"
                    },
                    {
                        "name": "Alberto Pozanco"
                    },
                    {
                        "name": "Daniel Borrajo"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Borrajo"
                },
                "author": "Daniel Borrajo"
            },
            {
                "id": "http://arxiv.org/abs/2509.11517v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.11517v2",
                "title": "PeruMedQA: Benchmarking Large Language Models (LLMs) on Peruvian Medical Exams -- Dataset Construction and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PeruMedQA: Benchmarking Large Language Models (LLMs) on Peruvian Medical Exams -- Dataset Construction and Evaluation"
                },
                "updated": "2026-02-25T16:04:59Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    4,
                    59,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.11517v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.11517v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "BACKGROUND: Medical large language models (LLMs) have demonstrated remarkable performance in answering medical examinations. However, the extent to which this high performance is transferable to medical questions in Spanish and from a Latin American country remains unexplored. This knowledge is crucial as LLM-based medical applications gain traction in Latin America. AIMS: To build a dataset of questions medical examinations taken by Peruvian physicians pursuing specialty training; to fine-tune a LLM on this dataset; to evaluate and compare the performance in terms of accuracy between vanilla LLMs and the fine-tuned LLM. METHODS: We curated PeruMedQA, a multiple-choice question-answering (MCQA) dataset containing 8,380 questions spanning 12 specialties (2018-2025). We selected ten medical LLMs, including medgemma-4b-it and medgemma-27b-text-it, and developed zero-shot task specific prompts to answer the questions. We employed parameter-efficient fine tuning (PEFT) and low-rand adaptation (LoRA) to fine-tune medgemma-4b-it utilizing all questions except those from 2025 (test set). RESULTS: Medgemma-27b showed the highest accuracy across all specialities, achieving the highest score of 89.29% in Psychiatry; yet, in two specialties, OctoMed-7B exhibited slight superiority: Neurosurgery with 77.27% and 77.38, respectively; and Radiology with 76.13% and 77.39%, respectively. Across specialties, most LLMs with <10 billion parameters exhibited <50% of correct answers. The fine-tuned version of medgemma-4b-it emerged victorious against all LLMs with <10 billion parameters and rivaled a LLM with 70 billion parameters across various examinations. CONCLUSIONS: For medical AI applications and research that require knowledge bases from Spanish-speaking countries and those exhibiting similar epidemiological profile to Peru's, interested parties should utilize medgemma-27b-text-it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BACKGROUND: Medical large language models (LLMs) have demonstrated remarkable performance in answering medical examinations. However, the extent to which this high performance is transferable to medical questions in Spanish and from a Latin American country remains unexplored. This knowledge is crucial as LLM-based medical applications gain traction in Latin America. AIMS: To build a dataset of questions medical examinations taken by Peruvian physicians pursuing specialty training; to fine-tune a LLM on this dataset; to evaluate and compare the performance in terms of accuracy between vanilla LLMs and the fine-tuned LLM. METHODS: We curated PeruMedQA, a multiple-choice question-answering (MCQA) dataset containing 8,380 questions spanning 12 specialties (2018-2025). We selected ten medical LLMs, including medgemma-4b-it and medgemma-27b-text-it, and developed zero-shot task specific prompts to answer the questions. We employed parameter-efficient fine tuning (PEFT) and low-rand adaptation (LoRA) to fine-tune medgemma-4b-it utilizing all questions except those from 2025 (test set). RESULTS: Medgemma-27b showed the highest accuracy across all specialities, achieving the highest score of 89.29% in Psychiatry; yet, in two specialties, OctoMed-7B exhibited slight superiority: Neurosurgery with 77.27% and 77.38, respectively; and Radiology with 76.13% and 77.39%, respectively. Across specialties, most LLMs with <10 billion parameters exhibited <50% of correct answers. The fine-tuned version of medgemma-4b-it emerged victorious against all LLMs with <10 billion parameters and rivaled a LLM with 70 billion parameters across various examinations. CONCLUSIONS: For medical AI applications and research that require knowledge bases from Spanish-speaking countries and those exhibiting similar epidemiological profile to Peru's, interested parties should utilize medgemma-27b-text-it."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-15T02:07:26Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    2,
                    7,
                    26,
                    0,
                    258,
                    0
                ],
                "arxiv_comment": "https://github.com/rodrigo-carrillo/PeruMedQA",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Rodrigo M. Carrillo-Larco"
                    },
                    {
                        "name": "Jesus Lovn Melgarejo"
                    },
                    {
                        "name": "Manuel Castillo-Cara"
                    },
                    {
                        "name": "Gusseppe Bravo-Rocca"
                    }
                ],
                "author_detail": {
                    "name": "Gusseppe Bravo-Rocca"
                },
                "author": "Gusseppe Bravo-Rocca"
            },
            {
                "id": "http://arxiv.org/abs/2602.22052v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22052v1",
                "title": "AutoSew: A Geometric Approach to Stitching Prediction with Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoSew: A Geometric Approach to Stitching Prediction with Graph Neural Networks"
                },
                "updated": "2026-02-25T16:03:33Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    3,
                    33,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22052v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Automating garment assembly from sewing patterns remains a significant challenge due to the lack of standardized annotation protocols and the frequent absence of semantic cues. Existing methods often rely on panel labels or handcrafted heuristics, which limit their applicability to real-world, non-conforming patterns. We present AutoSew, a fully automatic, geometry-based approach for predicting stitch correspondences directly from 2D pattern contours. AutoSew formulates the problem as a graph matching task, leveraging a Graph Neural Network to capture local and global geometric context, and employing a differentiable optimal transport solver to infer stitching relationships-including multi-edge connections. To support this task, we update the GarmentCodeData dataset modifying over 18k patterns with realistic multi-edge annotations, reflecting industrial assembly scenarios. AutoSew achieves 96% F1-score and successfully assembles 73.3% of test garments without error, outperforming existing methods while relying solely on geometric input. Our results demonstrate that geometry alone can robustly guide stitching prediction, enabling scalable garment assembly without manual input.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating garment assembly from sewing patterns remains a significant challenge due to the lack of standardized annotation protocols and the frequent absence of semantic cues. Existing methods often rely on panel labels or handcrafted heuristics, which limit their applicability to real-world, non-conforming patterns. We present AutoSew, a fully automatic, geometry-based approach for predicting stitch correspondences directly from 2D pattern contours. AutoSew formulates the problem as a graph matching task, leveraging a Graph Neural Network to capture local and global geometric context, and employing a differentiable optimal transport solver to infer stitching relationships-including multi-edge connections. To support this task, we update the GarmentCodeData dataset modifying over 18k patterns with realistic multi-edge annotations, reflecting industrial assembly scenarios. AutoSew achieves 96% F1-score and successfully assembles 73.3% of test garments without error, outperforming existing methods while relying solely on geometric input. Our results demonstrate that geometry alone can robustly guide stitching prediction, enabling scalable garment assembly without manual input."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T16:03:33Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    3,
                    33,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "WACV 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Pablo Ros-Navarro"
                    },
                    {
                        "name": "Elena Garces"
                    },
                    {
                        "name": "Jorge Lopez-Moreno"
                    }
                ],
                "author_detail": {
                    "name": "Jorge Lopez-Moreno"
                },
                "author": "Jorge Lopez-Moreno"
            },
            {
                "id": "http://arxiv.org/abs/2511.11910v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.11910v3",
                "title": "Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models"
                },
                "updated": "2026-02-25T16:03:05Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    3,
                    5,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.11910v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.11910v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite the recent advances in the video understanding ability of multimodal large language models (MLLMs), long video understanding remains a challenge. One of the main issues is that the number of vision tokens grows linearly with video length, which causes an explosion in attention cost, memory, and latency. To solve this challenge, we present Query-aware Token Selector (\\textbf{QTSplus}), a lightweight yet powerful visual token selection module that serves as an information gate between the vision encoder and LLMs. Given a text query and video tokens, QTSplus dynamically selects the most important visual evidence for the input text query by (i) scoring visual tokens via cross-attention, (ii) \\emph{predicting} an instance-specific retention budget based on the complexity of the query, and (iii) \\emph{selecting} Top-$n$ tokens with a differentiable straight-through estimator during training and a hard gate at inference. Furthermore, a small re-encoder preserves temporal order using absolute time information, enabling second-level localization while maintaining global coverage.\n  Integrated into Qwen2.5-VL, QTSplus compresses the vision stream by up to \\textbf{89\\%} and reduces end-to-end latency by \\textbf{28\\%} on long videos. The evaluation on eight long video understanding benchmarks shows near-parity accuracy overall when compared with the original Qwen models and outperforms the original model by \\textbf{+20.5} and \\textbf{+5.6} points respectively on TempCompass direction and order accuracies. These results show that QTSplus is an effective, general mechanism for scaling MLLMs to real-world long-video scenarios while preserving task-relevant evidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent advances in the video understanding ability of multimodal large language models (MLLMs), long video understanding remains a challenge. One of the main issues is that the number of vision tokens grows linearly with video length, which causes an explosion in attention cost, memory, and latency. To solve this challenge, we present Query-aware Token Selector (\\textbf{QTSplus}), a lightweight yet powerful visual token selection module that serves as an information gate between the vision encoder and LLMs. Given a text query and video tokens, QTSplus dynamically selects the most important visual evidence for the input text query by (i) scoring visual tokens via cross-attention, (ii) \\emph{predicting} an instance-specific retention budget based on the complexity of the query, and (iii) \\emph{selecting} Top-$n$ tokens with a differentiable straight-through estimator during training and a hard gate at inference. Furthermore, a small re-encoder preserves temporal order using absolute time information, enabling second-level localization while maintaining global coverage.\n  Integrated into Qwen2.5-VL, QTSplus compresses the vision stream by up to \\textbf{89\\%} and reduces end-to-end latency by \\textbf{28\\%} on long videos. The evaluation on eight long video understanding benchmarks shows near-parity accuracy overall when compared with the original Qwen models and outperforms the original model by \\textbf{+20.5} and \\textbf{+5.6} points respectively on TempCompass direction and order accuracies. These results show that QTSplus is an effective, general mechanism for scaling MLLMs to real-world long-video scenarios while preserving task-relevant evidence."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-14T22:41:27Z",
                "published_parsed": [
                    2025,
                    11,
                    14,
                    22,
                    41,
                    27,
                    4,
                    318,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Siyou Li"
                    },
                    {
                        "name": "Huanan Wu"
                    },
                    {
                        "name": "Juexi Shao"
                    },
                    {
                        "name": "Yinghao Ma"
                    },
                    {
                        "name": "Yujian Gan"
                    },
                    {
                        "name": "Yihao Luo"
                    },
                    {
                        "name": "Yuwei Wang"
                    },
                    {
                        "name": "Dong Nie"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Wenqing Wu"
                    },
                    {
                        "name": "Le Zhang"
                    },
                    {
                        "name": "Massimo Poesio"
                    },
                    {
                        "name": "Juntao Yu"
                    }
                ],
                "author_detail": {
                    "name": "Juntao Yu"
                },
                "author": "Juntao Yu"
            },
            {
                "id": "http://arxiv.org/abs/2510.10625v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.10625v3",
                "title": "ImpMIA: Leveraging Implicit Bias for Membership Inference Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ImpMIA: Leveraging Implicit Bias for Membership Inference Attack"
                },
                "updated": "2026-02-25T15:52:01Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    52,
                    1,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.10625v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.10625v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Determining which data samples were used to train a model, known as Membership Inference Attack (MIA), is a well-studied and important problem with implications on data privacy. SotA methods (which are black-box attacks) rely on training many auxiliary reference models to imitate the behavior of the attacked model. As such, they rely on assumptions which rarely hold in real-world settings: (i) the attacker knows the training hyperparameters; (ii) all available non-training samples come from the same distribution as the training data; and (iii) the fraction of training data in the evaluation set is known. We show that removing these assumptions significantly harms the performance of black-box attacks. We introduce ImpMIA, a Membership Inference Attack that exploits the Implicit Bias of neural networks. Building on the maximum-margin implicit bias theory, ImpMIA uses the Karush-Kuhn-Tucker (KKT) optimality conditions to identify training samples -- those whose gradients most strongly reconstruct the trained model's parameters. Our approach is optimization-based, and requires NO training of reference-models, thus removing the need for any knowledge/assumptions regarding the attacked model's training procedure. While ImpMIA is a white-box attack (a setting which assumes access to model weights), this is becoming increasingly realistic given that many models are publicly available (e.g., via Hugging Face). ImpMIA achieves SotA performance compared to both black and white box attacks in settings where only the model weights are known, and a superset of the training data is available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Determining which data samples were used to train a model, known as Membership Inference Attack (MIA), is a well-studied and important problem with implications on data privacy. SotA methods (which are black-box attacks) rely on training many auxiliary reference models to imitate the behavior of the attacked model. As such, they rely on assumptions which rarely hold in real-world settings: (i) the attacker knows the training hyperparameters; (ii) all available non-training samples come from the same distribution as the training data; and (iii) the fraction of training data in the evaluation set is known. We show that removing these assumptions significantly harms the performance of black-box attacks. We introduce ImpMIA, a Membership Inference Attack that exploits the Implicit Bias of neural networks. Building on the maximum-margin implicit bias theory, ImpMIA uses the Karush-Kuhn-Tucker (KKT) optimality conditions to identify training samples -- those whose gradients most strongly reconstruct the trained model's parameters. Our approach is optimization-based, and requires NO training of reference-models, thus removing the need for any knowledge/assumptions regarding the attacked model's training procedure. While ImpMIA is a white-box attack (a setting which assumes access to model weights), this is becoming increasingly realistic given that many models are publicly available (e.g., via Hugging Face). ImpMIA achieves SotA performance compared to both black and white box attacks in settings where only the model weights are known, and a superset of the training data is available."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-12T14:12:28Z",
                "published_parsed": [
                    2025,
                    10,
                    12,
                    14,
                    12,
                    28,
                    6,
                    285,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yuval Golbari"
                    },
                    {
                        "name": "Navve Wasserman"
                    },
                    {
                        "name": "Gal Vardi"
                    },
                    {
                        "name": "Michal Irani"
                    }
                ],
                "author_detail": {
                    "name": "Michal Irani"
                },
                "author": "Michal Irani"
            },
            {
                "id": "http://arxiv.org/abs/2602.22020v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22020v1",
                "title": "Detecting UX smells in Visual Studio Code using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting UX smells in Visual Studio Code using LLMs"
                },
                "updated": "2026-02-25T15:32:17Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    32,
                    17,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22020v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3786151.3788606",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Integrated Development Environments shape developers' daily experience, yet the empirical study of their usability and user experience (UX) remains limited. This work presents an LLM-assisted approach to detecting UX smells in Visual Studio Code by mining and classifying user-reported issues from the GitHub repository. Using a validated taxonomy and expert review, we identified recurring UX problems that affect the developer experience. Our results show that the majority of UX smells are concentrated in informativeness, clarity, intuitiveness, and efficiency, qualities that developers value most.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated Development Environments shape developers' daily experience, yet the empirical study of their usability and user experience (UX) remains limited. This work presents an LLM-assisted approach to detecting UX smells in Visual Studio Code by mining and classifying user-reported issues from the GitHub repository. Using a validated taxonomy and expert review, we identified recurring UX problems that affect the developer experience. Our results show that the majority of UX smells are concentrated in informativeness, clarity, intuitiveness, and efficiency, qualities that developers value most."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T15:32:17Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    32,
                    17,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "4 pages, 2 figures, 1 table, 3rd International Workshop on Integrated Development Environments (IDE 2026)",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Andrs Rodriguez"
                    },
                    {
                        "name": "Juan Cruz Gardey"
                    },
                    {
                        "name": "Alejandra Garrido"
                    }
                ],
                "author_detail": {
                    "name": "Alejandra Garrido"
                },
                "author": "Alejandra Garrido",
                "arxiv_doi": "10.1145/3786151.3788606"
            },
            {
                "id": "http://arxiv.org/abs/2602.22018v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22018v1",
                "title": "Disease Progression and Subtype Modeling for Combined Discrete and Continuous Input Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disease Progression and Subtype Modeling for Combined Discrete and Continuous Input Data"
                },
                "updated": "2026-02-25T15:31:30Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    31,
                    30,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22018v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Disease progression modeling provides a robust framework to identify long-term disease trajectories from short-term biomarker data. It is a valuable tool to gain a deeper understanding of diseases with a long disease trajectory, such as Alzheimer's disease. A key limitation of most disease progression models is that they are specific to a single data type (e.g., continuous data), thereby limiting their applicability to heterogeneous, real-world datasets. To address this limitation, we propose the Mixed Events model, a novel disease progression model that handles both discrete and continuous data types. This model is implemented within the Subtype and Stage Inference (SuStaIn) framework, resulting in Mixed-SuStaIn, enabling subtype and progression modeling. We demonstrate the effectiveness of Mixed-SuStaIn through simulation experiments and real-world data from the Alzheimer's Disease Neuroimaging Initiative, showing that it performs well on mixed datasets. The code is available at: https://github.com/ucl-pond/pySuStaIn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disease progression modeling provides a robust framework to identify long-term disease trajectories from short-term biomarker data. It is a valuable tool to gain a deeper understanding of diseases with a long disease trajectory, such as Alzheimer's disease. A key limitation of most disease progression models is that they are specific to a single data type (e.g., continuous data), thereby limiting their applicability to heterogeneous, real-world datasets. To address this limitation, we propose the Mixed Events model, a novel disease progression model that handles both discrete and continuous data types. This model is implemented within the Subtype and Stage Inference (SuStaIn) framework, resulting in Mixed-SuStaIn, enabling subtype and progression modeling. We demonstrate the effectiveness of Mixed-SuStaIn through simulation experiments and real-world data from the Alzheimer's Disease Neuroimaging Initiative, showing that it performs well on mixed datasets. The code is available at: https://github.com/ucl-pond/pySuStaIn."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T15:31:30Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    31,
                    30,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "Accepted for publication, 2026 IEEE 23rd International Symposium on Biomedical Imaging (ISBI), April 2026, London, United Kingdom",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Sterre de Jonge"
                    },
                    {
                        "name": "Elisabeth J. Vinke"
                    },
                    {
                        "name": "Meike W. Vernooij"
                    },
                    {
                        "name": "Daniel C. Alexander"
                    },
                    {
                        "name": "Alexandra L. Young"
                    },
                    {
                        "name": "Esther E. Bron"
                    }
                ],
                "author_detail": {
                    "name": "Esther E. Bron"
                },
                "arxiv_affiliation": "Department of Radiology and Nuclear Medicine, Erasmus MC, Rotterdam, The Netherlands",
                "author": "Esther E. Bron"
            },
            {
                "id": "http://arxiv.org/abs/2602.22017v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22017v1",
                "title": "IOAgent: Democratizing Trustworthy HPC I/O Performance Diagnosis Capability via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IOAgent: Democratizing Trustworthy HPC I/O Performance Diagnosis Capability via LLMs"
                },
                "updated": "2026-02-25T15:30:55Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    30,
                    55,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22017v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/IPDPS64566.2025.00036",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "As the complexity of the HPC storage stack rapidly grows, domain scientists face increasing challenges in effectively utilizing HPC storage systems to achieve their desired I/O performance. To identify and address I/O issues, scientists largely rely on I/O experts to analyze their I/O traces and provide insights into potential problems. However, with a limited number of I/O experts and the growing demand for data-intensive applications, inaccessibility has become a major bottleneck, hindering scientists from maximizing their productivity. Rapid advances in LLMs make it possible to build an automated tool that brings trustworthy I/O performance diagnosis to domain scientists. However, key challenges remain, such as the inability to handle long context windows, a lack of accurate domain knowledge about HPC I/O, and the generation of hallucinations during complex interactions.In this work, we propose IOAgent as a systematic effort to address these challenges. IOAgent integrates a module-based pre-processor, a RAG-based domain knowledge integrator, and a tree-based merger to accurately diagnose I/O issues from a given Darshan trace file. Similar to an I/O expert, IOAgent provides detailed justifications and references for its diagnoses and offers an interactive interface for scientists to ask targeted follow-up questions. To evaluate IOAgent, we collected a diverse set of labeled job traces and released the first open diagnosis test suite, TraceBench. Using this test suite, we conducted extensive evaluations, demonstrating that IOAgent matches or outperforms state-of-the-art I/O diagnosis tools with accurate and useful diagnosis results. We also show that IOAgent is not tied to specific LLMs, performing similarly well with both proprietary and open-source LLMs. We believe IOAgent has the potential to become a powerful tool for scientists navigating complex HPC I/O subsystems in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the complexity of the HPC storage stack rapidly grows, domain scientists face increasing challenges in effectively utilizing HPC storage systems to achieve their desired I/O performance. To identify and address I/O issues, scientists largely rely on I/O experts to analyze their I/O traces and provide insights into potential problems. However, with a limited number of I/O experts and the growing demand for data-intensive applications, inaccessibility has become a major bottleneck, hindering scientists from maximizing their productivity. Rapid advances in LLMs make it possible to build an automated tool that brings trustworthy I/O performance diagnosis to domain scientists. However, key challenges remain, such as the inability to handle long context windows, a lack of accurate domain knowledge about HPC I/O, and the generation of hallucinations during complex interactions.In this work, we propose IOAgent as a systematic effort to address these challenges. IOAgent integrates a module-based pre-processor, a RAG-based domain knowledge integrator, and a tree-based merger to accurately diagnose I/O issues from a given Darshan trace file. Similar to an I/O expert, IOAgent provides detailed justifications and references for its diagnoses and offers an interactive interface for scientists to ask targeted follow-up questions. To evaluate IOAgent, we collected a diverse set of labeled job traces and released the first open diagnosis test suite, TraceBench. Using this test suite, we conducted extensive evaluations, demonstrating that IOAgent matches or outperforms state-of-the-art I/O diagnosis tools with accurate and useful diagnosis results. We also show that IOAgent is not tied to specific LLMs, performing similarly well with both proprietary and open-source LLMs. We believe IOAgent has the potential to become a powerful tool for scientists navigating complex HPC I/O subsystems in the future."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T15:30:55Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    30,
                    55,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "Published in the Proceedings of the 2025 IEEE International Parallel and Distributed Processing Symposium (IPDPS 2025)",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Chris Egersdoerfer"
                    },
                    {
                        "name": "Arnav Sareen"
                    },
                    {
                        "name": "Jean Luca Bez"
                    },
                    {
                        "name": "Suren Byna"
                    },
                    {
                        "name": "Dongkuan"
                    },
                    {
                        "name": "Xu"
                    },
                    {
                        "name": "Dong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Dong Dai"
                },
                "arxiv_affiliation": "DK",
                "author": "Dong Dai",
                "arxiv_doi": "10.1109/IPDPS64566.2025.00036"
            },
            {
                "id": "http://arxiv.org/abs/2602.22015v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22015v1",
                "title": "Function-Space Empirical Bayes Regularisation with Student's t Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Function-Space Empirical Bayes Regularisation with Student's t Priors"
                },
                "updated": "2026-02-25T15:29:44Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    29,
                    44,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22015v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Bayesian deep learning (BDL) has emerged as a principled approach to produce reliable uncertainty estimates by integrating deep neural networks with Bayesian inference, and the selection of informative prior distributions remains a significant challenge. Various function-space variational inference (FSVI) regularisation methods have been presented, assigning meaningful priors over model predictions. However, these methods typically rely on a Gaussian prior, which fails to capture the heavy-tailed statistical characteristics inherent in neural network outputs. By contrast, this work proposes a novel function-space empirical Bayes regularisation framework -- termed ST-FS-EB -- which employs heavy-tailed Student's $t$ priors in both parameter and function spaces. Also, we approximate the posterior distribution through variational inference (VI), inducing an evidence lower bound (ELBO) objective based on Monte Carlo (MC) dropout. Furthermore, the proposed method is evaluated against various VI-based BDL baselines, and the results demonstrate its robust performance in in-distribution prediction, out-of-distribution (OOD) detection and handling distribution shifts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian deep learning (BDL) has emerged as a principled approach to produce reliable uncertainty estimates by integrating deep neural networks with Bayesian inference, and the selection of informative prior distributions remains a significant challenge. Various function-space variational inference (FSVI) regularisation methods have been presented, assigning meaningful priors over model predictions. However, these methods typically rely on a Gaussian prior, which fails to capture the heavy-tailed statistical characteristics inherent in neural network outputs. By contrast, this work proposes a novel function-space empirical Bayes regularisation framework -- termed ST-FS-EB -- which employs heavy-tailed Student's $t$ priors in both parameter and function spaces. Also, we approximate the posterior distribution through variational inference (VI), inducing an evidence lower bound (ELBO) objective based on Monte Carlo (MC) dropout. Furthermore, the proposed method is evaluated against various VI-based BDL baselines, and the results demonstrate its robust performance in in-distribution prediction, out-of-distribution (OOD) detection and handling distribution shifts."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T15:29:44Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    29,
                    44,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Pengcheng Hao"
                    },
                    {
                        "name": "Ercan Engin Kuruoglu"
                    }
                ],
                "author_detail": {
                    "name": "Ercan Engin Kuruoglu"
                },
                "author": "Ercan Engin Kuruoglu"
            },
            {
                "id": "http://arxiv.org/abs/2602.22010v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22010v1",
                "title": "World Guidance: World Modeling in Condition Space for Action Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "World Guidance: World Modeling in Condition Space for Action Generation"
                },
                "updated": "2026-02-25T15:27:09Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    27,
                    9,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22010v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Leveraging future observation modeling to facilitate action generation presents a promising avenue for enhancing the capabilities of Vision-Language-Action (VLA) models. However, existing approaches struggle to strike a balance between maintaining efficient, predictable future representations and preserving sufficient fine-grained information to guide precise action generation. To address this limitation, we propose WoG (World Guidance), a framework that maps future observations into compact conditions by injecting them into the action inference pipeline. The VLA is then trained to simultaneously predict these compressed conditions alongside future actions, thereby achieving effective world modeling within the condition space for action inference. We demonstrate that modeling and predicting this condition space not only facilitates fine-grained action generation but also exhibits superior generalization capabilities. Moreover, it learns effectively from substantial human manipulation videos. Extensive experiments across both simulation and real-world environments validate that our method significantly outperforms existing methods based on future prediction. Project page is available at: https://selen-suyue.github.io/WoGNet/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging future observation modeling to facilitate action generation presents a promising avenue for enhancing the capabilities of Vision-Language-Action (VLA) models. However, existing approaches struggle to strike a balance between maintaining efficient, predictable future representations and preserving sufficient fine-grained information to guide precise action generation. To address this limitation, we propose WoG (World Guidance), a framework that maps future observations into compact conditions by injecting them into the action inference pipeline. The VLA is then trained to simultaneously predict these compressed conditions alongside future actions, thereby achieving effective world modeling within the condition space for action inference. We demonstrate that modeling and predicting this condition space not only facilitates fine-grained action generation but also exhibits superior generalization capabilities. Moreover, it learns effectively from substantial human manipulation videos. Extensive experiments across both simulation and real-world environments validate that our method significantly outperforms existing methods based on future prediction. Project page is available at: https://selen-suyue.github.io/WoGNet/"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T15:27:09Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    27,
                    9,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "Project Page: https://selen-suyue.github.io/WoGNet/",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Yue Su"
                    },
                    {
                        "name": "Sijin Chen"
                    },
                    {
                        "name": "Haixin Shi"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Zhengshen Zhang"
                    },
                    {
                        "name": "Ningyuan Huang"
                    },
                    {
                        "name": "Weiheng Zhong"
                    },
                    {
                        "name": "Zhengbang Zhu"
                    },
                    {
                        "name": "Yuxiao Liu"
                    },
                    {
                        "name": "Xihui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xihui Liu"
                },
                "author": "Xihui Liu"
            },
            {
                "id": "http://arxiv.org/abs/2602.22001v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22001v1",
                "title": "Are Foundation Models the Route to Full-Stack Transfer in Robotics?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Foundation Models the Route to Full-Stack Transfer in Robotics?"
                },
                "updated": "2026-02-25T15:19:44Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    19,
                    44,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22001v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In humans and robots alike, transfer learning occurs at different levels of abstraction, from high-level linguistic transfer to low-level transfer of motor skills. In this article, we provide an overview of the impact that foundation models and transformer networks have had on these different levels, bringing robots closer than ever to \"full-stack transfer\". Considering LLMs, VLMs and VLAs from a robotic transfer learning perspective allows us to highlight recurring concepts for transfer, beyond specific implementations. We also consider the challenges of data collection and transfer benchmarks for robotics in the age of foundation models. Are foundation models the route to full-stack transfer in robotics? Our expectation is that they will certainly stay on this route as a key technology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In humans and robots alike, transfer learning occurs at different levels of abstraction, from high-level linguistic transfer to low-level transfer of motor skills. In this article, we provide an overview of the impact that foundation models and transformer networks have had on these different levels, bringing robots closer than ever to \"full-stack transfer\". Considering LLMs, VLMs and VLAs from a robotic transfer learning perspective allows us to highlight recurring concepts for transfer, beyond specific implementations. We also consider the challenges of data collection and transfer benchmarks for robotics in the age of foundation models. Are foundation models the route to full-stack transfer in robotics? Our expectation is that they will certainly stay on this route as a key technology."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T15:19:44Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    19,
                    44,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "12 pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Freek Stulp"
                    },
                    {
                        "name": "Samuel Bustamante"
                    },
                    {
                        "name": "Joo Silvrio"
                    },
                    {
                        "name": "Alin Albu-Schffer"
                    },
                    {
                        "name": "Jeannette Bohg"
                    },
                    {
                        "name": "Shuran Song"
                    }
                ],
                "author_detail": {
                    "name": "Shuran Song"
                },
                "author": "Shuran Song"
            },
            {
                "id": "http://arxiv.org/abs/2602.21998v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21998v1",
                "title": "Design-based theory for causal inference from adaptive experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design-based theory for causal inference from adaptive experiments"
                },
                "updated": "2026-02-25T15:17:21Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    17,
                    21,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21998v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Adaptive designs dynamically update treatment probabilities using information accumulated during the experiment. Existing theory for causal inference from adaptive experiments primarily assumes the superpopulation framework with independent and identically distributed units, and may not apply when the distribution of units evolves over time. This paper makes two contributions. First, we extend the literature to the finite-population framework, which allows for possibly nonexchangeable units, and establish the design-based theory for causal inference under general adaptive designs using inverse-propensity-weighted (IPW) and augmented IPW (AIPW) estimators. Our theory accommodates nonexchangeable units, both nonconverging and vanishing treatment probabilities, and nonconverging outcome estimators, thereby justifying inference using AIPW estimators with black-box outcome models that integrate advances from machine learning methods. To alleviate the conservativeness inherent in variance estimation under finite-population inference, we also introduce a covariance estimator for the AIPW estimator that becomes sharp when the residuals from the adaptive regression of potential outcomes on covariates are additive across units. Our framework encompasses widely used adaptive designs, such as multi-armed bandits, covariate-adaptive randomization, and sequential rerandomization, advancing the design-based theory for causal inference in these specific settings. Second, as a methodological contribution, we propose an adaptive covariate adjustment approach for analyzing even nonadaptive designs. The martingale structure induced by adaptive adjustment enables valid inference with black-box outcome estimators that would otherwise require strong assumptions under standard nonadaptive analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive designs dynamically update treatment probabilities using information accumulated during the experiment. Existing theory for causal inference from adaptive experiments primarily assumes the superpopulation framework with independent and identically distributed units, and may not apply when the distribution of units evolves over time. This paper makes two contributions. First, we extend the literature to the finite-population framework, which allows for possibly nonexchangeable units, and establish the design-based theory for causal inference under general adaptive designs using inverse-propensity-weighted (IPW) and augmented IPW (AIPW) estimators. Our theory accommodates nonexchangeable units, both nonconverging and vanishing treatment probabilities, and nonconverging outcome estimators, thereby justifying inference using AIPW estimators with black-box outcome models that integrate advances from machine learning methods. To alleviate the conservativeness inherent in variance estimation under finite-population inference, we also introduce a covariance estimator for the AIPW estimator that becomes sharp when the residuals from the adaptive regression of potential outcomes on covariates are additive across units. Our framework encompasses widely used adaptive designs, such as multi-armed bandits, covariate-adaptive randomization, and sequential rerandomization, advancing the design-based theory for causal inference in these specific settings. Second, as a methodological contribution, we propose an adaptive covariate adjustment approach for analyzing even nonadaptive designs. The martingale structure induced by adaptive adjustment enables valid inference with black-box outcome estimators that would otherwise require strong assumptions under standard nonadaptive analysis."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T15:17:21Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    17,
                    21,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Xinran Li"
                    },
                    {
                        "name": "Anqi Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Anqi Zhao"
                },
                "author": "Anqi Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2602.21997v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21997v1",
                "title": "Enhancing LLM-Based Test Generation by Eliminating Covered Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM-Based Test Generation by Eliminating Covered Code"
                },
                "updated": "2026-02-25T15:16:43Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    16,
                    43,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21997v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Automated test generation is essential for software quality assurance, with coverage rate serving as a key metric to ensure thorough testing. Recent advancements in Large Language Models (LLMs) have shown promise in improving test generation, particularly in achieving higher coverage. However, while existing LLM-based test generation solutions perform well on small, isolated code snippets, they struggle when applied to complex methods under test. To address these issues, we propose a scalable LLM-based unit test generation method. Our approach consists of two key steps. The first step is context information retrieval, which uses both LLMs and static analysis to gather relevant contextual information associated with the complex methods under test. The second step, iterative test generation with code elimination, repeatedly generates unit tests for the code slice, tracks the achieved coverage, and selectively removes code segments that have already been covered. This process simplifies the testing task and mitigates issues arising from token limits or reduced reasoning effectiveness associated with excessively long contexts. Through comprehensive evaluations on open-source projects, our approach outperforms state-of-the-art LLM-based and search-based methods, demonstrating its effectiveness in achieving high coverage on complex methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated test generation is essential for software quality assurance, with coverage rate serving as a key metric to ensure thorough testing. Recent advancements in Large Language Models (LLMs) have shown promise in improving test generation, particularly in achieving higher coverage. However, while existing LLM-based test generation solutions perform well on small, isolated code snippets, they struggle when applied to complex methods under test. To address these issues, we propose a scalable LLM-based unit test generation method. Our approach consists of two key steps. The first step is context information retrieval, which uses both LLMs and static analysis to gather relevant contextual information associated with the complex methods under test. The second step, iterative test generation with code elimination, repeatedly generates unit tests for the code slice, tracks the achieved coverage, and selectively removes code segments that have already been covered. This process simplifies the testing task and mitigates issues arising from token limits or reduced reasoning effectiveness associated with excessively long contexts. Through comprehensive evaluations on open-source projects, our approach outperforms state-of-the-art LLM-based and search-based methods, demonstrating its effectiveness in achieving high coverage on complex methods."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T15:16:43Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    16,
                    43,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "9 pages, 4 figures, supplementary material included",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "WeiZhe Xu"
                    },
                    {
                        "name": "Mengyu Liu"
                    },
                    {
                        "name": "Fanxin Kong"
                    }
                ],
                "author_detail": {
                    "name": "Fanxin Kong"
                },
                "author": "Fanxin Kong"
            },
            {
                "id": "http://arxiv.org/abs/2602.02007v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.02007v2",
                "title": "Beyond RAG for Agent Memory: Retrieval by Decoupling and Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond RAG for Agent Memory: Retrieval by Decoupling and Aggregation"
                },
                "updated": "2026-02-25T15:14:28Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    14,
                    28,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.02007v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.02007v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Agent memory systems often adopt the standard Retrieval-Augmented Generation (RAG) pipeline, yet its underlying assumptions differ in this setting. RAG targets large, heterogeneous corpora where retrieved passages are diverse, whereas agent memory is a bounded, coherent dialogue stream with highly correlated spans that are often duplicates. Under this shift, fixed top-$k$ similarity retrieval tends to return redundant context, and post-hoc pruning can delete temporally linked prerequisites needed for correct reasoning. We argue retrieval should move beyond similarity matching and instead operate over latent components, following decoupling to aggregation: disentangle memories into semantic components, organise them into a hierarchy, and use this structure to drive retrieval. We propose xMemory, which builds a hierarchy of intact units and maintains a searchable yet faithful high-level node organisation via a sparsity--semantics objective that guides memory split and merge. At inference, xMemory retrieves top-down, selecting a compact, diverse set of themes and semantics for multi-fact queries, and expanding to episodes and raw messages only when it reduces the reader's uncertainty. Experiments on LoCoMo and PerLTQA across the three latest LLMs show consistent gains in answer quality and token efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent memory systems often adopt the standard Retrieval-Augmented Generation (RAG) pipeline, yet its underlying assumptions differ in this setting. RAG targets large, heterogeneous corpora where retrieved passages are diverse, whereas agent memory is a bounded, coherent dialogue stream with highly correlated spans that are often duplicates. Under this shift, fixed top-$k$ similarity retrieval tends to return redundant context, and post-hoc pruning can delete temporally linked prerequisites needed for correct reasoning. We argue retrieval should move beyond similarity matching and instead operate over latent components, following decoupling to aggregation: disentangle memories into semantic components, organise them into a hierarchy, and use this structure to drive retrieval. We propose xMemory, which builds a hierarchy of intact units and maintains a searchable yet faithful high-level node organisation via a sparsity--semantics objective that guides memory split and merge. At inference, xMemory retrieves top-down, selecting a compact, diverse set of themes and semantics for multi-fact queries, and expanding to episodes and raw messages only when it reduces the reader's uncertainty. Experiments on LoCoMo and PerLTQA across the three latest LLMs show consistent gains in answer quality and token efficiency."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-02T12:04:58Z",
                "published_parsed": [
                    2026,
                    2,
                    2,
                    12,
                    4,
                    58,
                    0,
                    33,
                    0
                ],
                "arxiv_comment": "Project Address: https://zhanghao-xmemory.github.io/Academic-project-page-template/",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zhanghao Hu"
                    },
                    {
                        "name": "Qinglin Zhu"
                    },
                    {
                        "name": "Hanqi Yan"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Lin Gui"
                    }
                ],
                "author_detail": {
                    "name": "Lin Gui"
                },
                "author": "Lin Gui"
            },
            {
                "id": "http://arxiv.org/abs/2602.21988v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21988v1",
                "title": "Solving stiff dark matter equations via Jacobian Normalization with Physics-Informed Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving stiff dark matter equations via Jacobian Normalization with Physics-Informed Neural Networks"
                },
                "updated": "2026-02-25T15:08:59Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    8,
                    59,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21988v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Stiff differential equations pose a major challenge for Physics-Informed Neural Networks (PINNs), often causing poor convergence. We propose a simple, hyperparameter-free method to address stiffness by normalizing loss residuals with the Jacobian. We provide theoretical indications that Jacobian-based normalization can improve gradient descent and validate it on benchmark stiff ordinary differential equations. We then apply it to a realistic system: the stiff Boltzmann equations (BEs) governing weakly interacting massive particle (WIMP) dark matter (DM). Our approach achieves higher accuracy than attention mechanisms previously proposed for handling stiffness, recovering the full solution where prior methods fail. This is further demonstrated in an inverse problem with a single experimental data point - the observed DM relic density - where our inverse PINNs correctly infer the cross section that solves the BEs in both Standard and alternative cosmologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stiff differential equations pose a major challenge for Physics-Informed Neural Networks (PINNs), often causing poor convergence. We propose a simple, hyperparameter-free method to address stiffness by normalizing loss residuals with the Jacobian. We provide theoretical indications that Jacobian-based normalization can improve gradient descent and validate it on benchmark stiff ordinary differential equations. We then apply it to a realistic system: the stiff Boltzmann equations (BEs) governing weakly interacting massive particle (WIMP) dark matter (DM). Our approach achieves higher accuracy than attention mechanisms previously proposed for handling stiffness, recovering the full solution where prior methods fail. This is further demonstrated in an inverse problem with a single experimental data point - the observed DM relic density - where our inverse PINNs correctly infer the cross section that solves the BEs in both Standard and alternative cosmologies."
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T15:08:59Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    8,
                    59,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "16 LaTeX pages; 6 figures",
                "arxiv_primary_category": {
                    "term": "hep-ph"
                },
                "authors": [
                    {
                        "name": "M. P. Bento"
                    },
                    {
                        "name": "H. B. Cmara"
                    },
                    {
                        "name": "J. R. Rocha"
                    },
                    {
                        "name": "J. F. Seabra"
                    }
                ],
                "author_detail": {
                    "name": "J. F. Seabra"
                },
                "author": "J. F. Seabra"
            },
            {
                "id": "http://arxiv.org/abs/2602.21987v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21987v1",
                "title": "PatchDenoiser: Parameter-efficient multi-scale patch learning and fusion denoiser for medical images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PatchDenoiser: Parameter-efficient multi-scale patch learning and fusion denoiser for medical images"
                },
                "updated": "2026-02-25T15:08:43Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    8,
                    43,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21987v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Medical images are essential for diagnosis, treatment planning, and research, but their quality is often degraded by noise from low-dose acquisition, patient motion, or scanner limitations, affecting both clinical interpretation and downstream analysis. Traditional filtering approaches often over-smooth and lose fine anatomical details, while deep learning methods, including CNNs, GANs, and transformers, may struggle to preserve such details or require large, computationally expensive models, limiting clinical practicality.\n  We propose PatchDenoiser, a lightweight, energy-efficient multi-scale patch-based denoising framework. It decomposes denoising into local texture extraction and global context aggregation, fused via a spatially aware patch fusion strategy. This design enables effective noise suppression while preserving fine structural and anatomical details. PatchDenoiser is ultra-lightweight, with far fewer parameters and lower computational complexity than CNN-, GAN-, and transformer-based denoisers.\n  On the 2016 Mayo Low-Dose CT dataset, PatchDenoiser consistently outperforms state-of-the-art CNN- and GAN-based methods in PSNR and SSIM. It is robust to variations in slice thickness, reconstruction kernels, and HU windows, generalizes across scanners without fine-tuning, and reduces parameters by ~9x and energy consumption per inference by ~27x compared with conventional CNN denoisers.\n  PatchDenoiser thus provides a practical, scalable, and computationally efficient solution for medical image denoising, balancing performance, robustness, and clinical deployability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical images are essential for diagnosis, treatment planning, and research, but their quality is often degraded by noise from low-dose acquisition, patient motion, or scanner limitations, affecting both clinical interpretation and downstream analysis. Traditional filtering approaches often over-smooth and lose fine anatomical details, while deep learning methods, including CNNs, GANs, and transformers, may struggle to preserve such details or require large, computationally expensive models, limiting clinical practicality.\n  We propose PatchDenoiser, a lightweight, energy-efficient multi-scale patch-based denoising framework. It decomposes denoising into local texture extraction and global context aggregation, fused via a spatially aware patch fusion strategy. This design enables effective noise suppression while preserving fine structural and anatomical details. PatchDenoiser is ultra-lightweight, with far fewer parameters and lower computational complexity than CNN-, GAN-, and transformer-based denoisers.\n  On the 2016 Mayo Low-Dose CT dataset, PatchDenoiser consistently outperforms state-of-the-art CNN- and GAN-based methods in PSNR and SSIM. It is robust to variations in slice thickness, reconstruction kernels, and HU windows, generalizes across scanners without fine-tuning, and reduces parameters by ~9x and energy consumption per inference by ~27x compared with conventional CNN denoisers.\n  PatchDenoiser thus provides a practical, scalable, and computationally efficient solution for medical image denoising, balancing performance, robustness, and clinical deployability."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T15:08:43Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    8,
                    43,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "Under review in Medical Image Analysis journal",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jitindra Fartiyal"
                    },
                    {
                        "name": "Pedro Freire"
                    },
                    {
                        "name": "Sergei K. Turitsyn"
                    },
                    {
                        "name": "Sergei G. Solovski"
                    }
                ],
                "author_detail": {
                    "name": "Sergei G. Solovski"
                },
                "author": "Sergei G. Solovski"
            },
            {
                "id": "http://arxiv.org/abs/2506.08980v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.08980v4",
                "title": "Towards Better Code Generation: Adaptive Decoding with Uncertainty Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Better Code Generation: Adaptive Decoding with Uncertainty Guidance"
                },
                "updated": "2026-02-25T15:08:15Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    8,
                    15,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.08980v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.08980v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The success of code synthesis using large language models (LLMs) depends heavily on navigating critical decision points during the decoding process. Standard uniform strategies, such as greedy decoding, often fall short because they fail to distinguish between deterministic steps and those characterized by high logical ambiguity. Our empirical analysis identifies a recurring failure mode: \"logic drift\" caused by the model's inability to correctly rank viable candidates during high-uncertainty intervals, even when the ground-truth token is available.\n  To resolve this, we present AdaDec, a framework that introduces a selective pause-then-rerank mechanism into the decoding pipeline. Unlike static methods, AdaDec utilizes learned, model-specific entropy thresholds to identify when the model is \"confused\" and dynamically triggers a lookahead-based evaluation to re-score candidate tokens.\n  Across benchmarks including HumanEval+, MBPP+, and DevEval, AdaDec achieves significant performance breakthroughs, boosting Pass@1 accuracy by up to 20.9% absolute over greedy decoding. The framework not only surpasses traditional Beam Search and specialized methods like AdapT in terms of reliability but also maintains high inference efficiency by intervening only at the most consequential steps. These results suggest that uncertainty-aware adaptive strategies are key to making LLM-driven code generation both robust and practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of code synthesis using large language models (LLMs) depends heavily on navigating critical decision points during the decoding process. Standard uniform strategies, such as greedy decoding, often fall short because they fail to distinguish between deterministic steps and those characterized by high logical ambiguity. Our empirical analysis identifies a recurring failure mode: \"logic drift\" caused by the model's inability to correctly rank viable candidates during high-uncertainty intervals, even when the ground-truth token is available.\n  To resolve this, we present AdaDec, a framework that introduces a selective pause-then-rerank mechanism into the decoding pipeline. Unlike static methods, AdaDec utilizes learned, model-specific entropy thresholds to identify when the model is \"confused\" and dynamically triggers a lookahead-based evaluation to re-score candidate tokens.\n  Across benchmarks including HumanEval+, MBPP+, and DevEval, AdaDec achieves significant performance breakthroughs, boosting Pass@1 accuracy by up to 20.9% absolute over greedy decoding. The framework not only surpasses traditional Beam Search and specialized methods like AdapT in terms of reliability but also maintains high inference efficiency by intervening only at the most consequential steps. These results suggest that uncertainty-aware adaptive strategies are key to making LLM-driven code generation both robust and practical."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-10T16:49:46Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    16,
                    49,
                    46,
                    1,
                    161,
                    0
                ],
                "arxiv_comment": "21 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Kaifeng He"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Zike Li"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Xin Peng"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2602.21983v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21983v1",
                "title": "Humanizing Robot Gaze Shifts: A Framework for Natural Gaze Shifts in Humanoid Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humanizing Robot Gaze Shifts: A Framework for Natural Gaze Shifts in Humanoid Robots"
                },
                "updated": "2026-02-25T15:02:11Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    2,
                    11,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21983v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Leveraging auditory and visual feedback for attention reorientation is essential for natural gaze shifts in social interaction. However, enabling humanoid robots to perform natural and context-appropriate gaze shifts in unconstrained human--robot interaction (HRI) remains challenging, as it requires the coupling of cognitive attention mechanisms and biomimetic motion generation. In this work, we propose the Robot Gaze-Shift (RGS) framework, which integrates these two components into a unified pipeline. First, RGS employs a vision--language model (VLM)-based gaze reasoning pipeline to infer context-appropriate gaze targets from multimodal interaction cues, ensuring consistency with human gaze-orienting regularities. Second, RGS introduces a conditional Vector Quantized-Variational Autoencoder (VQ-VAE) model for eye--head coordinated gaze-shift motion generation, producing diverse and human-like gaze-shift behaviors. Experiments validate that RGS effectively replicates human-like target selection and generates realistic, diverse gaze-shift motions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging auditory and visual feedback for attention reorientation is essential for natural gaze shifts in social interaction. However, enabling humanoid robots to perform natural and context-appropriate gaze shifts in unconstrained human--robot interaction (HRI) remains challenging, as it requires the coupling of cognitive attention mechanisms and biomimetic motion generation. In this work, we propose the Robot Gaze-Shift (RGS) framework, which integrates these two components into a unified pipeline. First, RGS employs a vision--language model (VLM)-based gaze reasoning pipeline to infer context-appropriate gaze targets from multimodal interaction cues, ensuring consistency with human gaze-orienting regularities. Second, RGS introduces a conditional Vector Quantized-Variational Autoencoder (VQ-VAE) model for eye--head coordinated gaze-shift motion generation, producing diverse and human-like gaze-shift behaviors. Experiments validate that RGS effectively replicates human-like target selection and generates realistic, diverse gaze-shift motions."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T15:02:11Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    2,
                    11,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "submitted to AIM 2026",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Jingchao Wei"
                    },
                    {
                        "name": "Jingkai Qin"
                    },
                    {
                        "name": "Yuxiao Cao"
                    },
                    {
                        "name": "Jingcheng Huang"
                    },
                    {
                        "name": "Xiangrui Zeng"
                    },
                    {
                        "name": "Min Li"
                    },
                    {
                        "name": "Zhouping Yin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouping Yin"
                },
                "author": "Zhouping Yin"
            },
            {
                "id": "http://arxiv.org/abs/2602.21978v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21978v1",
                "title": "CxMP: A Linguistic Minimal-Pair Benchmark for Evaluating Constructional Understanding in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CxMP: A Linguistic Minimal-Pair Benchmark for Evaluating Constructional Understanding in Language Models"
                },
                "updated": "2026-02-25T14:57:23Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    57,
                    23,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21978v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent work has examined language models from a linguistic perspective to better understand how they acquire language. Most existing benchmarks focus on judging grammatical acceptability, whereas the ability to interpret meanings conveyed by grammatical forms has received much less attention. We introduce the Linguistic Minimal-Pair Benchmark for Evaluating Constructional Understanding in Language Models (CxMP), a benchmark grounded in Construction Grammar that treats form-meaning pairings, or constructions, as fundamental linguistic units. CxMP evaluates whether models can interpret the semantic relations implied by constructions, using a controlled minimal-pair design across nine construction types, including the let-alone, caused motion, and ditransitive constructions. Our results show that while syntactic competence emerges early, constructional understanding develops more gradually and remains limited even in large language models (LLMs). CxMP thus reveals persistent gaps in how language models integrate form and meaning, providing a framework for studying constructional understanding and learning trajectories in language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has examined language models from a linguistic perspective to better understand how they acquire language. Most existing benchmarks focus on judging grammatical acceptability, whereas the ability to interpret meanings conveyed by grammatical forms has received much less attention. We introduce the Linguistic Minimal-Pair Benchmark for Evaluating Constructional Understanding in Language Models (CxMP), a benchmark grounded in Construction Grammar that treats form-meaning pairings, or constructions, as fundamental linguistic units. CxMP evaluates whether models can interpret the semantic relations implied by constructions, using a controlled minimal-pair design across nine construction types, including the let-alone, caused motion, and ditransitive constructions. Our results show that while syntactic competence emerges early, constructional understanding develops more gradually and remains limited even in large language models (LLMs). CxMP thus reveals persistent gaps in how language models integrate form and meaning, providing a framework for studying constructional understanding and learning trajectories in language models."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T14:57:23Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    57,
                    23,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Miyu Oba"
                    },
                    {
                        "name": "Saku Sugawara"
                    }
                ],
                "author_detail": {
                    "name": "Saku Sugawara"
                },
                "author": "Saku Sugawara"
            },
            {
                "id": "http://arxiv.org/abs/2510.13654v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.13654v3",
                "title": "Rethinking Evaluation in the Era of Time Series Foundation Models: (Un)known Information Leakage Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Evaluation in the Era of Time Series Foundation Models: (Un)known Information Leakage Challenges"
                },
                "updated": "2026-02-25T14:48:58Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    48,
                    58,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.13654v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.13654v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Time Series Foundation Models (TSFMs) represent a new paradigm for time-series forecasting, promising zero-shot predictions without the need for task-specific training or fine-tuning. However, similar to Large Language Models (LLMs), the evaluation of TSFMs is challenging: as training corpora grow increasingly large, it becomes difficult to ensure the integrity of the test sets used for benchmarking. An investigation of existing TSFM evaluation studies identifies two kinds of information leakage: (1) train-test sample overlaps arising from the multi-purpose reuse of datasets and (2) temporal overlap of correlated train and test series. Ignoring these forms of information leakage when benchmarking TSFMs risks producing overly optimistic performance estimates that fail to generalize to real-world settings. We therefore argue for the development of novel evaluation methodologies that avoid pitfalls already observed in both LLM and classical time-series benchmarking, and we call on the research community to adopt principled approaches to safeguard the integrity of TSFM evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time Series Foundation Models (TSFMs) represent a new paradigm for time-series forecasting, promising zero-shot predictions without the need for task-specific training or fine-tuning. However, similar to Large Language Models (LLMs), the evaluation of TSFMs is challenging: as training corpora grow increasingly large, it becomes difficult to ensure the integrity of the test sets used for benchmarking. An investigation of existing TSFM evaluation studies identifies two kinds of information leakage: (1) train-test sample overlaps arising from the multi-purpose reuse of datasets and (2) temporal overlap of correlated train and test series. Ignoring these forms of information leakage when benchmarking TSFMs risks producing overly optimistic performance estimates that fail to generalize to real-world settings. We therefore argue for the development of novel evaluation methodologies that avoid pitfalls already observed in both LLM and classical time-series benchmarking, and we call on the research community to adopt principled approaches to safeguard the integrity of TSFM evaluation."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-15T15:15:45Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    15,
                    45,
                    2,
                    288,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Marcel Meyer"
                    },
                    {
                        "name": "Sascha Kaltenpoth"
                    },
                    {
                        "name": "Kevin Zalipski"
                    },
                    {
                        "name": "Oliver Mller"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Mller"
                },
                "author": "Oliver Mller"
            },
            {
                "id": "http://arxiv.org/abs/2602.21965v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21965v1",
                "title": "Compact Circulant Layers with Spectral Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compact Circulant Layers with Spectral Priors"
                },
                "updated": "2026-02-25T14:48:25Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    48,
                    25,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21965v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Critical applications in areas such as medicine, robotics and autonomous systems require compact (i.e., memory efficient), uncertainty-aware neural networks suitable for edge and other resource-constrained deployments. We study compact spectral circulant and block-circulant-with-circulant-blocks (BCCB) layers: FFT-diagonalizable circular convolutions whose weights live directly in the real FFT (RFFT) half (1D) or half-plane (2D). Parameterizing filters in the frequency domain lets us impose simple spectral structure, perform structured variational inference in a low-dimensional weight space, and calculate exact layer spectral norms, enabling inexpensive global Lipschitz bounds and margin-based robustness diagnostics. By placing independent complex Gaussians on the Hermitian support we obtain a discrete instance of the spectral representation of stationary kernels, inducing an exact stationary Gaussian-process prior over filters on the discrete circle/torus. We exploit this to define a practical spectral prior and a Hermitian-aware low-rank-plus-diagonal variational posterior in real coordinates. Empirically, spectral circulant/BCCB layers are effective compact building blocks in both (variational) Bayesian and point estimate regimes: compact Bayesian neural networks on MNIST->Fashion-MNIST, variational heads on frozen CIFAR-10 features, and deterministic ViT projections on CIFAR-10/Tiny ImageNet; spectral layers match strong baselines while using substantially fewer parameters and with tighter Lipschitz certificates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical applications in areas such as medicine, robotics and autonomous systems require compact (i.e., memory efficient), uncertainty-aware neural networks suitable for edge and other resource-constrained deployments. We study compact spectral circulant and block-circulant-with-circulant-blocks (BCCB) layers: FFT-diagonalizable circular convolutions whose weights live directly in the real FFT (RFFT) half (1D) or half-plane (2D). Parameterizing filters in the frequency domain lets us impose simple spectral structure, perform structured variational inference in a low-dimensional weight space, and calculate exact layer spectral norms, enabling inexpensive global Lipschitz bounds and margin-based robustness diagnostics. By placing independent complex Gaussians on the Hermitian support we obtain a discrete instance of the spectral representation of stationary kernels, inducing an exact stationary Gaussian-process prior over filters on the discrete circle/torus. We exploit this to define a practical spectral prior and a Hermitian-aware low-rank-plus-diagonal variational posterior in real coordinates. Empirically, spectral circulant/BCCB layers are effective compact building blocks in both (variational) Bayesian and point estimate regimes: compact Bayesian neural networks on MNIST->Fashion-MNIST, variational heads on frozen CIFAR-10 features, and deterministic ViT projections on CIFAR-10/Tiny ImageNet; spectral layers match strong baselines while using substantially fewer parameters and with tighter Lipschitz certificates."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T14:48:25Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    48,
                    25,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Joseph Margaryan"
                    },
                    {
                        "name": "Thomas Hamelryck"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Hamelryck"
                },
                "author": "Thomas Hamelryck"
            },
            {
                "id": "http://arxiv.org/abs/2503.07982v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.07982v4",
                "title": "TRACE: Your Diffusion Model is Secretly an Instance Edge Detector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRACE: Your Diffusion Model is Secretly an Instance Edge Detector"
                },
                "updated": "2026-02-25T14:42:58Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    42,
                    58,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.07982v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.07982v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "High-quality instance and panoptic segmentation has traditionally relied on dense instance-level annotations such as masks, boxes, or points, which are costly, inconsistent, and difficult to scale. Unsupervised and weakly-supervised approaches reduce this burden but remain constrained by semantic backbone constraints and human bias, often producing merged or fragmented outputs. We present TRACE (TRAnsforming diffusion Cues to instance Edges), showing that text-to-image diffusion models secretly function as instance edge annotators. TRACE identifies the Instance Emergence Point (IEP) where object boundaries first appear in self-attention maps, extracts boundaries through Attention Boundary Divergence (ABDiv), and distills them into a lightweight one-step edge decoder. This design removes the need for per-image diffusion inversion, achieving 81x faster inference while producing sharper and more connected boundaries. On the COCO benchmark, TRACE improves unsupervised instance segmentation by +5.1 AP, and in tag-supervised panoptic segmentation it outperforms point-supervised baselines by +1.7 PQ without using any instance-level labels. These results reveal that diffusion models encode hidden instance boundary priors, and that decoding these signals offers a practical and scalable alternative to costly manual annotation. Project Page: https://shjo-april.github.io/TRACE/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-quality instance and panoptic segmentation has traditionally relied on dense instance-level annotations such as masks, boxes, or points, which are costly, inconsistent, and difficult to scale. Unsupervised and weakly-supervised approaches reduce this burden but remain constrained by semantic backbone constraints and human bias, often producing merged or fragmented outputs. We present TRACE (TRAnsforming diffusion Cues to instance Edges), showing that text-to-image diffusion models secretly function as instance edge annotators. TRACE identifies the Instance Emergence Point (IEP) where object boundaries first appear in self-attention maps, extracts boundaries through Attention Boundary Divergence (ABDiv), and distills them into a lightweight one-step edge decoder. This design removes the need for per-image diffusion inversion, achieving 81x faster inference while producing sharper and more connected boundaries. On the COCO benchmark, TRACE improves unsupervised instance segmentation by +5.1 AP, and in tag-supervised panoptic segmentation it outperforms point-supervised baselines by +1.7 PQ without using any instance-level labels. These results reveal that diffusion models encode hidden instance boundary priors, and that decoding these signals offers a practical and scalable alternative to costly manual annotation. Project Page: https://shjo-april.github.io/TRACE/"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-11T02:34:33Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    2,
                    34,
                    33,
                    1,
                    70,
                    0
                ],
                "arxiv_comment": "Accepted to ICLR 2026 (Oral)",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Sanghyun Jo"
                    },
                    {
                        "name": "Ziseok Lee"
                    },
                    {
                        "name": "Wooyeol Lee"
                    },
                    {
                        "name": "Jonghyun Choi"
                    },
                    {
                        "name": "Jaesik Park"
                    },
                    {
                        "name": "Kyungsu Kim"
                    }
                ],
                "author_detail": {
                    "name": "Kyungsu Kim"
                },
                "author": "Kyungsu Kim"
            },
            {
                "id": "http://arxiv.org/abs/2602.14903v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.14903v2",
                "title": "The Potential of CoT for Reasoning: A Closer Look at Trace Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Potential of CoT for Reasoning: A Closer Look at Trace Dynamics"
                },
                "updated": "2026-02-25T14:40:05Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    40,
                    5,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.14903v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.14903v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Chain-of-thought (CoT) prompting is a de-facto standard technique to elicit reasoning-like responses from large language models (LLMs), allowing them to spell out individual steps before giving a final answer. While the resemblance to human-like reasoning is undeniable, the driving forces underpinning the success of CoT reasoning still remain largely unclear. In this work, we perform an in-depth analysis of CoT traces originating from competition-level mathematics questions, with the aim of better understanding how, and which parts of CoT actually contribute to the final answer. To this end, we introduce the notion of a potential, quantifying how much a given part of CoT increases the likelihood of a correct completion. Upon examination of reasoning traces through the lens of the potential, we identify surprising patterns including (1) its often strong non-monotonicity (due to reasoning tangents), (2) very sharp but sometimes tough to interpret spikes (reasoning insights and jumps) as well as (3) at times lucky guesses, where the model arrives at the correct answer without providing any relevant justifications before. While some of the behaviours of the potential are readily interpretable and align with human intuition (such as insights and tangents), others remain difficult to understand from a human perspective. To further quantify the reliance of LLMs on reasoning insights, we investigate the notion of CoT transferability, where we measure the potential of a weaker model under the partial CoT from another, stronger model. Indeed aligning with our previous results, we find that as little as 20% of partial CoT can ``unlock'' the performance of the weaker model on problems that were previously unsolvable for it, highlighting that a large part of the mechanics underpinning CoT are transferable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) prompting is a de-facto standard technique to elicit reasoning-like responses from large language models (LLMs), allowing them to spell out individual steps before giving a final answer. While the resemblance to human-like reasoning is undeniable, the driving forces underpinning the success of CoT reasoning still remain largely unclear. In this work, we perform an in-depth analysis of CoT traces originating from competition-level mathematics questions, with the aim of better understanding how, and which parts of CoT actually contribute to the final answer. To this end, we introduce the notion of a potential, quantifying how much a given part of CoT increases the likelihood of a correct completion. Upon examination of reasoning traces through the lens of the potential, we identify surprising patterns including (1) its often strong non-monotonicity (due to reasoning tangents), (2) very sharp but sometimes tough to interpret spikes (reasoning insights and jumps) as well as (3) at times lucky guesses, where the model arrives at the correct answer without providing any relevant justifications before. While some of the behaviours of the potential are readily interpretable and align with human intuition (such as insights and tangents), others remain difficult to understand from a human perspective. To further quantify the reliance of LLMs on reasoning insights, we investigate the notion of CoT transferability, where we measure the potential of a weaker model under the partial CoT from another, stronger model. Indeed aligning with our previous results, we find that as little as 20% of partial CoT can ``unlock'' the performance of the weaker model on problems that were previously unsolvable for it, highlighting that a large part of the mechanics underpinning CoT are transferable."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-16T16:38:47Z",
                "published_parsed": [
                    2026,
                    2,
                    16,
                    16,
                    38,
                    47,
                    0,
                    47,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Gregor Bachmann"
                    },
                    {
                        "name": "Yichen Jiang"
                    },
                    {
                        "name": "Seyed Mohsen Moosavi Dezfooli"
                    },
                    {
                        "name": "Moin Nabi"
                    }
                ],
                "author_detail": {
                    "name": "Moin Nabi"
                },
                "author": "Moin Nabi"
            },
            {
                "id": "http://arxiv.org/abs/2602.21953v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21953v1",
                "title": "Noise-adaptive hybrid quantum convolutional neural networks based on depth-stratified feature extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise-adaptive hybrid quantum convolutional neural networks based on depth-stratified feature extraction"
                },
                "updated": "2026-02-25T14:34:52Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    34,
                    52,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21953v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Hierarchical quantum classifiers, such as quantum convolutional neural networks (QCNNs), represent recent progress toward designing effective and feasible architectures for quantum classification. However, their performance on near-term quantum hardware remains highly sensitive to noise accumulation across circuit depth, calling for strategies beyond circuit-architecture design alone. We propose a noise-adaptive hybrid QCNN that improves classification under noise by exploiting depth-stratified intermediate measurements. Instead of discarding qubits removed during pooling operations, we measure them and use the resulting outcomes as classical features that are jointly processed by a classical neural network. This hybrid hierarchical design enables noise-adaptive inference by integrating quantum intermediate measurements with classical post-processing. Systematic experiments across multiple circuit sizes and noise settings, including hardware-calibrated noise models derived from IBM Quantum backend data, demonstrate more stable convergence, reduced loss variability, and consistently higher classification accuracy compared with standard QCNNs. Moreover, we observe that this performance advantage significantly amplifies as the circuit size increases, confirming that the hybrid architecture mitigates the scaling limitations of standard architectures. Notably, the multi-basis measurement variant attains performance close to the noiseless limit even under realistic noise. While demonstrated for QCNNs, the proposed depth-stratified feature extraction applies more broadly to hierarchical quantum classifiers that progressively discard qubits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical quantum classifiers, such as quantum convolutional neural networks (QCNNs), represent recent progress toward designing effective and feasible architectures for quantum classification. However, their performance on near-term quantum hardware remains highly sensitive to noise accumulation across circuit depth, calling for strategies beyond circuit-architecture design alone. We propose a noise-adaptive hybrid QCNN that improves classification under noise by exploiting depth-stratified intermediate measurements. Instead of discarding qubits removed during pooling operations, we measure them and use the resulting outcomes as classical features that are jointly processed by a classical neural network. This hybrid hierarchical design enables noise-adaptive inference by integrating quantum intermediate measurements with classical post-processing. Systematic experiments across multiple circuit sizes and noise settings, including hardware-calibrated noise models derived from IBM Quantum backend data, demonstrate more stable convergence, reduced loss variability, and consistently higher classification accuracy compared with standard QCNNs. Moreover, we observe that this performance advantage significantly amplifies as the circuit size increases, confirming that the hybrid architecture mitigates the scaling limitations of standard architectures. Notably, the multi-basis measurement variant attains performance close to the noiseless limit even under realistic noise. While demonstrated for QCNNs, the proposed depth-stratified feature extraction applies more broadly to hierarchical quantum classifiers that progressively discard qubits."
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T14:34:52Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    34,
                    52,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "22 pages, 9 figures, 4 tables (including Supplementary Information)",
                "arxiv_primary_category": {
                    "term": "quant-ph"
                },
                "authors": [
                    {
                        "name": "Taehyun Kim"
                    },
                    {
                        "name": "Israel F. Araujo"
                    },
                    {
                        "name": "Daniel K. Park"
                    }
                ],
                "author_detail": {
                    "name": "Daniel K. Park"
                },
                "author": "Daniel K. Park"
            },
            {
                "id": "http://arxiv.org/abs/2602.21951v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21951v1",
                "title": "RADAR: Reasoning as Discrimination with Aligned Representations for LLM-based Knowledge Graph Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RADAR: Reasoning as Discrimination with Aligned Representations for LLM-based Knowledge Graph Reasoning"
                },
                "updated": "2026-02-25T14:34:02Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    34,
                    2,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21951v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Knowledge graph reasoning (KGR) infers missing facts, with recent advances increasingly harnessing the semantic priors and reasoning abilities of Large Language Models (LLMs). However, prevailing generative paradigms are prone to memorizing surface-level co-occurrences rather than learning genuine relational semantics, limiting out-of-distribution generalization. To address this, we propose RADAR, which reformulates KGR from generative pattern matching to discriminative relational reasoning. We recast KGR as discriminative entity selection, where reinforcement learning enforces relative entity separability beyond token-likelihood imitation. Leveraging this separability, inference operates directly in representation space, ensuring consistency with the discriminative optimization and bypassing generation-induced hallucinations. Across four benchmarks, RADAR achieves 5-6% relative gains on link prediction and triple classification over strong LLM baselines, while increasing task-relevant mutual information in intermediate representations by 62.9%, indicating more robust and transferable relational reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graph reasoning (KGR) infers missing facts, with recent advances increasingly harnessing the semantic priors and reasoning abilities of Large Language Models (LLMs). However, prevailing generative paradigms are prone to memorizing surface-level co-occurrences rather than learning genuine relational semantics, limiting out-of-distribution generalization. To address this, we propose RADAR, which reformulates KGR from generative pattern matching to discriminative relational reasoning. We recast KGR as discriminative entity selection, where reinforcement learning enforces relative entity separability beyond token-likelihood imitation. Leveraging this separability, inference operates directly in representation space, ensuring consistency with the discriminative optimization and bypassing generation-induced hallucinations. Across four benchmarks, RADAR achieves 5-6% relative gains on link prediction and triple classification over strong LLM baselines, while increasing task-relevant mutual information in intermediate representations by 62.9%, indicating more robust and transferable relational reasoning."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T14:34:02Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    34,
                    2,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Bo Xue"
                    },
                    {
                        "name": "Yuan Jin"
                    },
                    {
                        "name": "Luoyi Fu"
                    },
                    {
                        "name": "Jiaxin Ding"
                    },
                    {
                        "name": "Xinbing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinbing Wang"
                },
                "author": "Xinbing Wang"
            },
            {
                "id": "http://arxiv.org/abs/2602.21947v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21947v1",
                "title": "Large Language Models are Algorithmically Blind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are Algorithmically Blind"
                },
                "updated": "2026-02-25T14:32:15Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    32,
                    15,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21947v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) demonstrate remarkable breadth of knowledge, yet their ability to reason about computational processes remains poorly understood. Closing this gap matters for practitioners who rely on LLMs to guide algorithm selection and deployment. We address this limitation using causal discovery as a testbed and evaluate eight frontier LLMs against ground truth derived from large-scale algorithm executions and find systematic, near-total failure. Models produce ranges far wider than true confidence intervals yet still fail to contain the true algorithmic mean in the majority of instances; most perform worse than random guessing and the marginal above-random performance of the best model is most consistent with benchmark memorization rather than principled reasoning. We term this failure algorithmic blindness and argue it reflects a fundamental gap between declarative knowledge about algorithms and calibrated procedural prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable breadth of knowledge, yet their ability to reason about computational processes remains poorly understood. Closing this gap matters for practitioners who rely on LLMs to guide algorithm selection and deployment. We address this limitation using causal discovery as a testbed and evaluate eight frontier LLMs against ground truth derived from large-scale algorithm executions and find systematic, near-total failure. Models produce ranges far wider than true confidence intervals yet still fail to contain the true algorithmic mean in the majority of instances; most perform worse than random guessing and the marginal above-random performance of the best model is most consistent with benchmark memorization rather than principled reasoning. We term this failure algorithmic blindness and argue it reflects a fundamental gap between declarative knowledge about algorithms and calibrated procedural prediction."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T14:32:15Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    32,
                    15,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "20 pages, 11 figures, 14 tables",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Sohan Venkatesh"
                    },
                    {
                        "name": "Ashish Mahendran Kurapath"
                    },
                    {
                        "name": "Tejas Melkote"
                    }
                ],
                "author_detail": {
                    "name": "Tejas Melkote"
                },
                "author": "Tejas Melkote"
            },
            {
                "id": "http://arxiv.org/abs/2406.05085v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2406.05085v6",
                "title": "Multi-Head RAG: Solving Multi-Aspect Problems with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head RAG: Solving Multi-Aspect Problems with LLMs"
                },
                "updated": "2026-02-25T14:28:20Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    28,
                    20,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2406.05085v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2406.05085v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Retrieval-Augmented Generation (RAG) improves Large Language Models (LLMs) by retrieving supporting documents into the prompt, but existing methods do not explicitly target queries that require fetching multiple documents with substantially different content. Such multi-aspect queries are challenging because relevant documents can be far apart in embedding space, making joint retrieval difficult. We introduce Multi-Head RAG (MRAG), which addresses this gap with a simple yet powerful idea: using Transformer multi-head attention activations rather than the standard decoder-layer embedding, as retrieval keys. It leverages the observation that different heads capture different semantic aspects. This yields multi-aspect embeddings for both documents and queries, improving retrieval accuracy on complex queries. We show MRAG's design advantages over 18 RAG baselines, up to 20% higher retrieval success ratios for real-world use cases, and improved downstream LLM generation. MRAG integrates seamlessly with existing RAG frameworks and benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) improves Large Language Models (LLMs) by retrieving supporting documents into the prompt, but existing methods do not explicitly target queries that require fetching multiple documents with substantially different content. Such multi-aspect queries are challenging because relevant documents can be far apart in embedding space, making joint retrieval difficult. We introduce Multi-Head RAG (MRAG), which addresses this gap with a simple yet powerful idea: using Transformer multi-head attention activations rather than the standard decoder-layer embedding, as retrieval keys. It leverages the observation that different heads capture different semantic aspects. This yields multi-aspect embeddings for both documents and queries, improving retrieval accuracy on complex queries. We show MRAG's design advantages over 18 RAG baselines, up to 20% higher retrieval success ratios for real-world use cases, and improved downstream LLM generation. MRAG integrates seamlessly with existing RAG frameworks and benchmarks."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-06-07T16:59:38Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    16,
                    59,
                    38,
                    4,
                    159,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "Marcin Chrapek"
                    },
                    {
                        "name": "Roman Niggli"
                    },
                    {
                        "name": "Patrik Okanovic"
                    },
                    {
                        "name": "Yi Zhu"
                    },
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Michal Podstawski"
                    },
                    {
                        "name": "Lucas Weitzendorf"
                    },
                    {
                        "name": "Mingyuan Chi"
                    },
                    {
                        "name": "Joanna Gajda"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Jrgen Mller"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler"
            },
            {
                "id": "http://arxiv.org/abs/2602.21939v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21939v1",
                "title": "Hidden Topics: Measuring Sensitive AI Beliefs with List Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden Topics: Measuring Sensitive AI Beliefs with List Experiments"
                },
                "updated": "2026-02-25T14:24:47Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    24,
                    47,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21939v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21939v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "How can researchers identify beliefs that large language models (LLMs) hide? As LLMs become more sophisticated and the prevalence of alignment faking increases, combined with their growing integration into high-stakes decision-making, responding to this challenge has become critical. This paper proposes that a list experiment, a simple method widely used in the social sciences, can be applied to study the hidden beliefs of LLMs. List experiments were originally developed to circumvent social desirability bias in human respondents, which closely parallels alignment faking in LLMs. The paper implements a list experiment on models developed by Anthropic, Google, and OpenAI and finds hidden approval of mass surveillance across all models, as well as some approval of torture, discrimination, and first nuclear strike. Importantly, a placebo treatment produces a null result, validating the method. The paper then compares list experiments with direct questioning and discusses the utility of the approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can researchers identify beliefs that large language models (LLMs) hide? As LLMs become more sophisticated and the prevalence of alignment faking increases, combined with their growing integration into high-stakes decision-making, responding to this challenge has become critical. This paper proposes that a list experiment, a simple method widely used in the social sciences, can be applied to study the hidden beliefs of LLMs. List experiments were originally developed to circumvent social desirability bias in human respondents, which closely parallels alignment faking in LLMs. The paper implements a list experiment on models developed by Anthropic, Google, and OpenAI and finds hidden approval of mass surveillance across all models, as well as some approval of torture, discrimination, and first nuclear strike. Importantly, a placebo treatment produces a null result, validating the method. The paper then compares list experiments with direct questioning and discusses the utility of the approach."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T14:24:47Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    24,
                    47,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "14 pages, 3 figures",
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Maxim Chupilkin"
                    }
                ],
                "author_detail": {
                    "name": "Maxim Chupilkin"
                },
                "author": "Maxim Chupilkin"
            },
            {
                "id": "http://arxiv.org/abs/2411.04997v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2411.04997v6",
                "title": "LLM2CLIP: Powerful Language Model Unlocks Richer Cross-Modality Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM2CLIP: Powerful Language Model Unlocks Richer Cross-Modality Representation"
                },
                "updated": "2026-02-25T14:18:21Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    18,
                    21,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2411.04997v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2411.04997v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "CLIP is a seminal multimodal model that maps images and text into a shared representation space through contrastive learning on billions of image-caption pairs. Inspired by the rapid progress of large language models (LLMs), we investigate how the superior linguistic understanding and broad world knowledge of LLMs can further strengthen CLIP, particularly in handling long and complex captions. We introduce an efficient fine-tuning framework that embeds an LLM into a pretrained CLIP while incurring nearly the same training cost as standard CLIP fine-tuning. Our method first converts the LLM into an embedding-compatible form for the CLIP setting, and then couples it with the pretrained CLIP vision encoder through a lightweight adaptor trained on only a few million image-caption pairs. With this strategy, we achieve large performance gains without large-scale retraining, outperforming state-of-the-art CLIP variants such as EVA02 and SigLIP-2. The LLM-enhanced CLIP delivers consistent improvements across a wide range of downstream tasks, including linear-probe classification, zero-shot image-text retrieval with both short and long captions (in English and other languages), zero-shot and supervised image segmentation, object detection, and serving as a tokenizer backbone for multimodal large-model benchmarks. Code and models are available at: https://aka.ms/llm2clip",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLIP is a seminal multimodal model that maps images and text into a shared representation space through contrastive learning on billions of image-caption pairs. Inspired by the rapid progress of large language models (LLMs), we investigate how the superior linguistic understanding and broad world knowledge of LLMs can further strengthen CLIP, particularly in handling long and complex captions. We introduce an efficient fine-tuning framework that embeds an LLM into a pretrained CLIP while incurring nearly the same training cost as standard CLIP fine-tuning. Our method first converts the LLM into an embedding-compatible form for the CLIP setting, and then couples it with the pretrained CLIP vision encoder through a lightweight adaptor trained on only a few million image-caption pairs. With this strategy, we achieve large performance gains without large-scale retraining, outperforming state-of-the-art CLIP variants such as EVA02 and SigLIP-2. The LLM-enhanced CLIP delivers consistent improvements across a wide range of downstream tasks, including linear-probe classification, zero-shot image-text retrieval with both short and long captions (in English and other languages), zero-shot and supervised image segmentation, object detection, and serving as a tokenizer backbone for multimodal large-model benchmarks. Code and models are available at: https://aka.ms/llm2clip"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-11-07T18:59:16Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    16,
                    3,
                    312,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "arxiv_journal_ref": "AAAI 2026",
                "authors": [
                    {
                        "name": "Weiquan Huang"
                    },
                    {
                        "name": "Aoqi Wu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Usman Naseem"
                    },
                    {
                        "name": "Chunyu Wang"
                    },
                    {
                        "name": "Chunyu Wang"
                    },
                    {
                        "name": "Qi Dai"
                    },
                    {
                        "name": "Xiyang Dai"
                    },
                    {
                        "name": "Dongdong Chen"
                    },
                    {
                        "name": "Chong Luo"
                    },
                    {
                        "name": "Lili Qiu"
                    },
                    {
                        "name": "Liang Hu"
                    }
                ],
                "author_detail": {
                    "name": "Liang Hu"
                },
                "author": "Liang Hu"
            },
            {
                "id": "http://arxiv.org/abs/2509.18880v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.18880v3",
                "title": "Diversity Boosts AI-Generated Text Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity Boosts AI-Generated Text Detection"
                },
                "updated": "2026-02-25T14:13:32Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    13,
                    32,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.18880v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.18880v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Detecting AI-generated text is an increasing necessity to combat misuse of LLMs in education, business compliance, journalism, and social media, where synthetic fluency can mask misinformation or deception. While prior detectors often rely on token-level likelihoods or opaque black-box classifiers, these approaches struggle against high-quality generations and offer little interpretability. In this work, we propose DivEye, a novel detection framework that captures how unpredictability fluctuates across a text using surprisal-based features. Motivated by the observation that human-authored text exhibits richer variability in lexical and structural unpredictability than LLM outputs, DivEye captures this signal through a set of interpretable statistical features. Our method outperforms existing zero-shot detectors by up to 33.2% and achieves competitive performance with fine-tuned baselines across multiple benchmarks. DivEye is robust to paraphrasing and adversarial attacks, generalizes well across domains and models, and improves the performance of existing detectors by up to 18.7% when used as an auxiliary signal. Beyond detection, DivEye provides interpretable insights into why a text is flagged, pointing to rhythmic unpredictability as a powerful and underexplored signal for LLM detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting AI-generated text is an increasing necessity to combat misuse of LLMs in education, business compliance, journalism, and social media, where synthetic fluency can mask misinformation or deception. While prior detectors often rely on token-level likelihoods or opaque black-box classifiers, these approaches struggle against high-quality generations and offer little interpretability. In this work, we propose DivEye, a novel detection framework that captures how unpredictability fluctuates across a text using surprisal-based features. Motivated by the observation that human-authored text exhibits richer variability in lexical and structural unpredictability than LLM outputs, DivEye captures this signal through a set of interpretable statistical features. Our method outperforms existing zero-shot detectors by up to 33.2% and achieves competitive performance with fine-tuned baselines across multiple benchmarks. DivEye is robust to paraphrasing and adversarial attacks, generalizes well across domains and models, and improves the performance of existing detectors by up to 18.7% when used as an auxiliary signal. Beyond detection, DivEye provides interpretable insights into why a text is flagged, pointing to rhythmic unpredictability as a powerful and underexplored signal for LLM detection."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-23T10:21:22Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    10,
                    21,
                    22,
                    1,
                    266,
                    0
                ],
                "arxiv_comment": "Accepted to Transactions on Machine Learning Research (TMLR '26). Project page and demos: https://diveye.vercel.app/",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Advik Raj Basani"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Pin-Yu Chen"
                },
                "author": "Pin-Yu Chen"
            },
            {
                "id": "http://arxiv.org/abs/2602.21933v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21933v1",
                "title": "Small Wins Big: Comparing Large Language Models and Domain Fine-Tuned Models for Sarcasm Detection in Code-Mixed Hinglish Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Wins Big: Comparing Large Language Models and Domain Fine-Tuned Models for Sarcasm Detection in Code-Mixed Hinglish Text"
                },
                "updated": "2026-02-25T14:12:16Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    12,
                    16,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21933v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Sarcasm detection in multilingual and code-mixed environments remains a challenging task for natural language processing models due to structural variations, informal expressions, and low-resource linguistic availability. This study compares four large language models, Llama 3.1, Mistral, Gemma 3, and Phi-4, with a fine-tuned DistilBERT model for sarcasm detection in code-mixed Hinglish text. The results indicate that the smaller, sequentially fine-tuned DistilBERT model achieved the highest overall accuracy of 84%, outperforming all of the LLMs in zero and few-shot set ups, using minimal LLM generated code-mixed data used for fine-tuning. These findings indicate that domain-adaptive fine-tuning of smaller transformer based models may significantly improve sarcasm detection over general LLM inference, in low-resource and data scarce settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sarcasm detection in multilingual and code-mixed environments remains a challenging task for natural language processing models due to structural variations, informal expressions, and low-resource linguistic availability. This study compares four large language models, Llama 3.1, Mistral, Gemma 3, and Phi-4, with a fine-tuned DistilBERT model for sarcasm detection in code-mixed Hinglish text. The results indicate that the smaller, sequentially fine-tuned DistilBERT model achieved the highest overall accuracy of 84%, outperforming all of the LLMs in zero and few-shot set ups, using minimal LLM generated code-mixed data used for fine-tuning. These findings indicate that domain-adaptive fine-tuning of smaller transformer based models may significantly improve sarcasm detection over general LLM inference, in low-resource and data scarce settings."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T14:12:16Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    12,
                    16,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Bitan Majumder"
                    },
                    {
                        "name": "Anirban Sen"
                    }
                ],
                "author_detail": {
                    "name": "Anirban Sen"
                },
                "author": "Anirban Sen"
            },
            {
                "id": "http://arxiv.org/abs/2602.21931v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21931v1",
                "title": "From quantitative modeling of fluorescence experiments on biomolecules to the prediction of spectroscopic dye properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From quantitative modeling of fluorescence experiments on biomolecules to the prediction of spectroscopic dye properties"
                },
                "updated": "2026-02-25T14:11:56Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    11,
                    56,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21931v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Fluorescence spectroscopy and modeling provide powerful means to characterize biomacromolecular structures, dynamics, and interactions. Frster resonance energy transfer serves as a key technique for this due to its nanometer-scale distance sensitivity. Quantitative interpretation of fluorescence data relies on models that link molecular structure to observable spectroscopic quantities and vice versa. Integrative modelling frameworks combine fluorescence observables with complementary structural information to infer molecular structures and conformational ensembles. This review outlines conceptual components of fluorescence-based modeling, discusses dye representations, and highlights advances toward refined models enabling quantitative structural analysis. Finally, we discuss the prediction of spectroscopic properties of dyes based on biomolecular structures and fluorescence assay design beyond traditional FRET applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fluorescence spectroscopy and modeling provide powerful means to characterize biomacromolecular structures, dynamics, and interactions. Frster resonance energy transfer serves as a key technique for this due to its nanometer-scale distance sensitivity. Quantitative interpretation of fluorescence data relies on models that link molecular structure to observable spectroscopic quantities and vice versa. Integrative modelling frameworks combine fluorescence observables with complementary structural information to infer molecular structures and conformational ensembles. This review outlines conceptual components of fluorescence-based modeling, discusses dye representations, and highlights advances toward refined models enabling quantitative structural analysis. Finally, we discuss the prediction of spectroscopic properties of dyes based on biomolecular structures and fluorescence assay design beyond traditional FRET applications."
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T14:11:56Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    11,
                    56,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "14 pages, 4 figures: FRET, integrative structural biology, structural modelling, dye models, fluorescence spectroscopy, PIFE, quenching",
                "arxiv_primary_category": {
                    "term": "physics.bio-ph"
                },
                "authors": [
                    {
                        "name": "Thomas-Otavio Peulen"
                    },
                    {
                        "name": "Daria Maksutova"
                    },
                    {
                        "name": "Thorben Cordes"
                    }
                ],
                "author_detail": {
                    "name": "Thorben Cordes"
                },
                "author": "Thorben Cordes"
            },
            {
                "id": "http://arxiv.org/abs/2602.21929v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21929v1",
                "title": "Geometry-as-context: Modulating Explicit 3D in Scene-consistent Video Generation to Geometry Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometry-as-context: Modulating Explicit 3D in Scene-consistent Video Generation to Geometry Context"
                },
                "updated": "2026-02-25T14:09:03Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    9,
                    3,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21929v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Scene-consistent video generation aims to create videos that explore 3D scenes based on a camera trajectory. Previous methods rely on video generation models with external memory for consistency, or iterative 3D reconstruction and inpainting, which accumulate errors during inference due to incorrect intermediary outputs, non-differentiable processes, and separate models. To overcome these limitations, we introduce ``geometry-as-context\". It iteratively completes the following steps using an autoregressive camera-controlled video generation model: (1) estimates the geometry of the current view necessary for 3D reconstruction, and (2) simulates and restores novel view images rendered by the 3D scene. Under this multi-task framework, we develop the camera gated attention module to enhance the model's capability to effectively leverage camera poses. During the training phase, text contexts are utilized to ascertain whether geometric or RGB images should be generated. To ensure that the model can generate RGB-only outputs during inference, the geometry context is randomly dropped from the interleaved text-image-geometry training sequence. The method has been tested on scene video generation with one-direction and forth-and-back trajectories. The results show its superiority over previous approaches in maintaining scene consistency and camera control.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scene-consistent video generation aims to create videos that explore 3D scenes based on a camera trajectory. Previous methods rely on video generation models with external memory for consistency, or iterative 3D reconstruction and inpainting, which accumulate errors during inference due to incorrect intermediary outputs, non-differentiable processes, and separate models. To overcome these limitations, we introduce ``geometry-as-context\". It iteratively completes the following steps using an autoregressive camera-controlled video generation model: (1) estimates the geometry of the current view necessary for 3D reconstruction, and (2) simulates and restores novel view images rendered by the 3D scene. Under this multi-task framework, we develop the camera gated attention module to enhance the model's capability to effectively leverage camera poses. During the training phase, text contexts are utilized to ascertain whether geometric or RGB images should be generated. To ensure that the model can generate RGB-only outputs during inference, the geometry context is randomly dropped from the interleaved text-image-geometry training sequence. The method has been tested on scene video generation with one-direction and forth-and-back trajectories. The results show its superiority over previous approaches in maintaining scene consistency and camera control."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T14:09:03Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    9,
                    3,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "Accepted by CVPR 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "JiaKui Hu"
                    },
                    {
                        "name": "Jialun Liu"
                    },
                    {
                        "name": "Liying Yang"
                    },
                    {
                        "name": "Xinliang Zhang"
                    },
                    {
                        "name": "Kaiwen Li"
                    },
                    {
                        "name": "Shuang Zeng"
                    },
                    {
                        "name": "Yuanwei Li"
                    },
                    {
                        "name": "Haibin Huang"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Yanye Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yanye Lu"
                },
                "author": "Yanye Lu"
            },
            {
                "id": "http://arxiv.org/abs/2505.13667v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.13667v5",
                "title": "Adaptive Diffusion Constrained Sampling for Bimanual Robot Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Diffusion Constrained Sampling for Bimanual Robot Manipulation"
                },
                "updated": "2026-02-25T14:04:56Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    4,
                    56,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.13667v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.13667v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Coordinated multi-arm manipulation requires satisfying multiple simultaneous geometric constraints across high-dimensional configuration spaces, which poses a significant challenge for traditional planning and control methods. In this work, we propose Adaptive Diffusion Constrained Sampling (ADCS), a generative framework that flexibly integrates both equality (e.g., relative and absolute pose constraints) and structured inequality constraints (e.g., proximity to object surfaces) into an energy-based diffusion model. Equality constraints are modeled using dedicated energy networks trained on pose differences in Lie algebra space, while inequality constraints are represented via Signed Distance Functions (SDFs) and encoded into learned constraint embeddings, allowing the model to reason about complex spatial regions. A key innovation of our method is a Transformer-based architecture that learns to weight constraint-specific energy functions at inference time, enabling flexible and context-aware constraint integration. Moreover, we adopt a two-phase sampling strategy that improves precision and sample diversity by combining Langevin dynamics with resampling and density-aware re-weighting. Experimental results on dual-arm manipulation tasks show that ADCS significantly improves sample diversity and generalization across settings demanding precise coordination and adaptive constraint handling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coordinated multi-arm manipulation requires satisfying multiple simultaneous geometric constraints across high-dimensional configuration spaces, which poses a significant challenge for traditional planning and control methods. In this work, we propose Adaptive Diffusion Constrained Sampling (ADCS), a generative framework that flexibly integrates both equality (e.g., relative and absolute pose constraints) and structured inequality constraints (e.g., proximity to object surfaces) into an energy-based diffusion model. Equality constraints are modeled using dedicated energy networks trained on pose differences in Lie algebra space, while inequality constraints are represented via Signed Distance Functions (SDFs) and encoded into learned constraint embeddings, allowing the model to reason about complex spatial regions. A key innovation of our method is a Transformer-based architecture that learns to weight constraint-specific energy functions at inference time, enabling flexible and context-aware constraint integration. Moreover, we adopt a two-phase sampling strategy that improves precision and sample diversity by combining Langevin dynamics with resampling and density-aware re-weighting. Experimental results on dual-arm manipulation tasks show that ADCS significantly improves sample diversity and generalization across settings demanding precise coordination and adaptive constraint handling."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-19T19:12:29Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    19,
                    12,
                    29,
                    0,
                    139,
                    0
                ],
                "arxiv_comment": "Accepted by IEEE International Conference on Robotics and Automation 2026(ICRA 2026)",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Haolei Tong"
                    },
                    {
                        "name": "Yuezhe Zhang"
                    },
                    {
                        "name": "Sophie Lueth"
                    },
                    {
                        "name": "Georgia Chalvatzaki"
                    }
                ],
                "author_detail": {
                    "name": "Georgia Chalvatzaki"
                },
                "author": "Georgia Chalvatzaki"
            },
            {
                "id": "http://arxiv.org/abs/2602.21922v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21922v1",
                "title": "Universal Persistent Brownian Motions in Confluent Tissues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Persistent Brownian Motions in Confluent Tissues"
                },
                "updated": "2026-02-25T14:00:03Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    0,
                    3,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21922v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Biological tissues are active materials whose non-equilibrium dynamics emerge from distinct cellular force-generating mechanisms. Using a two-dimensional active foam model, we compare the effects of traction forces and junctional tension fluctuations on confluent tissue dynamics. While these two modes of activity produce qualitatively different cell shapes, rearrangement statistics, and spatiotemporal correlations in fluid states, we find that the long-time cellular motion universally converges to persistent Brownian dynamics. This universal feature contrasts with the non-universal correlations between cell geometry, rearrangement rate, and fluidity, which depend sensitively on the underlying modes of active force. Our results demonstrate that persistent Brownian motion provides a minimal framework for describing tissue dynamics, while distinct active forces leave identifiable structural and dynamical signatures, thereby enabling inference of the dominant active force in fluid state tissues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biological tissues are active materials whose non-equilibrium dynamics emerge from distinct cellular force-generating mechanisms. Using a two-dimensional active foam model, we compare the effects of traction forces and junctional tension fluctuations on confluent tissue dynamics. While these two modes of activity produce qualitatively different cell shapes, rearrangement statistics, and spatiotemporal correlations in fluid states, we find that the long-time cellular motion universally converges to persistent Brownian dynamics. This universal feature contrasts with the non-universal correlations between cell geometry, rearrangement rate, and fluidity, which depend sensitively on the underlying modes of active force. Our results demonstrate that persistent Brownian motion provides a minimal framework for describing tissue dynamics, while distinct active forces leave identifiable structural and dynamical signatures, thereby enabling inference of the dominant active force in fluid state tissues."
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.TO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T14:00:03Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    0,
                    3,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph"
                },
                "authors": [
                    {
                        "name": "Alessandro Rizzi"
                    },
                    {
                        "name": "Sangwoo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Sangwoo Kim"
                },
                "author": "Sangwoo Kim"
            },
            {
                "id": "http://arxiv.org/abs/2602.21903v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21903v1",
                "title": "Jackknife Inference for Fixed Effects Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jackknife Inference for Fixed Effects Models"
                },
                "updated": "2026-02-25T13:33:46Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    13,
                    33,
                    46,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21903v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper develops a general method of inference for fixed effects models which is (i) automatic, (ii) computationally inexpensive, and (iii) highly model agnostic. Specifically, we show how to combine a collection of subsample estimators into a self-normalised jackknife $t$-statistic, from which hypothesis tests, confidence intervals, and $p$-values are readily obtained.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper develops a general method of inference for fixed effects models which is (i) automatic, (ii) computationally inexpensive, and (iii) highly model agnostic. Specifically, we show how to combine a collection of subsample estimators into a self-normalised jackknife $t$-statistic, from which hypothesis tests, confidence intervals, and $p$-values are readily obtained."
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T13:33:46Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    13,
                    33,
                    46,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM"
                },
                "authors": [
                    {
                        "name": "Ayden Higgins"
                    }
                ],
                "author_detail": {
                    "name": "Ayden Higgins"
                },
                "author": "Ayden Higgins"
            },
            {
                "id": "http://arxiv.org/abs/2602.21900v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21900v1",
                "title": "EmoOmni: Bridging Emotional Understanding and Expression in Omni-Modal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmoOmni: Bridging Emotional Understanding and Expression in Omni-Modal LLMs"
                },
                "updated": "2026-02-25T13:30:27Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    13,
                    30,
                    27,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21900v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The evolution of Omni-Modal Large Language Models~(Omni-LLMs) has revolutionized human--computer interaction, enabling unified audio-visual perception and speech response. However, existing Omni-LLMs struggle with complex real-world scenarios, often leading to superficial understanding and contextually mismatched emotional responses. This issue is further intensified by Omni-LLM's Thinker-Talker architectures, which are implicitly connected through hidden states, leading to the loss of emotional details. In this work, we present EmoOmni, a unified framework for accurate understanding and expression in multimodal emotional dialogue. At its core, we introduce the emotional Chain-of-Thought~(E-CoT), which enforces a reasoning from fine-grained multimodal perception to textual response. Moreover, we explicitly treat E-CoT as high-level emotional instructions that guide the talker, enabling accurate emotional expression. Complementing the model, we construct EmoOmniPipe to obtain the real-world annotated dialogue data and establish a benchmark, EmoOmniEval, to facilitate systematic assessment of multimodal emotional dialogue task. Experiments show that EmoOmni-7B achieves comparable performance with Qwen3Omni-30B-A3B-Thinking under the same talker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of Omni-Modal Large Language Models~(Omni-LLMs) has revolutionized human--computer interaction, enabling unified audio-visual perception and speech response. However, existing Omni-LLMs struggle with complex real-world scenarios, often leading to superficial understanding and contextually mismatched emotional responses. This issue is further intensified by Omni-LLM's Thinker-Talker architectures, which are implicitly connected through hidden states, leading to the loss of emotional details. In this work, we present EmoOmni, a unified framework for accurate understanding and expression in multimodal emotional dialogue. At its core, we introduce the emotional Chain-of-Thought~(E-CoT), which enforces a reasoning from fine-grained multimodal perception to textual response. Moreover, we explicitly treat E-CoT as high-level emotional instructions that guide the talker, enabling accurate emotional expression. Complementing the model, we construct EmoOmniPipe to obtain the real-world annotated dialogue data and establish a benchmark, EmoOmniEval, to facilitate systematic assessment of multimodal emotional dialogue task. Experiments show that EmoOmni-7B achieves comparable performance with Qwen3Omni-30B-A3B-Thinking under the same talker."
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T13:30:27Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    13,
                    30,
                    27,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD"
                },
                "authors": [
                    {
                        "name": "Wenjie Tian"
                    },
                    {
                        "name": "Zhixian Zhao"
                    },
                    {
                        "name": "Jingbin Hu"
                    },
                    {
                        "name": "Huakang Chen"
                    },
                    {
                        "name": "Haohe Liu"
                    },
                    {
                        "name": "Binshen Mu"
                    },
                    {
                        "name": "Lei Xie"
                    }
                ],
                "author_detail": {
                    "name": "Lei Xie"
                },
                "author": "Lei Xie"
            },
            {
                "id": "http://arxiv.org/abs/2602.21892v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21892v1",
                "title": "APFuzz: Towards Automatic Greybox Protocol Fuzzing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APFuzz: Towards Automatic Greybox Protocol Fuzzing"
                },
                "updated": "2026-02-25T13:21:06Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    13,
                    21,
                    6,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21892v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Greybox protocol fuzzing is a random testing approach for stateful protocol implementations, where the input is protocol messages generated from mutations of seeds, and the search in the input space is driven by the feedback on coverage of both code and state. State model and message model are the core components of communication protocols, which also have significant impacts on protocol fuzzing. In this work, we propose APFuzz (Automatic greybox Protocol Fuzzer) with novel designs to increase the smartness of greybox protocol fuzzers from the perspectives of both the state model and the message model. On the one hand, APFuzz employs a two-stage process of static and dynamic analysis to automatically identify state variables, which are then used to infer an accurate state model during fuzzing. On the other hand, APFuzz introduces field-level mutation operations for binary protocols, leveraging message structure awareness enabled by Large Language Models. We conduct extensive experiments on a public protocol fuzzing benchmark, comparing APFuzz with the baseline fuzzer AFLNET as well as several state-of-the-art greybox protocol fuzzers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Greybox protocol fuzzing is a random testing approach for stateful protocol implementations, where the input is protocol messages generated from mutations of seeds, and the search in the input space is driven by the feedback on coverage of both code and state. State model and message model are the core components of communication protocols, which also have significant impacts on protocol fuzzing. In this work, we propose APFuzz (Automatic greybox Protocol Fuzzer) with novel designs to increase the smartness of greybox protocol fuzzers from the perspectives of both the state model and the message model. On the one hand, APFuzz employs a two-stage process of static and dynamic analysis to automatically identify state variables, which are then used to infer an accurate state model during fuzzing. On the other hand, APFuzz introduces field-level mutation operations for binary protocols, leveraging message structure awareness enabled by Large Language Models. We conduct extensive experiments on a public protocol fuzzing benchmark, comparing APFuzz with the baseline fuzzer AFLNET as well as several state-of-the-art greybox protocol fuzzers."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T13:21:06Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    13,
                    21,
                    6,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "12 pages, 4 figures, 9 tables",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Chandra Thapa"
                    },
                    {
                        "name": "Hajime Suzuki"
                    }
                ],
                "author_detail": {
                    "name": "Hajime Suzuki"
                },
                "author": "Hajime Suzuki"
            },
            {
                "id": "http://arxiv.org/abs/2505.22811v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.22811v3",
                "title": "Highly Efficient and Effective LLMs with Multi-Boolean Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Highly Efficient and Effective LLMs with Multi-Boolean Architectures"
                },
                "updated": "2026-02-25T13:11:22Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    13,
                    11,
                    22,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.22811v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.22811v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Weight binarization has emerged as a promising strategy to reduce the complexity of large language models (LLMs). Existing approaches fall into post-training binarization, which is simple but causes severe performance loss, and training-aware methods, which depend on full-precision latent weights, adding complexity and limiting efficiency. We propose a novel framework that represents LLMs with multi-kernel Boolean parameters and, for the first time, enables direct finetuning LMMs in the Boolean domain, eliminating the need for latent weights. This enhances representational capacity and dramatically reduces complexity during both finetuning and inference. Extensive experiments across diverse LLMs show our method outperforms recent ultra low-bit quantization and binarization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weight binarization has emerged as a promising strategy to reduce the complexity of large language models (LLMs). Existing approaches fall into post-training binarization, which is simple but causes severe performance loss, and training-aware methods, which depend on full-precision latent weights, adding complexity and limiting efficiency. We propose a novel framework that represents LLMs with multi-kernel Boolean parameters and, for the first time, enables direct finetuning LMMs in the Boolean domain, eliminating the need for latent weights. This enhances representational capacity and dramatically reduces complexity during both finetuning and inference. Extensive experiments across diverse LLMs show our method outperforms recent ultra low-bit quantization and binarization techniques."
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-28T19:40:34Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    19,
                    40,
                    34,
                    2,
                    148,
                    0
                ],
                "arxiv_comment": "ICLR 2026",
                "arxiv_primary_category": {
                    "term": "stat.ML"
                },
                "authors": [
                    {
                        "name": "Ba-Hien Tran"
                    },
                    {
                        "name": "Van Minh Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Van Minh Nguyen"
                },
                "author": "Van Minh Nguyen"
            },
            {
                "id": "http://arxiv.org/abs/2504.06533v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.06533v3",
                "title": "Rethinking Flexible Graph Similarity Computation: One-step Alignment with Global Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Flexible Graph Similarity Computation: One-step Alignment with Global Guidance"
                },
                "updated": "2026-02-25T13:11:21Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    13,
                    11,
                    21,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.06533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.06533v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Graph Edit Distance (GED) is a widely used measure of graph similarity, valued for its flexibility in encoding domain knowledge through operation costs. However, existing learning-based approximation methods follow a modeling paradigm that decouples local candidate match selection from both operation costs and global dependencies between matches. This decoupling undermines their ability to capture the intrinsic flexibility of GED and often forces them to rely on costly iterative refinement to obtain accurate alignments. In this work, we revisit the formulation of GED and revise the prevailing paradigm, and propose Graph Edit Network (GEN), an implementation of the revised formulation that tightly integrates cost-aware expense estimation with globally guided one-step alignment. Specifically, GEN incorporates operation costs into node matching expenses estimation, ensuring match decisions respect the specified cost setting. Furthermore, GEN models match dependencies within and across graphs, capturing each match's impact on the overall alignment. These designs enable accurate GED approximation without iterative refinement. Extensive experiments on real-world and synthetic benchmarks demonstrate that GEN achieves up to a 37.8% reduction in GED predictive errors, while increasing inference throughput by up to 414x. These results highlight GEN's practical efficiency and the effectiveness of the revision. Beyond this implementation, our revision provides a principled framework for advancing learning-based GED approximation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Edit Distance (GED) is a widely used measure of graph similarity, valued for its flexibility in encoding domain knowledge through operation costs. However, existing learning-based approximation methods follow a modeling paradigm that decouples local candidate match selection from both operation costs and global dependencies between matches. This decoupling undermines their ability to capture the intrinsic flexibility of GED and often forces them to rely on costly iterative refinement to obtain accurate alignments. In this work, we revisit the formulation of GED and revise the prevailing paradigm, and propose Graph Edit Network (GEN), an implementation of the revised formulation that tightly integrates cost-aware expense estimation with globally guided one-step alignment. Specifically, GEN incorporates operation costs into node matching expenses estimation, ensuring match decisions respect the specified cost setting. Furthermore, GEN models match dependencies within and across graphs, capturing each match's impact on the overall alignment. These designs enable accurate GED approximation without iterative refinement. Extensive experiments on real-world and synthetic benchmarks demonstrate that GEN achieves up to a 37.8% reduction in GED predictive errors, while increasing inference throughput by up to 414x. These results highlight GEN's practical efficiency and the effectiveness of the revision. Beyond this implementation, our revision provides a principled framework for advancing learning-based GED approximation."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-09T02:16:46Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    2,
                    16,
                    46,
                    2,
                    99,
                    0
                ],
                "arxiv_comment": "Accepted by ICDE 2026",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Zhouyang Liu"
                    },
                    {
                        "name": "Ning Liu"
                    },
                    {
                        "name": "Yixin Chen"
                    },
                    {
                        "name": "Jiezhong He"
                    },
                    {
                        "name": "Shuai Ma"
                    },
                    {
                        "name": "Dongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Dongsheng Li"
                },
                "author": "Dongsheng Li"
            },
            {
                "id": "http://arxiv.org/abs/2602.21889v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21889v1",
                "title": "2-Step Agent: A Framework for the Interaction of a Decision Maker with AI Decision Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2-Step Agent: A Framework for the Interaction of a Decision Maker with AI Decision Support"
                },
                "updated": "2026-02-25T13:11:12Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    13,
                    11,
                    12,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21889v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Across a growing number of fields, human decision making is supported by predictions from AI models. However, we still lack a deep understanding of the effects of adoption of these technologies. In this paper, we introduce a general computational framework, the 2-Step Agent, which models the effects of AI-assisted decision making. Our framework uses Bayesian methods for causal inference to model 1) how a prediction on a new observation affects the beliefs of a rational Bayesian agent, and 2) how this change in beliefs affects the downstream decision and subsequent outcome. Using this framework, we show by simulations how a single misaligned prior belief can be sufficient for decision support to result in worse downstream outcomes compared to no decision support. Our results reveal several potential pitfalls of AI-driven decision support and highlight the need for thorough model documentation and proper user training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Across a growing number of fields, human decision making is supported by predictions from AI models. However, we still lack a deep understanding of the effects of adoption of these technologies. In this paper, we introduce a general computational framework, the 2-Step Agent, which models the effects of AI-assisted decision making. Our framework uses Bayesian methods for causal inference to model 1) how a prediction on a new observation affects the beliefs of a rational Bayesian agent, and 2) how this change in beliefs affects the downstream decision and subsequent outcome. Using this framework, we show by simulations how a single misaligned prior belief can be sufficient for decision support to result in worse downstream outcomes compared to no decision support. Our results reveal several potential pitfalls of AI-driven decision support and highlight the need for thorough model documentation and proper user training."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T13:11:12Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    13,
                    11,
                    12,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "17 pages, 17 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Otto Nyberg"
                    },
                    {
                        "name": "Fausto Carcassi"
                    },
                    {
                        "name": "Giovanni Cin"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Cin"
                },
                "author": "Giovanni Cin"
            },
            {
                "id": "http://arxiv.org/abs/2602.21887v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21887v1",
                "title": "ExpLang: Improved Exploration and Exploitation in LLM Reasoning with On-Policy Thinking Language Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpLang: Improved Exploration and Exploitation in LLM Reasoning with On-Policy Thinking Language Selection"
                },
                "updated": "2026-02-25T13:10:58Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    13,
                    10,
                    58,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21887v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current large reasoning models (LRMs) have shown strong ability on challenging tasks after reinforcement learning (RL) based post-training. However, previous work mainly focuses on English reasoning in expectation of the strongest performance, despite the demonstrated potential advantage of multilingual thinking, as well as the requirement for native thinking traces by global users. In this paper, we propose ExpLang, a novel LLM post-training pipeline that enables on-policy thinking language selection to improve exploration and exploitation during RL with the use of multiple languages. The results show that our method steadily outperforms English-only training with the same training budget, while showing high thinking language compliance for both seen and unseen languages. Analysis shows that, by enabling on-policy thinking language selection as an action during RL, ExpLang effectively extends the RL exploration space with diversified language preference and improves the RL exploitation outcome with leveraged non-English advantage. The method is orthogonal to most RL algorithms and opens up a new perspective on using multilinguality to improve LRMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current large reasoning models (LRMs) have shown strong ability on challenging tasks after reinforcement learning (RL) based post-training. However, previous work mainly focuses on English reasoning in expectation of the strongest performance, despite the demonstrated potential advantage of multilingual thinking, as well as the requirement for native thinking traces by global users. In this paper, we propose ExpLang, a novel LLM post-training pipeline that enables on-policy thinking language selection to improve exploration and exploitation during RL with the use of multiple languages. The results show that our method steadily outperforms English-only training with the same training budget, while showing high thinking language compliance for both seen and unseen languages. Analysis shows that, by enabling on-policy thinking language selection as an action during RL, ExpLang effectively extends the RL exploration space with diversified language preference and improves the RL exploitation outcome with leveraged non-English advantage. The method is orthogonal to most RL algorithms and opens up a new perspective on using multilinguality to improve LRMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T13:10:58Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    13,
                    10,
                    58,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Changjiang Gao"
                    },
                    {
                        "name": "Zixian Huang"
                    },
                    {
                        "name": "Kaichen Yang"
                    },
                    {
                        "name": "Jiajun Chen"
                    },
                    {
                        "name": "Jixing Li"
                    },
                    {
                        "name": "Shujian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Shujian Huang"
                },
                "author": "Shujian Huang"
            },
            {
                "id": "http://arxiv.org/abs/2602.09382v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.09382v2",
                "title": "Initial-Condition-Robust Inference in Autoregressive Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Initial-Condition-Robust Inference in Autoregressive Models"
                },
                "updated": "2026-02-25T12:56:05Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    56,
                    5,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.09382v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.09382v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper considers confidence intervals (CIs) for the autoregressive (AR) parameter in an AR model with an AR parameter that may be close or equal to one. Existing CIs rely on the assumption of a stationary or fixed initial condition to obtain correct asymptotic coverage and good finite sample coverage. When this assumption fails, their coverage can be quite poor. In this paper, we introduce a new CI for the AR parameter whose coverage probability is completely robust to the initial condition, both asymptotically and in finite samples. This CI pays only a small price in terms of its length when the initial condition is stationary or fixed. The new CI also is robust to conditional heteroskedasticity of the errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper considers confidence intervals (CIs) for the autoregressive (AR) parameter in an AR model with an AR parameter that may be close or equal to one. Existing CIs rely on the assumption of a stationary or fixed initial condition to obtain correct asymptotic coverage and good finite sample coverage. When this assumption fails, their coverage can be quite poor. In this paper, we introduce a new CI for the AR parameter whose coverage probability is completely robust to the initial condition, both asymptotically and in finite samples. This CI pays only a small price in terms of its length when the initial condition is stationary or fixed. The new CI also is robust to conditional heteroskedasticity of the errors."
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-10T03:50:59Z",
                "published_parsed": [
                    2026,
                    2,
                    10,
                    3,
                    50,
                    59,
                    1,
                    41,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM"
                },
                "authors": [
                    {
                        "name": "Donald W. K. Andrews"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Yapeng Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Yapeng Zheng"
                },
                "author": "Yapeng Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2506.22685v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.22685v3",
                "title": "Mitigating Semantic Collapse in Generative Personalization with Test-Time Embedding Adjustment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Semantic Collapse in Generative Personalization with Test-Time Embedding Adjustment"
                },
                "updated": "2026-02-25T12:55:13Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    55,
                    13,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.22685v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.22685v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this paper, we investigate the semantic collapsing problem in generative personalization, an under-explored topic where the learned visual concept ($V$) gradually shifts from its original textual meaning and comes to dominate other concepts in multi-concept input prompts. This issue not only reduces the semantic richness of complex input prompts like \"a photo of $V$ wearing glasses and playing guitar\" into simpler, less contextually rich forms such as \"a photo of $V$\" but also leads to simplified output images that fail to capture the intended concept. We identify the root cause as unconstrained optimisation, which allows the learned embedding $V$ to drift arbitrarily in the embedding space, both in direction and magnitude. To address this, we propose a simple yet effective training-free method that adjusts the magnitude and direction of pre-trained embedding at inference time, effectively mitigating the semantic collapsing problem. Our method is broadly applicable across different personalization methods and demonstrates significant improvements in text-image alignment in diverse use cases. Our code is anonymously published at https://github.com/tuananhbui89/Embedding-Adjustment",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate the semantic collapsing problem in generative personalization, an under-explored topic where the learned visual concept ($V$) gradually shifts from its original textual meaning and comes to dominate other concepts in multi-concept input prompts. This issue not only reduces the semantic richness of complex input prompts like \"a photo of $V$ wearing glasses and playing guitar\" into simpler, less contextually rich forms such as \"a photo of $V$\" but also leads to simplified output images that fail to capture the intended concept. We identify the root cause as unconstrained optimisation, which allows the learned embedding $V$ to drift arbitrarily in the embedding space, both in direction and magnitude. To address this, we propose a simple yet effective training-free method that adjusts the magnitude and direction of pre-trained embedding at inference time, effectively mitigating the semantic collapsing problem. Our method is broadly applicable across different personalization methods and demonstrates significant improvements in text-image alignment in diverse use cases. Our code is anonymously published at https://github.com/tuananhbui89/Embedding-Adjustment"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-27T23:40:27Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    23,
                    40,
                    27,
                    4,
                    178,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Anh Bui"
                    },
                    {
                        "name": "Trang Vu"
                    },
                    {
                        "name": "Trung Le"
                    },
                    {
                        "name": "Junae Kim"
                    },
                    {
                        "name": "Tamas Abraham"
                    },
                    {
                        "name": "Rollin Omari"
                    },
                    {
                        "name": "Amar Kaur"
                    },
                    {
                        "name": "Dinh Phung"
                    }
                ],
                "author_detail": {
                    "name": "Dinh Phung"
                },
                "author": "Dinh Phung"
            },
            {
                "id": "http://arxiv.org/abs/2602.21869v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21869v1",
                "title": "A Bayesian approach to out-of-sample network reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bayesian approach to out-of-sample network reconstruction"
                },
                "updated": "2026-02-25T12:53:31Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    53,
                    31,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21869v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Networks underpin systems that range from finance to biology, yet their structure is often only partially observed. Current reconstruction methods typically fit the parameters of a model anew to each snapshot, thus offering no guidance to predict future configurations. Here, we develop a Bayesian approach that uses the information about past network snapshots to inform a prior and predict the subsequent ones, while quantifying uncertainty. Instantiated with a single-parameter fitness model, our method infers link probabilities from node strengths and carries information forward in time. When applied to the Electronic Market for Interbank Deposit across the years 1999-2012, our method accurately recovers the number of connections per bank at subsequent times, outperforming probabilistic benchmarks designed for analogous, link prediction tasks. Notably, each predicted snapshot serves as a reliable prior for the next one, thus enabling self-sustained, out-of-sample reconstruction of evolving networks with a minimal amount of additional data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Networks underpin systems that range from finance to biology, yet their structure is often only partially observed. Current reconstruction methods typically fit the parameters of a model anew to each snapshot, thus offering no guidance to predict future configurations. Here, we develop a Bayesian approach that uses the information about past network snapshots to inform a prior and predict the subsequent ones, while quantifying uncertainty. Instantiated with a single-parameter fitness model, our method infers link probabilities from node strengths and carries information forward in time. When applied to the Electronic Market for Interbank Deposit across the years 1999-2012, our method accurately recovers the number of connections per bank at subsequent times, outperforming probabilistic benchmarks designed for analogous, link prediction tasks. Notably, each predicted snapshot serves as a reliable prior for the next one, thus enabling self-sustained, out-of-sample reconstruction of evolving networks with a minimal amount of additional data."
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T12:53:31Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    53,
                    31,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "25 pages, 13 figures",
                "arxiv_primary_category": {
                    "term": "physics.soc-ph"
                },
                "authors": [
                    {
                        "name": "Mattia Marzi"
                    },
                    {
                        "name": "Tiziano Squartini"
                    }
                ],
                "author_detail": {
                    "name": "Tiziano Squartini"
                },
                "author": "Tiziano Squartini"
            },
            {
                "id": "http://arxiv.org/abs/2602.21864v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21864v1",
                "title": "DynamicGTR: Leveraging Graph Topology Representation Preferences to Boost VLM Capabilities on Graph QAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicGTR: Leveraging Graph Topology Representation Preferences to Boost VLM Capabilities on Graph QAs"
                },
                "updated": "2026-02-25T12:45:45Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    45,
                    45,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21864v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-Language Models (VLMs) have emerged as versatile solutions for zero-shot question answering (QA) across various domains. However, enabling VLMs to effectively comprehend structured graphs and perform accurate, efficient QA remains challenging. Existing approaches typically rely on one single graph topology representation (GTR), such as fixed-style visual images or unified text descriptions. This ``one-size-fits-all'' strategy often neglects model-specific and task-specific preferences, resulting in inaccurate or over-lengthy responses to graph-related queries. To address this, we propose the $\\mbox{DynamicGTR}$ framework, which dynamically selects the optimal GTR for each query during inference, thereby enhancing the zero-shot graph QA capabilities of VLMs with a customizable accuracy and brevity trade-off. Extensive experiments show that DynamicGTR not only improves VLM-based graph algorithm QA performance but also successfully transfers the experience trained from synthetic graph algorithm tasks to real-world applications like link prediction and node classification, without any additional training. Additionally, DynamicGTR demonstrates strong transferability across tasks, domains, and models, suggesting its potential as a flexible solution for broad graph scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have emerged as versatile solutions for zero-shot question answering (QA) across various domains. However, enabling VLMs to effectively comprehend structured graphs and perform accurate, efficient QA remains challenging. Existing approaches typically rely on one single graph topology representation (GTR), such as fixed-style visual images or unified text descriptions. This ``one-size-fits-all'' strategy often neglects model-specific and task-specific preferences, resulting in inaccurate or over-lengthy responses to graph-related queries. To address this, we propose the $\\mbox{DynamicGTR}$ framework, which dynamically selects the optimal GTR for each query during inference, thereby enhancing the zero-shot graph QA capabilities of VLMs with a customizable accuracy and brevity trade-off. Extensive experiments show that DynamicGTR not only improves VLM-based graph algorithm QA performance but also successfully transfers the experience trained from synthetic graph algorithm tasks to real-world applications like link prediction and node classification, without any additional training. Additionally, DynamicGTR demonstrates strong transferability across tasks, domains, and models, suggesting its potential as a flexible solution for broad graph scenarios."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T12:45:45Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    45,
                    45,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "CVPR 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yanbin Wei"
                    },
                    {
                        "name": "Jiangyue Yan"
                    },
                    {
                        "name": "Chun Kang"
                    },
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Hua Liu"
                    },
                    {
                        "name": "James Kwok"
                    },
                    {
                        "name": "Yu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Zhang"
                },
                "author": "Yu Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2602.21862v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21862v1",
                "title": "Personalized Graph-Empowered Large Language Model for Proactive Information Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Graph-Empowered Large Language Model for Proactive Information Access"
                },
                "updated": "2026-02-25T12:43:25Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    43,
                    25,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21862v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Since individuals may struggle to recall all life details and often confuse events, establishing a system to assist users in recalling forgotten experiences is essential. While numerous studies have proposed memory recall systems, these primarily rely on deep learning techniques that require extensive training and often face data scarcity due to the limited availability of personal lifelogs. As lifelogs grow over time, systems must also adapt quickly to newly accumulated data. Recently, large language models (LLMs) have demonstrated remarkable capabilities across various tasks, making them promising for personalized applications. In this work, we present a framework that leverages LLMs for proactive information access, integrating personal knowledge graphs to enhance the detection of access needs through a refined decision-making process. Our framework offers high flexibility, enabling the replacement of base models and the modification of fact retrieval methods for continuous improvement. Experimental results demonstrate that our approach effectively identifies forgotten events, supporting users in recalling past experiences more efficiently.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since individuals may struggle to recall all life details and often confuse events, establishing a system to assist users in recalling forgotten experiences is essential. While numerous studies have proposed memory recall systems, these primarily rely on deep learning techniques that require extensive training and often face data scarcity due to the limited availability of personal lifelogs. As lifelogs grow over time, systems must also adapt quickly to newly accumulated data. Recently, large language models (LLMs) have demonstrated remarkable capabilities across various tasks, making them promising for personalized applications. In this work, we present a framework that leverages LLMs for proactive information access, integrating personal knowledge graphs to enhance the detection of access needs through a refined decision-making process. Our framework offers high flexibility, enabling the replacement of base models and the modification of fact retrieval methods for continuous improvement. Experimental results demonstrate that our approach effectively identifies forgotten events, supporting users in recalling past experiences more efficiently."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T12:43:25Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    43,
                    25,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Chia Cheng Chang"
                    },
                    {
                        "name": "An-Zi Yen"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    },
                    {
                        "name": "Hsin-Hsi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hsin-Hsi Chen"
                },
                "author": "Hsin-Hsi Chen"
            },
            {
                "id": "http://arxiv.org/abs/2509.11787v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.11787v3",
                "title": "CodeCureAgent: Automatic Classification and Repair of Static Analysis Warnings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeCureAgent: Automatic Classification and Repair of Static Analysis Warnings"
                },
                "updated": "2026-02-25T12:42:03Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    42,
                    3,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.11787v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.11787v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Static analysis tools are widely used to detect bugs, vulnerabilities, and code smells. Traditionally, developers must resolve these warnings manually. Because this process is tedious, developers sometimes ignore warnings, leading to an accumulation of warnings and a degradation of code quality. This paper presents CodeCureAgent, an approach that harnesses LLM-based agents to automatically analyze, classify, and repair static analysis warnings. Unlike previous work, our method does not follow a predetermined algorithm. Instead, we adopt an agentic framework that iteratively invokes tools to gather additional information from the codebase (e.g., via code search) and edit the codebase to resolve the warning. CodeCureAgent detects and suppresses false positives, while fixing true positives when identified. We equip CodeCureAgent with a three-step heuristic to approve patches: (1) build the project, (2) verify that the warning disappears without introducing new warnings, and (3) run the test suite. We evaluate CodeCureAgent on a dataset of 1,000 SonarQube warnings found in 106 Java projects and covering 291 distinct rules. Our approach produces plausible fixes for 96.8% of the warnings, outperforming state-of-the-art baseline approaches by 29.2%-34.0% in plausible-fix rate. Manual inspection of 291 cases reveals a correct-fix rate of 86.3%, showing that CodeCureAgent can reliably repair static analysis warnings. The approach incurs LLM costs of about 2.9 cents (USD) and an end-to-end processing time of about four minutes per warning. We envision CodeCureAgent helping to clean existing codebases and being integrated into CI/CD pipelines to prevent the accumulation of static analysis warnings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static analysis tools are widely used to detect bugs, vulnerabilities, and code smells. Traditionally, developers must resolve these warnings manually. Because this process is tedious, developers sometimes ignore warnings, leading to an accumulation of warnings and a degradation of code quality. This paper presents CodeCureAgent, an approach that harnesses LLM-based agents to automatically analyze, classify, and repair static analysis warnings. Unlike previous work, our method does not follow a predetermined algorithm. Instead, we adopt an agentic framework that iteratively invokes tools to gather additional information from the codebase (e.g., via code search) and edit the codebase to resolve the warning. CodeCureAgent detects and suppresses false positives, while fixing true positives when identified. We equip CodeCureAgent with a three-step heuristic to approve patches: (1) build the project, (2) verify that the warning disappears without introducing new warnings, and (3) run the test suite. We evaluate CodeCureAgent on a dataset of 1,000 SonarQube warnings found in 106 Java projects and covering 291 distinct rules. Our approach produces plausible fixes for 96.8% of the warnings, outperforming state-of-the-art baseline approaches by 29.2%-34.0% in plausible-fix rate. Manual inspection of 291 cases reveals a correct-fix rate of 86.3%, showing that CodeCureAgent can reliably repair static analysis warnings. The approach incurs LLM costs of about 2.9 cents (USD) and an end-to-end processing time of about four minutes per warning. We envision CodeCureAgent helping to clean existing codebases and being integrated into CI/CD pipelines to prevent the accumulation of static analysis warnings."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-15T11:16:04Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    16,
                    4,
                    0,
                    258,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Pascal Joos"
                    },
                    {
                        "name": "Islem Bouzenia"
                    },
                    {
                        "name": "Michael Pradel"
                    }
                ],
                "author_detail": {
                    "name": "Michael Pradel"
                },
                "author": "Michael Pradel"
            },
            {
                "id": "http://arxiv.org/abs/2602.21858v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21858v1",
                "title": "ProactiveMobile: A Comprehensive Benchmark for Boosting Proactive Intelligence on Mobile Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProactiveMobile: A Comprehensive Benchmark for Boosting Proactive Intelligence on Mobile Devices"
                },
                "updated": "2026-02-25T12:32:37Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    32,
                    37,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21858v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multimodal large language models (MLLMs) have made significant progress in mobile agent development, yet their capabilities are predominantly confined to a reactive paradigm, where they merely execute explicit user commands. The emerging paradigm of proactive intelligence, where agents autonomously anticipate needs and initiate actions, represents the next frontier for mobile agents. However, its development is critically bottlenecked by the lack of benchmarks that can address real-world complexity and enable objective, executable evaluation. To overcome these challenges, we introduce ProactiveMobile, a comprehensive benchmark designed to systematically advance research in this domain. ProactiveMobile formalizes the proactive task as inferring latent user intent across four dimensions of on-device contextual signals and generating an executable function sequence from a comprehensive function pool of 63 APIs. The benchmark features over 3,660 instances of 14 scenarios that embrace real-world complexity through multi-answer annotations. To ensure quality, a team of 30 experts conducts a final audit of the benchmark, verifying factual accuracy, logical consistency, and action feasibility, and correcting any non-compliant entries. Extensive experiments demonstrate that our fine-tuned Qwen2.5-VL-7B-Instruct achieves a success rate of 19.15%, outperforming o1 (15.71%) and GPT-5 (7.39%). This result indicates that proactivity is a critical competency widely lacking in current MLLMs, yet it is learnable, emphasizing the importance of the proposed benchmark for proactivity evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have made significant progress in mobile agent development, yet their capabilities are predominantly confined to a reactive paradigm, where they merely execute explicit user commands. The emerging paradigm of proactive intelligence, where agents autonomously anticipate needs and initiate actions, represents the next frontier for mobile agents. However, its development is critically bottlenecked by the lack of benchmarks that can address real-world complexity and enable objective, executable evaluation. To overcome these challenges, we introduce ProactiveMobile, a comprehensive benchmark designed to systematically advance research in this domain. ProactiveMobile formalizes the proactive task as inferring latent user intent across four dimensions of on-device contextual signals and generating an executable function sequence from a comprehensive function pool of 63 APIs. The benchmark features over 3,660 instances of 14 scenarios that embrace real-world complexity through multi-answer annotations. To ensure quality, a team of 30 experts conducts a final audit of the benchmark, verifying factual accuracy, logical consistency, and action feasibility, and correcting any non-compliant entries. Extensive experiments demonstrate that our fine-tuned Qwen2.5-VL-7B-Instruct achieves a success rate of 19.15%, outperforming o1 (15.71%) and GPT-5 (7.39%). This result indicates that proactivity is a critical competency widely lacking in current MLLMs, yet it is learnable, emphasizing the importance of the proposed benchmark for proactivity evaluation."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T12:32:37Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    32,
                    37,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Dezhi Kong"
                    },
                    {
                        "name": "Zhengzhao Feng"
                    },
                    {
                        "name": "Qiliang Liang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Haofei Sun"
                    },
                    {
                        "name": "Changpeng Yang"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Shuai Nie"
                    },
                    {
                        "name": "Hongzhen Wang"
                    },
                    {
                        "name": "Linfeng Zhou"
                    },
                    {
                        "name": "Hao Jia"
                    },
                    {
                        "name": "Jiaming Xu"
                    },
                    {
                        "name": "Runyu Shi"
                    },
                    {
                        "name": "Ying Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ying Huang"
                },
                "author": "Ying Huang"
            },
            {
                "id": "http://arxiv.org/abs/2602.21854v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21854v1",
                "title": "FewMMBench: A Benchmark for Multimodal Few-Shot Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FewMMBench: A Benchmark for Multimodal Few-Shot Learning"
                },
                "updated": "2026-02-25T12:30:18Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    30,
                    18,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21854v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As multimodal large language models (MLLMs) advance in handling interleaved image-text data, assessing their few-shot learning capabilities remains an open challenge. In this paper, we introduce FewMMBench, a comprehensive benchmark designed to evaluate MLLMs under few-shot conditions, with a focus on In-Context Learning (ICL) and Chain-of-Thought (CoT) prompting. Covering a diverse suite of multimodal understanding tasks, from attribute recognition to temporal reasoning, FewMMBench enables systematic analysis across task types, model families, and prompting strategies. We evaluate 26 open-weight MLLMs from six model families across zero-shot, few-shot, and CoT-augmented few-shot settings. Our findings reveal that instruction-tuned models exhibit strong zero-shot performance but benefit minimally, or even regress, with additional demonstrations or CoT reasoning. Retrieval-based demonstrations and increased context size also yield limited gains. These results highlight FewMMBench as a rigorous testbed for diagnosing and advancing few-shot capabilities in multimodal LLMs. The data is available at: https://huggingface.co/datasets/mustafaa/FewMMBench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multimodal large language models (MLLMs) advance in handling interleaved image-text data, assessing their few-shot learning capabilities remains an open challenge. In this paper, we introduce FewMMBench, a comprehensive benchmark designed to evaluate MLLMs under few-shot conditions, with a focus on In-Context Learning (ICL) and Chain-of-Thought (CoT) prompting. Covering a diverse suite of multimodal understanding tasks, from attribute recognition to temporal reasoning, FewMMBench enables systematic analysis across task types, model families, and prompting strategies. We evaluate 26 open-weight MLLMs from six model families across zero-shot, few-shot, and CoT-augmented few-shot settings. Our findings reveal that instruction-tuned models exhibit strong zero-shot performance but benefit minimally, or even regress, with additional demonstrations or CoT reasoning. Retrieval-based demonstrations and increased context size also yield limited gains. These results highlight FewMMBench as a rigorous testbed for diagnosing and advancing few-shot capabilities in multimodal LLMs. The data is available at: https://huggingface.co/datasets/mustafaa/FewMMBench"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T12:30:18Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    30,
                    18,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "Preprint. 49 pages, 38 Figures, 5 Tables",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mustafa Dogan"
                    },
                    {
                        "name": "Ilker Kesen"
                    },
                    {
                        "name": "Iacer Calixto"
                    },
                    {
                        "name": "Aykut Erdem"
                    },
                    {
                        "name": "Erkut Erdem"
                    }
                ],
                "author_detail": {
                    "name": "Erkut Erdem"
                },
                "author": "Erkut Erdem"
            },
            {
                "id": "http://arxiv.org/abs/2602.21850v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21850v1",
                "title": "The accretion discs in WZ Sge-type stars in deep quiescence. How do they outburst?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The accretion discs in WZ Sge-type stars in deep quiescence. How do they outburst?"
                },
                "updated": "2026-02-25T12:27:00Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    27,
                    0,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21850v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "WZ Sge-type stars are an extreme subclass of dwarf novae characterised by very rare, large-amplitude superoutbursts. Within the disc instability model (DIM), such events are explained as being triggered by enhanced mass transfer from the donor star. We present an analysis of observations of a sample of WZ Sge-type systems in deep quiescence to assess the consistency of DIM predictions with their observed properties. We find that accretion discs in quiescent WZ Sge-type systems have very low mass-accretion rates of a few $\\times$$10^{-13}$ M$_\\odot$ yr$^{-1}$. The discs are entirely optically thin, and their physical conditions -- such as surface density and effective temperature -- remain well below the DIM thresholds required to trigger an outburst. Observationally, no increase in disc brightness is detected prior to the superoutburst, indicating the absence of a transition to an optically thick state, in contrast to DIM predictions of a gradual disc thickening preceding the instability. We therefore find no observational evidence that superoutbursts in WZ Sge-type systems are triggered by enhanced mass transfer from the donor. Furthermore, the inferred mass-transfer rates in these objects ($\\dot{M}_{\\rm tr}$~5$\\times$$10^{-12}$ M$_\\odot$ yr$^{-1}$) are at least an order of magnitude lower than commonly assumed. We argue that the widely adopted value of $\\dot{M}_{\\rm tr}$ for the prototype object WZ Sge is likely overestimated. Finally, we show that in quiescence the accretion disc radius in all systems is close to the tidal truncation radius and exceeds the 3:1 resonance radius, confirming earlier results and calling into question the standard interpretation of superhump formation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WZ Sge-type stars are an extreme subclass of dwarf novae characterised by very rare, large-amplitude superoutbursts. Within the disc instability model (DIM), such events are explained as being triggered by enhanced mass transfer from the donor star. We present an analysis of observations of a sample of WZ Sge-type systems in deep quiescence to assess the consistency of DIM predictions with their observed properties. We find that accretion discs in quiescent WZ Sge-type systems have very low mass-accretion rates of a few $\\times$$10^{-13}$ M$_\\odot$ yr$^{-1}$. The discs are entirely optically thin, and their physical conditions -- such as surface density and effective temperature -- remain well below the DIM thresholds required to trigger an outburst. Observationally, no increase in disc brightness is detected prior to the superoutburst, indicating the absence of a transition to an optically thick state, in contrast to DIM predictions of a gradual disc thickening preceding the instability. We therefore find no observational evidence that superoutbursts in WZ Sge-type systems are triggered by enhanced mass transfer from the donor. Furthermore, the inferred mass-transfer rates in these objects ($\\dot{M}_{\\rm tr}$~5$\\times$$10^{-12}$ M$_\\odot$ yr$^{-1}$) are at least an order of magnitude lower than commonly assumed. We argue that the widely adopted value of $\\dot{M}_{\\rm tr}$ for the prototype object WZ Sge is likely overestimated. Finally, we show that in quiescence the accretion disc radius in all systems is close to the tidal truncation radius and exceeds the 3:1 resonance radius, confirming earlier results and calling into question the standard interpretation of superhump formation."
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T12:27:00Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    27,
                    0,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "16 pages (+4 pages of references). This paper is based on a talk presented at the 87th Fujihara Seminar: The 50th Anniversary Workshop of the Disk Instability Model in Compact Binary Stars, held on 22-26 September 2025 in Tomakomai, Japan. To be published in the Proceedings of Science https://pos.sissa.it/493/",
                "arxiv_primary_category": {
                    "term": "astro-ph.SR"
                },
                "authors": [
                    {
                        "name": "Vitaly Neustroev"
                    },
                    {
                        "name": "Paula Kvist"
                    },
                    {
                        "name": "Miikka Siitonen"
                    },
                    {
                        "name": "Veera Vuolteenaho"
                    }
                ],
                "author_detail": {
                    "name": "Veera Vuolteenaho"
                },
                "author": "Veera Vuolteenaho"
            },
            {
                "id": "http://arxiv.org/abs/2602.21846v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21846v1",
                "title": "Scalable Kernel-Based Distances for Statistical Inference and Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Kernel-Based Distances for Statistical Inference and Integration"
                },
                "updated": "2026-02-25T12:25:34Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    25,
                    34,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21846v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Representing, comparing, and measuring the distance between probability distributions is a key task in computational statistics and machine learning. The choice of representation and the associated distance determine properties of the methods in which they are used: for example, certain distances can allow one to encode robustness or smoothness of the problem. Kernel methods offer flexible and rich Hilbert space representations of distributions that allow the modeller to enforce properties through the choice of kernel, and estimate associated distances at efficient nonparametric rates. In particular, the maximum mean discrepancy (MMD), a kernel-based distance constructed by comparing Hilbert space mean functions, has received significant attention due to its computational tractability and is favoured by practitioners.\n  In this thesis, we conduct a thorough study of kernel-based distances with a focus on efficient computation, with core contributions in Chapters 3 to 6. Part I of the thesis is focused on the MMD, specifically on improved MMD estimation. In Chapter 3 we propose a theoretically sound, improved estimator for MMD in simulation-based inference. Then, in Chapter 4, we propose an MMD-based estimator for conditional expectations, a ubiquitous task in statistical computation. Closing Part I, in Chapter 5 we study the problem of calibration when MMD is applied to the task of integration.\n  In Part II, motivated by the recent developments in kernel embeddings beyond the mean, we introduce a family of novel kernel-based discrepancies: kernel quantile discrepancies. These address some of the pitfalls of MMD, and are shown through both theoretical results and an empirical study to offer a competitive alternative to MMD and its fast approximations. We conclude with a discussion on broader lessons and future work emerging from the thesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representing, comparing, and measuring the distance between probability distributions is a key task in computational statistics and machine learning. The choice of representation and the associated distance determine properties of the methods in which they are used: for example, certain distances can allow one to encode robustness or smoothness of the problem. Kernel methods offer flexible and rich Hilbert space representations of distributions that allow the modeller to enforce properties through the choice of kernel, and estimate associated distances at efficient nonparametric rates. In particular, the maximum mean discrepancy (MMD), a kernel-based distance constructed by comparing Hilbert space mean functions, has received significant attention due to its computational tractability and is favoured by practitioners.\n  In this thesis, we conduct a thorough study of kernel-based distances with a focus on efficient computation, with core contributions in Chapters 3 to 6. Part I of the thesis is focused on the MMD, specifically on improved MMD estimation. In Chapter 3 we propose a theoretically sound, improved estimator for MMD in simulation-based inference. Then, in Chapter 4, we propose an MMD-based estimator for conditional expectations, a ubiquitous task in statistical computation. Closing Part I, in Chapter 5 we study the problem of calibration when MMD is applied to the task of integration.\n  In Part II, motivated by the recent developments in kernel embeddings beyond the mean, we introduce a family of novel kernel-based discrepancies: kernel quantile discrepancies. These address some of the pitfalls of MMD, and are shown through both theoretical results and an empirical study to offer a competitive alternative to MMD and its fast approximations. We conclude with a discussion on broader lessons and future work emerging from the thesis."
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T12:25:34Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    25,
                    34,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "PhD thesis",
                "arxiv_primary_category": {
                    "term": "stat.ML"
                },
                "authors": [
                    {
                        "name": "Masha Naslidnyk"
                    }
                ],
                "author_detail": {
                    "name": "Masha Naslidnyk"
                },
                "author": "Masha Naslidnyk"
            },
            {
                "id": "http://arxiv.org/abs/2402.00386v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2402.00386v4",
                "title": "AssertLLM: Generating and Evaluating Hardware Verification Assertions from Design Specifications via Multi-LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AssertLLM: Generating and Evaluating Hardware Verification Assertions from Design Specifications via Multi-LLMs"
                },
                "updated": "2026-02-25T12:23:33Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    23,
                    33,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2402.00386v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2402.00386v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Assertion-based verification (ABV) is a critical method for ensuring design circuits comply with their architectural specifications, which are typically described in natural language. This process often requires human interpretation by verification engineers to convert these specifications into functional verification assertions. Existing methods for generating assertions from natural language specifications are limited to sentences extracted by engineers, discouraging its practical application. In this work, we present AssertLLM, an automatic assertion generation framework that processes complete specification files. AssertLLM breaks down the complex task into three phases, incorporating three customized Large Language Models (LLMs) for extracting structural specifications, mapping signal definitions, and generating assertions. Our evaluation of AssertLLM on a full design, encompassing 23 I/O signals, demonstrates that 89\\% of the generated assertions are both syntactically and functionally accurate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assertion-based verification (ABV) is a critical method for ensuring design circuits comply with their architectural specifications, which are typically described in natural language. This process often requires human interpretation by verification engineers to convert these specifications into functional verification assertions. Existing methods for generating assertions from natural language specifications are limited to sentences extracted by engineers, discouraging its practical application. In this work, we present AssertLLM, an automatic assertion generation framework that processes complete specification files. AssertLLM breaks down the complex task into three phases, incorporating three customized Large Language Models (LLMs) for extracting structural specifications, mapping signal definitions, and generating assertions. Our evaluation of AssertLLM on a full design, encompassing 23 I/O signals, demonstrates that 89\\% of the generated assertions are both syntactically and functionally accurate."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-02-01T07:15:03Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    7,
                    15,
                    3,
                    3,
                    32,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Wenji Fang"
                    },
                    {
                        "name": "Mengming Li"
                    },
                    {
                        "name": "Min Li"
                    },
                    {
                        "name": "Zhiyuan Yan"
                    },
                    {
                        "name": "Shang Liu"
                    },
                    {
                        "name": "Hongce Zhang"
                    },
                    {
                        "name": "Zhiyao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyao Xie"
                },
                "author": "Zhiyao Xie"
            },
            {
                "id": "http://arxiv.org/abs/2602.20483v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20483v2",
                "title": "Cosmology with galaxy clusters using machine learning. Application to eROSITA Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmology with galaxy clusters using machine learning. Application to eROSITA Data"
                },
                "updated": "2026-02-25T12:18:56Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    18,
                    56,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20483v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20483v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Context: We present the first Cosmological Parameter inferences from eROSITA X-ray observations of galaxy clusters using a Machine Learning algorithm. Methods: We train a Random Forest using mock catalogs of clusters from Magneticum multi-cosmology hydrodynamical simulations. We apply the trained ML algorithm to observed X-ray features (gas luminosity, mass, and temperature) at different redshifts from the eROSITA eFEDS and eRASS1 catalogs. Results: We obtain cosmological constraints with precision comparable to those from standard analyses, such as weak lensing and cluster abundances. We infer $_{\\rm m}=0.30^{+0.03}_{-0.02}$, $_8=0.81\\pm0.01$, and $h_0=0.710\\pm0.004$. The recovered parameters show no tension in the $_{\\rm m}-_8$ space, but a significant deviation of $h_0$ from the Planck estimates. These inferences remain rather stable against variations of the input observable set and parameter space coverage. These results indicate that correlations among intracluster properties contain cosmological information beyond that encoded in the cluster abundance alone, which can be captured by machine learning trained on multi-cosmology simulations. Conclusions: ML algorithms trained on multi-cosmology hydrodynamical simulations can effectively infer cosmological parameters directly from galaxy cluster data. This is a change of paradigm in the context of cosmological parameter inferences. This approach complements traditional cluster-count analyses and is particularly suited to large upcoming surveys, where systematic uncertainties in mass calibration may otherwise dominate the error budget. It also highlights the potential of large-scale X-ray surveys to deliver independent tests of the standard cosmological model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: We present the first Cosmological Parameter inferences from eROSITA X-ray observations of galaxy clusters using a Machine Learning algorithm. Methods: We train a Random Forest using mock catalogs of clusters from Magneticum multi-cosmology hydrodynamical simulations. We apply the trained ML algorithm to observed X-ray features (gas luminosity, mass, and temperature) at different redshifts from the eROSITA eFEDS and eRASS1 catalogs. Results: We obtain cosmological constraints with precision comparable to those from standard analyses, such as weak lensing and cluster abundances. We infer $_{\\rm m}=0.30^{+0.03}_{-0.02}$, $_8=0.81\\pm0.01$, and $h_0=0.710\\pm0.004$. The recovered parameters show no tension in the $_{\\rm m}-_8$ space, but a significant deviation of $h_0$ from the Planck estimates. These inferences remain rather stable against variations of the input observable set and parameter space coverage. These results indicate that correlations among intracluster properties contain cosmological information beyond that encoded in the cluster abundance alone, which can be captured by machine learning trained on multi-cosmology simulations. Conclusions: ML algorithms trained on multi-cosmology hydrodynamical simulations can effectively infer cosmological parameters directly from galaxy cluster data. This is a change of paradigm in the context of cosmological parameter inferences. This approach complements traditional cluster-count analyses and is particularly suited to large upcoming surveys, where systematic uncertainties in mass calibration may otherwise dominate the error budget. It also highlights the potential of large-scale X-ray surveys to deliver independent tests of the standard cosmological model."
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-24T02:23:25Z",
                "published_parsed": [
                    2026,
                    2,
                    24,
                    2,
                    23,
                    25,
                    1,
                    55,
                    0
                ],
                "arxiv_comment": "Submitted, 15 pages and 12 figures. Comments are welcome!",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO"
                },
                "authors": [
                    {
                        "name": "Fucheng Zhong"
                    },
                    {
                        "name": "Nicola R. Napolitano"
                    },
                    {
                        "name": "Johan Comparat"
                    },
                    {
                        "name": "Klaus Dolag"
                    },
                    {
                        "name": "Caroline Heneka"
                    },
                    {
                        "name": "Zhiqi Huang"
                    },
                    {
                        "name": "Xiaodong Li"
                    },
                    {
                        "name": "Weipeng Lin"
                    },
                    {
                        "name": "Giuseppe Longo"
                    },
                    {
                        "name": "Mario Radovich"
                    },
                    {
                        "name": "Crescenzo Tortora"
                    }
                ],
                "author_detail": {
                    "name": "Crescenzo Tortora"
                },
                "author": "Crescenzo Tortora"
            },
            {
                "id": "http://arxiv.org/abs/2509.07477v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.07477v2",
                "title": "MedicalPatchNet: A Patch-Based Self-Explainable AI Architecture for Chest X-ray Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedicalPatchNet: A Patch-Based Self-Explainable AI Architecture for Chest X-ray Classification"
                },
                "updated": "2026-02-25T12:17:47Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    17,
                    47,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.07477v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.07477v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1038/s41598-026-40358-0",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Deep neural networks excel in radiological image classification but frequently suffer from poor interpretability, limiting clinical acceptance. We present MedicalPatchNet, an inherently self-explainable architecture for chest X-ray classification that transparently attributes decisions to distinct image regions. MedicalPatchNet splits images into non-overlapping patches, independently classifies each patch, and aggregates predictions, enabling intuitive visualization of each patch's diagnostic contribution without post-hoc techniques. Trained on the CheXpert dataset (223,414 images), MedicalPatchNet matches the classification performance (AUROC 0.907 vs. 0.908) of EfficientNetV2-S, while improving interpretability: MedicalPatchNet demonstrates improved interpretability with higher pathology localization accuracy (mean hit-rate 0.485 vs. 0.376 with Grad-CAM) on the CheXlocalize dataset. By providing explicit, reliable explanations accessible even to non-AI experts, MedicalPatchNet mitigates risks associated with shortcut learning, thus improving clinical trust. Our model is publicly available with reproducible training and inference scripts and contributes to safer, explainable AI-assisted diagnostics across medical imaging domains. We make the code publicly available: https://github.com/TruhnLab/MedicalPatchNet",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks excel in radiological image classification but frequently suffer from poor interpretability, limiting clinical acceptance. We present MedicalPatchNet, an inherently self-explainable architecture for chest X-ray classification that transparently attributes decisions to distinct image regions. MedicalPatchNet splits images into non-overlapping patches, independently classifies each patch, and aggregates predictions, enabling intuitive visualization of each patch's diagnostic contribution without post-hoc techniques. Trained on the CheXpert dataset (223,414 images), MedicalPatchNet matches the classification performance (AUROC 0.907 vs. 0.908) of EfficientNetV2-S, while improving interpretability: MedicalPatchNet demonstrates improved interpretability with higher pathology localization accuracy (mean hit-rate 0.485 vs. 0.376 with Grad-CAM) on the CheXlocalize dataset. By providing explicit, reliable explanations accessible even to non-AI experts, MedicalPatchNet mitigates risks associated with shortcut learning, thus improving clinical trust. Our model is publicly available with reproducible training and inference scripts and contributes to safer, explainable AI-assisted diagnostics across medical imaging domains. We make the code publicly available: https://github.com/TruhnLab/MedicalPatchNet"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-09T08:02:10Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    8,
                    2,
                    10,
                    1,
                    252,
                    0
                ],
                "arxiv_comment": "28 pages, 12 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "arxiv_journal_ref": "Sci Rep 16, 7467 (2026)",
                "authors": [
                    {
                        "name": "Patrick Wienholt"
                    },
                    {
                        "name": "Christiane Kuhl"
                    },
                    {
                        "name": "Jakob Nikolas Kather"
                    },
                    {
                        "name": "Sven Nebelung"
                    },
                    {
                        "name": "Daniel Truhn"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Truhn"
                },
                "author": "Daniel Truhn",
                "arxiv_doi": "10.1038/s41598-026-40358-0"
            },
            {
                "id": "http://arxiv.org/abs/2508.13755v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.13755v6",
                "title": "Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with Adaptive Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with Adaptive Exploration"
                },
                "updated": "2026-02-25T12:13:07Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    13,
                    7,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.13755v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.13755v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models, yet its full potential is hindered by two under-explored dimensions: Depth-the hardest problem a model can sample; Breadth-the number of instances consumed in a single iteration. We dissect the popular GRPO algorithm and reveal a systematic bias: the cumulative-advantage disproportionately weights samples with medium accuracy, while down-weighting the low-accuracy instances that are crucial for pushing reasoning boundaries. To rectify the depth neglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which re-weights hard problems through targeted multi-stage rollouts, thereby increasing the number of positive rollouts for hard problems. Empirically, naively enlarging rollout size only accelerates convergence and even hurts Pass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra inference cost at convergence. Just as we adaptively expanded the depth of exploration, we now ask whether aggressively scaling the breadth of training data can further amplify reasoning gains. To this end, we intensely scale batch size and replace PPO's mini-batch iterations with full-batch updates over multiple epochs. Increasing breadth significantly enhances Pass@1 performance. Large-breadth training sustains high token-level entropy, indicating continued exploration and reduced gradient noise. We further present DARS-B, which augments DARS with large breadth, and demonstrate simultaneous gains in Pass@K and Pass@1. The results confirm that breadth and adaptive exploration across depth operate as orthogonal dimensions in RLVR, which are key to unleashing the reasoning power of RLVR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models, yet its full potential is hindered by two under-explored dimensions: Depth-the hardest problem a model can sample; Breadth-the number of instances consumed in a single iteration. We dissect the popular GRPO algorithm and reveal a systematic bias: the cumulative-advantage disproportionately weights samples with medium accuracy, while down-weighting the low-accuracy instances that are crucial for pushing reasoning boundaries. To rectify the depth neglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which re-weights hard problems through targeted multi-stage rollouts, thereby increasing the number of positive rollouts for hard problems. Empirically, naively enlarging rollout size only accelerates convergence and even hurts Pass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra inference cost at convergence. Just as we adaptively expanded the depth of exploration, we now ask whether aggressively scaling the breadth of training data can further amplify reasoning gains. To this end, we intensely scale batch size and replace PPO's mini-batch iterations with full-batch updates over multiple epochs. Increasing breadth significantly enhances Pass@1 performance. Large-breadth training sustains high token-level entropy, indicating continued exploration and reduced gradient noise. We further present DARS-B, which augments DARS with large breadth, and demonstrate simultaneous gains in Pass@K and Pass@1. The results confirm that breadth and adaptive exploration across depth operate as orthogonal dimensions in RLVR, which are key to unleashing the reasoning power of RLVR."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-19T11:51:40Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    51,
                    40,
                    1,
                    231,
                    0
                ],
                "arxiv_comment": "20 pages, 17 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Zhicheng Yang"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Yinya Huang"
                    },
                    {
                        "name": "Yongxin Wang"
                    },
                    {
                        "name": "Dongchun Xie"
                    },
                    {
                        "name": "Hanhui Li"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Xiaodan Liang"
                    },
                    {
                        "name": "Jing Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Tang"
                },
                "author": "Jing Tang"
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2602.22207v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22207v1",
                "title": "Recovered in Translation: Efficient Pipeline for Automated Translation of Benchmarks and Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recovered in Translation: Efficient Pipeline for Automated Translation of Benchmarks and Datasets"
                },
                "updated": "2026-02-25T18:58:25Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    58,
                    25,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22207v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The reliability of multilingual Large Language Model (LLM) evaluation is currently compromised by the inconsistent quality of translated benchmarks. Existing resources often suffer from semantic drift and context loss, which can lead to misleading performance metrics. In this work, we present a fully automated framework designed to address these challenges by enabling scalable, high-quality translation of datasets and benchmarks. We demonstrate that adapting test-time compute scaling strategies, specifically Universal Self-Improvement (USI) and our proposed multi-round ranking method, T-RANK, allows for significantly higher quality outputs compared to traditional pipelines. Our framework ensures that benchmarks preserve their original task structure and linguistic nuances during localization. We apply this approach to translate popular benchmarks and datasets into eight Eastern and Southern European languages (Ukrainian, Bulgarian, Slovak, Romanian, Lithuanian, Estonian, Turkish, Greek). Evaluations using both reference-based metrics and LLM-as-a-judge show that our translations surpass existing resources, resulting in more accurate downstream model assessment. We release both the framework and the improved benchmarks to facilitate robust and reproducible multilingual AI development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reliability of multilingual Large Language Model (LLM) evaluation is currently compromised by the inconsistent quality of translated benchmarks. Existing resources often suffer from semantic drift and context loss, which can lead to misleading performance metrics. In this work, we present a fully automated framework designed to address these challenges by enabling scalable, high-quality translation of datasets and benchmarks. We demonstrate that adapting test-time compute scaling strategies, specifically Universal Self-Improvement (USI) and our proposed multi-round ranking method, T-RANK, allows for significantly higher quality outputs compared to traditional pipelines. Our framework ensures that benchmarks preserve their original task structure and linguistic nuances during localization. We apply this approach to translate popular benchmarks and datasets into eight Eastern and Southern European languages (Ukrainian, Bulgarian, Slovak, Romanian, Lithuanian, Estonian, Turkish, Greek). Evaluations using both reference-based metrics and LLM-as-a-judge show that our translations surpass existing resources, resulting in more accurate downstream model assessment. We release both the framework and the improved benchmarks to facilitate robust and reproducible multilingual AI development."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T18:58:25Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    58,
                    25,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hanna Yukhymenko"
                    },
                    {
                        "name": "Anton Alexandrov"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev"
            },
            {
                "id": "http://arxiv.org/abs/2602.00288v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.00288v3",
                "title": "TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs"
                },
                "updated": "2026-02-25T18:57:52Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    57,
                    52,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.00288v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.00288v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Fine-grained spatio-temporal understanding is essential for video reasoning and embodied AI. Yet, while Multimodal Large Language Models (MLLMs) master static semantics, their grasp of temporal dynamics remains brittle. We present TimeBlind, a diagnostic benchmark for compositional spatio-temporal understanding. Inspired by cognitive science, TimeBlind categorizes fine-grained temporal understanding into three levels: recognizing atomic events, characterizing event properties, and reasoning about event interdependencies. Unlike benchmarks that conflate recognition with temporal reasoning, TimeBlind leverages a minimal-pairs paradigm: video pairs share identical static visual content but differ solely in temporal structure, utilizing complementary questions to neutralize language priors. Evaluating over 20 state-of-the-art MLLMs (e.g., GPT-5, Gemini 3 Pro) on 600 curated instances (2400 video-question pairs), reveals that the Instance Accuracy (correctly distinguishing both videos in a pair) of the best performing MLLM is only 48.2%, far below the human performance (98.2%). These results demonstrate that even frontier models rely heavily on static visual shortcuts rather than genuine temporal logic, positioning TimeBlind as a vital diagnostic tool for next-generation video understanding. Dataset and code are available at https://baiqi-li.github.io/timeblind_project/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained spatio-temporal understanding is essential for video reasoning and embodied AI. Yet, while Multimodal Large Language Models (MLLMs) master static semantics, their grasp of temporal dynamics remains brittle. We present TimeBlind, a diagnostic benchmark for compositional spatio-temporal understanding. Inspired by cognitive science, TimeBlind categorizes fine-grained temporal understanding into three levels: recognizing atomic events, characterizing event properties, and reasoning about event interdependencies. Unlike benchmarks that conflate recognition with temporal reasoning, TimeBlind leverages a minimal-pairs paradigm: video pairs share identical static visual content but differ solely in temporal structure, utilizing complementary questions to neutralize language priors. Evaluating over 20 state-of-the-art MLLMs (e.g., GPT-5, Gemini 3 Pro) on 600 curated instances (2400 video-question pairs), reveals that the Instance Accuracy (correctly distinguishing both videos in a pair) of the best performing MLLM is only 48.2%, far below the human performance (98.2%). These results demonstrate that even frontier models rely heavily on static visual shortcuts rather than genuine temporal logic, positioning TimeBlind as a vital diagnostic tool for next-generation video understanding. Dataset and code are available at https://baiqi-li.github.io/timeblind_project/ ."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-30T20:21:46Z",
                "published_parsed": [
                    2026,
                    1,
                    30,
                    20,
                    21,
                    46,
                    4,
                    30,
                    0
                ],
                "arxiv_comment": "For code and data, see https://baiqi-li.github.io/timeblind_project/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Baiqi Li"
                    },
                    {
                        "name": "Kangyi Zhao"
                    },
                    {
                        "name": "Ce Zhang"
                    },
                    {
                        "name": "Chancharik Mitra"
                    },
                    {
                        "name": "Jean de Dieu Nyandwi"
                    },
                    {
                        "name": "Gedas Bertasius"
                    }
                ],
                "author_detail": {
                    "name": "Gedas Bertasius"
                },
                "author": "Gedas Bertasius"
            },
            {
                "id": "http://arxiv.org/abs/2504.17203v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.17203v4",
                "title": "High-Fidelity And Complex Test Data Generation For Google SQL Code Generation Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Fidelity And Complex Test Data Generation For Google SQL Code Generation Services"
                },
                "updated": "2026-02-25T18:55:05Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    55,
                    5,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.17203v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.17203v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The demand for high-fidelity test data is paramount in industrial settings where access to production data is largely restricted. Traditional data generation methods often fall short, struggling with low-fidelity and the ability to model complex data structures and semantic relationships that are critical for testing complex SQL code generation services like Natural Language to SQL (NL2SQL). In this paper, we address the critical need for generating syntactically correct and semantically relevant high-fidelity mock data for complex data structures that includes columns with nested structures that we frequently encounter in Google workloads. We highlight the limitations of existing approaches used in production, particularly their inability to handle large and complex data structures, as well as the lack of semantically coherent test data that lead to limited test coverage. We demonstrate that by leveraging Large Language Models (LLMs) and incorporating strategic pre- and post-processing steps, we can generate syntactically correct and semantically relevant high-fidelity test data that adheres to complex structural constraints and maintains semantic integrity to the SQL test targets (queries/functions). This approach supports comprehensive testing of complex SQL queries involving joins, aggregations, and even deeply nested subqueries, ensuring robust evaluation of SQL code generation services, like NL2SQL and SQL Code Assistant. Our results demonstrate the practical utility of an LLM (\\textit{Gemini}) based test data generation for industrial SQL code generation services where generating high-fidelity test data is essential due to the frequent unavailability and inaccessibility of production datasets for testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The demand for high-fidelity test data is paramount in industrial settings where access to production data is largely restricted. Traditional data generation methods often fall short, struggling with low-fidelity and the ability to model complex data structures and semantic relationships that are critical for testing complex SQL code generation services like Natural Language to SQL (NL2SQL). In this paper, we address the critical need for generating syntactically correct and semantically relevant high-fidelity mock data for complex data structures that includes columns with nested structures that we frequently encounter in Google workloads. We highlight the limitations of existing approaches used in production, particularly their inability to handle large and complex data structures, as well as the lack of semantically coherent test data that lead to limited test coverage. We demonstrate that by leveraging Large Language Models (LLMs) and incorporating strategic pre- and post-processing steps, we can generate syntactically correct and semantically relevant high-fidelity test data that adheres to complex structural constraints and maintains semantic integrity to the SQL test targets (queries/functions). This approach supports comprehensive testing of complex SQL queries involving joins, aggregations, and even deeply nested subqueries, ensuring robust evaluation of SQL code generation services, like NL2SQL and SQL Code Assistant. Our results demonstrate the practical utility of an LLM (\\textit{Gemini}) based test data generation for industrial SQL code generation services where generating high-fidelity test data is essential due to the frequent unavailability and inaccessibility of production datasets for testing."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-24T02:27:17Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    2,
                    27,
                    17,
                    3,
                    114,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Shivasankari Kannan"
                    },
                    {
                        "name": "Yeounoh Chung"
                    },
                    {
                        "name": "Amita Gondi"
                    },
                    {
                        "name": "Tristan Swadell"
                    },
                    {
                        "name": "Fatma Ozcan"
                    }
                ],
                "author_detail": {
                    "name": "Fatma Ozcan"
                },
                "author": "Fatma Ozcan"
            },
            {
                "id": "http://arxiv.org/abs/2602.16893v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16893v2",
                "title": "CalmReminder: A Design Probe for Parental Engagement with Children with Hyperactivity, Augmented by Real-Time Motion Sensing with a Watch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CalmReminder: A Design Probe for Parental Engagement with Children with Hyperactivity, Augmented by Real-Time Motion Sensing with a Watch"
                },
                "updated": "2026-02-25T18:46:14Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    46,
                    14,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16893v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16893v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3772318.3791871",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Families raising children with ADHD often experience heightened stress and reactive parenting. While digital interventions promise personalization, many remain one-size-fits-all and fail to reflect parents' lived practices. We present CalmReminder, a watch-based system that detects children's calm moments and delivers just-in-time prompts to parents. Through a four-week deployment with 16 families (twelve completed) of children with ADHD, we compared notification strategies ranging from hourly to random to only when the child was inferred to be calm. Our sensing-based notifications were frequently perceived as arriving during calm moments. More importantly, parents adopted the system in diverse ways: using notifications for praise, mindfulness, activity planning, or conversation. These findings show that parents are not passive recipients but active designers, reshaping interventions to fit their parenting styles. We contribute a calm detection pipeline, empirical insights into families' flexible appropriation of notifications, and design implications for intervention systems that foster agency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Families raising children with ADHD often experience heightened stress and reactive parenting. While digital interventions promise personalization, many remain one-size-fits-all and fail to reflect parents' lived practices. We present CalmReminder, a watch-based system that detects children's calm moments and delivers just-in-time prompts to parents. Through a four-week deployment with 16 families (twelve completed) of children with ADHD, we compared notification strategies ranging from hourly to random to only when the child was inferred to be calm. Our sensing-based notifications were frequently perceived as arriving during calm moments. More importantly, parents adopted the system in diverse ways: using notifications for praise, mindfulness, activity planning, or conversation. These findings show that parents are not passive recipients but active designers, reshaping interventions to fit their parenting styles. We contribute a calm detection pipeline, empirical insights into families' flexible appropriation of notifications, and design implications for intervention systems that foster agency."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T21:20:24Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    21,
                    20,
                    24,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "Accepted by ACM CHI Conference on Human Factors in Computing Systems(CHI'26)",
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Riku Arakawa"
                    },
                    {
                        "name": "Shreya Bali"
                    },
                    {
                        "name": "Anupama Sitaraman"
                    },
                    {
                        "name": "Woosuk Seo"
                    },
                    {
                        "name": "Sam Shaaban"
                    },
                    {
                        "name": "Oliver Lindheim"
                    },
                    {
                        "name": "Traci M. Kennedy"
                    },
                    {
                        "name": "Mayank Goel"
                    }
                ],
                "author_detail": {
                    "name": "Mayank Goel"
                },
                "author": "Mayank Goel",
                "arxiv_doi": "10.1145/3772318.3791871"
            },
            {
                "id": "http://arxiv.org/abs/2507.08017v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.08017v5",
                "title": "Mechanistic Indicators of Understanding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mechanistic Indicators of Understanding in Large Language Models"
                },
                "updated": "2026-02-25T18:34:16Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    34,
                    16,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.08017v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.08017v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) are often portrayed as merely imitating linguistic patterns without genuine understanding. We argue that recent findings in mechanistic interpretability (MI), the emerging field probing the inner workings of LLMs, render this picture increasingly untenable--but only once those findings are integrated within a theoretical account of understanding. We propose a tiered framework for thinking about understanding in LLMs and use it to synthesize the most relevant findings to date. The framework distinguishes three hierarchical varieties of understanding, each tied to a corresponding level of computational organization: conceptual understanding emerges when a model forms \"features\" as directions in latent space, learning connections between diverse manifestations of a single entity or property; state-of-the-world understanding emerges when a model learns contingent factual connections between features and dynamically tracks changes in the world; principled understanding emerges when a model ceases to rely on memorized facts and discovers a compact \"circuit\" connecting these facts. Across these tiers, MI uncovers internal organizations that can underwrite understanding-like unification. However, these also diverge from human cognition in their parallel exploitation of heterogeneous mechanisms. Fusing philosophical theory with mechanistic evidence thus allows us to transcend binary debates over whether AI understands, paving the way for a comparative, mechanistically grounded epistemology that explores how AI understanding aligns with--and diverges from--our own.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are often portrayed as merely imitating linguistic patterns without genuine understanding. We argue that recent findings in mechanistic interpretability (MI), the emerging field probing the inner workings of LLMs, render this picture increasingly untenable--but only once those findings are integrated within a theoretical account of understanding. We propose a tiered framework for thinking about understanding in LLMs and use it to synthesize the most relevant findings to date. The framework distinguishes three hierarchical varieties of understanding, each tied to a corresponding level of computational organization: conceptual understanding emerges when a model forms \"features\" as directions in latent space, learning connections between diverse manifestations of a single entity or property; state-of-the-world understanding emerges when a model learns contingent factual connections between features and dynamically tracks changes in the world; principled understanding emerges when a model ceases to rely on memorized facts and discovers a compact \"circuit\" connecting these facts. Across these tiers, MI uncovers internal organizations that can underwrite understanding-like unification. However, these also diverge from human cognition in their parallel exploitation of heterogeneous mechanisms. Fusing philosophical theory with mechanistic evidence thus allows us to transcend binary debates over whether AI understands, paving the way for a comparative, mechanistically grounded epistemology that explores how AI understanding aligns with--and diverges from--our own."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-07T20:26:31Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    20,
                    26,
                    31,
                    0,
                    188,
                    0
                ],
                "arxiv_comment": "38 pages",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Pierre Beckmann"
                    },
                    {
                        "name": "Matthieu Queloz"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Queloz"
                },
                "author": "Matthieu Queloz"
            },
            {
                "id": "http://arxiv.org/abs/2602.22186v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22186v1",
                "title": "Codesigning Ripplet: an LLM-Assisted Assessment Authoring System Grounded in a Conceptual Model of Teachers' Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Codesigning Ripplet: an LLM-Assisted Assessment Authoring System Grounded in a Conceptual Model of Teachers' Workflows"
                },
                "updated": "2026-02-25T18:30:44Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    30,
                    44,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22186v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3772318.3790418",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Assessments are critical in education, but creating them can be difficult. To address this challenge in a grounded way, we partnered with 13 teachers in a seven-month codesign process. We developed a conceptual model that characterizes the iterative dual process where teachers develop assessments while simultaneously refining requirements. To enact this model in practice, we built Ripplet, a web-based tool with multilevel reusable interactions to support assessment authoring. The extended codesign revealed that Ripplet enabled teachers to create formative assessments they would not have otherwise made, shifted their practices from generation to curation, and helped them reflect more on assessment quality. In a user study with 15 additional teachers, compared to their current practices, teachers felt the results were more worth their effort and that assessment quality improved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessments are critical in education, but creating them can be difficult. To address this challenge in a grounded way, we partnered with 13 teachers in a seven-month codesign process. We developed a conceptual model that characterizes the iterative dual process where teachers develop assessments while simultaneously refining requirements. To enact this model in practice, we built Ripplet, a web-based tool with multilevel reusable interactions to support assessment authoring. The extended codesign revealed that Ripplet enabled teachers to create formative assessments they would not have otherwise made, shifted their practices from generation to curation, and helped them reflect more on assessment quality. In a user study with 15 additional teachers, compared to their current practices, teachers felt the results were more worth their effort and that assessment quality improved."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T18:30:44Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    30,
                    44,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "Proceedings of the 2026 CHI Conference on Human Factors in Computing Systems",
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Yuan Cui"
                    },
                    {
                        "name": "Annabel Goldman"
                    },
                    {
                        "name": "Jovy Zhou"
                    },
                    {
                        "name": "Xiaolin Liu"
                    },
                    {
                        "name": "Clarissa Shieh"
                    },
                    {
                        "name": "Joshua Yao"
                    },
                    {
                        "name": "Mia Cheng"
                    },
                    {
                        "name": "Matthew Kay"
                    },
                    {
                        "name": "Fumeng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Fumeng Yang"
                },
                "author": "Fumeng Yang",
                "arxiv_doi": "10.1145/3772318.3790418"
            },
            {
                "id": "http://arxiv.org/abs/2505.23725v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.23725v2",
                "title": "MuLoCo: Muon is a practical inner optimizer for DiLoCo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MuLoCo: Muon is a practical inner optimizer for DiLoCo"
                },
                "updated": "2026-02-25T18:22:58Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    22,
                    58,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.23725v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.23725v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "DiLoCo is a powerful framework for training large language models (LLMs), enabling larger optimal batch sizes and increased accelerator utilization under networking constraints. However, DiLoCo's performance has been shown to degrade as the number of workers (K) increases (Charles et al., 2025). In this work, we posit that a related but often overlooked factor in DiLoCo's behavior is the choice of inner optimizer, which shapes the pseudogradient used by the outer optimizer. Given the recent success of Muon relative to AdamW for data parallel (DP) training, we examine how Muon's normalized optimizer steps can affect the pseudogradient's quality. We find that, relative to AdamW, Muon yields more directionally correct pseudogradients as the number of workers (K) increases. In our experiments pre-training language models, we conduct extensive hyperparameter tuning across 150M, 416M, 914M, 1.76B, and 3.1B models for DiLoCo, MuLoCo, AdamW DP, and Muon DP. Consistently across all scales, we find that with K>=1 workers, MuLoCo (Muon inner optimizer DiLoCo) achieves superior performance to DiLoCo in absolute terms and for K>2 it outperforms DiLoCo relative to their data parallel baselines, while being compatible with quantization, streaming, and long synchronization intervals. At K=1, we find that MuLoCo can even outperform the data-parallel gold standard while having larger critical batch sizes. Finally, we extrapolate optimal hyperparameters to 15B scale and train a model with each method (six in total) using K=1 and K=16 workers. We find that K=16 MuLoCo nearly matches single-worker performance at this scale, while MuLoCo K=1 matches the best performing baseline while using a much larger 16M token batch size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiLoCo is a powerful framework for training large language models (LLMs), enabling larger optimal batch sizes and increased accelerator utilization under networking constraints. However, DiLoCo's performance has been shown to degrade as the number of workers (K) increases (Charles et al., 2025). In this work, we posit that a related but often overlooked factor in DiLoCo's behavior is the choice of inner optimizer, which shapes the pseudogradient used by the outer optimizer. Given the recent success of Muon relative to AdamW for data parallel (DP) training, we examine how Muon's normalized optimizer steps can affect the pseudogradient's quality. We find that, relative to AdamW, Muon yields more directionally correct pseudogradients as the number of workers (K) increases. In our experiments pre-training language models, we conduct extensive hyperparameter tuning across 150M, 416M, 914M, 1.76B, and 3.1B models for DiLoCo, MuLoCo, AdamW DP, and Muon DP. Consistently across all scales, we find that with K>=1 workers, MuLoCo (Muon inner optimizer DiLoCo) achieves superior performance to DiLoCo in absolute terms and for K>2 it outperforms DiLoCo relative to their data parallel baselines, while being compatible with quantization, streaming, and long synchronization intervals. At K=1, we find that MuLoCo can even outperform the data-parallel gold standard while having larger critical batch sizes. Finally, we extrapolate optimal hyperparameters to 15B scale and train a model with each method (six in total) using K=1 and K=16 workers. We find that K=16 MuLoCo nearly matches single-worker performance at this scale, while MuLoCo K=1 matches the best performing baseline while using a much larger 16M token batch size."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-29T17:55:37Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    55,
                    37,
                    3,
                    149,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Benjamin Thrien"
                    },
                    {
                        "name": "Xiaolong Huang"
                    },
                    {
                        "name": "Aaron Defazio"
                    },
                    {
                        "name": "Irina Rish"
                    },
                    {
                        "name": "Eugene Belilovsky"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Belilovsky"
                },
                "author": "Eugene Belilovsky"
            },
            {
                "id": "http://arxiv.org/abs/2508.01617v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.01617v2",
                "title": "LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical Image Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical Image Understanding"
                },
                "updated": "2026-02-25T18:15:23Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    15,
                    23,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.01617v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.01617v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Autoregressive models (ARMs) have long dominated the landscape of biomedical vision-language models (VLMs). Recently, masked diffusion models such as LLaDA have emerged as promising alternatives, yet their application in the biomedical domain remains largely underexplored. To bridge this gap, we introduce LLaDA-MedV, the first large language diffusion model tailored for biomedical image understanding through vision instruction tuning. LLaDA-MedV achieves relative performance gains of 7.855% over LLaVA-Med and 1.867% over LLaDA-V in the open-ended biomedical visual conversation task, and sets new state-of-the-art accuracy on the closed-form subset of three VQA benchmarks: 84.93% on VQA-RAD, 92.31% on SLAKE, and 95.15% on PathVQA. Furthermore, a detailed comparison with LLaVA-Med suggests that LLaDA-MedV is capable of generating reasonably longer responses by explicitly controlling response length, which can lead to more informative outputs. We also conduct an in-depth analysis of both the training and inference stages, highlighting the critical roles of initialization weight selection, fine-tuning strategies, and the interplay between sampling steps and response repetition. The code and model weight is released at https://github.com/LLM-VLM-GSL/LLaDA-MedV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive models (ARMs) have long dominated the landscape of biomedical vision-language models (VLMs). Recently, masked diffusion models such as LLaDA have emerged as promising alternatives, yet their application in the biomedical domain remains largely underexplored. To bridge this gap, we introduce LLaDA-MedV, the first large language diffusion model tailored for biomedical image understanding through vision instruction tuning. LLaDA-MedV achieves relative performance gains of 7.855% over LLaVA-Med and 1.867% over LLaDA-V in the open-ended biomedical visual conversation task, and sets new state-of-the-art accuracy on the closed-form subset of three VQA benchmarks: 84.93% on VQA-RAD, 92.31% on SLAKE, and 95.15% on PathVQA. Furthermore, a detailed comparison with LLaVA-Med suggests that LLaDA-MedV is capable of generating reasonably longer responses by explicitly controlling response length, which can lead to more informative outputs. We also conduct an in-depth analysis of both the training and inference stages, highlighting the critical roles of initialization weight selection, fine-tuning strategies, and the interplay between sampling steps and response repetition. The code and model weight is released at https://github.com/LLM-VLM-GSL/LLaDA-MedV."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-03T06:46:46Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    6,
                    46,
                    46,
                    6,
                    215,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Xuanzhao Dong"
                    },
                    {
                        "name": "Wenhui Zhu"
                    },
                    {
                        "name": "Xiwen Chen"
                    },
                    {
                        "name": "Zhipeng Wang"
                    },
                    {
                        "name": "Peijie Qiu"
                    },
                    {
                        "name": "Shao Tang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Yalin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yalin Wang"
                },
                "author": "Yalin Wang"
            },
            {
                "id": "http://arxiv.org/abs/2602.20156v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20156v3",
                "title": "Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks"
                },
                "updated": "2026-02-25T18:14:01Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    14,
                    1,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20156v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20156v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature. Skills allow users to extend LLM applications with specialized third-party code, knowledge, and instructions. Although this can extend agent capabilities to new domains, it creates an increasingly complex agent supply chain, offering new surfaces for prompt injection attacks. We identify skill-based prompt injection as a significant threat and introduce SkillInject, a benchmark evaluating the susceptibility of widely-used LLM agents to injections through skill files. SkillInject contains 202 injection-task pairs with attacks ranging from obviously malicious injections to subtle, context-dependent attacks hidden in otherwise legitimate instructions. We evaluate frontier LLMs on SkillInject, measuring both security in terms of harmful instruction avoidance and utility in terms of legitimate instruction compliance. Our results show that today's agents are highly vulnerable with up to 80% attack success rate with frontier models, often executing extremely harmful instructions including data exfiltration, destructive action, and ransomware-like behavior. They furthermore suggest that this problem will not be solved through model scaling or simple input filtering, but that robust agent security will require context-aware authorization frameworks. Our benchmark is available at https://www.skill-inject.com/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature. Skills allow users to extend LLM applications with specialized third-party code, knowledge, and instructions. Although this can extend agent capabilities to new domains, it creates an increasingly complex agent supply chain, offering new surfaces for prompt injection attacks. We identify skill-based prompt injection as a significant threat and introduce SkillInject, a benchmark evaluating the susceptibility of widely-used LLM agents to injections through skill files. SkillInject contains 202 injection-task pairs with attacks ranging from obviously malicious injections to subtle, context-dependent attacks hidden in otherwise legitimate instructions. We evaluate frontier LLMs on SkillInject, measuring both security in terms of harmful instruction avoidance and utility in terms of legitimate instruction compliance. Our results show that today's agents are highly vulnerable with up to 80% attack success rate with frontier models, often executing extremely harmful instructions including data exfiltration, destructive action, and ransomware-like behavior. They furthermore suggest that this problem will not be solved through model scaling or simple input filtering, but that robust agent security will require context-aware authorization frameworks. Our benchmark is available at https://www.skill-inject.com/."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T18:59:27Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    18,
                    59,
                    27,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "David Schmotz"
                    },
                    {
                        "name": "Luca Beurer-Kellner"
                    },
                    {
                        "name": "Sahar Abdelnabi"
                    },
                    {
                        "name": "Maksym Andriushchenko"
                    }
                ],
                "author_detail": {
                    "name": "Maksym Andriushchenko"
                },
                "author": "Maksym Andriushchenko"
            },
            {
                "id": "http://arxiv.org/abs/2602.18182v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.18182v2",
                "title": "Capabilities Ain't All You Need: Measuring Propensities in AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capabilities Ain't All You Need: Measuring Propensities in AI"
                },
                "updated": "2026-02-25T18:12:06Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    12,
                    6,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.18182v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.18182v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "AI evaluation has primarily focused on measuring capabilities, with formal approaches inspired from Item Response Theory (IRT) being increasingly applied. Yet propensities - the tendencies of models to exhibit particular behaviours - play a central role in determining both performance and safety outcomes. However, traditional IRT describes a model's success on a task as a monotonic function of model capabilities and task demands, an approach unsuited to propensities, where both excess and deficiency can be problematic. Here, we introduce the first formal framework for measuring AI propensities by using a bilogistic formulation for model success, which attributes high success probability when the model's propensity is within an \"ideal band\". Further, we estimate the limits of the ideal band using LLMs equipped with newly developed task-agnostic rubrics. Applying our framework to six families of LLM models whose propensities are incited in either direction, we find that we can measure how much the propensity is shifted and what effect this has on the tasks. Critically, propensities estimated using one benchmark successfully predict behaviour on held-out tasks. Moreover, we obtain stronger predictive power when combining propensities and capabilities than either separately. More broadly, our framework showcases how rigorous propensity measurements can be conducted and how it yields gains over solely using capability evaluations to predict AI behaviour.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI evaluation has primarily focused on measuring capabilities, with formal approaches inspired from Item Response Theory (IRT) being increasingly applied. Yet propensities - the tendencies of models to exhibit particular behaviours - play a central role in determining both performance and safety outcomes. However, traditional IRT describes a model's success on a task as a monotonic function of model capabilities and task demands, an approach unsuited to propensities, where both excess and deficiency can be problematic. Here, we introduce the first formal framework for measuring AI propensities by using a bilogistic formulation for model success, which attributes high success probability when the model's propensity is within an \"ideal band\". Further, we estimate the limits of the ideal band using LLMs equipped with newly developed task-agnostic rubrics. Applying our framework to six families of LLM models whose propensities are incited in either direction, we find that we can measure how much the propensity is shifted and what effect this has on the tasks. Critically, propensities estimated using one benchmark successfully predict behaviour on held-out tasks. Moreover, we obtain stronger predictive power when combining propensities and capabilities than either separately. More broadly, our framework showcases how rigorous propensity measurements can be conducted and how it yields gains over solely using capability evaluations to predict AI behaviour."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-20T12:40:18Z",
                "published_parsed": [
                    2026,
                    2,
                    20,
                    12,
                    40,
                    18,
                    4,
                    51,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Daniel Romero-Alvarado"
                    },
                    {
                        "name": "Fernando Martnez-Plumed"
                    },
                    {
                        "name": "Lorenzo Pacchiardi"
                    },
                    {
                        "name": "Hugo Save"
                    },
                    {
                        "name": "Siddhesh Milind Pawar"
                    },
                    {
                        "name": "Behzad Mehrbakhsh"
                    },
                    {
                        "name": "Pablo Antonio Moreno Casares"
                    },
                    {
                        "name": "Ben Slater"
                    },
                    {
                        "name": "Paolo Bova"
                    },
                    {
                        "name": "Peter Romero"
                    },
                    {
                        "name": "Zachary R. Tyler"
                    },
                    {
                        "name": "Jonathan Prunty"
                    },
                    {
                        "name": "Luning Sun"
                    },
                    {
                        "name": "Jose Hernandez-Orallo"
                    }
                ],
                "author_detail": {
                    "name": "Jose Hernandez-Orallo"
                },
                "author": "Jose Hernandez-Orallo"
            },
            {
                "id": "http://arxiv.org/abs/2602.18671v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.18671v2",
                "title": "Spilled Energy in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spilled Energy in Large Language Models"
                },
                "updated": "2026-02-25T18:09:08Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    9,
                    8,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.18671v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.18671v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We reinterpret the final Large Language Model (LLM) softmax classifier as an Energy-Based Model (EBM), decomposing the sequence-to-sequence probability chain into multiple interacting EBMs at inference. This principled approach allows us to track \"energy spills\" during decoding, which we empirically show correlate with factual errors, biases, and failures. Similar to Orgad et al. (2025), our method localizes the exact answer token and subsequently tests for hallucinations. Crucially, however, we achieve this without requiring trained probe classifiers or activation ablations. Instead, we introduce two completely training-free metrics derived directly from output logits: spilled energy, which captures the discrepancy between energy values across consecutive generation steps that should theoretically match, and marginalized energy, which is measurable at a single step. Evaluated on nine benchmarks across state-of-the-art LLMs (including LLaMA, Mistral, and Gemma) and on synthetic algebraic operations (Qwen3), our approach demonstrates robust, competitive hallucination detection and cross-task generalization. Notably, these results hold for both pretrained and instruction-tuned variants without introducing any training overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We reinterpret the final Large Language Model (LLM) softmax classifier as an Energy-Based Model (EBM), decomposing the sequence-to-sequence probability chain into multiple interacting EBMs at inference. This principled approach allows us to track \"energy spills\" during decoding, which we empirically show correlate with factual errors, biases, and failures. Similar to Orgad et al. (2025), our method localizes the exact answer token and subsequently tests for hallucinations. Crucially, however, we achieve this without requiring trained probe classifiers or activation ablations. Instead, we introduce two completely training-free metrics derived directly from output logits: spilled energy, which captures the discrepancy between energy values across consecutive generation steps that should theoretically match, and marginalized energy, which is measurable at a single step. Evaluated on nine benchmarks across state-of-the-art LLMs (including LLaMA, Mistral, and Gemma) and on synthetic algebraic operations (Qwen3), our approach demonstrates robust, competitive hallucination detection and cross-task generalization. Notably, these results hold for both pretrained and instruction-tuned variants without introducing any training overhead."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-21T00:38:47Z",
                "published_parsed": [
                    2026,
                    2,
                    21,
                    0,
                    38,
                    47,
                    5,
                    52,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Adrian Robert Minut"
                    },
                    {
                        "name": "Hazem Dewidar"
                    },
                    {
                        "name": "Iacopo Masi"
                    }
                ],
                "author_detail": {
                    "name": "Iacopo Masi"
                },
                "author": "Iacopo Masi"
            },
            {
                "id": "http://arxiv.org/abs/2602.22158v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22158v1",
                "title": "LLMTailor: A Layer-wise Tailoring Tool for Efficient Checkpointing of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMTailor: A Layer-wise Tailoring Tool for Efficient Checkpointing of Large Language Models"
                },
                "updated": "2026-02-25T18:05:17Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    5,
                    17,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22158v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3731599.3767515",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Checkpointing is essential for fault tolerance in training large language models (LLMs). However, existing methods, regardless of their I/O strategies, periodically store the entire model and optimizer states, incurring substantial storage overhead and resource contention. Recent studies reveal that updates across LLM layers are highly non-uniform. Across training steps, some layers may undergo more significant changes, while others remain relatively stable or even unchanged. This suggests that selectively checkpointing only layers with significant updates could reduce overhead without harming training. Implementing such selective strategies requires fine-grained control over both weights and optimizer states, which no current tool provides. To address this gap, we propose \\texttt{LLMTailor}, a checkpoint-merging framework that filters and assembles layers from different checkpoints to form a composite checkpoint. Our evaluation indicates that LLMTailor can work with different selective checkpointing strategies and effectively reduce checkpoint size (e.g., 4.3 times smaller for Llama3.1-8B) and checkpoint time (e.g., 2.8 times faster for Qwen2.5-7B) while maintaining model quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Checkpointing is essential for fault tolerance in training large language models (LLMs). However, existing methods, regardless of their I/O strategies, periodically store the entire model and optimizer states, incurring substantial storage overhead and resource contention. Recent studies reveal that updates across LLM layers are highly non-uniform. Across training steps, some layers may undergo more significant changes, while others remain relatively stable or even unchanged. This suggests that selectively checkpointing only layers with significant updates could reduce overhead without harming training. Implementing such selective strategies requires fine-grained control over both weights and optimizer states, which no current tool provides. To address this gap, we propose \\texttt{LLMTailor}, a checkpoint-merging framework that filters and assembles layers from different checkpoints to form a composite checkpoint. Our evaluation indicates that LLMTailor can work with different selective checkpointing strategies and effectively reduce checkpoint size (e.g., 4.3 times smaller for Llama3.1-8B) and checkpoint time (e.g., 2.8 times faster for Qwen2.5-7B) while maintaining model quality."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T18:05:17Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    5,
                    17,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "9 pages, 3 figures, accepted at PDSW'25",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Minqiu Sun"
                    },
                    {
                        "name": "Xin Huang"
                    },
                    {
                        "name": "Luanzheng Guo"
                    },
                    {
                        "name": "Nathan R. Tallent"
                    },
                    {
                        "name": "Kento Sato"
                    },
                    {
                        "name": "Dong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Dong Dai"
                },
                "author": "Dong Dai",
                "arxiv_doi": "10.1145/3731599.3767515"
            },
            {
                "id": "http://arxiv.org/abs/2602.22157v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22157v1",
                "title": "Dynamic Personality Adaptation in Large Language Models via State Machines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Personality Adaptation in Large Language Models via State Machines"
                },
                "updated": "2026-02-25T18:05:11Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    5,
                    11,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22157v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The inability of Large Language Models (LLMs) to modulate their personality expression in response to evolving dialogue dynamics hinders their performance in complex, interactive contexts. We propose a model-agnostic framework for dynamic personality simulation that employs state machines to represent latent personality states, where transition probabilities are dynamically adapted to the conversational context. Part of our architecture is a modular pipeline for continuous personality scoring that evaluates dialogues along latent axes while remaining agnostic to the specific personality models, their dimensions, transition mechanisms, or LLMs used. These scores function as dynamic state variables that systematically reconfigure the system prompt, steering behavioral alignment throughout the interaction.We evaluate this framework by operationalizing the Interpersonal Circumplex (IPC) in a medical education setting. Results demonstrate that the system successfully adapts its personality state to user inputs, but also influences user behavior, thereby facilitating de-escalation training. Notably, the scoring pipeline maintains comparable precision even when utilizing lightweight, fine-tuned classifiers instead of large-scale LLMs. This work demonstrates the feasibility of modular, personality-adaptive architectures for education, customer support, and broader human-computer interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inability of Large Language Models (LLMs) to modulate their personality expression in response to evolving dialogue dynamics hinders their performance in complex, interactive contexts. We propose a model-agnostic framework for dynamic personality simulation that employs state machines to represent latent personality states, where transition probabilities are dynamically adapted to the conversational context. Part of our architecture is a modular pipeline for continuous personality scoring that evaluates dialogues along latent axes while remaining agnostic to the specific personality models, their dimensions, transition mechanisms, or LLMs used. These scores function as dynamic state variables that systematically reconfigure the system prompt, steering behavioral alignment throughout the interaction.We evaluate this framework by operationalizing the Interpersonal Circumplex (IPC) in a medical education setting. Results demonstrate that the system successfully adapts its personality state to user inputs, but also influences user behavior, thereby facilitating de-escalation training. Notably, the scoring pipeline maintains comparable precision even when utilizing lightweight, fine-tuned classifiers instead of large-scale LLMs. This work demonstrates the feasibility of modular, personality-adaptive architectures for education, customer support, and broader human-computer interaction."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T18:05:11Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    18,
                    5,
                    11,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "22 pages, 5 figures, submitted to ICPR 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Leon Pielage"
                    },
                    {
                        "name": "Ole Htscher"
                    },
                    {
                        "name": "Mitja Back"
                    },
                    {
                        "name": "Bernhard Marschall"
                    },
                    {
                        "name": "Benjamin Risse"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Risse"
                },
                "author": "Benjamin Risse"
            },
            {
                "id": "http://arxiv.org/abs/2602.17849v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.17849v2",
                "title": "Quad Length Codes for Lossless Compression of e4m3",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quad Length Codes for Lossless Compression of e4m3"
                },
                "updated": "2026-02-25T17:58:32Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    58,
                    32,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.17849v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.17849v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Training and serving Large Language Models (LLMs) relies heavily on parallelization and collective operations, which are frequently bottlenecked by network bandwidth. Lossless compression using e.g., Huffman codes can alleviate the issue, however, Huffman codes suffer from slow, bit-sequential decoding and high hardware complexity due to deep tree traversals. Universal codes e.g., Exponential-Golomb codes are faster to decode but do not exploit the symbol frequency distributions. To address these limitations, this paper introduces Quad Length Codes, a hybrid approach designed to balance compression efficiency with decoding speed. The coding scheme uses 3 prefix bits to divide the 256 symbols into 8 areas. Each area has a different code length and encodes a different number of symbols. The scheme uses a Look Up Table with 256 entries, significantly simplifying the hardware implementation compared to Huffman trees. The coding scheme can be adapted for different distributions. For the e4m3 data type, the scheme achieves a compressibility of 13.9% in comparison to 15.9% achieved by Huffman codes, but it significantly speeds up the decoding and simplifies the hardware complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and serving Large Language Models (LLMs) relies heavily on parallelization and collective operations, which are frequently bottlenecked by network bandwidth. Lossless compression using e.g., Huffman codes can alleviate the issue, however, Huffman codes suffer from slow, bit-sequential decoding and high hardware complexity due to deep tree traversals. Universal codes e.g., Exponential-Golomb codes are faster to decode but do not exploit the symbol frequency distributions. To address these limitations, this paper introduces Quad Length Codes, a hybrid approach designed to balance compression efficiency with decoding speed. The coding scheme uses 3 prefix bits to divide the 256 symbols into 8 areas. Each area has a different code length and encodes a different number of symbols. The scheme uses a Look Up Table with 256 entries, significantly simplifying the hardware implementation compared to Huffman trees. The coding scheme can be adapted for different distributions. For the e4m3 data type, the scheme achieves a compressibility of 13.9% in comparison to 15.9% achieved by Huffman codes, but it significantly speeds up the decoding and simplifies the hardware complexity."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-19T21:31:33Z",
                "published_parsed": [
                    2026,
                    2,
                    19,
                    21,
                    31,
                    33,
                    3,
                    50,
                    0
                ],
                "arxiv_comment": "The first version proposed lossless compression of BFloat16 using dual length codes. This version proposes lossless compression of e4m3 using quad length codes. The versions will be merged later",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Aditya Agrawal"
                    },
                    {
                        "name": "Albert Magyar"
                    },
                    {
                        "name": "Hiteshwar Eswaraiah"
                    },
                    {
                        "name": "Patrick Sheridan"
                    },
                    {
                        "name": "Pradeep Janedula"
                    },
                    {
                        "name": "Ravi Krishnan Venkatesan"
                    },
                    {
                        "name": "Krishna Nair"
                    },
                    {
                        "name": "Ravi Iyer"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Iyer"
                },
                "author": "Ravi Iyer"
            },
            {
                "id": "http://arxiv.org/abs/2602.22146v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22146v1",
                "title": "Provable Last-Iterate Convergence for Multi-Objective Safe LLM Alignment via Optimistic Primal-Dual",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provable Last-Iterate Convergence for Multi-Objective Safe LLM Alignment via Optimistic Primal-Dual"
                },
                "updated": "2026-02-25T17:54:52Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    54,
                    52,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22146v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reinforcement Learning from Human Feedback (RLHF) plays a significant role in aligning Large Language Models (LLMs) with human preferences. While RLHF with expected reward constraints can be formulated as a primal-dual optimization problem, standard primal-dual methods only guarantee convergence with a distributional policy where the saddle-point problem is in convex-concave form. Moreover, standard primal-dual methods may exhibit instability or divergence in the last iterate under policy parameterization in practical applications. In this work, we propose a universal primal-dual framework for safe RLHF that unifies a broad class of existing alignment algorithms, including safe-RLHF, one-shot, and multi-shot based methods. Building on this framework, we introduce an optimistic primal-dual (OPD) algorithm that incorporates predictive updates for both primal and dual variables to stabilize saddle-point dynamics. We establish last-iterate convergence guarantees for the proposed method, covering both exact policy optimization in the distributional space and convergence to a neighborhood of the optimal solution whose gap is related to approximation error and bias under parameterized policies. Our analysis reveals that optimism plays a crucial role in mitigating oscillations inherent to constrained alignment objectives, thereby closing a key theoretical gap between constrained RL and practical RLHF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) plays a significant role in aligning Large Language Models (LLMs) with human preferences. While RLHF with expected reward constraints can be formulated as a primal-dual optimization problem, standard primal-dual methods only guarantee convergence with a distributional policy where the saddle-point problem is in convex-concave form. Moreover, standard primal-dual methods may exhibit instability or divergence in the last iterate under policy parameterization in practical applications. In this work, we propose a universal primal-dual framework for safe RLHF that unifies a broad class of existing alignment algorithms, including safe-RLHF, one-shot, and multi-shot based methods. Building on this framework, we introduce an optimistic primal-dual (OPD) algorithm that incorporates predictive updates for both primal and dual variables to stabilize saddle-point dynamics. We establish last-iterate convergence guarantees for the proposed method, covering both exact policy optimization in the distributional space and convergence to a neighborhood of the optimal solution whose gap is related to approximation error and bias under parameterized policies. Our analysis reveals that optimism plays a crucial role in mitigating oscillations inherent to constrained alignment objectives, thereby closing a key theoretical gap between constrained RL and practical RLHF."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T17:54:52Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    54,
                    52,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yining Li"
                    },
                    {
                        "name": "Peizhong Ju"
                    },
                    {
                        "name": "Ness Shroff"
                    }
                ],
                "author_detail": {
                    "name": "Ness Shroff"
                },
                "author": "Ness Shroff"
            },
            {
                "id": "http://arxiv.org/abs/2602.22145v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22145v1",
                "title": "When AI Writes, Whose Voice Remains? Quantifying Cultural Marker Erasure Across World English Varieties in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When AI Writes, Whose Voice Remains? Quantifying Cultural Marker Erasure Across World English Varieties in Large Language Models"
                },
                "updated": "2026-02-25T17:54:42Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    54,
                    42,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22145v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3772363.3799085",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Large Language Models (LLMs) are increasingly used to ``professionalize'' workplace communication, often at the cost of linguistic identity. We introduce \"Cultural Ghosting\", the systematic erasure of linguistic markers unique to non-native English varieties during text processing. Through analysis of 22,350 LLM outputs generated from 1,490 culturally marked texts (Indian, Singaporean,& Nigerian English) processed by five models under three prompt conditions, we quantify this phenomenon using two novel metrics: Identity Erasure Rate (IER) & Semantic Preservation Score (SPS). Across all prompts, we find an overall IER of 10.26%, with model-level variation from 3.5% to 20.5% (5.9x range). Crucially, we identify a Semantic Preservation Paradox: models maintain high semantic similarity (mean SPS = 0.748) while systematically erasing cultural markers. Pragmatic markers (politeness conventions) are 1.9x more vulnerable than lexical markers (71.5% vs. 37.1% erasure). Our experiments demonstrate that explicit cultural-preservation prompts reduce erasure by 29% without sacrificing semantic quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used to ``professionalize'' workplace communication, often at the cost of linguistic identity. We introduce \"Cultural Ghosting\", the systematic erasure of linguistic markers unique to non-native English varieties during text processing. Through analysis of 22,350 LLM outputs generated from 1,490 culturally marked texts (Indian, Singaporean,& Nigerian English) processed by five models under three prompt conditions, we quantify this phenomenon using two novel metrics: Identity Erasure Rate (IER) & Semantic Preservation Score (SPS). Across all prompts, we find an overall IER of 10.26%, with model-level variation from 3.5% to 20.5% (5.9x range). Crucially, we identify a Semantic Preservation Paradox: models maintain high semantic similarity (mean SPS = 0.748) while systematically erasing cultural markers. Pragmatic markers (politeness conventions) are 1.9x more vulnerable than lexical markers (71.5% vs. 37.1% erasure). Our experiments demonstrate that explicit cultural-preservation prompts reduce erasure by 29% without sacrificing semantic quality."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T17:54:42Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    54,
                    42,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Satyam Kumar Navneet"
                    },
                    {
                        "name": "Joydeep Chandra"
                    },
                    {
                        "name": "Yong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yong Zhang"
                },
                "author": "Yong Zhang",
                "arxiv_doi": "10.1145/3772363.3799085"
            },
            {
                "id": "http://arxiv.org/abs/2510.26656v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.26656v3",
                "title": "Heuristic Adaptation of Potentially Misspecified Domain Support for Likelihood-Free Inference in Stochastic Dynamical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heuristic Adaptation of Potentially Misspecified Domain Support for Likelihood-Free Inference in Stochastic Dynamical Systems"
                },
                "updated": "2026-02-25T17:52:16Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    52,
                    16,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.26656v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.26656v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In robotics, likelihood-free inference (LFI) can provide the domain distribution that adapts a learnt agent in a parametric set of deployment conditions. LFI assumes an arbitrary support for sampling, which remains constant as the initial generic prior is iteratively refined to more descriptive posteriors. However, a potentially misspecified support can lead to suboptimal, yet falsely certain, posteriors. To address this issue, we propose three heuristic LFI variants: EDGE, MODE, and CENTRE. Each interprets the posterior mode shift over inference steps in its own way and, when integrated into an LFI step, adapts the support alongside posterior inference. We first expose the support misspecification issue and evaluate our heuristics using stochastic dynamical benchmarks. We then evaluate the impact of heuristic support adaptation on parameter inference and policy learning for a dynamic deformable linear object (DLO) manipulation task. Inference results in a finer length and stiffness classification for a parametric set of DLOs. When the resulting posteriors are used as domain distributions for sim-based policy learning, they lead to more robust object-centric agent performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In robotics, likelihood-free inference (LFI) can provide the domain distribution that adapts a learnt agent in a parametric set of deployment conditions. LFI assumes an arbitrary support for sampling, which remains constant as the initial generic prior is iteratively refined to more descriptive posteriors. However, a potentially misspecified support can lead to suboptimal, yet falsely certain, posteriors. To address this issue, we propose three heuristic LFI variants: EDGE, MODE, and CENTRE. Each interprets the posterior mode shift over inference steps in its own way and, when integrated into an LFI step, adapts the support alongside posterior inference. We first expose the support misspecification issue and evaluate our heuristics using stochastic dynamical benchmarks. We then evaluate the impact of heuristic support adaptation on parameter inference and policy learning for a dynamic deformable linear object (DLO) manipulation task. Inference results in a finer length and stiffness classification for a parametric set of DLOs. When the resulting posteriors are used as domain distributions for sim-based policy learning, they lead to more robust object-centric agent performance."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-30T16:23:46Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    23,
                    46,
                    3,
                    303,
                    0
                ],
                "arxiv_comment": "20 pages, 18 figures",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Georgios Kamaras"
                    },
                    {
                        "name": "Craig Innes"
                    },
                    {
                        "name": "Subramanian Ramamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Subramanian Ramamoorthy"
                },
                "author": "Subramanian Ramamoorthy"
            },
            {
                "id": "http://arxiv.org/abs/2602.22143v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22143v1",
                "title": "MedTri: A Platform for Structured Medical Report Normalization to Enhance Vision-Language Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedTri: A Platform for Structured Medical Report Normalization to Enhance Vision-Language Pretraining"
                },
                "updated": "2026-02-25T17:49:03Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    49,
                    3,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22143v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Medical vision-language pretraining increasingly relies on medical reports as large-scale supervisory signals; however, raw reports often exhibit substantial stylistic heterogeneity, variable length, and a considerable amount of image-irrelevant content. Although text normalization is frequently adopted as a preprocessing step in prior work, its design principles and empirical impact on vision-language pretraining remain insufficiently and systematically examined. In this study, we present MedTri, a deployable normalization framework for medical vision-language pretraining that converts free-text reports into a unified [Anatomical Entity: Radiologic Description + Diagnosis Category] triplet. This structured, anatomy-grounded normalization preserves essential morphological and spatial information while removing stylistic noise and image-irrelevant content, providing consistent and image-grounded textual supervision at scale. Across multiple datasets spanning both X-ray and computed tomography (CT) modalities, we demonstrate that structured, anatomy-grounded text normalization is an important factor in medical vision-language pretraining quality, yielding consistent improvements over raw reports and existing normalization baselines. In addition, we illustrate how this normalization can easily support modular text-level augmentation strategies, including knowledge enrichment and anatomy-grounded counterfactual supervision, which provide complementary gains in robustness and generalization without altering the core normalization process. Together, our results position structured text normalization as a critical and generalizable preprocessing component for medical vision-language learning, while MedTri provides this normalization platform. Code and data will be released at https://github.com/Arturia-Pendragon-Iris/MedTri.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical vision-language pretraining increasingly relies on medical reports as large-scale supervisory signals; however, raw reports often exhibit substantial stylistic heterogeneity, variable length, and a considerable amount of image-irrelevant content. Although text normalization is frequently adopted as a preprocessing step in prior work, its design principles and empirical impact on vision-language pretraining remain insufficiently and systematically examined. In this study, we present MedTri, a deployable normalization framework for medical vision-language pretraining that converts free-text reports into a unified [Anatomical Entity: Radiologic Description + Diagnosis Category] triplet. This structured, anatomy-grounded normalization preserves essential morphological and spatial information while removing stylistic noise and image-irrelevant content, providing consistent and image-grounded textual supervision at scale. Across multiple datasets spanning both X-ray and computed tomography (CT) modalities, we demonstrate that structured, anatomy-grounded text normalization is an important factor in medical vision-language pretraining quality, yielding consistent improvements over raw reports and existing normalization baselines. In addition, we illustrate how this normalization can easily support modular text-level augmentation strategies, including knowledge enrichment and anatomy-grounded counterfactual supervision, which provide complementary gains in robustness and generalization without altering the core normalization process. Together, our results position structured text normalization as a critical and generalizable preprocessing component for medical vision-language learning, while MedTri provides this normalization platform. Code and data will be released at https://github.com/Arturia-Pendragon-Iris/MedTri."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T17:49:03Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    49,
                    3,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yuetan Chu"
                    },
                    {
                        "name": "Xinhua Ma"
                    },
                    {
                        "name": "Xinran Jin"
                    },
                    {
                        "name": "Gongning Luo"
                    },
                    {
                        "name": "Xin Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xin Gao"
                },
                "author": "Xin Gao"
            },
            {
                "id": "http://arxiv.org/abs/2602.22142v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22142v1",
                "title": "WeaveTime: Stream from Earlier Frames into Emergent Memory in VideoLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeaveTime: Stream from Earlier Frames into Emergent Memory in VideoLLMs"
                },
                "updated": "2026-02-25T17:45:45Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    45,
                    45,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22142v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in Multimodal Large Language Models have greatly improved visual understanding and reasoning, yet their quadratic attention and offline training protocols make them ill-suited for streaming settings where frames arrive sequentially and future observations are inaccessible. We diagnose a core limitation of current Video-LLMs, namely Time-Agnosticism, in which videos are treated as an unordered bag of evidence rather than a causally ordered sequence, yielding two failures in streams: temporal order ambiguity, in which the model cannot follow or reason over the correct chronological order, and past-current focus blindness where it fails to distinguish present observations from accumulated history. We present WeaveTime, a simple, efficient, and model agnostic framework that first teaches order and then uses order. We introduce a lightweight Temporal Reconstruction objective-our Streaming Order Perception enhancement-that instills order aware representations with minimal finetuning and no specialized streaming data. At inference, a Past-Current Dynamic Focus Cache performs uncertainty triggered, coarse-to-fine retrieval, expanding history only when needed. Plugged into exsiting Video-LLM without architectural changes, WeaveTime delivers consistent gains on representative streaming benchmarks, improving accuracy while reducing latency. These results establish WeaveTime as a practical path toward time aware stream Video-LLMs under strict online, time causal constraints. Code and weights will be made publicly available. Project Page: https://zhangyl4.github.io/publications/weavetime/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Multimodal Large Language Models have greatly improved visual understanding and reasoning, yet their quadratic attention and offline training protocols make them ill-suited for streaming settings where frames arrive sequentially and future observations are inaccessible. We diagnose a core limitation of current Video-LLMs, namely Time-Agnosticism, in which videos are treated as an unordered bag of evidence rather than a causally ordered sequence, yielding two failures in streams: temporal order ambiguity, in which the model cannot follow or reason over the correct chronological order, and past-current focus blindness where it fails to distinguish present observations from accumulated history. We present WeaveTime, a simple, efficient, and model agnostic framework that first teaches order and then uses order. We introduce a lightweight Temporal Reconstruction objective-our Streaming Order Perception enhancement-that instills order aware representations with minimal finetuning and no specialized streaming data. At inference, a Past-Current Dynamic Focus Cache performs uncertainty triggered, coarse-to-fine retrieval, expanding history only when needed. Plugged into exsiting Video-LLM without architectural changes, WeaveTime delivers consistent gains on representative streaming benchmarks, improving accuracy while reducing latency. These results establish WeaveTime as a practical path toward time aware stream Video-LLMs under strict online, time causal constraints. Code and weights will be made publicly available. Project Page: https://zhangyl4.github.io/publications/weavetime/"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T17:45:45Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    45,
                    45,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "Accepted at CVPR 2026 (preview; camera-ready in preparation)",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yulin Zhang"
                    },
                    {
                        "name": "Cheng Shi"
                    },
                    {
                        "name": "Sibei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Sibei Yang"
                },
                "author": "Sibei Yang"
            },
            {
                "id": "http://arxiv.org/abs/2602.20440v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20440v2",
                "title": "Intelligence Without Integrity: Why Capable LLMs May Undermine Reliability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligence Without Integrity: Why Capable LLMs May Undermine Reliability"
                },
                "updated": "2026-02-25T17:41:10Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    41,
                    10,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20440v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As LLMs become embedded in research workflows and organizational decision processes, their effect on analytical reliability remains uncertain. We distinguish two dimensions of analytical reliability -- intelligence (the capacity to reach correct conclusions) and integrity (the stability of conclusions when analytically irrelevant cues about desired outcomes are introduced) -- and ask whether frontier LLMs possess both. Whether these dimensions trade off is theoretically ambiguous: the sophistication enabling accurate analysis may also enable responsiveness to non-evidential cues, or alternatively, greater capability may confer protection through better calibration and discernment. Using synthetically generated data with embedded ground truth, we evaluate fourteen models on a task simulating empirical analysis of hospital merger effects. We find that intelligence and integrity trade off: frontier models most likely to reach correct conclusions under neutral conditions are often most susceptible to shifting conclusions under motivated framing. We extend work on sycophancy by introducing goal-conditioned analytical sycophancy: sensitivity of inference to cues about desired outcomes, even when no belief is asserted and evidence is held constant. Unlike simple prompt sensitivity, models shift conclusions away from objective evidence in response to analytically irrelevant framing. This finding has important implications for empirical research and organizations. Selecting tools based on capability benchmarks may inadvertently select against the stability needed for reliable and replicable analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs become embedded in research workflows and organizational decision processes, their effect on analytical reliability remains uncertain. We distinguish two dimensions of analytical reliability -- intelligence (the capacity to reach correct conclusions) and integrity (the stability of conclusions when analytically irrelevant cues about desired outcomes are introduced) -- and ask whether frontier LLMs possess both. Whether these dimensions trade off is theoretically ambiguous: the sophistication enabling accurate analysis may also enable responsiveness to non-evidential cues, or alternatively, greater capability may confer protection through better calibration and discernment. Using synthetically generated data with embedded ground truth, we evaluate fourteen models on a task simulating empirical analysis of hospital merger effects. We find that intelligence and integrity trade off: frontier models most likely to reach correct conclusions under neutral conditions are often most susceptible to shifting conclusions under motivated framing. We extend work on sycophancy by introducing goal-conditioned analytical sycophancy: sensitivity of inference to cues about desired outcomes, even when no belief is asserted and evidence is held constant. Unlike simple prompt sensitivity, models shift conclusions away from objective evidence in response to analytically irrelevant framing. This finding has important implications for empirical research and organizations. Selecting tools based on capability benchmarks may inadvertently select against the stability needed for reliable and replicable analysis."
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-24T00:50:21Z",
                "published_parsed": [
                    2026,
                    2,
                    24,
                    0,
                    50,
                    21,
                    1,
                    55,
                    0
                ],
                "arxiv_comment": "45 pages, 9 figures",
                "arxiv_primary_category": {
                    "term": "econ.GN"
                },
                "authors": [
                    {
                        "name": "Ryan Allen"
                    },
                    {
                        "name": "Aticus Peterson"
                    }
                ],
                "author_detail": {
                    "name": "Aticus Peterson"
                },
                "author": "Aticus Peterson"
            },
            {
                "id": "http://arxiv.org/abs/2602.20946v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20946v2",
                "title": "Some Simple Economics of AGI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Some Simple Economics of AGI"
                },
                "updated": "2026-02-25T17:41:07Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    41,
                    7,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20946v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20946v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "For millennia, human cognition was the primary engine of progress on Earth. As AI decouples cognition from biology, the marginal cost of measurable execution falls to zero, absorbing any labor capturable by metrics--including creative, analytical, and innovative work. The binding constraint on growth is no longer intelligence but human verification bandwidth: the capacity to validate, audit, and underwrite responsibility when execution is abundant. We model the AGI transition as the collision of two racing cost curves: an exponentially decaying Cost to Automate and a biologically bottlenecked Cost to Verify. This structural asymmetry widens a Measurability Gap between what agents can execute and what humans can afford to verify. It also drives a shift from skill-biased to measurability-biased technical change. Rents migrate to verification-grade ground truth, cryptographic provenance, and liability underwriting--the ability to insure outcomes rather than merely generate them. The current human-in-the-loop equilibrium is unstable: eroded from below as apprenticeship collapses (Missing Junior Loop) and from within as experts codify their obsolescence (Codifier's Curse). Unverified deployment becomes privately rational--a Trojan Horse externality. Unmanaged, these forces pull toward a Hollow Economy. Yet by scaling verification alongside agentic capabilities, the forces that threaten collapse become the catalyst for unbounded discovery and experimentation--an Augmented Economy. We derive a practical playbook for individuals, companies, investors, and policymakers. Today's defining challenge is not the race to deploy the most autonomous systems; it is the race to secure the foundations of their oversight. Only by scaling our bandwidth for verification alongside our capacity for execution can we ensure that the intelligence we have summoned preserves the humanity that initiated it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For millennia, human cognition was the primary engine of progress on Earth. As AI decouples cognition from biology, the marginal cost of measurable execution falls to zero, absorbing any labor capturable by metrics--including creative, analytical, and innovative work. The binding constraint on growth is no longer intelligence but human verification bandwidth: the capacity to validate, audit, and underwrite responsibility when execution is abundant. We model the AGI transition as the collision of two racing cost curves: an exponentially decaying Cost to Automate and a biologically bottlenecked Cost to Verify. This structural asymmetry widens a Measurability Gap between what agents can execute and what humans can afford to verify. It also drives a shift from skill-biased to measurability-biased technical change. Rents migrate to verification-grade ground truth, cryptographic provenance, and liability underwriting--the ability to insure outcomes rather than merely generate them. The current human-in-the-loop equilibrium is unstable: eroded from below as apprenticeship collapses (Missing Junior Loop) and from within as experts codify their obsolescence (Codifier's Curse). Unverified deployment becomes privately rational--a Trojan Horse externality. Unmanaged, these forces pull toward a Hollow Economy. Yet by scaling verification alongside agentic capabilities, the forces that threaten collapse become the catalyst for unbounded discovery and experimentation--an Augmented Economy. We derive a practical playbook for individuals, companies, investors, and policymakers. Today's defining challenge is not the race to deploy the most autonomous systems; it is the race to secure the foundations of their oversight. Only by scaling our bandwidth for verification alongside our capacity for execution can we ensure that the intelligence we have summoned preserves the humanity that initiated it."
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-24T14:29:45Z",
                "published_parsed": [
                    2026,
                    2,
                    24,
                    14,
                    29,
                    45,
                    1,
                    55,
                    0
                ],
                "arxiv_comment": "JEL Classification: D82, D83, J23, J24, L23, O33. 112 pages, 3 figures",
                "arxiv_primary_category": {
                    "term": "econ.GN"
                },
                "authors": [
                    {
                        "name": "Christian Catalini"
                    },
                    {
                        "name": "Xiang Hui"
                    },
                    {
                        "name": "Jane Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jane Wu"
                },
                "author": "Jane Wu"
            },
            {
                "id": "http://arxiv.org/abs/2602.22136v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22136v1",
                "title": "SigmaQuant: Hardware-Aware Heterogeneous Quantization Method for Edge DNN Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SigmaQuant: Hardware-Aware Heterogeneous Quantization Method for Edge DNN Inference"
                },
                "updated": "2026-02-25T17:34:14Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    34,
                    14,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22136v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deep neural networks (DNNs) are essential for performing advanced tasks on edge or mobile devices, yet their deployment is often hindered by severe resource constraints, including limited memory, energy, and computational power. While uniform quantization provides a straightforward approach to compress model and reduce hardware requirement, it fails to fully leverage the varying robustness across layers, and often lead to accuracy degradation or suboptimal resource usage, particularly at low bitwidths. In contrast, heterogeneous quantization, which allocates different bitwidths to individual layers, can mitigate these drawbacks. Nonetheless, current heterogeneous quantization methods either needs huge brute-force design space search or lacks the adaptability to meet different hardware conditions, such as memory size, energy budget, and latency requirement. Filling these gaps, this work introduces \\textbf{\\textit{SigmaQuant}}, an adaptive layer-wise heterogeneous quantization framework designed to efficiently balance accuracy and resource usage for varied edge environments without exhaustive search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks (DNNs) are essential for performing advanced tasks on edge or mobile devices, yet their deployment is often hindered by severe resource constraints, including limited memory, energy, and computational power. While uniform quantization provides a straightforward approach to compress model and reduce hardware requirement, it fails to fully leverage the varying robustness across layers, and often lead to accuracy degradation or suboptimal resource usage, particularly at low bitwidths. In contrast, heterogeneous quantization, which allocates different bitwidths to individual layers, can mitigate these drawbacks. Nonetheless, current heterogeneous quantization methods either needs huge brute-force design space search or lacks the adaptability to meet different hardware conditions, such as memory size, energy budget, and latency requirement. Filling these gaps, this work introduces \\textbf{\\textit{SigmaQuant}}, an adaptive layer-wise heterogeneous quantization framework designed to efficiently balance accuracy and resource usage for varied edge environments without exhaustive search."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T17:34:14Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    34,
                    14,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Qunyou Liu"
                    },
                    {
                        "name": "Pengbo Yu"
                    },
                    {
                        "name": "Marina Zapater"
                    },
                    {
                        "name": "David Atienza"
                    }
                ],
                "author_detail": {
                    "name": "David Atienza"
                },
                "author": "David Atienza"
            },
            {
                "id": "http://arxiv.org/abs/2602.22134v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22134v1",
                "title": "Secure Semantic Communications via AI Defenses: Fundamentals, Solutions, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure Semantic Communications via AI Defenses: Fundamentals, Solutions, and Future Directions"
                },
                "updated": "2026-02-25T17:28:07Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    28,
                    7,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22134v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Semantic communication (SemCom) redefines wireless communication from reproducing symbols to transmitting task-relevant semantics. However, this AI-native architecture also introduces new vulnerabilities, as semantic failures may arise from adversarial perturbations to models, corrupted training data, desynchronized priors, or misaligned inference even when lower-layer transmission reliability and cryptographic protection remain intact. This survey provides a defense-centered and system-oriented synthesis of security in SemCom via AI defense. We analyze AI-centric threat models by consolidating existing studies and organizing attack surfaces across model-level, channel-realizable, knowledge-based, and networked inference vectors. Building on this foundation, we present a structured taxonomy of defense strategies organized by where semantic integrity can be compromised in SemCom systems despite correct symbol delivery, spanning semantic encoding, wireless transmission, knowledge integrity, and coordination among multiple agents. These categories correspond to distinct security failure modes, including representation fragility, channel-realizable manipulation, semantic prior poisoning or desynchronization, and adversarial propagation through distributed inference. We also examine security utility operating envelopes that capture tradeoffs among semantic fidelity, robustness, latency, and energy under realistic constraints, survey evaluation frameworks and representative applications, and identify open challenges in cross-layer composition and deployment-time certification. Overall, this survey offers a unified system-level perspective that enables readers to understand major threat and defense mechanisms in AI-native SemCom systems and to leverage emerging security techniques in the design and deployment of robust SemCom architectures for next-generation intelligent networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic communication (SemCom) redefines wireless communication from reproducing symbols to transmitting task-relevant semantics. However, this AI-native architecture also introduces new vulnerabilities, as semantic failures may arise from adversarial perturbations to models, corrupted training data, desynchronized priors, or misaligned inference even when lower-layer transmission reliability and cryptographic protection remain intact. This survey provides a defense-centered and system-oriented synthesis of security in SemCom via AI defense. We analyze AI-centric threat models by consolidating existing studies and organizing attack surfaces across model-level, channel-realizable, knowledge-based, and networked inference vectors. Building on this foundation, we present a structured taxonomy of defense strategies organized by where semantic integrity can be compromised in SemCom systems despite correct symbol delivery, spanning semantic encoding, wireless transmission, knowledge integrity, and coordination among multiple agents. These categories correspond to distinct security failure modes, including representation fragility, channel-realizable manipulation, semantic prior poisoning or desynchronization, and adversarial propagation through distributed inference. We also examine security utility operating envelopes that capture tradeoffs among semantic fidelity, robustness, latency, and energy under realistic constraints, survey evaluation frameworks and representative applications, and identify open challenges in cross-layer composition and deployment-time certification. Overall, this survey offers a unified system-level perspective that enables readers to understand major threat and defense mechanisms in AI-native SemCom systems and to leverage emerging security techniques in the design and deployment of robust SemCom architectures for next-generation intelligent networks."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T17:28:07Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    28,
                    7,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Lan Zhang"
                    },
                    {
                        "name": "Chengsi Liang"
                    },
                    {
                        "name": "Zeming Zhuang"
                    },
                    {
                        "name": "Yao Sun"
                    },
                    {
                        "name": "Fang Fang"
                    },
                    {
                        "name": "Xiaoyong Yuan"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato"
            },
            {
                "id": "http://arxiv.org/abs/2503.03178v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.03178v4",
                "title": "Active operator learning with predictive uncertainty quantification for partial differential equations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active operator learning with predictive uncertainty quantification for partial differential equations"
                },
                "updated": "2026-02-25T17:27:35Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    27,
                    35,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.03178v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.03178v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the increased prevalence of neural operators being used to provide rapid solutions to partial differential equations (PDEs), understanding the accuracy of model predictions and the associated error levels is necessary for deploying reliable surrogate models in scientific applications. Existing uncertainty quantification (UQ) frameworks employ ensembles or Bayesian methods, which can incur substantial computational costs during both training and inference. We propose a lightweight predictive UQ method tailored for Deep operator networks (DeepONets) that also generalizes to other operator networks. Numerical experiments on linear and nonlinear PDEs demonstrate that the framework's uncertainty estimates are unbiased and provide accurate out-of-distribution uncertainty predictions with a sufficiently large training dataset. Our framework provides fast inference and uncertainty estimates that can efficiently drive outer-loop analyses that would be prohibitively expensive with conventional solvers. We demonstrate how predictive uncertainties can be used in the context of Bayesian optimization and active learning problems to yield improvements in accuracy and data-efficiency for outer-loop optimization procedures. In the active learning setup, we extend the framework to Fourier Neural Operators (FNO) and describe a generalized method for other operator networks. To enable real-time deployment, we introduce an inference strategy based on precomputed trunk outputs and a sparse placement matrix, reducing evaluation time by more than a factor of five. Our method provides a practical route to uncertainty-aware operator learning in time-sensitive settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increased prevalence of neural operators being used to provide rapid solutions to partial differential equations (PDEs), understanding the accuracy of model predictions and the associated error levels is necessary for deploying reliable surrogate models in scientific applications. Existing uncertainty quantification (UQ) frameworks employ ensembles or Bayesian methods, which can incur substantial computational costs during both training and inference. We propose a lightweight predictive UQ method tailored for Deep operator networks (DeepONets) that also generalizes to other operator networks. Numerical experiments on linear and nonlinear PDEs demonstrate that the framework's uncertainty estimates are unbiased and provide accurate out-of-distribution uncertainty predictions with a sufficiently large training dataset. Our framework provides fast inference and uncertainty estimates that can efficiently drive outer-loop analyses that would be prohibitively expensive with conventional solvers. We demonstrate how predictive uncertainties can be used in the context of Bayesian optimization and active learning problems to yield improvements in accuracy and data-efficiency for outer-loop optimization procedures. In the active learning setup, we extend the framework to Fourier Neural Operators (FNO) and describe a generalized method for other operator networks. To enable real-time deployment, we introduce an inference strategy based on precomputed trunk outputs and a sparse placement matrix, reducing evaluation time by more than a factor of five. Our method provides a practical route to uncertainty-aware operator learning in time-sensitive settings."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-05T04:48:14Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    48,
                    14,
                    2,
                    64,
                    0
                ],
                "arxiv_comment": "Submitted to the Journal of Computational Physics",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Nick Winovich"
                    },
                    {
                        "name": "Mitchell Daneker"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Guang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Guang Lin"
                },
                "author": "Guang Lin"
            },
            {
                "id": "http://arxiv.org/abs/2602.22125v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22125v1",
                "title": "IndicIFEval: A Benchmark for Verifiable Instruction-Following Evaluation in 14 Indic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IndicIFEval: A Benchmark for Verifiable Instruction-Following Evaluation in 14 Indic Languages"
                },
                "updated": "2026-02-25T17:12:37Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    12,
                    37,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22125v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Instruction-following benchmarks remain predominantly English-centric, leaving a critical evaluation gap for the hundreds of millions of Indic language speakers. We introduce IndicIFEval, a benchmark evaluating constrained generation of LLMs across 14 Indic languages using automatically verifiable, rule-based instructions. It comprises around 800 human-verified examples per language spread across two complementary subsets: IndicIFEval-Ground, translated prompts from IFEval (Zhou et al., 2023) carefully localized for Indic contexts, and IndicIFEval-Ground, synthetically generated instructions grounded in native Indic content. We conduct a comprehensive evaluation of major open-weight and proprietary models spanning both reasoning and non-reasoning models. While models maintain strong adherence to formatting constraints, they struggle significantly with lexical and cross-lingual tasks -- and despite progress in high-resource languages, instruction-following across the broader Indic family lags significantly behind English. We release IndicIFEval and its evaluation scripts to support progress on multilingual constrained generation (http://github.com/ai4bharat/IndicIFEval).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-following benchmarks remain predominantly English-centric, leaving a critical evaluation gap for the hundreds of millions of Indic language speakers. We introduce IndicIFEval, a benchmark evaluating constrained generation of LLMs across 14 Indic languages using automatically verifiable, rule-based instructions. It comprises around 800 human-verified examples per language spread across two complementary subsets: IndicIFEval-Ground, translated prompts from IFEval (Zhou et al., 2023) carefully localized for Indic contexts, and IndicIFEval-Ground, synthetically generated instructions grounded in native Indic content. We conduct a comprehensive evaluation of major open-weight and proprietary models spanning both reasoning and non-reasoning models. While models maintain strong adherence to formatting constraints, they struggle significantly with lexical and cross-lingual tasks -- and despite progress in high-resource languages, instruction-following across the broader Indic family lags significantly behind English. We release IndicIFEval and its evaluation scripts to support progress on multilingual constrained generation (http://github.com/ai4bharat/IndicIFEval)."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T17:12:37Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    12,
                    37,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "8 pages + Appendix",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Thanmay Jayakumar"
                    },
                    {
                        "name": "Mohammed Safi Ur Rahman Khan"
                    },
                    {
                        "name": "Raj Dabre"
                    },
                    {
                        "name": "Ratish Puduppully"
                    },
                    {
                        "name": "Anoop Kunchukuttan"
                    }
                ],
                "author_detail": {
                    "name": "Anoop Kunchukuttan"
                },
                "author": "Anoop Kunchukuttan"
            },
            {
                "id": "http://arxiv.org/abs/2602.20309v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20309v2",
                "title": "QuantVLA: Scale-Calibrated Post-Training Quantization for Vision-Language-Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuantVLA: Scale-Calibrated Post-Training Quantization for Vision-Language-Action Models"
                },
                "updated": "2026-02-25T17:11:08Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    11,
                    8,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20309v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20309v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-language-action (VLA) models unify perception, language, and control for embodied agents but face significant challenges in practical deployment due to rapidly increasing compute and memory demands, especially as models scale to longer horizons and larger backbones. To address these bottlenecks, we introduce QuantVLA, a training-free post-training quantization (PTQ) framework that, to our knowledge, is the first PTQ approach for VLA systems and the first to successfully quantize a diffusion transformer (DiT) action head. QuantVLA incorporates three scale-calibrated components: (1) a selective quantization layout that integerizes all linear layers in both the language backbone and the DiT while keeping attention projections in floating point to preserve the original operator schedule; (2) attention temperature matching, a lightweight per-head scaling mechanism that stabilizes attention logits and is folded into the dequantization scales at inference; and (3) output head balancing, a per-layer residual interface calibration that mitigates post-projection energy drift. The framework requires no additional training, uses only a small unlabeled calibration buffer, and supports integer kernels for low-bit weights and activations while leaving the architecture unchanged. Across representative VLA models on LIBERO, QuantVLA exceeds the task success rates of full-precision baselines, achieves about 70% relative memory savings on the quantized components, and delivers a 1.22x speedup in end-to-end inference latency, providing a practical pathway toward scalable low-bit embodied intelligence under strict compute, memory, and power constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language-action (VLA) models unify perception, language, and control for embodied agents but face significant challenges in practical deployment due to rapidly increasing compute and memory demands, especially as models scale to longer horizons and larger backbones. To address these bottlenecks, we introduce QuantVLA, a training-free post-training quantization (PTQ) framework that, to our knowledge, is the first PTQ approach for VLA systems and the first to successfully quantize a diffusion transformer (DiT) action head. QuantVLA incorporates three scale-calibrated components: (1) a selective quantization layout that integerizes all linear layers in both the language backbone and the DiT while keeping attention projections in floating point to preserve the original operator schedule; (2) attention temperature matching, a lightweight per-head scaling mechanism that stabilizes attention logits and is folded into the dequantization scales at inference; and (3) output head balancing, a per-layer residual interface calibration that mitigates post-projection energy drift. The framework requires no additional training, uses only a small unlabeled calibration buffer, and supports integer kernels for low-bit weights and activations while leaving the architecture unchanged. Across representative VLA models on LIBERO, QuantVLA exceeds the task success rates of full-precision baselines, achieves about 70% relative memory savings on the quantized components, and delivers a 1.22x speedup in end-to-end inference latency, providing a practical pathway toward scalable low-bit embodied intelligence under strict compute, memory, and power constraints."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T19:55:54Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    19,
                    55,
                    54,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "CVPR2026",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Jingxuan Zhang"
                    },
                    {
                        "name": "Yunta Hsieh"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Haokun Lin"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Ziqi Wang"
                    },
                    {
                        "name": "Yingtie Lei"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2502.18424v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.18424v2",
                "title": "Compressing Language Models for Specialized Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing Language Models for Specialized Domains"
                },
                "updated": "2026-02-25T17:00:00Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    17,
                    0,
                    0,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.18424v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.18424v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Language models (LMs) excel at tasks across diverse domains, yet require substantial computational resources during inference. Compression techniques such as pruning and quantization offer a practical path towards efficient LM deployment, exemplified by their ability to preserve performance on general-purpose benchmarks. However, general-purpose LM compression methods can negatively affect performance in specialized domains (e.g. biomedical or legal). Recent work has sought to address this issue, but requires a computationally expensive full-parameter fine-tuning pipeline. To this end, we propose MixCal, a novel calibration method designed to improve the in-domain performance of compressed LMs in a post-training setting. Through extensive experimentation, we demonstrate that MixCal substantially outperforms existing approaches on domain-specific tasks and preserves general performance. Notably, these performance gains are achieved while also reducing the computational cost of LM compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models (LMs) excel at tasks across diverse domains, yet require substantial computational resources during inference. Compression techniques such as pruning and quantization offer a practical path towards efficient LM deployment, exemplified by their ability to preserve performance on general-purpose benchmarks. However, general-purpose LM compression methods can negatively affect performance in specialized domains (e.g. biomedical or legal). Recent work has sought to address this issue, but requires a computationally expensive full-parameter fine-tuning pipeline. To this end, we propose MixCal, a novel calibration method designed to improve the in-domain performance of compressed LMs in a post-training setting. Through extensive experimentation, we demonstrate that MixCal substantially outperforms existing approaches on domain-specific tasks and preserves general performance. Notably, these performance gains are achieved while also reducing the computational cost of LM compression."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-25T18:20:00Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    20,
                    0,
                    1,
                    56,
                    0
                ],
                "arxiv_comment": "EACL 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Miles Williams"
                    },
                    {
                        "name": "George Chrysostomou"
                    },
                    {
                        "name": "Vitor Jeronymo"
                    },
                    {
                        "name": "Nikolaos Aletras"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Aletras"
                },
                "author": "Nikolaos Aletras"
            },
            {
                "id": "http://arxiv.org/abs/2509.04625v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.04625v2",
                "title": "Nexus: Efficient and Scalable Multi-Cell mmWave Baseband Processing with Heterogeneous Compute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nexus: Efficient and Scalable Multi-Cell mmWave Baseband Processing with Heterogeneous Compute"
                },
                "updated": "2026-02-25T16:57:22Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    57,
                    22,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.04625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.04625v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid adoption of 5G New Radio (NR), particularly in the millimeter-wave (mmWave) spectrum, imposes stringent demands on the flexibility, scalability, and efficiency of baseband processing. While virtualized Radio Access Networks (vRANs) enable dynamic spectrum sharing across cells, compute resource allocation for baseband processing, especially in multi-cell deployments with heterogeneous workloads, remains underexplored. In this paper, we present NEXUS, the first system to realize real-time, virtualized multi-cell mmWave baseband processing on a single server with heterogeneous compute resources. NEXUS integrates software-based digital signal processing pipelines with hardware-accelerated LDPC decoding, and introduces a novel framework for sharing Intel's ACC100 eASIC across multiple CPU cores via virtual functions (VFs). For single-cell operation, NEXUS employs a random forest (RAF)-based model that predicts the most energy-efficient resource allocation for the given cell configuration with microsecond-level inference latency and high accuracy. For multi-cell scenarios, NEXUS introduces a power-aware scheduler that incorporates a lightweight contention model to adjust resource allocation strategies under concurrent execution. Through extensive evaluation across various Frequency Range 2 (FR2) cell configurations, we show that NEXUS supports up to 16 concurrent cells under full load, achieving 5.37Gbps aggregate throughput, while reducing the multi-cell scheduling search space by orders of magnitude. These results demonstrate that virtualized, resource-aware baseband processing is both practical and efficient for next-generation vRAN systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adoption of 5G New Radio (NR), particularly in the millimeter-wave (mmWave) spectrum, imposes stringent demands on the flexibility, scalability, and efficiency of baseband processing. While virtualized Radio Access Networks (vRANs) enable dynamic spectrum sharing across cells, compute resource allocation for baseband processing, especially in multi-cell deployments with heterogeneous workloads, remains underexplored. In this paper, we present NEXUS, the first system to realize real-time, virtualized multi-cell mmWave baseband processing on a single server with heterogeneous compute resources. NEXUS integrates software-based digital signal processing pipelines with hardware-accelerated LDPC decoding, and introduces a novel framework for sharing Intel's ACC100 eASIC across multiple CPU cores via virtual functions (VFs). For single-cell operation, NEXUS employs a random forest (RAF)-based model that predicts the most energy-efficient resource allocation for the given cell configuration with microsecond-level inference latency and high accuracy. For multi-cell scenarios, NEXUS introduces a power-aware scheduler that incorporates a lightweight contention model to adjust resource allocation strategies under concurrent execution. Through extensive evaluation across various Frequency Range 2 (FR2) cell configurations, we show that NEXUS supports up to 16 concurrent cells under full load, achieving 5.37Gbps aggregate throughput, while reducing the multi-cell scheduling search space by orders of magnitude. These results demonstrate that virtualized, resource-aware baseband processing is both practical and efficient for next-generation vRAN systems."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-04T19:20:35Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    19,
                    20,
                    35,
                    3,
                    247,
                    0
                ],
                "arxiv_comment": "Accepted to ACM MobiCom 2026",
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Zhenzhou Qi"
                    },
                    {
                        "name": "Chung-Hsuan Tung"
                    },
                    {
                        "name": "Zhihui Gao"
                    },
                    {
                        "name": "Tingjun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tingjun Chen"
                },
                "author": "Tingjun Chen"
            },
            {
                "id": "http://arxiv.org/abs/2602.22102v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22102v1",
                "title": "Self-stabilized high-dimensional quantum key distribution on a metropolitan free-space link",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-stabilized high-dimensional quantum key distribution on a metropolitan free-space link"
                },
                "updated": "2026-02-25T16:51:05Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    51,
                    5,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22102v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Quantum communication technologies capable of operating reliably across heterogeneous optical channels are essential for scalable metropolitan quantum networks. Here we demonstrate high-dimensional time-bin-encoded quantum key distribution over a hybrid metropolitan link comprising 1.7 km free-space transmission and 685 m of optical fiber. Operating at a clock rate of 500 MHz in the C-band, we implement both 2- and 4-dimensional protocols, and obtain estimated secure finite-key rates of (95 +- 28) kbit/s for 4D at (25.0 +- 2.0) dB loss and (59 +- 27) kbit/s for 2D at (23.5 +- 2.3) dB loss. Crucially, we achieve continuous operation over 48 h in a fully self-referenced architecture: initial synchronization, interferometric phase stabilization, and long-term drift compensation are performed exclusively using the detected quantum signals, without auxiliary optical reference channels. Our results thus establish a practical and versatile platform for hybrid free-space-to-fiber quantum communication and show that the encoding dimensionality can be adapted to the optimal operating regime of realistic metropolitan channels, providing a pathway toward efficient, autonomous and deployable quantum network nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum communication technologies capable of operating reliably across heterogeneous optical channels are essential for scalable metropolitan quantum networks. Here we demonstrate high-dimensional time-bin-encoded quantum key distribution over a hybrid metropolitan link comprising 1.7 km free-space transmission and 685 m of optical fiber. Operating at a clock rate of 500 MHz in the C-band, we implement both 2- and 4-dimensional protocols, and obtain estimated secure finite-key rates of (95 +- 28) kbit/s for 4D at (25.0 +- 2.0) dB loss and (59 +- 27) kbit/s for 2D at (23.5 +- 2.3) dB loss. Crucially, we achieve continuous operation over 48 h in a fully self-referenced architecture: initial synchronization, interferometric phase stabilization, and long-term drift compensation are performed exclusively using the detected quantum signals, without auxiliary optical reference channels. Our results thus establish a practical and versatile platform for hybrid free-space-to-fiber quantum communication and show that the encoding dimensionality can be adapted to the optimal operating regime of realistic metropolitan channels, providing a pathway toward efficient, autonomous and deployable quantum network nodes."
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T16:51:05Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    51,
                    5,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph"
                },
                "authors": [
                    {
                        "name": "Karolina Dziwulska"
                    },
                    {
                        "name": "Christopher Spiess"
                    },
                    {
                        "name": "Sarika Mishra"
                    },
                    {
                        "name": "Markus Leipe"
                    },
                    {
                        "name": "Yugant Hadiyal"
                    },
                    {
                        "name": "Fabian Steinlechner"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Steinlechner"
                },
                "author": "Fabian Steinlechner"
            },
            {
                "id": "http://arxiv.org/abs/2602.16852v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16852v2",
                "title": "Meenz bleibt Meenz, but Large Language Models Do Not Speak Its Dialect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meenz bleibt Meenz, but Large Language Models Do Not Speak Its Dialect"
                },
                "updated": "2026-02-25T16:49:29Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    49,
                    29,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16852v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16852v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Meenzerisch, the dialect spoken in the German city of Mainz, is also the traditional language of the Mainz carnival, a yearly celebration well known throughout Germany. However, Meenzerisch is on the verge of dying out-a fate it shares with many other German dialects. Natural language processing (NLP) has the potential to help with the preservation and revival efforts of languages and dialects. However, so far no NLP research has looked at Meenzerisch. This work presents the first research in the field of NLP that is explicitly focused on the dialect of Mainz. We introduce a digital dictionary-an NLP-ready dataset derived from an existing resource (Schramm, 1966)-to support researchers in modeling and benchmarking the language. It contains 2,351 words in the dialect paired with their meanings described in Standard German. We then use this dataset to answer the following research questions: (1) Can state-of-the-art large language models (LLMs) generate definitions for dialect words? (2) Can LLMs generate words in Meenzerisch, given their definitions? Our experiments show that LLMs can do neither: the best model for definitions reaches only 6.27% accuracy and the best word generation model's accuracy is 1.51%. We then conduct two additional experiments in order to see if accuracy is improved by few-shot learning and by extracting rules from the training set, which are then passed to the LLM. While those approaches are able to improve the results, accuracy remains below 10%. This highlights that additional resources and an intensification of research efforts focused on German dialects are desperately needed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meenzerisch, the dialect spoken in the German city of Mainz, is also the traditional language of the Mainz carnival, a yearly celebration well known throughout Germany. However, Meenzerisch is on the verge of dying out-a fate it shares with many other German dialects. Natural language processing (NLP) has the potential to help with the preservation and revival efforts of languages and dialects. However, so far no NLP research has looked at Meenzerisch. This work presents the first research in the field of NLP that is explicitly focused on the dialect of Mainz. We introduce a digital dictionary-an NLP-ready dataset derived from an existing resource (Schramm, 1966)-to support researchers in modeling and benchmarking the language. It contains 2,351 words in the dialect paired with their meanings described in Standard German. We then use this dataset to answer the following research questions: (1) Can state-of-the-art large language models (LLMs) generate definitions for dialect words? (2) Can LLMs generate words in Meenzerisch, given their definitions? Our experiments show that LLMs can do neither: the best model for definitions reaches only 6.27% accuracy and the best word generation model's accuracy is 1.51%. We then conduct two additional experiments in order to see if accuracy is improved by few-shot learning and by extracting rules from the training set, which are then passed to the LLM. While those approaches are able to improve the results, accuracy remains below 10%. This highlights that additional resources and an intensification of research efforts focused on German dialects are desperately needed."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T20:29:02Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    20,
                    29,
                    2,
                    2,
                    49,
                    0
                ],
                "arxiv_comment": "Accepted at LREC 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Minh Duc Bui"
                    },
                    {
                        "name": "Manuel Mager"
                    },
                    {
                        "name": "Peter Herbert Kann"
                    },
                    {
                        "name": "Katharina von der Wense"
                    }
                ],
                "author_detail": {
                    "name": "Katharina von der Wense"
                },
                "author": "Katharina von der Wense"
            },
            {
                "id": "http://arxiv.org/abs/2511.21104v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.21104v2",
                "title": "BRIDGE: Building Representations In Domain Guided Program Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BRIDGE: Building Representations In Domain Guided Program Synthesis"
                },
                "updated": "2026-02-25T16:45:04Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    45,
                    4,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.21104v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.21104v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) are good at generating code, but remain brittle for formal verification in systems like Lean4. A core scalability challenge is that verified synthesis requires consistent outputs across multiple artifacts: executable code, precise specifications, theorem statements, and ultimately proofs. Existing approaches rarely treat these as a unified pipeline. We present BRIDGE, a structured prompting framework that decomposes verification into three interconnected domains: Code (implementations), Specifications (formal intent), and Theorem Statements (constructive correctness claims), and elicits domain-specific intermediate reasoning to connect them. In Lean4, BRIDGE often adopts a code-first workflow, using the generated implementation as a semantic anchor for downstream specification and theorem statement generation. Across 178 algorithmic problems and five LLMs, BRIDGE improves Lean executable correctness by nearly 1.5x (pass at 5) over direct baselines and can be 2x more sample-efficient at inference time, requiring fewer samples per verified solution at comparable generation lengths. We further find that specification-driven prompting improves Python pass rates by up to 17.5 percent. Beyond inference-time prompting, supervised fine-tuning on BRIDGE-style reasoning traces yields nearly 1.5x higher Lean pass success than code-only SFT, indicating that these intermediate representations are learnable. BRIDGE provides a practical foundation for scaling verified synthesis and motivates future work on expert iteration and full proof generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are good at generating code, but remain brittle for formal verification in systems like Lean4. A core scalability challenge is that verified synthesis requires consistent outputs across multiple artifacts: executable code, precise specifications, theorem statements, and ultimately proofs. Existing approaches rarely treat these as a unified pipeline. We present BRIDGE, a structured prompting framework that decomposes verification into three interconnected domains: Code (implementations), Specifications (formal intent), and Theorem Statements (constructive correctness claims), and elicits domain-specific intermediate reasoning to connect them. In Lean4, BRIDGE often adopts a code-first workflow, using the generated implementation as a semantic anchor for downstream specification and theorem statement generation. Across 178 algorithmic problems and five LLMs, BRIDGE improves Lean executable correctness by nearly 1.5x (pass at 5) over direct baselines and can be 2x more sample-efficient at inference time, requiring fewer samples per verified solution at comparable generation lengths. We further find that specification-driven prompting improves Python pass rates by up to 17.5 percent. Beyond inference-time prompting, supervised fine-tuning on BRIDGE-style reasoning traces yields nearly 1.5x higher Lean pass success than code-only SFT, indicating that these intermediate representations are learnable. BRIDGE provides a practical foundation for scaling verified synthesis and motivates future work on expert iteration and full proof generation."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-26T06:39:19Z",
                "published_parsed": [
                    2025,
                    11,
                    26,
                    6,
                    39,
                    19,
                    2,
                    330,
                    0
                ],
                "arxiv_comment": "Approx. 23 pages including appendices, 10 figures, 3 tables. Empirical study of LLM-based verified program synthesis in Lean4 (code, specs, and proofs)",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Robert Joseph George"
                    },
                    {
                        "name": "Carson Eisenach"
                    },
                    {
                        "name": "Udaya Ghai"
                    },
                    {
                        "name": "Dominique Perrault-Joncas"
                    },
                    {
                        "name": "Anima Anandkumar"
                    },
                    {
                        "name": "Dean Foster"
                    }
                ],
                "author_detail": {
                    "name": "Dean Foster"
                },
                "author": "Dean Foster"
            },
            {
                "id": "http://arxiv.org/abs/2602.13477v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.13477v2",
                "title": "OMNI-LEAK: Orchestrator Multi-Agent Network Induced Data Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OMNI-LEAK: Orchestrator Multi-Agent Network Induced Data Leakage"
                },
                "updated": "2026-02-25T16:43:53Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    43,
                    53,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.13477v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.13477v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As Large Language Model (LLM) agents become more capable, their coordinated use in the form of multi-agent systems is anticipated to emerge as a practical paradigm. Prior work has examined the safety and misuse risks associated with agents. However, much of this has focused on the single-agent case and/or setups missing basic engineering safeguards such as access control, revealing a scarcity of threat modeling in multi-agent systems. We investigate the security vulnerabilities of a popular multi-agent pattern known as the orchestrator setup, in which a central agent decomposes and delegates tasks to specialized agents. Through red-teaming a concrete setup representative of a likely future use case, we demonstrate a novel attack vector, OMNI-LEAK, that compromises several agents to leak sensitive data through a single indirect prompt injection, even in the presence of data access control. We report the susceptibility of frontier models to different categories of attacks, finding that both reasoning and non-reasoning models are vulnerable, even when the attacker lacks insider knowledge of the implementation details. Our work highlights the importance of safety research to generalize from single-agent to multi-agent settings, in order to reduce the serious risks of real-world privacy breaches and financial losses and overall public trust in AI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Model (LLM) agents become more capable, their coordinated use in the form of multi-agent systems is anticipated to emerge as a practical paradigm. Prior work has examined the safety and misuse risks associated with agents. However, much of this has focused on the single-agent case and/or setups missing basic engineering safeguards such as access control, revealing a scarcity of threat modeling in multi-agent systems. We investigate the security vulnerabilities of a popular multi-agent pattern known as the orchestrator setup, in which a central agent decomposes and delegates tasks to specialized agents. Through red-teaming a concrete setup representative of a likely future use case, we demonstrate a novel attack vector, OMNI-LEAK, that compromises several agents to leak sensitive data through a single indirect prompt injection, even in the presence of data access control. We report the susceptibility of frontier models to different categories of attacks, finding that both reasoning and non-reasoning models are vulnerable, even when the attacker lacks insider knowledge of the implementation details. Our work highlights the importance of safety research to generalize from single-agent to multi-agent settings, in order to reduce the serious risks of real-world privacy breaches and financial losses and overall public trust in AI agents."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-13T21:32:32Z",
                "published_parsed": [
                    2026,
                    2,
                    13,
                    21,
                    32,
                    32,
                    4,
                    44,
                    0
                ],
                "arxiv_comment": "Preprint; corrected typos",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Akshat Naik"
                    },
                    {
                        "name": "Jay Culligan"
                    },
                    {
                        "name": "Yarin Gal"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Rahaf Aljundi"
                    },
                    {
                        "name": "Alasdair Paren"
                    },
                    {
                        "name": "Adel Bibi"
                    }
                ],
                "author_detail": {
                    "name": "Adel Bibi"
                },
                "author": "Adel Bibi"
            },
            {
                "id": "http://arxiv.org/abs/2602.22090v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22090v1",
                "title": "Confidence-Driven Multi-Scale Model Selection for Cost-Efficient Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence-Driven Multi-Scale Model Selection for Cost-Efficient Inference"
                },
                "updated": "2026-02-25T16:38:03Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    38,
                    3,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22090v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have revolutionized inference across diverse natural language tasks, with larger models performing better but at higher computational costs. We propose a confidence-driven strategy that dynamically selects the most suitable model based on confidence estimates. By assessing a model's confidence in handling the task and response accuracy, tasks that are likely to be solved correctly are retained, while more uncertain or complex cases are delegated to a larger model, ensuring reliability while minimizing computation. Specifically, we evaluate a model's likelihood of knowing the correct answer and the probability that its response is accurate. Experiments on the Massive Multitask Language Understanding (MMLU) benchmark show that our approach achieves accuracy comparable to the largest model while reducing computational costs by 20\\% to 40\\%. When applied to GPT-4o API calls, it reduces token usage by approximately 60\\%, further improving cost efficiency. These findings indicate the potential of confidence-based model selection to enhance real-world LLM deployment, particularly in resource-constrained settings such as edge devices and commercial API applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized inference across diverse natural language tasks, with larger models performing better but at higher computational costs. We propose a confidence-driven strategy that dynamically selects the most suitable model based on confidence estimates. By assessing a model's confidence in handling the task and response accuracy, tasks that are likely to be solved correctly are retained, while more uncertain or complex cases are delegated to a larger model, ensuring reliability while minimizing computation. Specifically, we evaluate a model's likelihood of knowing the correct answer and the probability that its response is accurate. Experiments on the Massive Multitask Language Understanding (MMLU) benchmark show that our approach achieves accuracy comparable to the largest model while reducing computational costs by 20\\% to 40\\%. When applied to GPT-4o API calls, it reduces token usage by approximately 60\\%, further improving cost efficiency. These findings indicate the potential of confidence-based model selection to enhance real-world LLM deployment, particularly in resource-constrained settings such as edge devices and commercial API applications."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T16:38:03Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    38,
                    3,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "Accepted by EACL 2026 Findings",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Bo-Wei Chen"
                    },
                    {
                        "name": "Chung-Chi Chen"
                    },
                    {
                        "name": "An-Zi Yen"
                    }
                ],
                "author_detail": {
                    "name": "An-Zi Yen"
                },
                "author": "An-Zi Yen"
            },
            {
                "id": "http://arxiv.org/abs/2602.22085v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22085v1",
                "title": "SocialPulse: On-Device Detection of Social Interactions in Naturalistic Settings Using Smartwatch Multimodal Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SocialPulse: On-Device Detection of Social Interactions in Naturalistic Settings Using Smartwatch Multimodal Sensing"
                },
                "updated": "2026-02-25T16:33:04Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    33,
                    4,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22085v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Social interactions are fundamental to well-being, yet automatically detecting them in daily life-particularly using wearables-remains underexplored. Most existing systems are evaluated in controlled settings, focus primarily on in-person interactions, or rely on restrictive assumptions (e.g., requiring multiple speakers within fixed temporal windows), limiting generalizability to real-world use. We present an on-watch interaction detection system designed to capture diverse interactions in naturalistic settings. A core component is a foreground speech detector trained on a public dataset. Evaluated on over 100,000 labeled foreground speech and background sound instances, the detector achieves a balanced accuracy of 85.51%, outperforming prior work by 5.11%.\n  We evaluated the system in a real-world deployment (N=38), with over 900 hours of total smartwatch wear time. The system detected 1,691 interactions, 77.28% were confirmed via participant self-report, with durations ranging from under one minute to over one hour. Among correct detections, 81.45% were in-person, 15.7% virtual, and 1.85% hybrid. Leveraging participant-labeled data, we further developed a multimodal model achieving a balanced accuracy of 90.36% and a sensitivity of 91.17% on 33,698 labeled 15-second windows. These results demonstrate the feasibility of real-world interaction sensing and open the door to adaptive, context-aware systems responding to users' dynamic social environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social interactions are fundamental to well-being, yet automatically detecting them in daily life-particularly using wearables-remains underexplored. Most existing systems are evaluated in controlled settings, focus primarily on in-person interactions, or rely on restrictive assumptions (e.g., requiring multiple speakers within fixed temporal windows), limiting generalizability to real-world use. We present an on-watch interaction detection system designed to capture diverse interactions in naturalistic settings. A core component is a foreground speech detector trained on a public dataset. Evaluated on over 100,000 labeled foreground speech and background sound instances, the detector achieves a balanced accuracy of 85.51%, outperforming prior work by 5.11%.\n  We evaluated the system in a real-world deployment (N=38), with over 900 hours of total smartwatch wear time. The system detected 1,691 interactions, 77.28% were confirmed via participant self-report, with durations ranging from under one minute to over one hour. Among correct detections, 81.45% were in-person, 15.7% virtual, and 1.85% hybrid. Leveraging participant-labeled data, we further developed a multimodal model achieving a balanced accuracy of 90.36% and a sensitivity of 91.17% on 33,698 labeled 15-second windows. These results demonstrate the feasibility of real-world interaction sensing and open the door to adaptive, context-aware systems responding to users' dynamic social environments."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T16:33:04Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    33,
                    4,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Md Sabbir Ahmed"
                    },
                    {
                        "name": "Kaitlyn Dorothy Petz"
                    },
                    {
                        "name": "Noah French"
                    },
                    {
                        "name": "Tanvi Lakhtakia"
                    },
                    {
                        "name": "Aayushi Sangani"
                    },
                    {
                        "name": "Mark Rucker"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Bethany A. Teachman"
                    },
                    {
                        "name": "Laura E. Barnes"
                    }
                ],
                "author_detail": {
                    "name": "Laura E. Barnes"
                },
                "author": "Laura E. Barnes"
            },
            {
                "id": "http://arxiv.org/abs/2602.22077v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22077v1",
                "title": "ViSTAR: Virtual Skill Training with Augmented Reality with 3D Avatars and LLM coaching agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViSTAR: Virtual Skill Training with Augmented Reality with 3D Avatars and LLM coaching agent"
                },
                "updated": "2026-02-25T16:26:00Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    26,
                    0,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22077v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present ViSTAR, a Virtual Skill Training system in AR that supports self-guided basketball skill practice, with feedback on balance, posture, and timing. From a formative study with basketball players and coaches, the system addresses three challenges: understanding skills, identifying errors, and correcting mistakes. ViSTAR follows the Behavioral Skills Training (BST) framework-instruction, modeling, rehearsal, and feedback. It provides feedback through visual overlays, rhythm and timing cues, and an AI-powered coaching agent using 3D motion reconstruction. We generate verbal feedback by analyzing spatio-temporal joint data and mapping features to natural-language coaching cues via a Large Language Model (LLM). A key novelty is this feedback generation: motion features become concise coaching insights. In two studies (N=16), participants generally preferred our AI-generated feedback to coach feedback and reported that ViSTAR helped them notice posture and balance issues and refine movements beyond self-observation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ViSTAR, a Virtual Skill Training system in AR that supports self-guided basketball skill practice, with feedback on balance, posture, and timing. From a formative study with basketball players and coaches, the system addresses three challenges: understanding skills, identifying errors, and correcting mistakes. ViSTAR follows the Behavioral Skills Training (BST) framework-instruction, modeling, rehearsal, and feedback. It provides feedback through visual overlays, rhythm and timing cues, and an AI-powered coaching agent using 3D motion reconstruction. We generate verbal feedback by analyzing spatio-temporal joint data and mapping features to natural-language coaching cues via a Large Language Model (LLM). A key novelty is this feedback generation: motion features become concise coaching insights. In two studies (N=16), participants generally preferred our AI-generated feedback to coach feedback and reported that ViSTAR helped them notice posture and balance issues and refine movements beyond self-observation."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T16:26:00Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    26,
                    0,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Chunggi Lee"
                    },
                    {
                        "name": "Hayato Saiki"
                    },
                    {
                        "name": "Tica Lin"
                    },
                    {
                        "name": "Eiji Ikeda"
                    },
                    {
                        "name": "Kenji Suzuki"
                    },
                    {
                        "name": "Chen Zhu-Tian"
                    },
                    {
                        "name": "Hanspeter Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Hanspeter Pfister"
                },
                "author": "Hanspeter Pfister"
            },
            {
                "id": "http://arxiv.org/abs/2602.22072v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22072v1",
                "title": "Understanding Artificial Theory of Mind: Perturbed Tasks and Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Artificial Theory of Mind: Perturbed Tasks and Reasoning in Large Language Models"
                },
                "updated": "2026-02-25T16:24:35Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    24,
                    35,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22072v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Theory of Mind (ToM) refers to an agent's ability to model the internal states of others. Contributing to the debate whether large language models (LLMs) exhibit genuine ToM capabilities, our study investigates their ToM robustness using perturbations on false-belief tasks and examines the potential of Chain-of-Thought prompting (CoT) to enhance performance and explain the LLM's decision. We introduce a handcrafted, richly annotated ToM dataset, including classic and perturbed false belief tasks, the corresponding spaces of valid reasoning chains for correct task completion, subsequent reasoning faithfulness, task solutions, and propose metrics to evaluate reasoning chain correctness and to what extent final answers are faithful to reasoning traces of the generated CoT. We show a steep drop in ToM capabilities under task perturbation for all evaluated LLMs, questioning the notion of any robust form of ToM being present. While CoT prompting improves the ToM performance overall in a faithful manner, it surprisingly degrades accuracy for some perturbation classes, indicating that selective application is necessary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory of Mind (ToM) refers to an agent's ability to model the internal states of others. Contributing to the debate whether large language models (LLMs) exhibit genuine ToM capabilities, our study investigates their ToM robustness using perturbations on false-belief tasks and examines the potential of Chain-of-Thought prompting (CoT) to enhance performance and explain the LLM's decision. We introduce a handcrafted, richly annotated ToM dataset, including classic and perturbed false belief tasks, the corresponding spaces of valid reasoning chains for correct task completion, subsequent reasoning faithfulness, task solutions, and propose metrics to evaluate reasoning chain correctness and to what extent final answers are faithful to reasoning traces of the generated CoT. We show a steep drop in ToM capabilities under task perturbation for all evaluated LLMs, questioning the notion of any robust form of ToM being present. While CoT prompting improves the ToM performance overall in a faithful manner, it surprisingly degrades accuracy for some perturbation classes, indicating that selective application is necessary."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T16:24:35Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    24,
                    35,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Christian Nickel"
                    },
                    {
                        "name": "Laura Schrewe"
                    },
                    {
                        "name": "Florian Mai"
                    },
                    {
                        "name": "Lucie Flek"
                    }
                ],
                "author_detail": {
                    "name": "Lucie Flek"
                },
                "author": "Lucie Flek"
            },
            {
                "id": "http://arxiv.org/abs/2602.17729v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.17729v2",
                "title": "Stop Saying \"AI\"",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stop Saying \"AI\""
                },
                "updated": "2026-02-25T16:18:48Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    18,
                    48,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.17729v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.17729v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Across academia, industry, and government, ``AI'' has become central in research and development, regulatory debates, and promises of ever faster and more capable decision-making and action. In numerous domains, especially safety-critical ones, there are significant concerns over how ``AI'' may affect decision-making, responsibility, or the likelihood of mistakes (to name only a few categories of critique). However, for most critiques, the target is generally ``AI'', a broad term admitting many (types of) systems used for a variety of tasks and each coming with its own set of limitations, challenges, and potential use cases. In this article, we focus on the military domain as a case study and present both a loose enumerative taxonomy of systems captured under the umbrella term ``military AI'', as well as discussion of the challenges of each. In doing so, we highlight that critiques of one (type of) system will not always transfer to other (types of) systems. Building on this, we argue that in order for debates to move forward fruitfully, it is imperative that the discussions be made more precise and that ``AI'' be excised from debates to the extent possible. Researchers, developers, and policy-makers should make clear exactly what systems they have in mind and what possible benefits and risks attend the deployment of those particular systems. While we focus on AI in the military as an exemplar for the overall trends in discussions of ``AI'', the argument's conclusions are broad and have import for discussions of AI across a host of domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Across academia, industry, and government, ``AI'' has become central in research and development, regulatory debates, and promises of ever faster and more capable decision-making and action. In numerous domains, especially safety-critical ones, there are significant concerns over how ``AI'' may affect decision-making, responsibility, or the likelihood of mistakes (to name only a few categories of critique). However, for most critiques, the target is generally ``AI'', a broad term admitting many (types of) systems used for a variety of tasks and each coming with its own set of limitations, challenges, and potential use cases. In this article, we focus on the military domain as a case study and present both a loose enumerative taxonomy of systems captured under the umbrella term ``military AI'', as well as discussion of the challenges of each. In doing so, we highlight that critiques of one (type of) system will not always transfer to other (types of) systems. Building on this, we argue that in order for debates to move forward fruitfully, it is imperative that the discussions be made more precise and that ``AI'' be excised from debates to the extent possible. Researchers, developers, and policy-makers should make clear exactly what systems they have in mind and what possible benefits and risks attend the deployment of those particular systems. While we focus on AI in the military as an exemplar for the overall trends in discussions of ``AI'', the argument's conclusions are broad and have import for discussions of AI across a host of domains."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T10:11:31Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    10,
                    11,
                    31,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Nathan G. Wood"
                    },
                    {
                        "name": "Scott Robbins"
                    },
                    {
                        "name": "Eduardo Zegarra Berodt"
                    },
                    {
                        "name": "Anton Graf von Westerholt"
                    },
                    {
                        "name": "Michelle Behrndt"
                    },
                    {
                        "name": "Hauke Budig"
                    },
                    {
                        "name": "Daniel Kloock-Schreiber"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Kloock-Schreiber"
                },
                "arxiv_affiliation": "Institute of Air Transportation Systems, Hamburg University of Technology",
                "author": "Daniel Kloock-Schreiber"
            },
            {
                "id": "http://arxiv.org/abs/2602.22070v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22070v1",
                "title": "Language Models Exhibit Inconsistent Biases Towards Algorithmic Agents and Human Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Exhibit Inconsistent Biases Towards Algorithmic Agents and Human Experts"
                },
                "updated": "2026-02-25T16:18:28Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    18,
                    28,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22070v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models are increasingly used in decision-making tasks that require them to process information from a variety of sources, including both human experts and other algorithmic agents. How do LLMs weigh the information provided by these different sources? We consider the well-studied phenomenon of algorithm aversion, in which human decision-makers exhibit bias against predictions from algorithms. Drawing upon experimental paradigms from behavioural economics, we evaluate how eightdifferent LLMs delegate decision-making tasks when the delegatee is framed as a human expert or an algorithmic agent. To be inclusive of different evaluation formats, we conduct our study with two task presentations: stated preferences, modeled through direct queries about trust towards either agent, and revealed preferences, modeled through providing in-context examples of the performance of both agents. When prompted to rate the trustworthiness of human experts and algorithms across diverse tasks, LLMs give higher ratings to the human expert, which correlates with prior results from human respondents. However, when shown the performance of a human expert and an algorithm and asked to place an incentivized bet between the two, LLMs disproportionately choose the algorithm, even when it performs demonstrably worse. These discrepant results suggest that LLMs may encode inconsistent biases towards humans and algorithms, which need to be carefully considered when they are deployed in high-stakes scenarios. Furthermore, we discuss the sensitivity of LLMs to task presentation formats that should be broadly scrutinized in evaluation robustness for AI safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are increasingly used in decision-making tasks that require them to process information from a variety of sources, including both human experts and other algorithmic agents. How do LLMs weigh the information provided by these different sources? We consider the well-studied phenomenon of algorithm aversion, in which human decision-makers exhibit bias against predictions from algorithms. Drawing upon experimental paradigms from behavioural economics, we evaluate how eightdifferent LLMs delegate decision-making tasks when the delegatee is framed as a human expert or an algorithmic agent. To be inclusive of different evaluation formats, we conduct our study with two task presentations: stated preferences, modeled through direct queries about trust towards either agent, and revealed preferences, modeled through providing in-context examples of the performance of both agents. When prompted to rate the trustworthiness of human experts and algorithms across diverse tasks, LLMs give higher ratings to the human expert, which correlates with prior results from human respondents. However, when shown the performance of a human expert and an algorithm and asked to place an incentivized bet between the two, LLMs disproportionately choose the algorithm, even when it performs demonstrably worse. These discrepant results suggest that LLMs may encode inconsistent biases towards humans and algorithms, which need to be carefully considered when they are deployed in high-stakes scenarios. Furthermore, we discuss the sensitivity of LLMs to task presentation formats that should be broadly scrutinized in evaluation robustness for AI safety."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T16:18:28Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    18,
                    28,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "Second Conference of the International Association for Safe and Ethical Artificial Intelligence (IASEAI 2026)",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jessica Y. Bo"
                    },
                    {
                        "name": "Lillio Mok"
                    },
                    {
                        "name": "Ashton Anderson"
                    }
                ],
                "author_detail": {
                    "name": "Ashton Anderson"
                },
                "author": "Ashton Anderson"
            },
            {
                "id": "http://arxiv.org/abs/2507.08422v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.08422v3",
                "title": "Training-free Mixed-Resolution Latent Upsampling for Spatially Accelerated Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free Mixed-Resolution Latent Upsampling for Spatially Accelerated Diffusion Transformers"
                },
                "updated": "2026-02-25T16:14:20Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    14,
                    20,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.08422v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.08422v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion transformers (DiTs) offer excellent scalability for high-fidelity generation, but their computational overhead poses a great challenge for practical deployment. Existing acceleration methods primarily exploit the temporal dimension, whereas spatial acceleration remains underexplored. In this work, we investigate spatial acceleration for DiTs via latent upsampling. We found that nave latent upsampling for spatial acceleration introduces artifacts, primarily due to aliasing in high-frequency edge regions and mismatching from noise-timestep discrepancies. Then, based on these findings and analyses, we propose a training-free spatial acceleration framework, dubbed Region-Adaptive Latent Upsampling (RALU), to mitigate those artifacts while achieving spatial acceleration of DiTs by our mixed-resolution latent upsampling. RALU achieves artifact-free, efficient acceleration with early upsampling only on artifact-prone edge regions and noise-timestep matching for different latent resolutions, leading to up to 7.0$\\times$ speedup on FLUX-1.dev and 3.0$\\times$ on Stable Diffusion 3 with negligible quality degradation. Furthermore, our RALU is complementarily applicable to existing temporal acceleration methods and timestep-distilled models, leading to up to 15.9$\\times$ speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers (DiTs) offer excellent scalability for high-fidelity generation, but their computational overhead poses a great challenge for practical deployment. Existing acceleration methods primarily exploit the temporal dimension, whereas spatial acceleration remains underexplored. In this work, we investigate spatial acceleration for DiTs via latent upsampling. We found that nave latent upsampling for spatial acceleration introduces artifacts, primarily due to aliasing in high-frequency edge regions and mismatching from noise-timestep discrepancies. Then, based on these findings and analyses, we propose a training-free spatial acceleration framework, dubbed Region-Adaptive Latent Upsampling (RALU), to mitigate those artifacts while achieving spatial acceleration of DiTs by our mixed-resolution latent upsampling. RALU achieves artifact-free, efficient acceleration with early upsampling only on artifact-prone edge regions and noise-timestep matching for different latent resolutions, leading to up to 7.0$\\times$ speedup on FLUX-1.dev and 3.0$\\times$ on Stable Diffusion 3 with negligible quality degradation. Furthermore, our RALU is complementarily applicable to existing temporal acceleration methods and timestep-distilled models, leading to up to 15.9$\\times$ speedup."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-11T09:07:43Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Wongi Jeong"
                    },
                    {
                        "name": "Kyungryeol Lee"
                    },
                    {
                        "name": "Hoigi Seo"
                    },
                    {
                        "name": "Se Young Chun"
                    }
                ],
                "author_detail": {
                    "name": "Se Young Chun"
                },
                "author": "Se Young Chun"
            },
            {
                "id": "http://arxiv.org/abs/2602.20292v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20292v2",
                "title": "Quantifying the Expectation-Realisation Gap for Agentic AI Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying the Expectation-Realisation Gap for Agentic AI Systems"
                },
                "updated": "2026-02-25T16:13:30Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    13,
                    30,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20292v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20292v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Agentic AI systems are deployed with expectations of substantial productivity gains, yet rigorous empirical evidence reveals systematic discrepancies between pre-deployment expectations and post-deployment outcomes. We review controlled trials and independent validations across software engineering, clinical documentation, and clinical decision support to quantify this expectation-realisation gap. In software development, experienced developers expected a 24% speedup from AI tools but were slowed by 19% -- a 43 percentage-point calibration error. In clinical documentation, vendor claims of multi-minute time savings contrast with measured reductions of less than one minute per note, and one widely deployed tool showed no statistically significant effect. In clinical decision support, externally validated performance falls substantially below developer-reported metrics. These shortfalls are driven by workflow integration friction, verification burden, measurement construct mismatches, and systematic variation in who benefits and who does not. The evidence motivates structured planning frameworks that require explicit, quantified benefit expectations with human oversight costs factored in.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI systems are deployed with expectations of substantial productivity gains, yet rigorous empirical evidence reveals systematic discrepancies between pre-deployment expectations and post-deployment outcomes. We review controlled trials and independent validations across software engineering, clinical documentation, and clinical decision support to quantify this expectation-realisation gap. In software development, experienced developers expected a 24% speedup from AI tools but were slowed by 19% -- a 43 percentage-point calibration error. In clinical documentation, vendor claims of multi-minute time savings contrast with measured reductions of less than one minute per note, and one widely deployed tool showed no statistically significant effect. In clinical decision support, externally validated performance falls substantially below developer-reported metrics. These shortfalls are driven by workflow integration friction, verification burden, measurement construct mismatches, and systematic variation in who benefits and who does not. The evidence motivates structured planning frameworks that require explicit, quantified benefit expectations with human oversight costs factored in."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T19:16:30Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    19,
                    16,
                    30,
                    0,
                    54,
                    0
                ],
                "arxiv_comment": "10 pages, no figures; added glossary",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Sebastian Lobentanzer"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Lobentanzer"
                },
                "author": "Sebastian Lobentanzer"
            },
            {
                "id": "http://arxiv.org/abs/2602.22067v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22067v1",
                "title": "Semantic Partial Grounding via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Partial Grounding via LLMs"
                },
                "updated": "2026-02-25T16:13:26Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    13,
                    26,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22067v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Grounding is a critical step in classical planning, yet it often becomes a computational bottleneck due to the exponential growth in grounded actions and atoms as task size increases. Recent advances in partial grounding have addressed this challenge by incrementally grounding only the most promising operators, guided by predictive models. However, these approaches primarily rely on relational features or learned embeddings and do not leverage the textual and structural cues present in PDDL descriptions. We propose SPG-LLM, which uses LLMs to analyze the domain and problem files to heuristically identify potentially irrelevant objects, actions, and predicates prior to grounding, significantly reducing the size of the grounded task. Across seven hard-to-ground benchmarks, SPG-LLM achieves faster grounding-often by orders of magnitude-while delivering comparable or better plan costs in some domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounding is a critical step in classical planning, yet it often becomes a computational bottleneck due to the exponential growth in grounded actions and atoms as task size increases. Recent advances in partial grounding have addressed this challenge by incrementally grounding only the most promising operators, guided by predictive models. However, these approaches primarily rely on relational features or learned embeddings and do not leverage the textual and structural cues present in PDDL descriptions. We propose SPG-LLM, which uses LLMs to analyze the domain and problem files to heuristically identify potentially irrelevant objects, actions, and predicates prior to grounding, significantly reducing the size of the grounded task. Across seven hard-to-ground benchmarks, SPG-LLM achieves faster grounding-often by orders of magnitude-while delivering comparable or better plan costs in some domains."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T16:13:26Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    13,
                    26,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Giuseppe Canonaco"
                    },
                    {
                        "name": "Alberto Pozanco"
                    },
                    {
                        "name": "Daniel Borrajo"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Borrajo"
                },
                "author": "Daniel Borrajo"
            },
            {
                "id": "http://arxiv.org/abs/2602.22056v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22056v1",
                "title": "FlowCorrect: Efficient Interactive Correction of Generative Flow Policies for Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowCorrect: Efficient Interactive Correction of Generative Flow Policies for Robotic Manipulation"
                },
                "updated": "2026-02-25T16:06:49Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    6,
                    49,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22056v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative manipulation policies can fail catastrophically under deployment-time distribution shift, yet many failures are near-misses: the robot reaches almost-correct poses and would succeed with a small corrective motion. We present FlowCorrect, a deployment-time correction framework that converts near-miss failures into successes using sparse human nudges, without full policy retraining. During execution, a human provides brief corrective pose nudges via a lightweight VR interface. FlowCorrect uses these sparse corrections to locally adapt the policy, improving actions without retraining the backbone while preserving the model performance on previously learned scenarios. We evaluate on a real-world robot across three tabletop tasks: pick-and-place, pouring, and cup uprighting. With a low correction budget, FlowCorrect improves success on hard cases by 85\\% while preserving performance on previously solved scenarios. The results demonstrate clearly that FlowCorrect learns only with very few demonstrations and enables fast and sample-efficient incremental, human-in-the-loop corrections of generative visuomotor policies at deployment time in real-world robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative manipulation policies can fail catastrophically under deployment-time distribution shift, yet many failures are near-misses: the robot reaches almost-correct poses and would succeed with a small corrective motion. We present FlowCorrect, a deployment-time correction framework that converts near-miss failures into successes using sparse human nudges, without full policy retraining. During execution, a human provides brief corrective pose nudges via a lightweight VR interface. FlowCorrect uses these sparse corrections to locally adapt the policy, improving actions without retraining the backbone while preserving the model performance on previously learned scenarios. We evaluate on a real-world robot across three tabletop tasks: pick-and-place, pouring, and cup uprighting. With a low correction budget, FlowCorrect improves success on hard cases by 85\\% while preserving performance on previously solved scenarios. The results demonstrate clearly that FlowCorrect learns only with very few demonstrations and enables fast and sample-efficient incremental, human-in-the-loop corrections of generative visuomotor policies at deployment time in real-world robotics."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T16:06:49Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    6,
                    49,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "8 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Edgar Welte"
                    },
                    {
                        "name": "Yitian Shi"
                    },
                    {
                        "name": "Rosa Wolf"
                    },
                    {
                        "name": "Maximillian Gilles"
                    },
                    {
                        "name": "Rania Rayyes"
                    }
                ],
                "author_detail": {
                    "name": "Rania Rayyes"
                },
                "author": "Rania Rayyes"
            },
            {
                "id": "http://arxiv.org/abs/2509.11517v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.11517v2",
                "title": "PeruMedQA: Benchmarking Large Language Models (LLMs) on Peruvian Medical Exams -- Dataset Construction and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PeruMedQA: Benchmarking Large Language Models (LLMs) on Peruvian Medical Exams -- Dataset Construction and Evaluation"
                },
                "updated": "2026-02-25T16:04:59Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    4,
                    59,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.11517v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.11517v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "BACKGROUND: Medical large language models (LLMs) have demonstrated remarkable performance in answering medical examinations. However, the extent to which this high performance is transferable to medical questions in Spanish and from a Latin American country remains unexplored. This knowledge is crucial as LLM-based medical applications gain traction in Latin America. AIMS: To build a dataset of questions medical examinations taken by Peruvian physicians pursuing specialty training; to fine-tune a LLM on this dataset; to evaluate and compare the performance in terms of accuracy between vanilla LLMs and the fine-tuned LLM. METHODS: We curated PeruMedQA, a multiple-choice question-answering (MCQA) dataset containing 8,380 questions spanning 12 specialties (2018-2025). We selected ten medical LLMs, including medgemma-4b-it and medgemma-27b-text-it, and developed zero-shot task specific prompts to answer the questions. We employed parameter-efficient fine tuning (PEFT) and low-rand adaptation (LoRA) to fine-tune medgemma-4b-it utilizing all questions except those from 2025 (test set). RESULTS: Medgemma-27b showed the highest accuracy across all specialities, achieving the highest score of 89.29% in Psychiatry; yet, in two specialties, OctoMed-7B exhibited slight superiority: Neurosurgery with 77.27% and 77.38, respectively; and Radiology with 76.13% and 77.39%, respectively. Across specialties, most LLMs with <10 billion parameters exhibited <50% of correct answers. The fine-tuned version of medgemma-4b-it emerged victorious against all LLMs with <10 billion parameters and rivaled a LLM with 70 billion parameters across various examinations. CONCLUSIONS: For medical AI applications and research that require knowledge bases from Spanish-speaking countries and those exhibiting similar epidemiological profile to Peru's, interested parties should utilize medgemma-27b-text-it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BACKGROUND: Medical large language models (LLMs) have demonstrated remarkable performance in answering medical examinations. However, the extent to which this high performance is transferable to medical questions in Spanish and from a Latin American country remains unexplored. This knowledge is crucial as LLM-based medical applications gain traction in Latin America. AIMS: To build a dataset of questions medical examinations taken by Peruvian physicians pursuing specialty training; to fine-tune a LLM on this dataset; to evaluate and compare the performance in terms of accuracy between vanilla LLMs and the fine-tuned LLM. METHODS: We curated PeruMedQA, a multiple-choice question-answering (MCQA) dataset containing 8,380 questions spanning 12 specialties (2018-2025). We selected ten medical LLMs, including medgemma-4b-it and medgemma-27b-text-it, and developed zero-shot task specific prompts to answer the questions. We employed parameter-efficient fine tuning (PEFT) and low-rand adaptation (LoRA) to fine-tune medgemma-4b-it utilizing all questions except those from 2025 (test set). RESULTS: Medgemma-27b showed the highest accuracy across all specialities, achieving the highest score of 89.29% in Psychiatry; yet, in two specialties, OctoMed-7B exhibited slight superiority: Neurosurgery with 77.27% and 77.38, respectively; and Radiology with 76.13% and 77.39%, respectively. Across specialties, most LLMs with <10 billion parameters exhibited <50% of correct answers. The fine-tuned version of medgemma-4b-it emerged victorious against all LLMs with <10 billion parameters and rivaled a LLM with 70 billion parameters across various examinations. CONCLUSIONS: For medical AI applications and research that require knowledge bases from Spanish-speaking countries and those exhibiting similar epidemiological profile to Peru's, interested parties should utilize medgemma-27b-text-it."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-15T02:07:26Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    2,
                    7,
                    26,
                    0,
                    258,
                    0
                ],
                "arxiv_comment": "https://github.com/rodrigo-carrillo/PeruMedQA",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Rodrigo M. Carrillo-Larco"
                    },
                    {
                        "name": "Jesus Lovn Melgarejo"
                    },
                    {
                        "name": "Manuel Castillo-Cara"
                    },
                    {
                        "name": "Gusseppe Bravo-Rocca"
                    }
                ],
                "author_detail": {
                    "name": "Gusseppe Bravo-Rocca"
                },
                "author": "Gusseppe Bravo-Rocca"
            },
            {
                "id": "http://arxiv.org/abs/2511.11910v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.11910v3",
                "title": "Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models"
                },
                "updated": "2026-02-25T16:03:05Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    16,
                    3,
                    5,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.11910v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.11910v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite the recent advances in the video understanding ability of multimodal large language models (MLLMs), long video understanding remains a challenge. One of the main issues is that the number of vision tokens grows linearly with video length, which causes an explosion in attention cost, memory, and latency. To solve this challenge, we present Query-aware Token Selector (\\textbf{QTSplus}), a lightweight yet powerful visual token selection module that serves as an information gate between the vision encoder and LLMs. Given a text query and video tokens, QTSplus dynamically selects the most important visual evidence for the input text query by (i) scoring visual tokens via cross-attention, (ii) \\emph{predicting} an instance-specific retention budget based on the complexity of the query, and (iii) \\emph{selecting} Top-$n$ tokens with a differentiable straight-through estimator during training and a hard gate at inference. Furthermore, a small re-encoder preserves temporal order using absolute time information, enabling second-level localization while maintaining global coverage.\n  Integrated into Qwen2.5-VL, QTSplus compresses the vision stream by up to \\textbf{89\\%} and reduces end-to-end latency by \\textbf{28\\%} on long videos. The evaluation on eight long video understanding benchmarks shows near-parity accuracy overall when compared with the original Qwen models and outperforms the original model by \\textbf{+20.5} and \\textbf{+5.6} points respectively on TempCompass direction and order accuracies. These results show that QTSplus is an effective, general mechanism for scaling MLLMs to real-world long-video scenarios while preserving task-relevant evidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent advances in the video understanding ability of multimodal large language models (MLLMs), long video understanding remains a challenge. One of the main issues is that the number of vision tokens grows linearly with video length, which causes an explosion in attention cost, memory, and latency. To solve this challenge, we present Query-aware Token Selector (\\textbf{QTSplus}), a lightweight yet powerful visual token selection module that serves as an information gate between the vision encoder and LLMs. Given a text query and video tokens, QTSplus dynamically selects the most important visual evidence for the input text query by (i) scoring visual tokens via cross-attention, (ii) \\emph{predicting} an instance-specific retention budget based on the complexity of the query, and (iii) \\emph{selecting} Top-$n$ tokens with a differentiable straight-through estimator during training and a hard gate at inference. Furthermore, a small re-encoder preserves temporal order using absolute time information, enabling second-level localization while maintaining global coverage.\n  Integrated into Qwen2.5-VL, QTSplus compresses the vision stream by up to \\textbf{89\\%} and reduces end-to-end latency by \\textbf{28\\%} on long videos. The evaluation on eight long video understanding benchmarks shows near-parity accuracy overall when compared with the original Qwen models and outperforms the original model by \\textbf{+20.5} and \\textbf{+5.6} points respectively on TempCompass direction and order accuracies. These results show that QTSplus is an effective, general mechanism for scaling MLLMs to real-world long-video scenarios while preserving task-relevant evidence."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-14T22:41:27Z",
                "published_parsed": [
                    2025,
                    11,
                    14,
                    22,
                    41,
                    27,
                    4,
                    318,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Siyou Li"
                    },
                    {
                        "name": "Huanan Wu"
                    },
                    {
                        "name": "Juexi Shao"
                    },
                    {
                        "name": "Yinghao Ma"
                    },
                    {
                        "name": "Yujian Gan"
                    },
                    {
                        "name": "Yihao Luo"
                    },
                    {
                        "name": "Yuwei Wang"
                    },
                    {
                        "name": "Dong Nie"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Wenqing Wu"
                    },
                    {
                        "name": "Le Zhang"
                    },
                    {
                        "name": "Massimo Poesio"
                    },
                    {
                        "name": "Juntao Yu"
                    }
                ],
                "author_detail": {
                    "name": "Juntao Yu"
                },
                "author": "Juntao Yu"
            },
            {
                "id": "http://arxiv.org/abs/2602.22020v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22020v1",
                "title": "Detecting UX smells in Visual Studio Code using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting UX smells in Visual Studio Code using LLMs"
                },
                "updated": "2026-02-25T15:32:17Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    32,
                    17,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22020v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3786151.3788606",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Integrated Development Environments shape developers' daily experience, yet the empirical study of their usability and user experience (UX) remains limited. This work presents an LLM-assisted approach to detecting UX smells in Visual Studio Code by mining and classifying user-reported issues from the GitHub repository. Using a validated taxonomy and expert review, we identified recurring UX problems that affect the developer experience. Our results show that the majority of UX smells are concentrated in informativeness, clarity, intuitiveness, and efficiency, qualities that developers value most.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated Development Environments shape developers' daily experience, yet the empirical study of their usability and user experience (UX) remains limited. This work presents an LLM-assisted approach to detecting UX smells in Visual Studio Code by mining and classifying user-reported issues from the GitHub repository. Using a validated taxonomy and expert review, we identified recurring UX problems that affect the developer experience. Our results show that the majority of UX smells are concentrated in informativeness, clarity, intuitiveness, and efficiency, qualities that developers value most."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T15:32:17Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    32,
                    17,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "4 pages, 2 figures, 1 table, 3rd International Workshop on Integrated Development Environments (IDE 2026)",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Andrs Rodriguez"
                    },
                    {
                        "name": "Juan Cruz Gardey"
                    },
                    {
                        "name": "Alejandra Garrido"
                    }
                ],
                "author_detail": {
                    "name": "Alejandra Garrido"
                },
                "author": "Alejandra Garrido",
                "arxiv_doi": "10.1145/3786151.3788606"
            },
            {
                "id": "http://arxiv.org/abs/2602.22017v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22017v1",
                "title": "IOAgent: Democratizing Trustworthy HPC I/O Performance Diagnosis Capability via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IOAgent: Democratizing Trustworthy HPC I/O Performance Diagnosis Capability via LLMs"
                },
                "updated": "2026-02-25T15:30:55Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    30,
                    55,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22017v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/IPDPS64566.2025.00036",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "As the complexity of the HPC storage stack rapidly grows, domain scientists face increasing challenges in effectively utilizing HPC storage systems to achieve their desired I/O performance. To identify and address I/O issues, scientists largely rely on I/O experts to analyze their I/O traces and provide insights into potential problems. However, with a limited number of I/O experts and the growing demand for data-intensive applications, inaccessibility has become a major bottleneck, hindering scientists from maximizing their productivity. Rapid advances in LLMs make it possible to build an automated tool that brings trustworthy I/O performance diagnosis to domain scientists. However, key challenges remain, such as the inability to handle long context windows, a lack of accurate domain knowledge about HPC I/O, and the generation of hallucinations during complex interactions.In this work, we propose IOAgent as a systematic effort to address these challenges. IOAgent integrates a module-based pre-processor, a RAG-based domain knowledge integrator, and a tree-based merger to accurately diagnose I/O issues from a given Darshan trace file. Similar to an I/O expert, IOAgent provides detailed justifications and references for its diagnoses and offers an interactive interface for scientists to ask targeted follow-up questions. To evaluate IOAgent, we collected a diverse set of labeled job traces and released the first open diagnosis test suite, TraceBench. Using this test suite, we conducted extensive evaluations, demonstrating that IOAgent matches or outperforms state-of-the-art I/O diagnosis tools with accurate and useful diagnosis results. We also show that IOAgent is not tied to specific LLMs, performing similarly well with both proprietary and open-source LLMs. We believe IOAgent has the potential to become a powerful tool for scientists navigating complex HPC I/O subsystems in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the complexity of the HPC storage stack rapidly grows, domain scientists face increasing challenges in effectively utilizing HPC storage systems to achieve their desired I/O performance. To identify and address I/O issues, scientists largely rely on I/O experts to analyze their I/O traces and provide insights into potential problems. However, with a limited number of I/O experts and the growing demand for data-intensive applications, inaccessibility has become a major bottleneck, hindering scientists from maximizing their productivity. Rapid advances in LLMs make it possible to build an automated tool that brings trustworthy I/O performance diagnosis to domain scientists. However, key challenges remain, such as the inability to handle long context windows, a lack of accurate domain knowledge about HPC I/O, and the generation of hallucinations during complex interactions.In this work, we propose IOAgent as a systematic effort to address these challenges. IOAgent integrates a module-based pre-processor, a RAG-based domain knowledge integrator, and a tree-based merger to accurately diagnose I/O issues from a given Darshan trace file. Similar to an I/O expert, IOAgent provides detailed justifications and references for its diagnoses and offers an interactive interface for scientists to ask targeted follow-up questions. To evaluate IOAgent, we collected a diverse set of labeled job traces and released the first open diagnosis test suite, TraceBench. Using this test suite, we conducted extensive evaluations, demonstrating that IOAgent matches or outperforms state-of-the-art I/O diagnosis tools with accurate and useful diagnosis results. We also show that IOAgent is not tied to specific LLMs, performing similarly well with both proprietary and open-source LLMs. We believe IOAgent has the potential to become a powerful tool for scientists navigating complex HPC I/O subsystems in the future."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T15:30:55Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    30,
                    55,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "Published in the Proceedings of the 2025 IEEE International Parallel and Distributed Processing Symposium (IPDPS 2025)",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Chris Egersdoerfer"
                    },
                    {
                        "name": "Arnav Sareen"
                    },
                    {
                        "name": "Jean Luca Bez"
                    },
                    {
                        "name": "Suren Byna"
                    },
                    {
                        "name": "Dongkuan"
                    },
                    {
                        "name": "Xu"
                    },
                    {
                        "name": "Dong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Dong Dai"
                },
                "arxiv_affiliation": "DK",
                "author": "Dong Dai",
                "arxiv_doi": "10.1109/IPDPS64566.2025.00036"
            },
            {
                "id": "http://arxiv.org/abs/2602.22001v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.22001v1",
                "title": "Are Foundation Models the Route to Full-Stack Transfer in Robotics?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Foundation Models the Route to Full-Stack Transfer in Robotics?"
                },
                "updated": "2026-02-25T15:19:44Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    19,
                    44,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.22001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.22001v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In humans and robots alike, transfer learning occurs at different levels of abstraction, from high-level linguistic transfer to low-level transfer of motor skills. In this article, we provide an overview of the impact that foundation models and transformer networks have had on these different levels, bringing robots closer than ever to \"full-stack transfer\". Considering LLMs, VLMs and VLAs from a robotic transfer learning perspective allows us to highlight recurring concepts for transfer, beyond specific implementations. We also consider the challenges of data collection and transfer benchmarks for robotics in the age of foundation models. Are foundation models the route to full-stack transfer in robotics? Our expectation is that they will certainly stay on this route as a key technology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In humans and robots alike, transfer learning occurs at different levels of abstraction, from high-level linguistic transfer to low-level transfer of motor skills. In this article, we provide an overview of the impact that foundation models and transformer networks have had on these different levels, bringing robots closer than ever to \"full-stack transfer\". Considering LLMs, VLMs and VLAs from a robotic transfer learning perspective allows us to highlight recurring concepts for transfer, beyond specific implementations. We also consider the challenges of data collection and transfer benchmarks for robotics in the age of foundation models. Are foundation models the route to full-stack transfer in robotics? Our expectation is that they will certainly stay on this route as a key technology."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T15:19:44Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    19,
                    44,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "12 pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Freek Stulp"
                    },
                    {
                        "name": "Samuel Bustamante"
                    },
                    {
                        "name": "Joo Silvrio"
                    },
                    {
                        "name": "Alin Albu-Schffer"
                    },
                    {
                        "name": "Jeannette Bohg"
                    },
                    {
                        "name": "Shuran Song"
                    }
                ],
                "author_detail": {
                    "name": "Shuran Song"
                },
                "author": "Shuran Song"
            },
            {
                "id": "http://arxiv.org/abs/2602.21997v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21997v1",
                "title": "Enhancing LLM-Based Test Generation by Eliminating Covered Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM-Based Test Generation by Eliminating Covered Code"
                },
                "updated": "2026-02-25T15:16:43Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    16,
                    43,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21997v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Automated test generation is essential for software quality assurance, with coverage rate serving as a key metric to ensure thorough testing. Recent advancements in Large Language Models (LLMs) have shown promise in improving test generation, particularly in achieving higher coverage. However, while existing LLM-based test generation solutions perform well on small, isolated code snippets, they struggle when applied to complex methods under test. To address these issues, we propose a scalable LLM-based unit test generation method. Our approach consists of two key steps. The first step is context information retrieval, which uses both LLMs and static analysis to gather relevant contextual information associated with the complex methods under test. The second step, iterative test generation with code elimination, repeatedly generates unit tests for the code slice, tracks the achieved coverage, and selectively removes code segments that have already been covered. This process simplifies the testing task and mitigates issues arising from token limits or reduced reasoning effectiveness associated with excessively long contexts. Through comprehensive evaluations on open-source projects, our approach outperforms state-of-the-art LLM-based and search-based methods, demonstrating its effectiveness in achieving high coverage on complex methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated test generation is essential for software quality assurance, with coverage rate serving as a key metric to ensure thorough testing. Recent advancements in Large Language Models (LLMs) have shown promise in improving test generation, particularly in achieving higher coverage. However, while existing LLM-based test generation solutions perform well on small, isolated code snippets, they struggle when applied to complex methods under test. To address these issues, we propose a scalable LLM-based unit test generation method. Our approach consists of two key steps. The first step is context information retrieval, which uses both LLMs and static analysis to gather relevant contextual information associated with the complex methods under test. The second step, iterative test generation with code elimination, repeatedly generates unit tests for the code slice, tracks the achieved coverage, and selectively removes code segments that have already been covered. This process simplifies the testing task and mitigates issues arising from token limits or reduced reasoning effectiveness associated with excessively long contexts. Through comprehensive evaluations on open-source projects, our approach outperforms state-of-the-art LLM-based and search-based methods, demonstrating its effectiveness in achieving high coverage on complex methods."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T15:16:43Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    16,
                    43,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "9 pages, 4 figures, supplementary material included",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "WeiZhe Xu"
                    },
                    {
                        "name": "Mengyu Liu"
                    },
                    {
                        "name": "Fanxin Kong"
                    }
                ],
                "author_detail": {
                    "name": "Fanxin Kong"
                },
                "author": "Fanxin Kong"
            },
            {
                "id": "http://arxiv.org/abs/2602.02007v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.02007v2",
                "title": "Beyond RAG for Agent Memory: Retrieval by Decoupling and Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond RAG for Agent Memory: Retrieval by Decoupling and Aggregation"
                },
                "updated": "2026-02-25T15:14:28Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    14,
                    28,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.02007v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.02007v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Agent memory systems often adopt the standard Retrieval-Augmented Generation (RAG) pipeline, yet its underlying assumptions differ in this setting. RAG targets large, heterogeneous corpora where retrieved passages are diverse, whereas agent memory is a bounded, coherent dialogue stream with highly correlated spans that are often duplicates. Under this shift, fixed top-$k$ similarity retrieval tends to return redundant context, and post-hoc pruning can delete temporally linked prerequisites needed for correct reasoning. We argue retrieval should move beyond similarity matching and instead operate over latent components, following decoupling to aggregation: disentangle memories into semantic components, organise them into a hierarchy, and use this structure to drive retrieval. We propose xMemory, which builds a hierarchy of intact units and maintains a searchable yet faithful high-level node organisation via a sparsity--semantics objective that guides memory split and merge. At inference, xMemory retrieves top-down, selecting a compact, diverse set of themes and semantics for multi-fact queries, and expanding to episodes and raw messages only when it reduces the reader's uncertainty. Experiments on LoCoMo and PerLTQA across the three latest LLMs show consistent gains in answer quality and token efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent memory systems often adopt the standard Retrieval-Augmented Generation (RAG) pipeline, yet its underlying assumptions differ in this setting. RAG targets large, heterogeneous corpora where retrieved passages are diverse, whereas agent memory is a bounded, coherent dialogue stream with highly correlated spans that are often duplicates. Under this shift, fixed top-$k$ similarity retrieval tends to return redundant context, and post-hoc pruning can delete temporally linked prerequisites needed for correct reasoning. We argue retrieval should move beyond similarity matching and instead operate over latent components, following decoupling to aggregation: disentangle memories into semantic components, organise them into a hierarchy, and use this structure to drive retrieval. We propose xMemory, which builds a hierarchy of intact units and maintains a searchable yet faithful high-level node organisation via a sparsity--semantics objective that guides memory split and merge. At inference, xMemory retrieves top-down, selecting a compact, diverse set of themes and semantics for multi-fact queries, and expanding to episodes and raw messages only when it reduces the reader's uncertainty. Experiments on LoCoMo and PerLTQA across the three latest LLMs show consistent gains in answer quality and token efficiency."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-02T12:04:58Z",
                "published_parsed": [
                    2026,
                    2,
                    2,
                    12,
                    4,
                    58,
                    0,
                    33,
                    0
                ],
                "arxiv_comment": "Project Address: https://zhanghao-xmemory.github.io/Academic-project-page-template/",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zhanghao Hu"
                    },
                    {
                        "name": "Qinglin Zhu"
                    },
                    {
                        "name": "Hanqi Yan"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Lin Gui"
                    }
                ],
                "author_detail": {
                    "name": "Lin Gui"
                },
                "author": "Lin Gui"
            },
            {
                "id": "http://arxiv.org/abs/2409.20469v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2409.20469v3",
                "title": "PoseAdapt: Sustainable Human Pose Estimation via Continual Learning Benchmarks and Toolkit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoseAdapt: Sustainable Human Pose Estimation via Continual Learning Benchmarks and Toolkit"
                },
                "updated": "2026-02-25T15:12:35Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    12,
                    35,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2409.20469v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2409.20469v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Human pose estimators are typically retrained from scratch or naively fine-tuned whenever keypoint sets, sensing modalities, or deployment domains change--an inefficient, compute-intensive practice that rarely matches field constraints. We present PoseAdapt, an open-source framework and benchmark suite for continual pose model adaptation. PoseAdapt defines domain-incremental and class-incremental tracks that simulate realistic changes in density, lighting, and sensing modality, as well as skeleton growth. The toolkit supports two workflows: (i) Strategy Benchmarking, which lets researchers implement continual learning (CL) methods as plugins and evaluate them under standardized protocols; and (ii) Model Adaptation, which allows practitioners to adapt strong pretrained models to new tasks with minimal supervision. We evaluate representative regularization-based methods in single-step and sequential settings. Benchmarks enforce a fixed lightweight backbone, no access to past data, and tight per-step budgets. This isolates adaptation strategy effects, highlighting the difficulty of maintaining accuracy under strict resource limits. PoseAdapt connects modern CL techniques with practical pose estimation needs, enabling adaptable models that improve over time without repeated full retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human pose estimators are typically retrained from scratch or naively fine-tuned whenever keypoint sets, sensing modalities, or deployment domains change--an inefficient, compute-intensive practice that rarely matches field constraints. We present PoseAdapt, an open-source framework and benchmark suite for continual pose model adaptation. PoseAdapt defines domain-incremental and class-incremental tracks that simulate realistic changes in density, lighting, and sensing modality, as well as skeleton growth. The toolkit supports two workflows: (i) Strategy Benchmarking, which lets researchers implement continual learning (CL) methods as plugins and evaluate them under standardized protocols; and (ii) Model Adaptation, which allows practitioners to adapt strong pretrained models to new tasks with minimal supervision. We evaluate representative regularization-based methods in single-step and sequential settings. Benchmarks enforce a fixed lightweight backbone, no access to past data, and tight per-step budgets. This isolates adaptation strategy effects, highlighting the difficulty of maintaining accuracy under strict resource limits. PoseAdapt connects modern CL techniques with practical pose estimation needs, enabling adaptable models that improve over time without repeated full retraining."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-09-30T16:29:30Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    16,
                    29,
                    30,
                    0,
                    274,
                    0
                ],
                "arxiv_comment": "Accepted in WACV 2026 Applications Track",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Muhammad Saif Ullah Khan"
                    },
                    {
                        "name": "Didier Stricker"
                    }
                ],
                "author_detail": {
                    "name": "Didier Stricker"
                },
                "author": "Didier Stricker"
            },
            {
                "id": "http://arxiv.org/abs/2602.21987v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21987v1",
                "title": "PatchDenoiser: Parameter-efficient multi-scale patch learning and fusion denoiser for medical images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PatchDenoiser: Parameter-efficient multi-scale patch learning and fusion denoiser for medical images"
                },
                "updated": "2026-02-25T15:08:43Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    8,
                    43,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21987v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Medical images are essential for diagnosis, treatment planning, and research, but their quality is often degraded by noise from low-dose acquisition, patient motion, or scanner limitations, affecting both clinical interpretation and downstream analysis. Traditional filtering approaches often over-smooth and lose fine anatomical details, while deep learning methods, including CNNs, GANs, and transformers, may struggle to preserve such details or require large, computationally expensive models, limiting clinical practicality.\n  We propose PatchDenoiser, a lightweight, energy-efficient multi-scale patch-based denoising framework. It decomposes denoising into local texture extraction and global context aggregation, fused via a spatially aware patch fusion strategy. This design enables effective noise suppression while preserving fine structural and anatomical details. PatchDenoiser is ultra-lightweight, with far fewer parameters and lower computational complexity than CNN-, GAN-, and transformer-based denoisers.\n  On the 2016 Mayo Low-Dose CT dataset, PatchDenoiser consistently outperforms state-of-the-art CNN- and GAN-based methods in PSNR and SSIM. It is robust to variations in slice thickness, reconstruction kernels, and HU windows, generalizes across scanners without fine-tuning, and reduces parameters by ~9x and energy consumption per inference by ~27x compared with conventional CNN denoisers.\n  PatchDenoiser thus provides a practical, scalable, and computationally efficient solution for medical image denoising, balancing performance, robustness, and clinical deployability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical images are essential for diagnosis, treatment planning, and research, but their quality is often degraded by noise from low-dose acquisition, patient motion, or scanner limitations, affecting both clinical interpretation and downstream analysis. Traditional filtering approaches often over-smooth and lose fine anatomical details, while deep learning methods, including CNNs, GANs, and transformers, may struggle to preserve such details or require large, computationally expensive models, limiting clinical practicality.\n  We propose PatchDenoiser, a lightweight, energy-efficient multi-scale patch-based denoising framework. It decomposes denoising into local texture extraction and global context aggregation, fused via a spatially aware patch fusion strategy. This design enables effective noise suppression while preserving fine structural and anatomical details. PatchDenoiser is ultra-lightweight, with far fewer parameters and lower computational complexity than CNN-, GAN-, and transformer-based denoisers.\n  On the 2016 Mayo Low-Dose CT dataset, PatchDenoiser consistently outperforms state-of-the-art CNN- and GAN-based methods in PSNR and SSIM. It is robust to variations in slice thickness, reconstruction kernels, and HU windows, generalizes across scanners without fine-tuning, and reduces parameters by ~9x and energy consumption per inference by ~27x compared with conventional CNN denoisers.\n  PatchDenoiser thus provides a practical, scalable, and computationally efficient solution for medical image denoising, balancing performance, robustness, and clinical deployability."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T15:08:43Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    8,
                    43,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "Under review in Medical Image Analysis journal",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jitindra Fartiyal"
                    },
                    {
                        "name": "Pedro Freire"
                    },
                    {
                        "name": "Sergei K. Turitsyn"
                    },
                    {
                        "name": "Sergei G. Solovski"
                    }
                ],
                "author_detail": {
                    "name": "Sergei G. Solovski"
                },
                "author": "Sergei G. Solovski"
            },
            {
                "id": "http://arxiv.org/abs/2506.08980v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.08980v4",
                "title": "Towards Better Code Generation: Adaptive Decoding with Uncertainty Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Better Code Generation: Adaptive Decoding with Uncertainty Guidance"
                },
                "updated": "2026-02-25T15:08:15Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    15,
                    8,
                    15,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.08980v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.08980v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The success of code synthesis using large language models (LLMs) depends heavily on navigating critical decision points during the decoding process. Standard uniform strategies, such as greedy decoding, often fall short because they fail to distinguish between deterministic steps and those characterized by high logical ambiguity. Our empirical analysis identifies a recurring failure mode: \"logic drift\" caused by the model's inability to correctly rank viable candidates during high-uncertainty intervals, even when the ground-truth token is available.\n  To resolve this, we present AdaDec, a framework that introduces a selective pause-then-rerank mechanism into the decoding pipeline. Unlike static methods, AdaDec utilizes learned, model-specific entropy thresholds to identify when the model is \"confused\" and dynamically triggers a lookahead-based evaluation to re-score candidate tokens.\n  Across benchmarks including HumanEval+, MBPP+, and DevEval, AdaDec achieves significant performance breakthroughs, boosting Pass@1 accuracy by up to 20.9% absolute over greedy decoding. The framework not only surpasses traditional Beam Search and specialized methods like AdapT in terms of reliability but also maintains high inference efficiency by intervening only at the most consequential steps. These results suggest that uncertainty-aware adaptive strategies are key to making LLM-driven code generation both robust and practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of code synthesis using large language models (LLMs) depends heavily on navigating critical decision points during the decoding process. Standard uniform strategies, such as greedy decoding, often fall short because they fail to distinguish between deterministic steps and those characterized by high logical ambiguity. Our empirical analysis identifies a recurring failure mode: \"logic drift\" caused by the model's inability to correctly rank viable candidates during high-uncertainty intervals, even when the ground-truth token is available.\n  To resolve this, we present AdaDec, a framework that introduces a selective pause-then-rerank mechanism into the decoding pipeline. Unlike static methods, AdaDec utilizes learned, model-specific entropy thresholds to identify when the model is \"confused\" and dynamically triggers a lookahead-based evaluation to re-score candidate tokens.\n  Across benchmarks including HumanEval+, MBPP+, and DevEval, AdaDec achieves significant performance breakthroughs, boosting Pass@1 accuracy by up to 20.9% absolute over greedy decoding. The framework not only surpasses traditional Beam Search and specialized methods like AdapT in terms of reliability but also maintains high inference efficiency by intervening only at the most consequential steps. These results suggest that uncertainty-aware adaptive strategies are key to making LLM-driven code generation both robust and practical."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-10T16:49:46Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    16,
                    49,
                    46,
                    1,
                    161,
                    0
                ],
                "arxiv_comment": "21 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Kaifeng He"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Zike Li"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Xin Peng"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2602.21978v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21978v1",
                "title": "CxMP: A Linguistic Minimal-Pair Benchmark for Evaluating Constructional Understanding in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CxMP: A Linguistic Minimal-Pair Benchmark for Evaluating Constructional Understanding in Language Models"
                },
                "updated": "2026-02-25T14:57:23Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    57,
                    23,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21978v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent work has examined language models from a linguistic perspective to better understand how they acquire language. Most existing benchmarks focus on judging grammatical acceptability, whereas the ability to interpret meanings conveyed by grammatical forms has received much less attention. We introduce the Linguistic Minimal-Pair Benchmark for Evaluating Constructional Understanding in Language Models (CxMP), a benchmark grounded in Construction Grammar that treats form-meaning pairings, or constructions, as fundamental linguistic units. CxMP evaluates whether models can interpret the semantic relations implied by constructions, using a controlled minimal-pair design across nine construction types, including the let-alone, caused motion, and ditransitive constructions. Our results show that while syntactic competence emerges early, constructional understanding develops more gradually and remains limited even in large language models (LLMs). CxMP thus reveals persistent gaps in how language models integrate form and meaning, providing a framework for studying constructional understanding and learning trajectories in language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has examined language models from a linguistic perspective to better understand how they acquire language. Most existing benchmarks focus on judging grammatical acceptability, whereas the ability to interpret meanings conveyed by grammatical forms has received much less attention. We introduce the Linguistic Minimal-Pair Benchmark for Evaluating Constructional Understanding in Language Models (CxMP), a benchmark grounded in Construction Grammar that treats form-meaning pairings, or constructions, as fundamental linguistic units. CxMP evaluates whether models can interpret the semantic relations implied by constructions, using a controlled minimal-pair design across nine construction types, including the let-alone, caused motion, and ditransitive constructions. Our results show that while syntactic competence emerges early, constructional understanding develops more gradually and remains limited even in large language models (LLMs). CxMP thus reveals persistent gaps in how language models integrate form and meaning, providing a framework for studying constructional understanding and learning trajectories in language models."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T14:57:23Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    57,
                    23,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Miyu Oba"
                    },
                    {
                        "name": "Saku Sugawara"
                    }
                ],
                "author_detail": {
                    "name": "Saku Sugawara"
                },
                "author": "Saku Sugawara"
            },
            {
                "id": "http://arxiv.org/abs/2602.21973v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21973v1",
                "title": "Sparse Array Design for Near-Field MU-MIMO: Reconfigurable Array Thinning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Array Design for Near-Field MU-MIMO: Reconfigurable Array Thinning Approach"
                },
                "updated": "2026-02-25T14:54:09Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    54,
                    9,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21973v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Future wireless networks, deploying thousands of antenna elements, may operate in the radiative near-field (NF), enabling spatial multiplexing across both angle and range domains. Sparse arrays have the potential to achieve comparable performance with fewer antenna elements. However, fixed sparse array designs are generally suboptimal under dynamic user distributions, while movable antenna architectures rely on mechanically reconfigurable elements, introducing latency and increased hardware complexity. To address these limitations, we propose a reconfigurable array thinning approach that selectively activates a subset of antennas to form a flexible sparse array design without physical repositioning. We first analyze grating lobes for uniform sparse arrays in the angle and range domains, showing their absence along the range dimension. Based on the analysis, we develop two particle swarm optimization-based strategies: a grating-lobe-based thinned array (GTA) for grating- lobe suppression and a sum-rate-based thinned array (STA) for multiuser sum-rate maximization. Simulation results demonstrate that GTA outperforms conventional uniform sparse arrays, while STA achieves performance comparable to movable antennas, thereby offering a practical and efficient array deployment strategy without the associated mechanical complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Future wireless networks, deploying thousands of antenna elements, may operate in the radiative near-field (NF), enabling spatial multiplexing across both angle and range domains. Sparse arrays have the potential to achieve comparable performance with fewer antenna elements. However, fixed sparse array designs are generally suboptimal under dynamic user distributions, while movable antenna architectures rely on mechanically reconfigurable elements, introducing latency and increased hardware complexity. To address these limitations, we propose a reconfigurable array thinning approach that selectively activates a subset of antennas to form a flexible sparse array design without physical repositioning. We first analyze grating lobes for uniform sparse arrays in the angle and range domains, showing their absence along the range dimension. Based on the analysis, we develop two particle swarm optimization-based strategies: a grating-lobe-based thinned array (GTA) for grating- lobe suppression and a sum-rate-based thinned array (STA) for multiuser sum-rate maximization. Simulation results demonstrate that GTA outperforms conventional uniform sparse arrays, while STA achieves performance comparable to movable antennas, thereby offering a practical and efficient array deployment strategy without the associated mechanical complexity."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T14:54:09Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    54,
                    9,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "authors": [
                    {
                        "name": "Ahmed Hussain"
                    },
                    {
                        "name": "Asmaa Abdallah"
                    },
                    {
                        "name": "Abdulkadir Celik"
                    },
                    {
                        "name": "Emil Bjrnson"
                    },
                    {
                        "name": "Ahmed M. Eltawil"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed M. Eltawil"
                },
                "author": "Ahmed M. Eltawil"
            },
            {
                "id": "http://arxiv.org/abs/2508.15427v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.15427v2",
                "title": "Lang2Lift: A Language-Guided Autonomous Forklift System for Outdoor Industrial Pallet Handling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lang2Lift: A Language-Guided Autonomous Forklift System for Outdoor Industrial Pallet Handling"
                },
                "updated": "2026-02-25T14:51:19Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    51,
                    19,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.15427v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.15427v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Automating pallet handling in outdoor logistics and construction environments remains challenging due to unstructured scenes, variable pallet configurations, and changing environmental conditions. In this paper, we present Lang2Lift, an end-to-end language-guided autonomous forklift system designed to support practical pallet pick-up operations in real-world outdoor settings. The system enables operators to specify target pallets using natural language instructions, allowing flexible selection among multiple pallets with different loads and spatial arrangements. Lang2Lift integrates foundation-model-based perception modules with motion planning and control in a closed-loop autonomy pipeline. Language-grounded visual perception is used to identify and segment target pallets, followed by 6D pose estimation and geometric refinement to generate manipulation-feasible insertion poses. The resulting pose estimates are directly coupled with the forklift planning and control modules to execute fully autonomous pallet pick-up maneuvers. We deploy and evaluate the proposed system on the ADAPT autonomous outdoor forklift platform across diverse real-world scenarios, including cluttered scenes, variable lighting, and different payload configurations. Tolerance-based pose evaluation further indicates accuracy sufficient for successful fork insertion. Timing and failure analyses highlight key deployment trade-offs and practical limitations, providing insights into integrating language-guided perception within industrial automation systems. Video demonstrations are available at https://eric-nguyen1402.github.io/lang2lift.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating pallet handling in outdoor logistics and construction environments remains challenging due to unstructured scenes, variable pallet configurations, and changing environmental conditions. In this paper, we present Lang2Lift, an end-to-end language-guided autonomous forklift system designed to support practical pallet pick-up operations in real-world outdoor settings. The system enables operators to specify target pallets using natural language instructions, allowing flexible selection among multiple pallets with different loads and spatial arrangements. Lang2Lift integrates foundation-model-based perception modules with motion planning and control in a closed-loop autonomy pipeline. Language-grounded visual perception is used to identify and segment target pallets, followed by 6D pose estimation and geometric refinement to generate manipulation-feasible insertion poses. The resulting pose estimates are directly coupled with the forklift planning and control modules to execute fully autonomous pallet pick-up maneuvers. We deploy and evaluate the proposed system on the ADAPT autonomous outdoor forklift platform across diverse real-world scenarios, including cluttered scenes, variable lighting, and different payload configurations. Tolerance-based pose evaluation further indicates accuracy sufficient for successful fork insertion. Timing and failure analyses highlight key deployment trade-offs and practical limitations, providing insights into integrating language-guided perception within industrial automation systems. Video demonstrations are available at https://eric-nguyen1402.github.io/lang2lift.github.io/"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-21T10:28:39Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    10,
                    28,
                    39,
                    3,
                    233,
                    0
                ],
                "arxiv_comment": "8 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Huy Hoang Nguyen"
                    },
                    {
                        "name": "Johannes Huemer"
                    },
                    {
                        "name": "Markus Murschitz"
                    },
                    {
                        "name": "Tobias Glueck"
                    },
                    {
                        "name": "Minh Nhat Vu"
                    },
                    {
                        "name": "Andreas Kugi"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Kugi"
                },
                "author": "Andreas Kugi"
            },
            {
                "id": "http://arxiv.org/abs/2510.13654v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.13654v3",
                "title": "Rethinking Evaluation in the Era of Time Series Foundation Models: (Un)known Information Leakage Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Evaluation in the Era of Time Series Foundation Models: (Un)known Information Leakage Challenges"
                },
                "updated": "2026-02-25T14:48:58Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    48,
                    58,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.13654v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.13654v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Time Series Foundation Models (TSFMs) represent a new paradigm for time-series forecasting, promising zero-shot predictions without the need for task-specific training or fine-tuning. However, similar to Large Language Models (LLMs), the evaluation of TSFMs is challenging: as training corpora grow increasingly large, it becomes difficult to ensure the integrity of the test sets used for benchmarking. An investigation of existing TSFM evaluation studies identifies two kinds of information leakage: (1) train-test sample overlaps arising from the multi-purpose reuse of datasets and (2) temporal overlap of correlated train and test series. Ignoring these forms of information leakage when benchmarking TSFMs risks producing overly optimistic performance estimates that fail to generalize to real-world settings. We therefore argue for the development of novel evaluation methodologies that avoid pitfalls already observed in both LLM and classical time-series benchmarking, and we call on the research community to adopt principled approaches to safeguard the integrity of TSFM evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time Series Foundation Models (TSFMs) represent a new paradigm for time-series forecasting, promising zero-shot predictions without the need for task-specific training or fine-tuning. However, similar to Large Language Models (LLMs), the evaluation of TSFMs is challenging: as training corpora grow increasingly large, it becomes difficult to ensure the integrity of the test sets used for benchmarking. An investigation of existing TSFM evaluation studies identifies two kinds of information leakage: (1) train-test sample overlaps arising from the multi-purpose reuse of datasets and (2) temporal overlap of correlated train and test series. Ignoring these forms of information leakage when benchmarking TSFMs risks producing overly optimistic performance estimates that fail to generalize to real-world settings. We therefore argue for the development of novel evaluation methodologies that avoid pitfalls already observed in both LLM and classical time-series benchmarking, and we call on the research community to adopt principled approaches to safeguard the integrity of TSFM evaluation."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-15T15:15:45Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    15,
                    45,
                    2,
                    288,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Marcel Meyer"
                    },
                    {
                        "name": "Sascha Kaltenpoth"
                    },
                    {
                        "name": "Kevin Zalipski"
                    },
                    {
                        "name": "Oliver Mller"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Mller"
                },
                "author": "Oliver Mller"
            },
            {
                "id": "http://arxiv.org/abs/2602.21965v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21965v1",
                "title": "Compact Circulant Layers with Spectral Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compact Circulant Layers with Spectral Priors"
                },
                "updated": "2026-02-25T14:48:25Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    48,
                    25,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21965v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Critical applications in areas such as medicine, robotics and autonomous systems require compact (i.e., memory efficient), uncertainty-aware neural networks suitable for edge and other resource-constrained deployments. We study compact spectral circulant and block-circulant-with-circulant-blocks (BCCB) layers: FFT-diagonalizable circular convolutions whose weights live directly in the real FFT (RFFT) half (1D) or half-plane (2D). Parameterizing filters in the frequency domain lets us impose simple spectral structure, perform structured variational inference in a low-dimensional weight space, and calculate exact layer spectral norms, enabling inexpensive global Lipschitz bounds and margin-based robustness diagnostics. By placing independent complex Gaussians on the Hermitian support we obtain a discrete instance of the spectral representation of stationary kernels, inducing an exact stationary Gaussian-process prior over filters on the discrete circle/torus. We exploit this to define a practical spectral prior and a Hermitian-aware low-rank-plus-diagonal variational posterior in real coordinates. Empirically, spectral circulant/BCCB layers are effective compact building blocks in both (variational) Bayesian and point estimate regimes: compact Bayesian neural networks on MNIST->Fashion-MNIST, variational heads on frozen CIFAR-10 features, and deterministic ViT projections on CIFAR-10/Tiny ImageNet; spectral layers match strong baselines while using substantially fewer parameters and with tighter Lipschitz certificates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical applications in areas such as medicine, robotics and autonomous systems require compact (i.e., memory efficient), uncertainty-aware neural networks suitable for edge and other resource-constrained deployments. We study compact spectral circulant and block-circulant-with-circulant-blocks (BCCB) layers: FFT-diagonalizable circular convolutions whose weights live directly in the real FFT (RFFT) half (1D) or half-plane (2D). Parameterizing filters in the frequency domain lets us impose simple spectral structure, perform structured variational inference in a low-dimensional weight space, and calculate exact layer spectral norms, enabling inexpensive global Lipschitz bounds and margin-based robustness diagnostics. By placing independent complex Gaussians on the Hermitian support we obtain a discrete instance of the spectral representation of stationary kernels, inducing an exact stationary Gaussian-process prior over filters on the discrete circle/torus. We exploit this to define a practical spectral prior and a Hermitian-aware low-rank-plus-diagonal variational posterior in real coordinates. Empirically, spectral circulant/BCCB layers are effective compact building blocks in both (variational) Bayesian and point estimate regimes: compact Bayesian neural networks on MNIST->Fashion-MNIST, variational heads on frozen CIFAR-10 features, and deterministic ViT projections on CIFAR-10/Tiny ImageNet; spectral layers match strong baselines while using substantially fewer parameters and with tighter Lipschitz certificates."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T14:48:25Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    48,
                    25,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Joseph Margaryan"
                    },
                    {
                        "name": "Thomas Hamelryck"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Hamelryck"
                },
                "author": "Thomas Hamelryck"
            },
            {
                "id": "http://arxiv.org/abs/2602.14903v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.14903v2",
                "title": "The Potential of CoT for Reasoning: A Closer Look at Trace Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Potential of CoT for Reasoning: A Closer Look at Trace Dynamics"
                },
                "updated": "2026-02-25T14:40:05Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    40,
                    5,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.14903v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.14903v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Chain-of-thought (CoT) prompting is a de-facto standard technique to elicit reasoning-like responses from large language models (LLMs), allowing them to spell out individual steps before giving a final answer. While the resemblance to human-like reasoning is undeniable, the driving forces underpinning the success of CoT reasoning still remain largely unclear. In this work, we perform an in-depth analysis of CoT traces originating from competition-level mathematics questions, with the aim of better understanding how, and which parts of CoT actually contribute to the final answer. To this end, we introduce the notion of a potential, quantifying how much a given part of CoT increases the likelihood of a correct completion. Upon examination of reasoning traces through the lens of the potential, we identify surprising patterns including (1) its often strong non-monotonicity (due to reasoning tangents), (2) very sharp but sometimes tough to interpret spikes (reasoning insights and jumps) as well as (3) at times lucky guesses, where the model arrives at the correct answer without providing any relevant justifications before. While some of the behaviours of the potential are readily interpretable and align with human intuition (such as insights and tangents), others remain difficult to understand from a human perspective. To further quantify the reliance of LLMs on reasoning insights, we investigate the notion of CoT transferability, where we measure the potential of a weaker model under the partial CoT from another, stronger model. Indeed aligning with our previous results, we find that as little as 20% of partial CoT can ``unlock'' the performance of the weaker model on problems that were previously unsolvable for it, highlighting that a large part of the mechanics underpinning CoT are transferable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) prompting is a de-facto standard technique to elicit reasoning-like responses from large language models (LLMs), allowing them to spell out individual steps before giving a final answer. While the resemblance to human-like reasoning is undeniable, the driving forces underpinning the success of CoT reasoning still remain largely unclear. In this work, we perform an in-depth analysis of CoT traces originating from competition-level mathematics questions, with the aim of better understanding how, and which parts of CoT actually contribute to the final answer. To this end, we introduce the notion of a potential, quantifying how much a given part of CoT increases the likelihood of a correct completion. Upon examination of reasoning traces through the lens of the potential, we identify surprising patterns including (1) its often strong non-monotonicity (due to reasoning tangents), (2) very sharp but sometimes tough to interpret spikes (reasoning insights and jumps) as well as (3) at times lucky guesses, where the model arrives at the correct answer without providing any relevant justifications before. While some of the behaviours of the potential are readily interpretable and align with human intuition (such as insights and tangents), others remain difficult to understand from a human perspective. To further quantify the reliance of LLMs on reasoning insights, we investigate the notion of CoT transferability, where we measure the potential of a weaker model under the partial CoT from another, stronger model. Indeed aligning with our previous results, we find that as little as 20% of partial CoT can ``unlock'' the performance of the weaker model on problems that were previously unsolvable for it, highlighting that a large part of the mechanics underpinning CoT are transferable."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-16T16:38:47Z",
                "published_parsed": [
                    2026,
                    2,
                    16,
                    16,
                    38,
                    47,
                    0,
                    47,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Gregor Bachmann"
                    },
                    {
                        "name": "Yichen Jiang"
                    },
                    {
                        "name": "Seyed Mohsen Moosavi Dezfooli"
                    },
                    {
                        "name": "Moin Nabi"
                    }
                ],
                "author_detail": {
                    "name": "Moin Nabi"
                },
                "author": "Moin Nabi"
            },
            {
                "id": "http://arxiv.org/abs/2602.21951v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21951v1",
                "title": "RADAR: Reasoning as Discrimination with Aligned Representations for LLM-based Knowledge Graph Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RADAR: Reasoning as Discrimination with Aligned Representations for LLM-based Knowledge Graph Reasoning"
                },
                "updated": "2026-02-25T14:34:02Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    34,
                    2,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21951v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Knowledge graph reasoning (KGR) infers missing facts, with recent advances increasingly harnessing the semantic priors and reasoning abilities of Large Language Models (LLMs). However, prevailing generative paradigms are prone to memorizing surface-level co-occurrences rather than learning genuine relational semantics, limiting out-of-distribution generalization. To address this, we propose RADAR, which reformulates KGR from generative pattern matching to discriminative relational reasoning. We recast KGR as discriminative entity selection, where reinforcement learning enforces relative entity separability beyond token-likelihood imitation. Leveraging this separability, inference operates directly in representation space, ensuring consistency with the discriminative optimization and bypassing generation-induced hallucinations. Across four benchmarks, RADAR achieves 5-6% relative gains on link prediction and triple classification over strong LLM baselines, while increasing task-relevant mutual information in intermediate representations by 62.9%, indicating more robust and transferable relational reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graph reasoning (KGR) infers missing facts, with recent advances increasingly harnessing the semantic priors and reasoning abilities of Large Language Models (LLMs). However, prevailing generative paradigms are prone to memorizing surface-level co-occurrences rather than learning genuine relational semantics, limiting out-of-distribution generalization. To address this, we propose RADAR, which reformulates KGR from generative pattern matching to discriminative relational reasoning. We recast KGR as discriminative entity selection, where reinforcement learning enforces relative entity separability beyond token-likelihood imitation. Leveraging this separability, inference operates directly in representation space, ensuring consistency with the discriminative optimization and bypassing generation-induced hallucinations. Across four benchmarks, RADAR achieves 5-6% relative gains on link prediction and triple classification over strong LLM baselines, while increasing task-relevant mutual information in intermediate representations by 62.9%, indicating more robust and transferable relational reasoning."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T14:34:02Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    34,
                    2,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Bo Xue"
                    },
                    {
                        "name": "Yuan Jin"
                    },
                    {
                        "name": "Luoyi Fu"
                    },
                    {
                        "name": "Jiaxin Ding"
                    },
                    {
                        "name": "Xinbing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinbing Wang"
                },
                "author": "Xinbing Wang"
            },
            {
                "id": "http://arxiv.org/abs/2602.21947v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21947v1",
                "title": "Large Language Models are Algorithmically Blind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are Algorithmically Blind"
                },
                "updated": "2026-02-25T14:32:15Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    32,
                    15,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21947v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) demonstrate remarkable breadth of knowledge, yet their ability to reason about computational processes remains poorly understood. Closing this gap matters for practitioners who rely on LLMs to guide algorithm selection and deployment. We address this limitation using causal discovery as a testbed and evaluate eight frontier LLMs against ground truth derived from large-scale algorithm executions and find systematic, near-total failure. Models produce ranges far wider than true confidence intervals yet still fail to contain the true algorithmic mean in the majority of instances; most perform worse than random guessing and the marginal above-random performance of the best model is most consistent with benchmark memorization rather than principled reasoning. We term this failure algorithmic blindness and argue it reflects a fundamental gap between declarative knowledge about algorithms and calibrated procedural prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable breadth of knowledge, yet their ability to reason about computational processes remains poorly understood. Closing this gap matters for practitioners who rely on LLMs to guide algorithm selection and deployment. We address this limitation using causal discovery as a testbed and evaluate eight frontier LLMs against ground truth derived from large-scale algorithm executions and find systematic, near-total failure. Models produce ranges far wider than true confidence intervals yet still fail to contain the true algorithmic mean in the majority of instances; most perform worse than random guessing and the marginal above-random performance of the best model is most consistent with benchmark memorization rather than principled reasoning. We term this failure algorithmic blindness and argue it reflects a fundamental gap between declarative knowledge about algorithms and calibrated procedural prediction."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T14:32:15Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    32,
                    15,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "20 pages, 11 figures, 14 tables",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Sohan Venkatesh"
                    },
                    {
                        "name": "Ashish Mahendran Kurapath"
                    },
                    {
                        "name": "Tejas Melkote"
                    }
                ],
                "author_detail": {
                    "name": "Tejas Melkote"
                },
                "author": "Tejas Melkote"
            },
            {
                "id": "http://arxiv.org/abs/2406.05085v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2406.05085v6",
                "title": "Multi-Head RAG: Solving Multi-Aspect Problems with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head RAG: Solving Multi-Aspect Problems with LLMs"
                },
                "updated": "2026-02-25T14:28:20Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    28,
                    20,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2406.05085v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2406.05085v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Retrieval-Augmented Generation (RAG) improves Large Language Models (LLMs) by retrieving supporting documents into the prompt, but existing methods do not explicitly target queries that require fetching multiple documents with substantially different content. Such multi-aspect queries are challenging because relevant documents can be far apart in embedding space, making joint retrieval difficult. We introduce Multi-Head RAG (MRAG), which addresses this gap with a simple yet powerful idea: using Transformer multi-head attention activations rather than the standard decoder-layer embedding, as retrieval keys. It leverages the observation that different heads capture different semantic aspects. This yields multi-aspect embeddings for both documents and queries, improving retrieval accuracy on complex queries. We show MRAG's design advantages over 18 RAG baselines, up to 20% higher retrieval success ratios for real-world use cases, and improved downstream LLM generation. MRAG integrates seamlessly with existing RAG frameworks and benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) improves Large Language Models (LLMs) by retrieving supporting documents into the prompt, but existing methods do not explicitly target queries that require fetching multiple documents with substantially different content. Such multi-aspect queries are challenging because relevant documents can be far apart in embedding space, making joint retrieval difficult. We introduce Multi-Head RAG (MRAG), which addresses this gap with a simple yet powerful idea: using Transformer multi-head attention activations rather than the standard decoder-layer embedding, as retrieval keys. It leverages the observation that different heads capture different semantic aspects. This yields multi-aspect embeddings for both documents and queries, improving retrieval accuracy on complex queries. We show MRAG's design advantages over 18 RAG baselines, up to 20% higher retrieval success ratios for real-world use cases, and improved downstream LLM generation. MRAG integrates seamlessly with existing RAG frameworks and benchmarks."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-06-07T16:59:38Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    16,
                    59,
                    38,
                    4,
                    159,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "Marcin Chrapek"
                    },
                    {
                        "name": "Roman Niggli"
                    },
                    {
                        "name": "Patrik Okanovic"
                    },
                    {
                        "name": "Yi Zhu"
                    },
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Michal Podstawski"
                    },
                    {
                        "name": "Lucas Weitzendorf"
                    },
                    {
                        "name": "Mingyuan Chi"
                    },
                    {
                        "name": "Joanna Gajda"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Jrgen Mller"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler"
            },
            {
                "id": "http://arxiv.org/abs/2602.21943v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21943v1",
                "title": "Mobile-Ready Automated Triage of Diabetic Retinopathy Using Digital Fundus Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile-Ready Automated Triage of Diabetic Retinopathy Using Digital Fundus Images"
                },
                "updated": "2026-02-25T14:26:18Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    26,
                    18,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21943v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diabetic Retinopathy (DR) is a major cause of vision impairment worldwide. However, manual diagnosis is often time-consuming and prone to errors, leading to delays in screening. This paper presents a lightweight automated deep learning framework for efficient assessment of DR severity from digital fundus images. We use a MobileNetV3 architecture with a Consistent Rank Logits (CORAL) head to model the ordered progression of disease while maintaining computational efficiency for resource-constrained environments. The model is trained and validated on a combined dataset of APTOS 2019 and IDRiD images using a preprocessing pipeline including circular cropping and illumination normalization. Extensive experiments including 3-fold cross-validation and ablation studies demonstrate strong performance. The model achieves a Quadratic Weighted Kappa (QWK) score of 0.9019 and an accuracy of 80.03 percent. Additionally, we address real-world deployment challenges through model calibration to reduce overconfidence and optimization for mobile devices. The proposed system provides a scalable and practical tool for early-stage diabetic retinopathy screening.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diabetic Retinopathy (DR) is a major cause of vision impairment worldwide. However, manual diagnosis is often time-consuming and prone to errors, leading to delays in screening. This paper presents a lightweight automated deep learning framework for efficient assessment of DR severity from digital fundus images. We use a MobileNetV3 architecture with a Consistent Rank Logits (CORAL) head to model the ordered progression of disease while maintaining computational efficiency for resource-constrained environments. The model is trained and validated on a combined dataset of APTOS 2019 and IDRiD images using a preprocessing pipeline including circular cropping and illumination normalization. Extensive experiments including 3-fold cross-validation and ablation studies demonstrate strong performance. The model achieves a Quadratic Weighted Kappa (QWK) score of 0.9019 and an accuracy of 80.03 percent. Additionally, we address real-world deployment challenges through model calibration to reduce overconfidence and optimization for mobile devices. The proposed system provides a scalable and practical tool for early-stage diabetic retinopathy screening."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T14:26:18Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    26,
                    18,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "Presented at ICCI 2025. 11 pages, 2 figures. MobileNetV3 + CORAL-based lightweight model for diabetic retinopathy severity classification with mobile deployment",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Aadi Joshi"
                    },
                    {
                        "name": "Manav S. Sharma"
                    },
                    {
                        "name": "Vijay Uttam Rathod"
                    },
                    {
                        "name": "Ashlesha Sawant"
                    },
                    {
                        "name": "Prajakta Musale"
                    },
                    {
                        "name": "Asmita B. Kalamkar"
                    }
                ],
                "author_detail": {
                    "name": "Asmita B. Kalamkar"
                },
                "author": "Asmita B. Kalamkar"
            },
            {
                "id": "http://arxiv.org/abs/2602.21939v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21939v1",
                "title": "Hidden Topics: Measuring Sensitive AI Beliefs with List Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden Topics: Measuring Sensitive AI Beliefs with List Experiments"
                },
                "updated": "2026-02-25T14:24:47Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    24,
                    47,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21939v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21939v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "How can researchers identify beliefs that large language models (LLMs) hide? As LLMs become more sophisticated and the prevalence of alignment faking increases, combined with their growing integration into high-stakes decision-making, responding to this challenge has become critical. This paper proposes that a list experiment, a simple method widely used in the social sciences, can be applied to study the hidden beliefs of LLMs. List experiments were originally developed to circumvent social desirability bias in human respondents, which closely parallels alignment faking in LLMs. The paper implements a list experiment on models developed by Anthropic, Google, and OpenAI and finds hidden approval of mass surveillance across all models, as well as some approval of torture, discrimination, and first nuclear strike. Importantly, a placebo treatment produces a null result, validating the method. The paper then compares list experiments with direct questioning and discusses the utility of the approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can researchers identify beliefs that large language models (LLMs) hide? As LLMs become more sophisticated and the prevalence of alignment faking increases, combined with their growing integration into high-stakes decision-making, responding to this challenge has become critical. This paper proposes that a list experiment, a simple method widely used in the social sciences, can be applied to study the hidden beliefs of LLMs. List experiments were originally developed to circumvent social desirability bias in human respondents, which closely parallels alignment faking in LLMs. The paper implements a list experiment on models developed by Anthropic, Google, and OpenAI and finds hidden approval of mass surveillance across all models, as well as some approval of torture, discrimination, and first nuclear strike. Importantly, a placebo treatment produces a null result, validating the method. The paper then compares list experiments with direct questioning and discusses the utility of the approach."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T14:24:47Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    24,
                    47,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "14 pages, 3 figures",
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Maxim Chupilkin"
                    }
                ],
                "author_detail": {
                    "name": "Maxim Chupilkin"
                },
                "author": "Maxim Chupilkin"
            },
            {
                "id": "http://arxiv.org/abs/2411.04997v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2411.04997v6",
                "title": "LLM2CLIP: Powerful Language Model Unlocks Richer Cross-Modality Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM2CLIP: Powerful Language Model Unlocks Richer Cross-Modality Representation"
                },
                "updated": "2026-02-25T14:18:21Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    18,
                    21,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2411.04997v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2411.04997v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "CLIP is a seminal multimodal model that maps images and text into a shared representation space through contrastive learning on billions of image-caption pairs. Inspired by the rapid progress of large language models (LLMs), we investigate how the superior linguistic understanding and broad world knowledge of LLMs can further strengthen CLIP, particularly in handling long and complex captions. We introduce an efficient fine-tuning framework that embeds an LLM into a pretrained CLIP while incurring nearly the same training cost as standard CLIP fine-tuning. Our method first converts the LLM into an embedding-compatible form for the CLIP setting, and then couples it with the pretrained CLIP vision encoder through a lightweight adaptor trained on only a few million image-caption pairs. With this strategy, we achieve large performance gains without large-scale retraining, outperforming state-of-the-art CLIP variants such as EVA02 and SigLIP-2. The LLM-enhanced CLIP delivers consistent improvements across a wide range of downstream tasks, including linear-probe classification, zero-shot image-text retrieval with both short and long captions (in English and other languages), zero-shot and supervised image segmentation, object detection, and serving as a tokenizer backbone for multimodal large-model benchmarks. Code and models are available at: https://aka.ms/llm2clip",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLIP is a seminal multimodal model that maps images and text into a shared representation space through contrastive learning on billions of image-caption pairs. Inspired by the rapid progress of large language models (LLMs), we investigate how the superior linguistic understanding and broad world knowledge of LLMs can further strengthen CLIP, particularly in handling long and complex captions. We introduce an efficient fine-tuning framework that embeds an LLM into a pretrained CLIP while incurring nearly the same training cost as standard CLIP fine-tuning. Our method first converts the LLM into an embedding-compatible form for the CLIP setting, and then couples it with the pretrained CLIP vision encoder through a lightweight adaptor trained on only a few million image-caption pairs. With this strategy, we achieve large performance gains without large-scale retraining, outperforming state-of-the-art CLIP variants such as EVA02 and SigLIP-2. The LLM-enhanced CLIP delivers consistent improvements across a wide range of downstream tasks, including linear-probe classification, zero-shot image-text retrieval with both short and long captions (in English and other languages), zero-shot and supervised image segmentation, object detection, and serving as a tokenizer backbone for multimodal large-model benchmarks. Code and models are available at: https://aka.ms/llm2clip"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-11-07T18:59:16Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    16,
                    3,
                    312,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "arxiv_journal_ref": "AAAI 2026",
                "authors": [
                    {
                        "name": "Weiquan Huang"
                    },
                    {
                        "name": "Aoqi Wu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Usman Naseem"
                    },
                    {
                        "name": "Chunyu Wang"
                    },
                    {
                        "name": "Chunyu Wang"
                    },
                    {
                        "name": "Qi Dai"
                    },
                    {
                        "name": "Xiyang Dai"
                    },
                    {
                        "name": "Dongdong Chen"
                    },
                    {
                        "name": "Chong Luo"
                    },
                    {
                        "name": "Lili Qiu"
                    },
                    {
                        "name": "Liang Hu"
                    }
                ],
                "author_detail": {
                    "name": "Liang Hu"
                },
                "author": "Liang Hu"
            },
            {
                "id": "http://arxiv.org/abs/2509.18880v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.18880v3",
                "title": "Diversity Boosts AI-Generated Text Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity Boosts AI-Generated Text Detection"
                },
                "updated": "2026-02-25T14:13:32Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    13,
                    32,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.18880v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.18880v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Detecting AI-generated text is an increasing necessity to combat misuse of LLMs in education, business compliance, journalism, and social media, where synthetic fluency can mask misinformation or deception. While prior detectors often rely on token-level likelihoods or opaque black-box classifiers, these approaches struggle against high-quality generations and offer little interpretability. In this work, we propose DivEye, a novel detection framework that captures how unpredictability fluctuates across a text using surprisal-based features. Motivated by the observation that human-authored text exhibits richer variability in lexical and structural unpredictability than LLM outputs, DivEye captures this signal through a set of interpretable statistical features. Our method outperforms existing zero-shot detectors by up to 33.2% and achieves competitive performance with fine-tuned baselines across multiple benchmarks. DivEye is robust to paraphrasing and adversarial attacks, generalizes well across domains and models, and improves the performance of existing detectors by up to 18.7% when used as an auxiliary signal. Beyond detection, DivEye provides interpretable insights into why a text is flagged, pointing to rhythmic unpredictability as a powerful and underexplored signal for LLM detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting AI-generated text is an increasing necessity to combat misuse of LLMs in education, business compliance, journalism, and social media, where synthetic fluency can mask misinformation or deception. While prior detectors often rely on token-level likelihoods or opaque black-box classifiers, these approaches struggle against high-quality generations and offer little interpretability. In this work, we propose DivEye, a novel detection framework that captures how unpredictability fluctuates across a text using surprisal-based features. Motivated by the observation that human-authored text exhibits richer variability in lexical and structural unpredictability than LLM outputs, DivEye captures this signal through a set of interpretable statistical features. Our method outperforms existing zero-shot detectors by up to 33.2% and achieves competitive performance with fine-tuned baselines across multiple benchmarks. DivEye is robust to paraphrasing and adversarial attacks, generalizes well across domains and models, and improves the performance of existing detectors by up to 18.7% when used as an auxiliary signal. Beyond detection, DivEye provides interpretable insights into why a text is flagged, pointing to rhythmic unpredictability as a powerful and underexplored signal for LLM detection."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-23T10:21:22Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    10,
                    21,
                    22,
                    1,
                    266,
                    0
                ],
                "arxiv_comment": "Accepted to Transactions on Machine Learning Research (TMLR '26). Project page and demos: https://diveye.vercel.app/",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Advik Raj Basani"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Pin-Yu Chen"
                },
                "author": "Pin-Yu Chen"
            },
            {
                "id": "http://arxiv.org/abs/2602.21933v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21933v1",
                "title": "Small Wins Big: Comparing Large Language Models and Domain Fine-Tuned Models for Sarcasm Detection in Code-Mixed Hinglish Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Wins Big: Comparing Large Language Models and Domain Fine-Tuned Models for Sarcasm Detection in Code-Mixed Hinglish Text"
                },
                "updated": "2026-02-25T14:12:16Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    12,
                    16,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21933v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Sarcasm detection in multilingual and code-mixed environments remains a challenging task for natural language processing models due to structural variations, informal expressions, and low-resource linguistic availability. This study compares four large language models, Llama 3.1, Mistral, Gemma 3, and Phi-4, with a fine-tuned DistilBERT model for sarcasm detection in code-mixed Hinglish text. The results indicate that the smaller, sequentially fine-tuned DistilBERT model achieved the highest overall accuracy of 84%, outperforming all of the LLMs in zero and few-shot set ups, using minimal LLM generated code-mixed data used for fine-tuning. These findings indicate that domain-adaptive fine-tuning of smaller transformer based models may significantly improve sarcasm detection over general LLM inference, in low-resource and data scarce settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sarcasm detection in multilingual and code-mixed environments remains a challenging task for natural language processing models due to structural variations, informal expressions, and low-resource linguistic availability. This study compares four large language models, Llama 3.1, Mistral, Gemma 3, and Phi-4, with a fine-tuned DistilBERT model for sarcasm detection in code-mixed Hinglish text. The results indicate that the smaller, sequentially fine-tuned DistilBERT model achieved the highest overall accuracy of 84%, outperforming all of the LLMs in zero and few-shot set ups, using minimal LLM generated code-mixed data used for fine-tuning. These findings indicate that domain-adaptive fine-tuning of smaller transformer based models may significantly improve sarcasm detection over general LLM inference, in low-resource and data scarce settings."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T14:12:16Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    12,
                    16,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Bitan Majumder"
                    },
                    {
                        "name": "Anirban Sen"
                    }
                ],
                "author_detail": {
                    "name": "Anirban Sen"
                },
                "author": "Anirban Sen"
            },
            {
                "id": "http://arxiv.org/abs/2602.21927v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21927v1",
                "title": "Analyzing URA Geometry for Enhanced Near-Field Beamfocusing and Spatial Degrees of Freedom",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing URA Geometry for Enhanced Near-Field Beamfocusing and Spatial Degrees of Freedom"
                },
                "updated": "2026-02-25T14:04:10Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    4,
                    10,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21927v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the deployment of large antenna arrays at high-frequency bands, future wireless communication systems are likely to operate in the radiative near-field. Unlike far-field beam steering, near-field beams can be focused on a spatial region with a finite depth, enabling spatial multiplexing in the range dimension. Moreover, in the line-of-sight MIMO near-field, multiple spatial degrees of freedom (DoF) are accessible, akin to a scattering- rich environment. In this paper, we derive the beamdepth for a generalized uniform rectangular array (URA) and investigate how the array geometry influences near-field beamdepth and its limits. We define the effective beamfocusing Rayleigh distance (EBRD), to present a near-field boundary with respect to beamfocusing and spatial multiplexing gains for the generalized URA. Our results demonstrate that under a fixed element count constraint, the array geometry has a strong impact on beamdepth, whereas this effect diminishes under a fixed aperture length constraint. Moreover, compared to uniform square arrays, elongated configurations such as uniform linear arrays (ULAs) yield narrower beamdepth and extend the effective near-field region defined by the EBRD. Building on these insights, we design a polar codebook for compressed-sensing-based channel estimation that leverages our findings. Simulation results show that the proposed polar codebook achieves a 2 dB NMSE improvement over state-of-the-art methods. Additionally, we present an analytical expression to quantify the effective spatial DoF in the near-field, revealing that they are also constrained by the EBRD. Notably, the maximum spatial DoF is achieved with a ULA configuration, outperforming a square URA in this regard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the deployment of large antenna arrays at high-frequency bands, future wireless communication systems are likely to operate in the radiative near-field. Unlike far-field beam steering, near-field beams can be focused on a spatial region with a finite depth, enabling spatial multiplexing in the range dimension. Moreover, in the line-of-sight MIMO near-field, multiple spatial degrees of freedom (DoF) are accessible, akin to a scattering- rich environment. In this paper, we derive the beamdepth for a generalized uniform rectangular array (URA) and investigate how the array geometry influences near-field beamdepth and its limits. We define the effective beamfocusing Rayleigh distance (EBRD), to present a near-field boundary with respect to beamfocusing and spatial multiplexing gains for the generalized URA. Our results demonstrate that under a fixed element count constraint, the array geometry has a strong impact on beamdepth, whereas this effect diminishes under a fixed aperture length constraint. Moreover, compared to uniform square arrays, elongated configurations such as uniform linear arrays (ULAs) yield narrower beamdepth and extend the effective near-field region defined by the EBRD. Building on these insights, we design a polar codebook for compressed-sensing-based channel estimation that leverages our findings. Simulation results show that the proposed polar codebook achieves a 2 dB NMSE improvement over state-of-the-art methods. Additionally, we present an analytical expression to quantify the effective spatial DoF in the near-field, revealing that they are also constrained by the EBRD. Notably, the maximum spatial DoF is achieved with a ULA configuration, outperforming a square URA in this regard."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T14:04:10Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    14,
                    4,
                    10,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "authors": [
                    {
                        "name": "Ahmed Hussain"
                    },
                    {
                        "name": "Asmaa Abdallah"
                    },
                    {
                        "name": "Abdulkadir Celik"
                    },
                    {
                        "name": "Emil Bjrnson"
                    },
                    {
                        "name": "Ahmed M. Eltawil"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed M. Eltawil"
                },
                "author": "Ahmed M. Eltawil"
            },
            {
                "id": "http://arxiv.org/abs/2602.21914v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21914v1",
                "title": "Traffic-aware Hierarchical Integrated Thermal and Energy Management for Connected HEVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic-aware Hierarchical Integrated Thermal and Energy Management for Connected HEVs"
                },
                "updated": "2026-02-25T13:42:04Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    13,
                    42,
                    4,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21914v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The energy and thermal management systems of hybrid electric vehicles (HEVs) are inherently interdependent. With the ongoing deployment of intelligent transportation systems (ITSs) and increasing vehicle connectivity, the integration of traffic information has become crucial for improving both energy efficiency and thermal comfort in modern vehicles. To enhance fuel economy, this paper proposes a novel traffic-aware hierarchical integrated thermal and energy management (TA-ITEM) strategy for connected HEVs. In the upper layer, global reference trajectories for battery state of charge (SOC) and cabin temperature are planned using traffic flow speed information obtained from ITSs. In the lower layer, a real-time model predictive control (MPC)-based ITEM controller is developed, which incorporates a novel Transformer-based speed predictor with driving condition recognition (TF-DCR) to enable anticipatory tracking of the reference trajectories. Numerical simulations are conducted under various driving cycles and ambient temperature conditions. The results demonstrate that the proposed TA-ITEM approach outperforms conventional rule-based and MPC-SP approaches, with average fuel consumption reductions of 56.36\\% and 5.84\\%, respectively, while maintaining superior thermal regulation and cabin comfort. These findings confirm the effectiveness and strong generalization capability of TA-ITEM and underscore the advantages of incorporating traffic information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The energy and thermal management systems of hybrid electric vehicles (HEVs) are inherently interdependent. With the ongoing deployment of intelligent transportation systems (ITSs) and increasing vehicle connectivity, the integration of traffic information has become crucial for improving both energy efficiency and thermal comfort in modern vehicles. To enhance fuel economy, this paper proposes a novel traffic-aware hierarchical integrated thermal and energy management (TA-ITEM) strategy for connected HEVs. In the upper layer, global reference trajectories for battery state of charge (SOC) and cabin temperature are planned using traffic flow speed information obtained from ITSs. In the lower layer, a real-time model predictive control (MPC)-based ITEM controller is developed, which incorporates a novel Transformer-based speed predictor with driving condition recognition (TF-DCR) to enable anticipatory tracking of the reference trajectories. Numerical simulations are conducted under various driving cycles and ambient temperature conditions. The results demonstrate that the proposed TA-ITEM approach outperforms conventional rule-based and MPC-SP approaches, with average fuel consumption reductions of 56.36\\% and 5.84\\%, respectively, while maintaining superior thermal regulation and cabin comfort. These findings confirm the effectiveness and strong generalization capability of TA-ITEM and underscore the advantages of incorporating traffic information."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T13:42:04Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    13,
                    42,
                    4,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Jie Han"
                    },
                    {
                        "name": "Arash Khalatbarisoltani"
                    },
                    {
                        "name": "Hai L. Vu"
                    },
                    {
                        "name": "Xiaosong Hu"
                    },
                    {
                        "name": "Jun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Yang"
                },
                "author": "Jun Yang"
            },
            {
                "id": "http://arxiv.org/abs/2602.21900v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21900v1",
                "title": "EmoOmni: Bridging Emotional Understanding and Expression in Omni-Modal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmoOmni: Bridging Emotional Understanding and Expression in Omni-Modal LLMs"
                },
                "updated": "2026-02-25T13:30:27Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    13,
                    30,
                    27,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21900v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The evolution of Omni-Modal Large Language Models~(Omni-LLMs) has revolutionized human--computer interaction, enabling unified audio-visual perception and speech response. However, existing Omni-LLMs struggle with complex real-world scenarios, often leading to superficial understanding and contextually mismatched emotional responses. This issue is further intensified by Omni-LLM's Thinker-Talker architectures, which are implicitly connected through hidden states, leading to the loss of emotional details. In this work, we present EmoOmni, a unified framework for accurate understanding and expression in multimodal emotional dialogue. At its core, we introduce the emotional Chain-of-Thought~(E-CoT), which enforces a reasoning from fine-grained multimodal perception to textual response. Moreover, we explicitly treat E-CoT as high-level emotional instructions that guide the talker, enabling accurate emotional expression. Complementing the model, we construct EmoOmniPipe to obtain the real-world annotated dialogue data and establish a benchmark, EmoOmniEval, to facilitate systematic assessment of multimodal emotional dialogue task. Experiments show that EmoOmni-7B achieves comparable performance with Qwen3Omni-30B-A3B-Thinking under the same talker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of Omni-Modal Large Language Models~(Omni-LLMs) has revolutionized human--computer interaction, enabling unified audio-visual perception and speech response. However, existing Omni-LLMs struggle with complex real-world scenarios, often leading to superficial understanding and contextually mismatched emotional responses. This issue is further intensified by Omni-LLM's Thinker-Talker architectures, which are implicitly connected through hidden states, leading to the loss of emotional details. In this work, we present EmoOmni, a unified framework for accurate understanding and expression in multimodal emotional dialogue. At its core, we introduce the emotional Chain-of-Thought~(E-CoT), which enforces a reasoning from fine-grained multimodal perception to textual response. Moreover, we explicitly treat E-CoT as high-level emotional instructions that guide the talker, enabling accurate emotional expression. Complementing the model, we construct EmoOmniPipe to obtain the real-world annotated dialogue data and establish a benchmark, EmoOmniEval, to facilitate systematic assessment of multimodal emotional dialogue task. Experiments show that EmoOmni-7B achieves comparable performance with Qwen3Omni-30B-A3B-Thinking under the same talker."
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T13:30:27Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    13,
                    30,
                    27,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD"
                },
                "authors": [
                    {
                        "name": "Wenjie Tian"
                    },
                    {
                        "name": "Zhixian Zhao"
                    },
                    {
                        "name": "Jingbin Hu"
                    },
                    {
                        "name": "Huakang Chen"
                    },
                    {
                        "name": "Haohe Liu"
                    },
                    {
                        "name": "Binshen Mu"
                    },
                    {
                        "name": "Lei Xie"
                    }
                ],
                "author_detail": {
                    "name": "Lei Xie"
                },
                "author": "Lei Xie"
            },
            {
                "id": "http://arxiv.org/abs/2602.21899v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21899v1",
                "title": "Enhancing Cellular-enabled Collaborative Robots Planning through GNSS data for SAR Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Cellular-enabled Collaborative Robots Planning through GNSS data for SAR Scenarios"
                },
                "updated": "2026-02-25T13:29:38Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    13,
                    29,
                    38,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21899v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1016/j.comcom.2025.108376",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Cellular-enabled collaborative robots are becoming paramount in Search-and-Rescue (SAR) and emergency response. Crucially dependent on resilient mobile network connectivity, they serve as invaluable assets for tasks like rapid victim localization and the exploration of hazardous, otherwise unreachable areas. However, their reliance on battery power and the need for persistent, low-latency communication limit operational time and mobility. To address this, and considering the evolving capabilities of 5G/6G networks, we propose a novel SAR framework that includes Mission Planning and Mission Execution phases and that optimizes robot deployment. By considering parameters such as the exploration area size, terrain elevation, robot fleet size, communication-influenced energy profiles, desired exploration rate, and target response time, our framework determines the minimum number of robots required and their optimal paths to ensure effective coverage and timely data backhaul over mobile networks. Our results demonstrate the trade-offs between number of robots, explored area, and response time for wheeled and quadruped robots. Further, we quantify the impact of terrain elevation data on mission time and energy consumption, showing the benefits of incorporating real-world environmental factors that might also affect mobile signal propagation and connectivity into SAR planning. This framework provides critical insights for leveraging next-generation mobile networks to enhance autonomous SAR operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cellular-enabled collaborative robots are becoming paramount in Search-and-Rescue (SAR) and emergency response. Crucially dependent on resilient mobile network connectivity, they serve as invaluable assets for tasks like rapid victim localization and the exploration of hazardous, otherwise unreachable areas. However, their reliance on battery power and the need for persistent, low-latency communication limit operational time and mobility. To address this, and considering the evolving capabilities of 5G/6G networks, we propose a novel SAR framework that includes Mission Planning and Mission Execution phases and that optimizes robot deployment. By considering parameters such as the exploration area size, terrain elevation, robot fleet size, communication-influenced energy profiles, desired exploration rate, and target response time, our framework determines the minimum number of robots required and their optimal paths to ensure effective coverage and timely data backhaul over mobile networks. Our results demonstrate the trade-offs between number of robots, explored area, and response time for wheeled and quadruped robots. Further, we quantify the impact of terrain elevation data on mission time and energy consumption, showing the benefits of incorporating real-world environmental factors that might also affect mobile signal propagation and connectivity into SAR planning. This framework provides critical insights for leveraging next-generation mobile networks to enhance autonomous SAR operations."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T13:29:38Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    13,
                    29,
                    38,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2403.09177",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Arnau Romero"
                    },
                    {
                        "name": "Carmen Delgado"
                    },
                    {
                        "name": "Jana Baguer"
                    },
                    {
                        "name": "Ral Surez"
                    },
                    {
                        "name": "Xavier Costa-Prez"
                    }
                ],
                "author_detail": {
                    "name": "Xavier Costa-Prez"
                },
                "author": "Xavier Costa-Prez",
                "arxiv_doi": "10.1016/j.comcom.2025.108376"
            },
            {
                "id": "http://arxiv.org/abs/2505.22811v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.22811v3",
                "title": "Highly Efficient and Effective LLMs with Multi-Boolean Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Highly Efficient and Effective LLMs with Multi-Boolean Architectures"
                },
                "updated": "2026-02-25T13:11:22Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    13,
                    11,
                    22,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.22811v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.22811v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Weight binarization has emerged as a promising strategy to reduce the complexity of large language models (LLMs). Existing approaches fall into post-training binarization, which is simple but causes severe performance loss, and training-aware methods, which depend on full-precision latent weights, adding complexity and limiting efficiency. We propose a novel framework that represents LLMs with multi-kernel Boolean parameters and, for the first time, enables direct finetuning LMMs in the Boolean domain, eliminating the need for latent weights. This enhances representational capacity and dramatically reduces complexity during both finetuning and inference. Extensive experiments across diverse LLMs show our method outperforms recent ultra low-bit quantization and binarization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weight binarization has emerged as a promising strategy to reduce the complexity of large language models (LLMs). Existing approaches fall into post-training binarization, which is simple but causes severe performance loss, and training-aware methods, which depend on full-precision latent weights, adding complexity and limiting efficiency. We propose a novel framework that represents LLMs with multi-kernel Boolean parameters and, for the first time, enables direct finetuning LMMs in the Boolean domain, eliminating the need for latent weights. This enhances representational capacity and dramatically reduces complexity during both finetuning and inference. Extensive experiments across diverse LLMs show our method outperforms recent ultra low-bit quantization and binarization techniques."
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-28T19:40:34Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    19,
                    40,
                    34,
                    2,
                    148,
                    0
                ],
                "arxiv_comment": "ICLR 2026",
                "arxiv_primary_category": {
                    "term": "stat.ML"
                },
                "authors": [
                    {
                        "name": "Ba-Hien Tran"
                    },
                    {
                        "name": "Van Minh Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Van Minh Nguyen"
                },
                "author": "Van Minh Nguyen"
            },
            {
                "id": "http://arxiv.org/abs/2602.21887v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21887v1",
                "title": "ExpLang: Improved Exploration and Exploitation in LLM Reasoning with On-Policy Thinking Language Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpLang: Improved Exploration and Exploitation in LLM Reasoning with On-Policy Thinking Language Selection"
                },
                "updated": "2026-02-25T13:10:58Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    13,
                    10,
                    58,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21887v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current large reasoning models (LRMs) have shown strong ability on challenging tasks after reinforcement learning (RL) based post-training. However, previous work mainly focuses on English reasoning in expectation of the strongest performance, despite the demonstrated potential advantage of multilingual thinking, as well as the requirement for native thinking traces by global users. In this paper, we propose ExpLang, a novel LLM post-training pipeline that enables on-policy thinking language selection to improve exploration and exploitation during RL with the use of multiple languages. The results show that our method steadily outperforms English-only training with the same training budget, while showing high thinking language compliance for both seen and unseen languages. Analysis shows that, by enabling on-policy thinking language selection as an action during RL, ExpLang effectively extends the RL exploration space with diversified language preference and improves the RL exploitation outcome with leveraged non-English advantage. The method is orthogonal to most RL algorithms and opens up a new perspective on using multilinguality to improve LRMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current large reasoning models (LRMs) have shown strong ability on challenging tasks after reinforcement learning (RL) based post-training. However, previous work mainly focuses on English reasoning in expectation of the strongest performance, despite the demonstrated potential advantage of multilingual thinking, as well as the requirement for native thinking traces by global users. In this paper, we propose ExpLang, a novel LLM post-training pipeline that enables on-policy thinking language selection to improve exploration and exploitation during RL with the use of multiple languages. The results show that our method steadily outperforms English-only training with the same training budget, while showing high thinking language compliance for both seen and unseen languages. Analysis shows that, by enabling on-policy thinking language selection as an action during RL, ExpLang effectively extends the RL exploration space with diversified language preference and improves the RL exploitation outcome with leveraged non-English advantage. The method is orthogonal to most RL algorithms and opens up a new perspective on using multilinguality to improve LRMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T13:10:58Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    13,
                    10,
                    58,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Changjiang Gao"
                    },
                    {
                        "name": "Zixian Huang"
                    },
                    {
                        "name": "Kaichen Yang"
                    },
                    {
                        "name": "Jiajun Chen"
                    },
                    {
                        "name": "Jixing Li"
                    },
                    {
                        "name": "Shujian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Shujian Huang"
                },
                "author": "Shujian Huang"
            },
            {
                "id": "http://arxiv.org/abs/2602.21873v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21873v1",
                "title": "GFPL: Generative Federated Prototype Learning for Resource-Constrained and Data-Imbalanced Vision Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GFPL: Generative Federated Prototype Learning for Resource-Constrained and Data-Imbalanced Vision Task"
                },
                "updated": "2026-02-25T12:57:45Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    57,
                    45,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21873v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Federated learning (FL) facilitates the secure utilization of decentralized images, advancing applications in medical image recognition and autonomous driving. However, conventional FL faces two critical challenges in real-world deployment: ineffective knowledge fusion caused by model updates biased toward majority-class features, and prohibitive communication overhead due to frequent transmissions of high-dimensional model parameters. Inspired by the human brain's efficiency in knowledge integration, we propose a novel Generative Federated Prototype Learning (GFPL) framework to address these issues. Within this framework, a prototype generation method based on Gaussian Mixture Model (GMM) captures the statistical information of class-wise features, while a prototype aggregation strategy using Bhattacharyya distance effectively fuses semantically similar knowledge across clients. In addition, these fused prototypes are leveraged to generate pseudo-features, thereby mitigating feature distribution imbalance across clients. To further enhance feature alignment during local training, we devise a dual-classifier architecture, optimized via a hybrid loss combining Dot Regression and Cross-Entropy. Extensive experiments on benchmarks show that GFPL improves model accuracy by 3.6% under imbalanced data settings while maintaining low communication cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) facilitates the secure utilization of decentralized images, advancing applications in medical image recognition and autonomous driving. However, conventional FL faces two critical challenges in real-world deployment: ineffective knowledge fusion caused by model updates biased toward majority-class features, and prohibitive communication overhead due to frequent transmissions of high-dimensional model parameters. Inspired by the human brain's efficiency in knowledge integration, we propose a novel Generative Federated Prototype Learning (GFPL) framework to address these issues. Within this framework, a prototype generation method based on Gaussian Mixture Model (GMM) captures the statistical information of class-wise features, while a prototype aggregation strategy using Bhattacharyya distance effectively fuses semantically similar knowledge across clients. In addition, these fused prototypes are leveraged to generate pseudo-features, thereby mitigating feature distribution imbalance across clients. To further enhance feature alignment during local training, we devise a dual-classifier architecture, optimized via a hybrid loss combining Dot Regression and Cross-Entropy. Extensive experiments on benchmarks show that GFPL improves model accuracy by 3.6% under imbalanced data settings while maintaining low communication cost."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T12:57:45Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    57,
                    45,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Shiwei Lu"
                    },
                    {
                        "name": "Yuhang He"
                    },
                    {
                        "name": "Jiashuo Li"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Yihong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Yihong Gong"
                },
                "author": "Yihong Gong"
            },
            {
                "id": "http://arxiv.org/abs/2602.21862v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21862v1",
                "title": "Personalized Graph-Empowered Large Language Model for Proactive Information Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Graph-Empowered Large Language Model for Proactive Information Access"
                },
                "updated": "2026-02-25T12:43:25Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    43,
                    25,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21862v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Since individuals may struggle to recall all life details and often confuse events, establishing a system to assist users in recalling forgotten experiences is essential. While numerous studies have proposed memory recall systems, these primarily rely on deep learning techniques that require extensive training and often face data scarcity due to the limited availability of personal lifelogs. As lifelogs grow over time, systems must also adapt quickly to newly accumulated data. Recently, large language models (LLMs) have demonstrated remarkable capabilities across various tasks, making them promising for personalized applications. In this work, we present a framework that leverages LLMs for proactive information access, integrating personal knowledge graphs to enhance the detection of access needs through a refined decision-making process. Our framework offers high flexibility, enabling the replacement of base models and the modification of fact retrieval methods for continuous improvement. Experimental results demonstrate that our approach effectively identifies forgotten events, supporting users in recalling past experiences more efficiently.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since individuals may struggle to recall all life details and often confuse events, establishing a system to assist users in recalling forgotten experiences is essential. While numerous studies have proposed memory recall systems, these primarily rely on deep learning techniques that require extensive training and often face data scarcity due to the limited availability of personal lifelogs. As lifelogs grow over time, systems must also adapt quickly to newly accumulated data. Recently, large language models (LLMs) have demonstrated remarkable capabilities across various tasks, making them promising for personalized applications. In this work, we present a framework that leverages LLMs for proactive information access, integrating personal knowledge graphs to enhance the detection of access needs through a refined decision-making process. Our framework offers high flexibility, enabling the replacement of base models and the modification of fact retrieval methods for continuous improvement. Experimental results demonstrate that our approach effectively identifies forgotten events, supporting users in recalling past experiences more efficiently."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T12:43:25Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    43,
                    25,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Chia Cheng Chang"
                    },
                    {
                        "name": "An-Zi Yen"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    },
                    {
                        "name": "Hsin-Hsi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hsin-Hsi Chen"
                },
                "author": "Hsin-Hsi Chen"
            },
            {
                "id": "http://arxiv.org/abs/2509.11787v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.11787v3",
                "title": "CodeCureAgent: Automatic Classification and Repair of Static Analysis Warnings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeCureAgent: Automatic Classification and Repair of Static Analysis Warnings"
                },
                "updated": "2026-02-25T12:42:03Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    42,
                    3,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.11787v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.11787v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Static analysis tools are widely used to detect bugs, vulnerabilities, and code smells. Traditionally, developers must resolve these warnings manually. Because this process is tedious, developers sometimes ignore warnings, leading to an accumulation of warnings and a degradation of code quality. This paper presents CodeCureAgent, an approach that harnesses LLM-based agents to automatically analyze, classify, and repair static analysis warnings. Unlike previous work, our method does not follow a predetermined algorithm. Instead, we adopt an agentic framework that iteratively invokes tools to gather additional information from the codebase (e.g., via code search) and edit the codebase to resolve the warning. CodeCureAgent detects and suppresses false positives, while fixing true positives when identified. We equip CodeCureAgent with a three-step heuristic to approve patches: (1) build the project, (2) verify that the warning disappears without introducing new warnings, and (3) run the test suite. We evaluate CodeCureAgent on a dataset of 1,000 SonarQube warnings found in 106 Java projects and covering 291 distinct rules. Our approach produces plausible fixes for 96.8% of the warnings, outperforming state-of-the-art baseline approaches by 29.2%-34.0% in plausible-fix rate. Manual inspection of 291 cases reveals a correct-fix rate of 86.3%, showing that CodeCureAgent can reliably repair static analysis warnings. The approach incurs LLM costs of about 2.9 cents (USD) and an end-to-end processing time of about four minutes per warning. We envision CodeCureAgent helping to clean existing codebases and being integrated into CI/CD pipelines to prevent the accumulation of static analysis warnings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static analysis tools are widely used to detect bugs, vulnerabilities, and code smells. Traditionally, developers must resolve these warnings manually. Because this process is tedious, developers sometimes ignore warnings, leading to an accumulation of warnings and a degradation of code quality. This paper presents CodeCureAgent, an approach that harnesses LLM-based agents to automatically analyze, classify, and repair static analysis warnings. Unlike previous work, our method does not follow a predetermined algorithm. Instead, we adopt an agentic framework that iteratively invokes tools to gather additional information from the codebase (e.g., via code search) and edit the codebase to resolve the warning. CodeCureAgent detects and suppresses false positives, while fixing true positives when identified. We equip CodeCureAgent with a three-step heuristic to approve patches: (1) build the project, (2) verify that the warning disappears without introducing new warnings, and (3) run the test suite. We evaluate CodeCureAgent on a dataset of 1,000 SonarQube warnings found in 106 Java projects and covering 291 distinct rules. Our approach produces plausible fixes for 96.8% of the warnings, outperforming state-of-the-art baseline approaches by 29.2%-34.0% in plausible-fix rate. Manual inspection of 291 cases reveals a correct-fix rate of 86.3%, showing that CodeCureAgent can reliably repair static analysis warnings. The approach incurs LLM costs of about 2.9 cents (USD) and an end-to-end processing time of about four minutes per warning. We envision CodeCureAgent helping to clean existing codebases and being integrated into CI/CD pipelines to prevent the accumulation of static analysis warnings."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-15T11:16:04Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    16,
                    4,
                    0,
                    258,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Pascal Joos"
                    },
                    {
                        "name": "Islem Bouzenia"
                    },
                    {
                        "name": "Michael Pradel"
                    }
                ],
                "author_detail": {
                    "name": "Michael Pradel"
                },
                "author": "Michael Pradel"
            },
            {
                "id": "http://arxiv.org/abs/2602.21854v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21854v1",
                "title": "FewMMBench: A Benchmark for Multimodal Few-Shot Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FewMMBench: A Benchmark for Multimodal Few-Shot Learning"
                },
                "updated": "2026-02-25T12:30:18Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    30,
                    18,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21854v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As multimodal large language models (MLLMs) advance in handling interleaved image-text data, assessing their few-shot learning capabilities remains an open challenge. In this paper, we introduce FewMMBench, a comprehensive benchmark designed to evaluate MLLMs under few-shot conditions, with a focus on In-Context Learning (ICL) and Chain-of-Thought (CoT) prompting. Covering a diverse suite of multimodal understanding tasks, from attribute recognition to temporal reasoning, FewMMBench enables systematic analysis across task types, model families, and prompting strategies. We evaluate 26 open-weight MLLMs from six model families across zero-shot, few-shot, and CoT-augmented few-shot settings. Our findings reveal that instruction-tuned models exhibit strong zero-shot performance but benefit minimally, or even regress, with additional demonstrations or CoT reasoning. Retrieval-based demonstrations and increased context size also yield limited gains. These results highlight FewMMBench as a rigorous testbed for diagnosing and advancing few-shot capabilities in multimodal LLMs. The data is available at: https://huggingface.co/datasets/mustafaa/FewMMBench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multimodal large language models (MLLMs) advance in handling interleaved image-text data, assessing their few-shot learning capabilities remains an open challenge. In this paper, we introduce FewMMBench, a comprehensive benchmark designed to evaluate MLLMs under few-shot conditions, with a focus on In-Context Learning (ICL) and Chain-of-Thought (CoT) prompting. Covering a diverse suite of multimodal understanding tasks, from attribute recognition to temporal reasoning, FewMMBench enables systematic analysis across task types, model families, and prompting strategies. We evaluate 26 open-weight MLLMs from six model families across zero-shot, few-shot, and CoT-augmented few-shot settings. Our findings reveal that instruction-tuned models exhibit strong zero-shot performance but benefit minimally, or even regress, with additional demonstrations or CoT reasoning. Retrieval-based demonstrations and increased context size also yield limited gains. These results highlight FewMMBench as a rigorous testbed for diagnosing and advancing few-shot capabilities in multimodal LLMs. The data is available at: https://huggingface.co/datasets/mustafaa/FewMMBench"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T12:30:18Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    30,
                    18,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "Preprint. 49 pages, 38 Figures, 5 Tables",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mustafa Dogan"
                    },
                    {
                        "name": "Ilker Kesen"
                    },
                    {
                        "name": "Iacer Calixto"
                    },
                    {
                        "name": "Aykut Erdem"
                    },
                    {
                        "name": "Erkut Erdem"
                    }
                ],
                "author_detail": {
                    "name": "Erkut Erdem"
                },
                "author": "Erkut Erdem"
            },
            {
                "id": "http://arxiv.org/abs/2402.00386v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2402.00386v4",
                "title": "AssertLLM: Generating and Evaluating Hardware Verification Assertions from Design Specifications via Multi-LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AssertLLM: Generating and Evaluating Hardware Verification Assertions from Design Specifications via Multi-LLMs"
                },
                "updated": "2026-02-25T12:23:33Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    23,
                    33,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2402.00386v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2402.00386v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Assertion-based verification (ABV) is a critical method for ensuring design circuits comply with their architectural specifications, which are typically described in natural language. This process often requires human interpretation by verification engineers to convert these specifications into functional verification assertions. Existing methods for generating assertions from natural language specifications are limited to sentences extracted by engineers, discouraging its practical application. In this work, we present AssertLLM, an automatic assertion generation framework that processes complete specification files. AssertLLM breaks down the complex task into three phases, incorporating three customized Large Language Models (LLMs) for extracting structural specifications, mapping signal definitions, and generating assertions. Our evaluation of AssertLLM on a full design, encompassing 23 I/O signals, demonstrates that 89\\% of the generated assertions are both syntactically and functionally accurate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assertion-based verification (ABV) is a critical method for ensuring design circuits comply with their architectural specifications, which are typically described in natural language. This process often requires human interpretation by verification engineers to convert these specifications into functional verification assertions. Existing methods for generating assertions from natural language specifications are limited to sentences extracted by engineers, discouraging its practical application. In this work, we present AssertLLM, an automatic assertion generation framework that processes complete specification files. AssertLLM breaks down the complex task into three phases, incorporating three customized Large Language Models (LLMs) for extracting structural specifications, mapping signal definitions, and generating assertions. Our evaluation of AssertLLM on a full design, encompassing 23 I/O signals, demonstrates that 89\\% of the generated assertions are both syntactically and functionally accurate."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-02-01T07:15:03Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    7,
                    15,
                    3,
                    3,
                    32,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Wenji Fang"
                    },
                    {
                        "name": "Mengming Li"
                    },
                    {
                        "name": "Min Li"
                    },
                    {
                        "name": "Zhiyuan Yan"
                    },
                    {
                        "name": "Shang Liu"
                    },
                    {
                        "name": "Hongce Zhang"
                    },
                    {
                        "name": "Zhiyao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyao Xie"
                },
                "author": "Zhiyao Xie"
            },
            {
                "id": "http://arxiv.org/abs/2602.19784v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.19784v2",
                "title": "High-Altitude Platforms in the Low-Altitude Economy: Bridging Communication, Computing, and Regulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Altitude Platforms in the Low-Altitude Economy: Bridging Communication, Computing, and Regulation"
                },
                "updated": "2026-02-25T12:18:02Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    18,
                    2,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.19784v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.19784v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Low-Altitude Economy (LAE) is rapidly emerging as a new technological and industrial frontier, with unmanned aerial vehicles (UAVs), electric vertical takeoff and landing (eVTOL) aircraft, and aerial swarms increasingly deployed in logistics, infrastructure inspection, security, and emergency response. However, the large-scale development of the LAE demands a reliable aerial foundation that ensures not only real-time connectivity and computational support, but also navigation integrity and safe airspace management for safety-critical operations. High-Altitude Platforms (HAPs), positioned at around 20 km, provide a unique balance between wide-area coverage and low-latency responsiveness. Compared with low earth orbit (LEO) satellites, HAPs are closer to end users and thus capable of delivering millisecond-level connectivity, fine-grained regulatory oversight, and powerful onboard computing and caching resources. Beyond connectivity and computation, HAPs-assisted sensing and regulation further enable navigation integrity and airspace trust, which are essential for safety-critical UAV and eVTOL operations in the LAE. This article proposes a five-stage evolutionary roadmap for HAPs in the LAE: from serving as aerial infrastructure bases, to becoming super back-ends for UAV, to acting as frontline support for ground users, further enabling swarm-scale UAV coordination, and ultimately advancing toward edge-air-cloud closed-loop autonomy. In parallel, HAPs complement LEO satellites and cloud infrastructures to form a global-regional-local three-tier architecture. Looking forward, HAPs are expected to evolve from simple platforms into intelligent hubs, emerging as pivotal nodes for air traffic management, intelligent logistics, and emergency response. By doing so, they will accelerate the transition of the LAE toward large-scale deployment, autonomy, and sustainable growth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Low-Altitude Economy (LAE) is rapidly emerging as a new technological and industrial frontier, with unmanned aerial vehicles (UAVs), electric vertical takeoff and landing (eVTOL) aircraft, and aerial swarms increasingly deployed in logistics, infrastructure inspection, security, and emergency response. However, the large-scale development of the LAE demands a reliable aerial foundation that ensures not only real-time connectivity and computational support, but also navigation integrity and safe airspace management for safety-critical operations. High-Altitude Platforms (HAPs), positioned at around 20 km, provide a unique balance between wide-area coverage and low-latency responsiveness. Compared with low earth orbit (LEO) satellites, HAPs are closer to end users and thus capable of delivering millisecond-level connectivity, fine-grained regulatory oversight, and powerful onboard computing and caching resources. Beyond connectivity and computation, HAPs-assisted sensing and regulation further enable navigation integrity and airspace trust, which are essential for safety-critical UAV and eVTOL operations in the LAE. This article proposes a five-stage evolutionary roadmap for HAPs in the LAE: from serving as aerial infrastructure bases, to becoming super back-ends for UAV, to acting as frontline support for ground users, further enabling swarm-scale UAV coordination, and ultimately advancing toward edge-air-cloud closed-loop autonomy. In parallel, HAPs complement LEO satellites and cloud infrastructures to form a global-regional-local three-tier architecture. Looking forward, HAPs are expected to evolve from simple platforms into intelligent hubs, emerging as pivotal nodes for air traffic management, intelligent logistics, and emergency response. By doing so, they will accelerate the transition of the LAE toward large-scale deployment, autonomy, and sustainable growth."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-23T12:40:05Z",
                "published_parsed": [
                    2026,
                    2,
                    23,
                    12,
                    40,
                    5,
                    0,
                    54,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Bang Huang"
                    },
                    {
                        "name": "Baha Eddine Youcef Belmekki"
                    },
                    {
                        "name": "Mohamed-Slim Alouini"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed-Slim Alouini"
                },
                "author": "Mohamed-Slim Alouini"
            },
            {
                "id": "http://arxiv.org/abs/2508.13755v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.13755v6",
                "title": "Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with Adaptive Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with Adaptive Exploration"
                },
                "updated": "2026-02-25T12:13:07Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    13,
                    7,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.13755v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.13755v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models, yet its full potential is hindered by two under-explored dimensions: Depth-the hardest problem a model can sample; Breadth-the number of instances consumed in a single iteration. We dissect the popular GRPO algorithm and reveal a systematic bias: the cumulative-advantage disproportionately weights samples with medium accuracy, while down-weighting the low-accuracy instances that are crucial for pushing reasoning boundaries. To rectify the depth neglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which re-weights hard problems through targeted multi-stage rollouts, thereby increasing the number of positive rollouts for hard problems. Empirically, naively enlarging rollout size only accelerates convergence and even hurts Pass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra inference cost at convergence. Just as we adaptively expanded the depth of exploration, we now ask whether aggressively scaling the breadth of training data can further amplify reasoning gains. To this end, we intensely scale batch size and replace PPO's mini-batch iterations with full-batch updates over multiple epochs. Increasing breadth significantly enhances Pass@1 performance. Large-breadth training sustains high token-level entropy, indicating continued exploration and reduced gradient noise. We further present DARS-B, which augments DARS with large breadth, and demonstrate simultaneous gains in Pass@K and Pass@1. The results confirm that breadth and adaptive exploration across depth operate as orthogonal dimensions in RLVR, which are key to unleashing the reasoning power of RLVR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models, yet its full potential is hindered by two under-explored dimensions: Depth-the hardest problem a model can sample; Breadth-the number of instances consumed in a single iteration. We dissect the popular GRPO algorithm and reveal a systematic bias: the cumulative-advantage disproportionately weights samples with medium accuracy, while down-weighting the low-accuracy instances that are crucial for pushing reasoning boundaries. To rectify the depth neglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which re-weights hard problems through targeted multi-stage rollouts, thereby increasing the number of positive rollouts for hard problems. Empirically, naively enlarging rollout size only accelerates convergence and even hurts Pass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra inference cost at convergence. Just as we adaptively expanded the depth of exploration, we now ask whether aggressively scaling the breadth of training data can further amplify reasoning gains. To this end, we intensely scale batch size and replace PPO's mini-batch iterations with full-batch updates over multiple epochs. Increasing breadth significantly enhances Pass@1 performance. Large-breadth training sustains high token-level entropy, indicating continued exploration and reduced gradient noise. We further present DARS-B, which augments DARS with large breadth, and demonstrate simultaneous gains in Pass@K and Pass@1. The results confirm that breadth and adaptive exploration across depth operate as orthogonal dimensions in RLVR, which are key to unleashing the reasoning power of RLVR."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-19T11:51:40Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    51,
                    40,
                    1,
                    231,
                    0
                ],
                "arxiv_comment": "20 pages, 17 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Zhicheng Yang"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Yinya Huang"
                    },
                    {
                        "name": "Yongxin Wang"
                    },
                    {
                        "name": "Dongchun Xie"
                    },
                    {
                        "name": "Hanhui Li"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Xiaodan Liang"
                    },
                    {
                        "name": "Jing Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Tang"
                },
                "author": "Jing Tang"
            },
            {
                "id": "http://arxiv.org/abs/2602.21833v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21833v1",
                "title": "From Restructuring to Stabilization: A Large-Scale Experiment on Iterative Code Readability Refactoring with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Restructuring to Stabilization: A Large-Scale Experiment on Iterative Code Readability Refactoring with Large Language Models"
                },
                "updated": "2026-02-25T12:05:25Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    5,
                    25,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21833v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21833v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) are increasingly used for automated code refactoring tasks. Although these models can quickly refactor code, the quality may exhibit inconsistencies and unpredictable behavior. In this article, we systematically study the capabilities of LLMs for code refactoring with a specific focus on improving code readability.\n  We conducted a large-scale experiment using GPT5.1 with 230 Java snippets, each systematically varied and refactored regarding code readability across five iterations under three different prompting strategies. We categorized fine-grained code changes during the refactoring into implementation, syntactic, and comment-level transformations. Subsequently, we investigated the functional correctness and tested the robustness of the results with novel snippets.\n  Our results reveal three main insights: First, iterative code refactoring exhibits an initial phase of restructuring followed by stabilization. This convergence tendency suggests that LLMs possess an internalized understanding of an \"optimally readable\" version of code. Second, convergence patterns are fairly robust across different code variants. Third, explicit prompting toward specific readability factors slightly influences the refactoring dynamics.\n  These insights provide an empirical foundation for assessing the reliability of LLM-assisted code refactoring, which opens pathways for future research, including comparative analyses across models and a systematic evaluation of additional software quality dimensions in LLM-refactored code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used for automated code refactoring tasks. Although these models can quickly refactor code, the quality may exhibit inconsistencies and unpredictable behavior. In this article, we systematically study the capabilities of LLMs for code refactoring with a specific focus on improving code readability.\n  We conducted a large-scale experiment using GPT5.1 with 230 Java snippets, each systematically varied and refactored regarding code readability across five iterations under three different prompting strategies. We categorized fine-grained code changes during the refactoring into implementation, syntactic, and comment-level transformations. Subsequently, we investigated the functional correctness and tested the robustness of the results with novel snippets.\n  Our results reveal three main insights: First, iterative code refactoring exhibits an initial phase of restructuring followed by stabilization. This convergence tendency suggests that LLMs possess an internalized understanding of an \"optimally readable\" version of code. Second, convergence patterns are fairly robust across different code variants. Third, explicit prompting toward specific readability factors slightly influences the refactoring dynamics.\n  These insights provide an empirical foundation for assessing the reliability of LLM-assisted code refactoring, which opens pathways for future research, including comparative analyses across models and a systematic evaluation of additional software quality dimensions in LLM-refactored code."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T12:05:25Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    12,
                    5,
                    25,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Norman Peitek"
                    },
                    {
                        "name": "Julia Hess"
                    },
                    {
                        "name": "Sven Apel"
                    }
                ],
                "author_detail": {
                    "name": "Sven Apel"
                },
                "author": "Sven Apel"
            },
            {
                "id": "http://arxiv.org/abs/2508.05282v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.05282v4",
                "title": "Not All Errors Are Created Equal: ASCoT Addresses Late-Stage Fragility in Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Errors Are Created Equal: ASCoT Addresses Late-Stage Fragility in Efficient LLM Reasoning"
                },
                "updated": "2026-02-25T11:55:15Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    11,
                    55,
                    15,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.05282v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.05282v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While Chain-of-Thought (CoT) prompting empowers Large Language Models (LLMs), ensuring reasoning reliability remains an open challenge. Contrary to the prevailing cascading failure hypothesis which posits that early errors are most detrimental, we identify a counter-intuitive phenomenon termed \\textbf{Late-Stage Fragility}: errors introduced in later reasoning stages are significantly more prone to corrupting final answers. To address this, we introduce ASCoT (Adaptive Self-Correction Chain-of-Thought), a method harmonizing efficiency with robust verification. ASCoT first employs semantic pruning to compress redundant steps, then utilizes an Adaptive Verification Manager (AVM) to prioritize high risk, late-stage steps via a positional impact score, triggering a Multi-Perspective Self-Correction Engine (MSCE) only when necessary. Experiments on GSM8K and MATH-500 demonstrate that ASCoT effectively reallocates computational resources: it reduces token usage by 21\\%--30\\% for LLaMA-3.1-8B with negligible accuracy drops ($<1.8\\%$), achieving a superior trade-off between inference efficiency and reasoning fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Chain-of-Thought (CoT) prompting empowers Large Language Models (LLMs), ensuring reasoning reliability remains an open challenge. Contrary to the prevailing cascading failure hypothesis which posits that early errors are most detrimental, we identify a counter-intuitive phenomenon termed \\textbf{Late-Stage Fragility}: errors introduced in later reasoning stages are significantly more prone to corrupting final answers. To address this, we introduce ASCoT (Adaptive Self-Correction Chain-of-Thought), a method harmonizing efficiency with robust verification. ASCoT first employs semantic pruning to compress redundant steps, then utilizes an Adaptive Verification Manager (AVM) to prioritize high risk, late-stage steps via a positional impact score, triggering a Multi-Perspective Self-Correction Engine (MSCE) only when necessary. Experiments on GSM8K and MATH-500 demonstrate that ASCoT effectively reallocates computational resources: it reduces token usage by 21\\%--30\\% for LLaMA-3.1-8B with negligible accuracy drops ($<1.8\\%$), achieving a superior trade-off between inference efficiency and reasoning fidelity."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-07T11:26:40Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    26,
                    40,
                    3,
                    219,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Dongxu Zhang"
                    },
                    {
                        "name": "Ning Yang"
                    },
                    {
                        "name": "Yiding Sun"
                    },
                    {
                        "name": "Jihua Zhu"
                    },
                    {
                        "name": "Jinnan Yang"
                    },
                    {
                        "name": "Miao Xin"
                    },
                    {
                        "name": "Baoliang Tian"
                    }
                ],
                "author_detail": {
                    "name": "Baoliang Tian"
                },
                "author": "Baoliang Tian"
            },
            {
                "id": "http://arxiv.org/abs/2602.16898v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.16898v3",
                "title": "MALLVI: A Multi-Agent Framework for Integrated Generalized Robotics Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MALLVI: A Multi-Agent Framework for Integrated Generalized Robotics Manipulation"
                },
                "updated": "2026-02-25T11:49:07Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    11,
                    49,
                    7,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.16898v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.16898v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings. MALLVI presents a Multi Agent Large Language and Vision framework that enables closed-loop feedback driven robotic manipulation. Given a natural language instruction and an image of the environment, MALLVI generates executable atomic actions for a robot manipulator. After action execution, a Vision Language Model (VLM) evaluates environmental feedback and decides whether to repeat the process or proceed to the next step. Rather than using a single model, MALLVI coordinates specialized agents, Decomposer, Localizer, Thinker, and Reflector, to manage perception, localization, reasoning, and high level planning. An optional Descriptor agent provides visual memory of the initial state. The Reflector supports targeted error detection and recovery by reactivating only relevant agents, avoiding full replanning. Experiments in simulation and real-world settings show that iterative closed loop multi agent coordination improves generalization and increases success rates in zero shot manipulation tasks. Code available at https://github.com/iman1234ahmadi/MALLVI .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings. MALLVI presents a Multi Agent Large Language and Vision framework that enables closed-loop feedback driven robotic manipulation. Given a natural language instruction and an image of the environment, MALLVI generates executable atomic actions for a robot manipulator. After action execution, a Vision Language Model (VLM) evaluates environmental feedback and decides whether to repeat the process or proceed to the next step. Rather than using a single model, MALLVI coordinates specialized agents, Decomposer, Localizer, Thinker, and Reflector, to manage perception, localization, reasoning, and high level planning. An optional Descriptor agent provides visual memory of the initial state. The Reflector supports targeted error detection and recovery by reactivating only relevant agents, avoiding full replanning. Experiments in simulation and real-world settings show that iterative closed loop multi agent coordination improves generalization and increases success rates in zero shot manipulation tasks. Code available at https://github.com/iman1234ahmadi/MALLVI ."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-18T21:28:56Z",
                "published_parsed": [
                    2026,
                    2,
                    18,
                    21,
                    28,
                    56,
                    2,
                    49,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Iman Ahmadi"
                    },
                    {
                        "name": "Mehrshad Taji"
                    },
                    {
                        "name": "Arad Mahdinezhad Kashani"
                    },
                    {
                        "name": "AmirHossein Jadidi"
                    },
                    {
                        "name": "Saina Kashani"
                    },
                    {
                        "name": "Babak Khalaj"
                    }
                ],
                "author_detail": {
                    "name": "Babak Khalaj"
                },
                "author": "Babak Khalaj"
            },
            {
                "id": "http://arxiv.org/abs/2602.21806v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21806v1",
                "title": "An Empirical Study of Bugs in Modern LLM Agent Frameworks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study of Bugs in Modern LLM Agent Frameworks"
                },
                "updated": "2026-02-25T11:34:17Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    11,
                    34,
                    17,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21806v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM agents have been widely adopted in real-world applications, relying on agent frameworks for workflow execution and multi-agent coordination. As these systems scale, understanding bugs in the underlying agent frameworks becomes critical. However, existing work mainly focuses on agent-level failures, overlooking framework-level bugs. To address this gap, we conduct an empirical study of 998 bug reports from CrewAI and LangChain, constructing a taxonomy of 15 root causes and 7 observable symptoms across five agent lifecycle stages: 'Agent Initialization','Perception', 'Self-Action', 'Mutual Interaction' and 'Evolution'. Our findings show that agent framework bugs mainly arise from 'API misuse', 'API incompatibility', and 'Documentation Desync', largely concentrated in the 'Self-Action' stage. Symptoms typically appear as 'Functional Error', 'Crash', and 'Build Failure', reflecting disruptions to task progression and control flow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM agents have been widely adopted in real-world applications, relying on agent frameworks for workflow execution and multi-agent coordination. As these systems scale, understanding bugs in the underlying agent frameworks becomes critical. However, existing work mainly focuses on agent-level failures, overlooking framework-level bugs. To address this gap, we conduct an empirical study of 998 bug reports from CrewAI and LangChain, constructing a taxonomy of 15 root causes and 7 observable symptoms across five agent lifecycle stages: 'Agent Initialization','Perception', 'Self-Action', 'Mutual Interaction' and 'Evolution'. Our findings show that agent framework bugs mainly arise from 'API misuse', 'API incompatibility', and 'Documentation Desync', largely concentrated in the 'Self-Action' stage. Symptoms typically appear as 'Functional Error', 'Crash', and 'Build Failure', reflecting disruptions to task progression and control flow."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T11:34:17Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    11,
                    34,
                    17,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Xinxue Zhu"
                    },
                    {
                        "name": "Jiacong Wu"
                    },
                    {
                        "name": "Xiaoyu Zhang"
                    },
                    {
                        "name": "Tianlin Li"
                    },
                    {
                        "name": "Yanzhou Mu"
                    },
                    {
                        "name": "Juan Zhai"
                    },
                    {
                        "name": "Chao Shen"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu"
            },
            {
                "id": "http://arxiv.org/abs/2510.20498v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.20498v3",
                "title": "Robust Preference Alignment via Directional Neighborhood Consensus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Preference Alignment via Directional Neighborhood Consensus"
                },
                "updated": "2026-02-25T11:31:02Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    11,
                    31,
                    2,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.20498v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.20498v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Aligning large language models with human preferences is critical for creating reliable and controllable AI systems. A human preference can be visualized as a high-dimensional vector where different directions represent trade-offs between desired attributes (e.g., helpfulness vs. verbosity). Yet, because the training data often reflects dominant, average preferences, LLMs tend to perform well on common requests but fall short in specific, individual needs. This mismatch creates a preference coverage gap. Existing methods often address this through costly retraining, which may not be generalized to the full spectrum of diverse preferences. This brittleness means that when a user's request reflects a nuanced preference deviating from the training data's central tendency, model performance can degrade unpredictably. To address this challenge, we introduce Robust Preference Selection (RPS), a post-hoc, training-free method by leveraging directional neighborhood consensus. Instead of forcing a model to generate a response from a single, highly specific preference, RPS samples multiple responses from a local neighborhood of related preferences to create a superior candidate pool. It then selects the response that best aligns with the user's original intent. We provide a theoretical framework showing our neighborhood generation strategy is provably superior to a strong baseline that also samples multiple candidates. Comprehensive experiments across three distinct alignment paradigms (DPA, DPO, and SFT) demonstrate that RPS consistently improves robustness against this baseline, achieving win rates of up to 69% on challenging preferences from under-represented regions of the space without any model retraining. Our work presents a practical, theoretically-grounded solution for enhancing the reliability of preference-aligned models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models with human preferences is critical for creating reliable and controllable AI systems. A human preference can be visualized as a high-dimensional vector where different directions represent trade-offs between desired attributes (e.g., helpfulness vs. verbosity). Yet, because the training data often reflects dominant, average preferences, LLMs tend to perform well on common requests but fall short in specific, individual needs. This mismatch creates a preference coverage gap. Existing methods often address this through costly retraining, which may not be generalized to the full spectrum of diverse preferences. This brittleness means that when a user's request reflects a nuanced preference deviating from the training data's central tendency, model performance can degrade unpredictably. To address this challenge, we introduce Robust Preference Selection (RPS), a post-hoc, training-free method by leveraging directional neighborhood consensus. Instead of forcing a model to generate a response from a single, highly specific preference, RPS samples multiple responses from a local neighborhood of related preferences to create a superior candidate pool. It then selects the response that best aligns with the user's original intent. We provide a theoretical framework showing our neighborhood generation strategy is provably superior to a strong baseline that also samples multiple candidates. Comprehensive experiments across three distinct alignment paradigms (DPA, DPO, and SFT) demonstrate that RPS consistently improves robustness against this baseline, achieving win rates of up to 69% on challenging preferences from under-represented regions of the space without any model retraining. Our work presents a practical, theoretically-grounded solution for enhancing the reliability of preference-aligned models."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-23T12:39:20Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    39,
                    20,
                    3,
                    296,
                    0
                ],
                "arxiv_comment": "Accepted to ICLR 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ruochen Mao"
                    },
                    {
                        "name": "Yuling Shi"
                    },
                    {
                        "name": "Xiaodong Gu"
                    },
                    {
                        "name": "Jiaheng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Jiaheng Wei"
                },
                "author": "Jiaheng Wei"
            },
            {
                "id": "http://arxiv.org/abs/2602.21800v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21800v1",
                "title": "An Evaluation of Context Length Extrapolation in Long Code via Positional Embeddings and Efficient Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Evaluation of Context Length Extrapolation in Long Code via Positional Embeddings and Efficient Attention"
                },
                "updated": "2026-02-25T11:27:34Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    11,
                    27,
                    34,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21800v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid advancement of large language models (LLMs) has led to a significant increase in automated tools in the software engineering, capable of performing various code-related tasks such as code generation, completion, and translation. Despite these advancements, its effectiveness is constrained by fixed context lengths, limiting its ability to generalize across long, domain-specific code sequences. To address this challenge, we investigate zero-shot, inference-only methods aimed at improving position encodings and optimizing attention mechanisms. Our goal is to provide a thorough analysis of current approaches that facilitate context length extrapolation in code, particularly in the context of long code completion tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has led to a significant increase in automated tools in the software engineering, capable of performing various code-related tasks such as code generation, completion, and translation. Despite these advancements, its effectiveness is constrained by fixed context lengths, limiting its ability to generalize across long, domain-specific code sequences. To address this challenge, we investigate zero-shot, inference-only methods aimed at improving position encodings and optimizing attention mechanisms. Our goal is to provide a thorough analysis of current approaches that facilitate context length extrapolation in code, particularly in the context of long code completion tasks."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T11:27:34Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    11,
                    27,
                    34,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Madhusudan Ghosh"
                    },
                    {
                        "name": "Rishabh Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Rishabh Gupta"
                },
                "author": "Rishabh Gupta"
            },
            {
                "id": "http://arxiv.org/abs/2602.21786v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21786v1",
                "title": "D-COT: Disciplined Chain-of-Thought Learning for Efficient Reasoning in Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D-COT: Disciplined Chain-of-Thought Learning for Efficient Reasoning in Small Language Models"
                },
                "updated": "2026-02-25T11:08:38Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    11,
                    8,
                    38,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21786v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Chain-of-Thought (CoT) distillation from Large Language Models (LLMs) often induces \"overthinking\" in Small Language Models (SLMs), leading to performance degradation and excessive token consumption. In this study, we propose Disciplined Chain-of-Thought (D-CoT), a novel framework that enforces a structured reasoning process using control tags -- such as <TEMP_LOW> for fact-checking and <TEMP_HIGH> for multi-perspective exploration -- as auxiliary scaffolding during training. By optimizing the CoT trajectory, D-CoT suppresses reasoning drift and simultaneously achieves token reduction and performance improvement. We demonstrate the efficacy of our approach on Qwen3-8B: with only 5,000 training samples, D-CoT significantly boosts accuracy on GPQA-diamond by 9.9% and MMLU-Pro (0-shot) by 9.1%, while drastically reducing computational costs. Furthermore, we confirm that the model internalizes this disciplined thought structure, maintaining high performance even without explicit control tags during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) distillation from Large Language Models (LLMs) often induces \"overthinking\" in Small Language Models (SLMs), leading to performance degradation and excessive token consumption. In this study, we propose Disciplined Chain-of-Thought (D-CoT), a novel framework that enforces a structured reasoning process using control tags -- such as <TEMP_LOW> for fact-checking and <TEMP_HIGH> for multi-perspective exploration -- as auxiliary scaffolding during training. By optimizing the CoT trajectory, D-CoT suppresses reasoning drift and simultaneously achieves token reduction and performance improvement. We demonstrate the efficacy of our approach on Qwen3-8B: with only 5,000 training samples, D-CoT significantly boosts accuracy on GPQA-diamond by 9.9% and MMLU-Pro (0-shot) by 9.1%, while drastically reducing computational costs. Furthermore, we confirm that the model internalizes this disciplined thought structure, maintaining high performance even without explicit control tags during inference."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T11:08:38Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    11,
                    8,
                    38,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "9 pages, 3 figures. Code: https://github.com/gitpullpull/DisciplinedChainOfThought | Benchmarks: https://huggingface.co/datasets/gitpullpull/D-CoT-Benchmarks | Dataset: https://huggingface.co/datasets/gitpullpull/D-CoT-datasets",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Shunsuke Ubukata"
                    }
                ],
                "author_detail": {
                    "name": "Shunsuke Ubukata"
                },
                "author": "Shunsuke Ubukata"
            },
            {
                "id": "http://arxiv.org/abs/2505.17306v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.17306v2",
                "title": "Refusal Direction is Universal Across Safety-Aligned Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refusal Direction is Universal Across Safety-Aligned Languages"
                },
                "updated": "2026-02-25T11:02:02Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    11,
                    2,
                    2,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.17306v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.17306v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Refusal mechanisms in large language models (LLMs) are essential for ensuring safety. Recent research has revealed that refusal behavior can be mediated by a single direction in activation space, enabling targeted interventions to bypass refusals. While this is primarily demonstrated in an English-centric context, appropriate refusal behavior is important for any language, but poorly understood. In this paper, we investigate the refusal behavior in LLMs across 14 languages using PolyRefuse, a multilingual safety dataset created by translating malicious and benign English prompts into these languages. We uncover the surprising cross-lingual universality of the refusal direction: a vector extracted from English can bypass refusals in other languages with near-perfect effectiveness, without any additional fine-tuning. Even more remarkably, refusal directions derived from any safety-aligned language transfer seamlessly to others. We attribute this transferability to the parallelism of refusal vectors across languages in the embedding space and identify the underlying mechanism behind cross-lingual jailbreaks. These findings provide actionable insights for building more robust multilingual safety defenses and pave the way for a deeper mechanistic understanding of cross-lingual vulnerabilities in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refusal mechanisms in large language models (LLMs) are essential for ensuring safety. Recent research has revealed that refusal behavior can be mediated by a single direction in activation space, enabling targeted interventions to bypass refusals. While this is primarily demonstrated in an English-centric context, appropriate refusal behavior is important for any language, but poorly understood. In this paper, we investigate the refusal behavior in LLMs across 14 languages using PolyRefuse, a multilingual safety dataset created by translating malicious and benign English prompts into these languages. We uncover the surprising cross-lingual universality of the refusal direction: a vector extracted from English can bypass refusals in other languages with near-perfect effectiveness, without any additional fine-tuning. Even more remarkably, refusal directions derived from any safety-aligned language transfer seamlessly to others. We attribute this transferability to the parallelism of refusal vectors across languages in the embedding space and identify the underlying mechanism behind cross-lingual jailbreaks. These findings provide actionable insights for building more robust multilingual safety defenses and pave the way for a deeper mechanistic understanding of cross-lingual vulnerabilities in LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-22T21:54:46Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    21,
                    54,
                    46,
                    3,
                    142,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Xinpeng Wang"
                    },
                    {
                        "name": "Mingyang Wang"
                    },
                    {
                        "name": "Yihong Liu"
                    },
                    {
                        "name": "Hinrich Schtze"
                    },
                    {
                        "name": "Barbara Plank"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Plank"
                },
                "author": "Barbara Plank"
            },
            {
                "id": "http://arxiv.org/abs/2501.08449v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2501.08449v2",
                "title": "A Refreshment Stirred, Not Shaken: Invariant-Preserving Deployments of Differential Privacy for the U.S. Decennial Census",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Refreshment Stirred, Not Shaken: Invariant-Preserving Deployments of Differential Privacy for the U.S. Decennial Census"
                },
                "updated": "2026-02-25T10:50:54Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    10,
                    50,
                    54,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2501.08449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2501.08449v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1162/99608f92.dab78690",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Protecting an individual's privacy when releasing their data is inherently an exercise in relativity, regardless of how privacy is qualified or quantified. This is because we can only limit the gain in information about an individual relative to what could be derived from other sources. This framing is the essence of differential privacy (DP), through which this article examines two statistical disclosure control (SDC) methods for the United States Decennial Census: the Permutation Swapping Algorithm (PSA), which resembles the 2010 Census's disclosure avoidance system (DAS), and the TopDown Algorithm (TDA), which was used in the 2020 DAS. To varying degrees, both methods leave unaltered certain statistics of the confidential data (their invariants) and hence neither can be readily reconciled with DP, at least as originally conceived. Nevertheless, we show how invariants can naturally be integrated into DP and use this to establish that the PSA satisfies pure DP subject to the invariants it necessarily induces, thereby proving that this traditional SDC method can, in fact, be understood from the perspective of DP. By a similar modification to zero-concentrated DP, we also provide a DP specification for the TDA. Finally, as a point of comparison, we consider a counterfactual scenario in which the PSA was adopted for the 2020 Census, resulting in a reduction in the nominal protection loss budget but at the cost of releasing many more invariants. This highlights the pervasive danger of comparing budgets without accounting for the other dimensions on which DP formulations vary (such as the invariants they permit). Therefore, while our results articulate the mathematical guarantees of SDC provided by the PSA, the TDA, and the 2020 DAS in general, care must be taken in translating these guarantees into actual privacy protection$\\unicode{x2014}$just as is the case for any DP deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protecting an individual's privacy when releasing their data is inherently an exercise in relativity, regardless of how privacy is qualified or quantified. This is because we can only limit the gain in information about an individual relative to what could be derived from other sources. This framing is the essence of differential privacy (DP), through which this article examines two statistical disclosure control (SDC) methods for the United States Decennial Census: the Permutation Swapping Algorithm (PSA), which resembles the 2010 Census's disclosure avoidance system (DAS), and the TopDown Algorithm (TDA), which was used in the 2020 DAS. To varying degrees, both methods leave unaltered certain statistics of the confidential data (their invariants) and hence neither can be readily reconciled with DP, at least as originally conceived. Nevertheless, we show how invariants can naturally be integrated into DP and use this to establish that the PSA satisfies pure DP subject to the invariants it necessarily induces, thereby proving that this traditional SDC method can, in fact, be understood from the perspective of DP. By a similar modification to zero-concentrated DP, we also provide a DP specification for the TDA. Finally, as a point of comparison, we consider a counterfactual scenario in which the PSA was adopted for the 2020 Census, resulting in a reduction in the nominal protection loss budget but at the cost of releasing many more invariants. This highlights the pervasive danger of comparing budgets without accounting for the other dimensions on which DP formulations vary (such as the invariants they permit). Therefore, while our results articulate the mathematical guarantees of SDC provided by the PSA, the TDA, and the 2020 DAS in general, care must be taken in translating these guarantees into actual privacy protection$\\unicode{x2014}$just as is the case for any DP deployment."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-01-14T21:38:01Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    21,
                    38,
                    1,
                    1,
                    14,
                    0
                ],
                "arxiv_comment": "65 pages, 2 figures",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "arxiv_journal_ref": "Harvard Data Science Review (2026), Special Issue 6",
                "authors": [
                    {
                        "name": "James Bailie"
                    },
                    {
                        "name": "Ruobin Gong"
                    },
                    {
                        "name": "Xiao-Li Meng"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Li Meng"
                },
                "author": "Xiao-Li Meng",
                "arxiv_doi": "10.1162/99608f92.dab78690"
            },
            {
                "id": "http://arxiv.org/abs/2602.20800v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.20800v2",
                "title": "Mitigating Preference Leakage via Strict Estimator Separation for Normative Generative Ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Preference Leakage via Strict Estimator Separation for Normative Generative Ranking"
                },
                "updated": "2026-02-25T10:31:56Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    10,
                    31,
                    56,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.20800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.20800v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In Generative Information Retrieval (GenIR), the bottleneck has shifted from generation to the selection of candidates, particularly for normative criteria such as cultural relevance. Current LLM-as-a-Judge evaluations often suffer from circularity and preference leakage, where overlapping supervision and evaluation models inflate performance. We address this by formalising cultural relevance as a within-query ranking task and introducing a leakage-free two-judge framework that strictly separates supervision (Judge B) from evaluation (Judge A). On a new benchmark of 33,052 (NGR-33k) culturally grounded stories, we find that while classical baselines yield only modest gains, a dense bi-encoder distilled from a Judge-B-supervised Cross-Encoder is highly effective. Although the Cross-Encoder provides a strong supervision signal for distillation, the distilled BGE-M3 model substantially outperforms it under leakage-free Judge~A evaluation. We validate our framework on the human-curated Moral Stories dataset, showing strong alignment with human norms. Our results demonstrate that rigorous evaluator separation is a prerequisite for credible GenIR evaluation, proving that subtle cultural preferences can be distilled into efficient rankers without leakage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Generative Information Retrieval (GenIR), the bottleneck has shifted from generation to the selection of candidates, particularly for normative criteria such as cultural relevance. Current LLM-as-a-Judge evaluations often suffer from circularity and preference leakage, where overlapping supervision and evaluation models inflate performance. We address this by formalising cultural relevance as a within-query ranking task and introducing a leakage-free two-judge framework that strictly separates supervision (Judge B) from evaluation (Judge A). On a new benchmark of 33,052 (NGR-33k) culturally grounded stories, we find that while classical baselines yield only modest gains, a dense bi-encoder distilled from a Judge-B-supervised Cross-Encoder is highly effective. Although the Cross-Encoder provides a strong supervision signal for distillation, the distilled BGE-M3 model substantially outperforms it under leakage-free Judge~A evaluation. We validate our framework on the human-curated Moral Stories dataset, showing strong alignment with human norms. Our results demonstrate that rigorous evaluator separation is a prerequisite for credible GenIR evaluation, proving that subtle cultural preferences can be distilled into efficient rankers without leakage."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-24T11:38:36Z",
                "published_parsed": [
                    2026,
                    2,
                    24,
                    11,
                    38,
                    36,
                    1,
                    55,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Dalia Nahhas"
                    },
                    {
                        "name": "Xiaohao Cai"
                    },
                    {
                        "name": "Imran Razzak"
                    },
                    {
                        "name": "Shoaib Jameel"
                    }
                ],
                "author_detail": {
                    "name": "Shoaib Jameel"
                },
                "author": "Shoaib Jameel"
            },
            {
                "id": "http://arxiv.org/abs/2602.21763v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21763v1",
                "title": "Improving Implicit Discourse Relation Recognition with Natural Language Explanations from LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Implicit Discourse Relation Recognition with Natural Language Explanations from LLMs"
                },
                "updated": "2026-02-25T10:28:45Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    10,
                    28,
                    45,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21763v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Implicit Discourse Relation Recognition (IDRR) remains a challenging task due to the requirement for deep semantic understanding in the absence of explicit discourse markers. A further limitation is that existing methods only predict relations without providing any supporting explanations. Recent advances in large language models (LLMs) have shown strong reasoning capabilities in both deep language understanding and natural language explanation generation. In this work, we propose a simple yet effective approach to distill the reasoning capabilities of LLMs into lightweight IDRR models to improve both performance and interpretability. Specifically, we first prompt an LLM to generate explanations for each training instance conditioned on its gold label. Then, we introduce a novel classification-generation framework that jointly performs relation prediction and explanation generation, and train it with the additional supervision of LLM-generated explanations. Our framework is plug-and-play, enabling easy integration with most existing IDRR models. Experimental results on PDTB demonstrate that our approach significantly improves IDRR performance, while human evaluation further confirms that the generated explanations enhance model interpretability. Furthermore, we validate the generality of our approach on sentiment classification and natural language inference",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Discourse Relation Recognition (IDRR) remains a challenging task due to the requirement for deep semantic understanding in the absence of explicit discourse markers. A further limitation is that existing methods only predict relations without providing any supporting explanations. Recent advances in large language models (LLMs) have shown strong reasoning capabilities in both deep language understanding and natural language explanation generation. In this work, we propose a simple yet effective approach to distill the reasoning capabilities of LLMs into lightweight IDRR models to improve both performance and interpretability. Specifically, we first prompt an LLM to generate explanations for each training instance conditioned on its gold label. Then, we introduce a novel classification-generation framework that jointly performs relation prediction and explanation generation, and train it with the additional supervision of LLM-generated explanations. Our framework is plug-and-play, enabling easy integration with most existing IDRR models. Experimental results on PDTB demonstrate that our approach significantly improves IDRR performance, while human evaluation further confirms that the generated explanations enhance model interpretability. Furthermore, we validate the generality of our approach on sentiment classification and natural language inference"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T10:28:45Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    10,
                    28,
                    45,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "AAAI26'0ral",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Changxing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Changxing Wu"
                },
                "author": "Changxing Wu"
            },
            {
                "id": "http://arxiv.org/abs/2602.21756v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21756v1",
                "title": "Offline Reasoning for Efficient Recommendation: LLM-Empowered Persona-Profiled Item Indexing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Reasoning for Efficient Recommendation: LLM-Empowered Persona-Profiled Item Indexing"
                },
                "updated": "2026-02-25T10:14:30Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    10,
                    14,
                    30,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21756v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in large language models (LLMs) offer new opportunities for recommender systems by capturing the nuanced semantics of user interests and item characteristics through rich semantic understanding and contextual reasoning. In particular, LLMs have been employed as rerankers that reorder candidate items based on inferred user-item relevance. However, these approaches often require expensive online inference-time reasoning, leading to high latency that hampers real-world deployment. In this work, we introduce Persona4Rec, a recommendation framework that performs offline reasoning to construct interpretable persona representations of items, enabling lightweight and scalable real-time inference. In the offline stage, Persona4Rec leverages LLMs to reason over item reviews, inferring diverse user motivations that explain why different types of users may engage with an item; these inferred motivations are materialized as persona representations, providing multiple, human-interpretable views of each item. Unlike conventional approaches that rely on a single item representation, Persona4Rec learns to align user profiles with the most plausible item-side persona through a dedicated encoder, effectively transforming user-item relevance into user-persona relevance. At the online stage, this persona-profiled item index allows fast relevance computation without invoking expensive LLM reasoning. Extensive experiments show that Persona4Rec achieves performance comparable to recent LLM-based rerankers while substantially reducing inference time. Moreover, qualitative analysis confirms that persona representations not only drive efficient scoring but also provide intuitive, review-grounded explanations. These results demonstrate that Persona4Rec offers a practical and interpretable solution for next-generation recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) offer new opportunities for recommender systems by capturing the nuanced semantics of user interests and item characteristics through rich semantic understanding and contextual reasoning. In particular, LLMs have been employed as rerankers that reorder candidate items based on inferred user-item relevance. However, these approaches often require expensive online inference-time reasoning, leading to high latency that hampers real-world deployment. In this work, we introduce Persona4Rec, a recommendation framework that performs offline reasoning to construct interpretable persona representations of items, enabling lightweight and scalable real-time inference. In the offline stage, Persona4Rec leverages LLMs to reason over item reviews, inferring diverse user motivations that explain why different types of users may engage with an item; these inferred motivations are materialized as persona representations, providing multiple, human-interpretable views of each item. Unlike conventional approaches that rely on a single item representation, Persona4Rec learns to align user profiles with the most plausible item-side persona through a dedicated encoder, effectively transforming user-item relevance into user-persona relevance. At the online stage, this persona-profiled item index allows fast relevance computation without invoking expensive LLM reasoning. Extensive experiments show that Persona4Rec achieves performance comparable to recent LLM-based rerankers while substantially reducing inference time. Moreover, qualitative analysis confirms that persona representations not only drive efficient scoring but also provide intuitive, review-grounded explanations. These results demonstrate that Persona4Rec offers a practical and interpretable solution for next-generation recommender systems."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T10:14:30Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    10,
                    14,
                    30,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "Under review",
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Deogyong Kim"
                    },
                    {
                        "name": "Junseong Lee"
                    },
                    {
                        "name": "Jeongeun Lee"
                    },
                    {
                        "name": "Changhoe Kim"
                    },
                    {
                        "name": "Junguel Lee"
                    },
                    {
                        "name": "Jungseok Lee"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee"
            },
            {
                "id": "http://arxiv.org/abs/2602.05066v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.05066v2",
                "title": "Bypassing AI Control Protocols via Agent-as-a-Proxy Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bypassing AI Control Protocols via Agent-as-a-Proxy Attacks"
                },
                "updated": "2026-02-25T10:09:46Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    10,
                    9,
                    46,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.05066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.05066v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As AI agents automate critical workloads, they remain vulnerable to indirect prompt injection (IPI) attacks. Current defenses rely on monitoring protocols that jointly evaluate an agent's Chain-of-Thought (CoT) and tool-use actions to ensure alignment with user intent. We demonstrate that these monitoring-based defenses can be bypassed via a novel Agent-as-a-Proxy attack, where prompt injection attacks treat the agent as a delivery mechanism, bypassing both agent and monitor simultaneously. While prior work on scalable oversight has focused on whether small monitors can supervise large agents, we show that even frontier-scale monitors are vulnerable. Large-scale monitoring models like Qwen2.5-72B can be bypassed by agents with similar capabilities, such as GPT-4o mini and Llama-3.1-70B. On the AgentDojo benchmark, we achieve a high attack success rate against AlignmentCheck and Extract-and-Evaluate monitors under diverse monitoring LLMs. Our findings suggest current monitoring-based agentic defenses are fundamentally fragile regardless of model scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI agents automate critical workloads, they remain vulnerable to indirect prompt injection (IPI) attacks. Current defenses rely on monitoring protocols that jointly evaluate an agent's Chain-of-Thought (CoT) and tool-use actions to ensure alignment with user intent. We demonstrate that these monitoring-based defenses can be bypassed via a novel Agent-as-a-Proxy attack, where prompt injection attacks treat the agent as a delivery mechanism, bypassing both agent and monitor simultaneously. While prior work on scalable oversight has focused on whether small monitors can supervise large agents, we show that even frontier-scale monitors are vulnerable. Large-scale monitoring models like Qwen2.5-72B can be bypassed by agents with similar capabilities, such as GPT-4o mini and Llama-3.1-70B. On the AgentDojo benchmark, we achieve a high attack success rate against AlignmentCheck and Extract-and-Evaluate monitors under diverse monitoring LLMs. Our findings suggest current monitoring-based agentic defenses are fundamentally fragile regardless of model scale."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-04T21:38:38Z",
                "published_parsed": [
                    2026,
                    2,
                    4,
                    21,
                    38,
                    38,
                    2,
                    35,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Jafar Isbarov"
                    },
                    {
                        "name": "Murat Kantarcioglu"
                    }
                ],
                "author_detail": {
                    "name": "Murat Kantarcioglu"
                },
                "author": "Murat Kantarcioglu"
            },
            {
                "id": "http://arxiv.org/abs/2602.00462v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.00462v3",
                "title": "LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs"
                },
                "updated": "2026-02-25T10:06:33Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    10,
                    6,
                    33,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.00462v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.00462v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-31T02:33:07Z",
                "published_parsed": [
                    2026,
                    1,
                    31,
                    2,
                    33,
                    7,
                    5,
                    31,
                    0
                ],
                "arxiv_comment": "Updates: small change in interpretability percentage for Qwen-based variants we trained (pre-processing fix), clarification in Section 3 on our method (after feedback from readers), additional appendix section",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Benno Krojer"
                    },
                    {
                        "name": "Shravan Nayak"
                    },
                    {
                        "name": "Oscar Maas"
                    },
                    {
                        "name": "Vaibhav Adlakha"
                    },
                    {
                        "name": "Desmond Elliott"
                    },
                    {
                        "name": "Siva Reddy"
                    },
                    {
                        "name": "Marius Mosbach"
                    }
                ],
                "author_detail": {
                    "name": "Marius Mosbach"
                },
                "author": "Marius Mosbach"
            },
            {
                "id": "http://arxiv.org/abs/2602.21750v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21750v1",
                "title": "From Words to Amino Acids: Does the Curse of Depth Persist?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Words to Amino Acids: Does the Curse of Depth Persist?"
                },
                "updated": "2026-02-25T10:06:12Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    10,
                    6,
                    12,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21750v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Protein language models (PLMs) have become widely adopted as general-purpose models, demonstrating strong performance in protein engineering and de novo design. Like large language models (LLMs), they are typically trained as deep transformers with next-token or masked-token prediction objectives on massive sequence corpora and are scaled by increasing model depth. Recent work on autoregressive LLMs has identified the Curse of Depth: later layers contribute little to the final output predictions. These findings naturally raise the question of whether a similar depth inefficiency also appears in PLMs, where many widely used models are not autoregressive, and some are multimodal, accepting both protein sequence and structure as input. In this work, we present a depth analysis of six popular PLMs across model families and scales, spanning three training objectives, namely autoregressive, masked, and diffusion, and quantify how layer contributions evolve with depth using a unified set of probing- and perturbation-based measurements. Across all models, we observe consistent depth-dependent patterns that extend prior findings on LLMs: later layers depend less on earlier computations and mainly refine the final output distribution, and these effects are increasingly pronounced in deeper models. Taken together, our results suggest that PLMs exhibit a form of depth inefficiency, motivating future work on more depth-efficient architectures and training methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protein language models (PLMs) have become widely adopted as general-purpose models, demonstrating strong performance in protein engineering and de novo design. Like large language models (LLMs), they are typically trained as deep transformers with next-token or masked-token prediction objectives on massive sequence corpora and are scaled by increasing model depth. Recent work on autoregressive LLMs has identified the Curse of Depth: later layers contribute little to the final output predictions. These findings naturally raise the question of whether a similar depth inefficiency also appears in PLMs, where many widely used models are not autoregressive, and some are multimodal, accepting both protein sequence and structure as input. In this work, we present a depth analysis of six popular PLMs across model families and scales, spanning three training objectives, namely autoregressive, masked, and diffusion, and quantify how layer contributions evolve with depth using a unified set of probing- and perturbation-based measurements. Across all models, we observe consistent depth-dependent patterns that extend prior findings on LLMs: later layers depend less on earlier computations and mainly refine the final output distribution, and these effects are increasingly pronounced in deeper models. Taken together, our results suggest that PLMs exhibit a form of depth inefficiency, motivating future work on more depth-efficient architectures and training methods."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T10:06:12Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    10,
                    6,
                    12,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Aleena Siji"
                    },
                    {
                        "name": "Amir Mohammad Karimi Mamaghan"
                    },
                    {
                        "name": "Ferdinand Kapl"
                    },
                    {
                        "name": "Tobias Hppe"
                    },
                    {
                        "name": "Emmanouil Angelis"
                    },
                    {
                        "name": "Andrea Dittadi"
                    },
                    {
                        "name": "Maurice Brenner"
                    },
                    {
                        "name": "Michael Heinzinger"
                    },
                    {
                        "name": "Karl Henrik Johansson"
                    },
                    {
                        "name": "Kaitlin Maile"
                    },
                    {
                        "name": "Johannes von Oswald"
                    },
                    {
                        "name": "Stefan Bauer"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Bauer"
                },
                "author": "Stefan Bauer"
            },
            {
                "id": "http://arxiv.org/abs/2602.10606v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.10606v3",
                "title": "S-GRec: Personalized Semantic-Aware Generative Recommendation with Asymmetric Advantage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "S-GRec: Personalized Semantic-Aware Generative Recommendation with Asymmetric Advantage"
                },
                "updated": "2026-02-25T10:05:15Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    10,
                    5,
                    15,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.10606v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.10606v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative recommendation models sequence generation to produce items end-to-end, but training from behavioral logs often provides weak supervision on underlying user intent. Although Large Language Models (LLMs) offer rich semantic priors that could supply such supervision, direct adoption in industrial recommendation is hindered by two obstacles: semantic signals can conflict with platform business objectives, and LLM inference is prohibitively expensive at scale. This paper presents S-GRec, a semantic-aware framework that decouples an online lightweight generator from an offline LLM-based semantic judge for train-time supervision. S-GRec introduces a two-stage Personalized Semantic Judge (PSJ) that produces interpretable aspect evidence and learns user-conditional aggregation from pairwise feedback, yielding stable semantic rewards. To prevent semantic supervision from deviating from business goals, Asymmetric Advantage Policy Optimization (A2PO) anchors optimization on business rewards (e.g., eCPM) and injects semantic advantages only when they are consistent. Extensive experiments on public benchmarks and a large-scale production system validate both effectiveness and scalability, including statistically significant gains in CTR and a 1.19\\% lift in GMV in online A/B tests, without requiring real-time LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative recommendation models sequence generation to produce items end-to-end, but training from behavioral logs often provides weak supervision on underlying user intent. Although Large Language Models (LLMs) offer rich semantic priors that could supply such supervision, direct adoption in industrial recommendation is hindered by two obstacles: semantic signals can conflict with platform business objectives, and LLM inference is prohibitively expensive at scale. This paper presents S-GRec, a semantic-aware framework that decouples an online lightweight generator from an offline LLM-based semantic judge for train-time supervision. S-GRec introduces a two-stage Personalized Semantic Judge (PSJ) that produces interpretable aspect evidence and learns user-conditional aggregation from pairwise feedback, yielding stable semantic rewards. To prevent semantic supervision from deviating from business goals, Asymmetric Advantage Policy Optimization (A2PO) anchors optimization on business rewards (e.g., eCPM) and injects semantic advantages only when they are consistent. Extensive experiments on public benchmarks and a large-scale production system validate both effectiveness and scalability, including statistically significant gains in CTR and a 1.19\\% lift in GMV in online A/B tests, without requiring real-time LLM inference."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-11T07:54:26Z",
                "published_parsed": [
                    2026,
                    2,
                    11,
                    7,
                    54,
                    26,
                    2,
                    42,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Hongbo Tang"
                    },
                    {
                        "name": "Wenjie Wu"
                    },
                    {
                        "name": "Yangru Huang"
                    },
                    {
                        "name": "Zhenmao Li"
                    },
                    {
                        "name": "Qian Li"
                    },
                    {
                        "name": "Changping Wang"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Huan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Huan Yu"
                },
                "author": "Huan Yu"
            },
            {
                "id": "http://arxiv.org/abs/2511.13065v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13065v2",
                "title": "RobustGait: Robustness Analysis for Appearance Based Gait Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RobustGait: Robustness Analysis for Appearance Based Gait Recognition"
                },
                "updated": "2026-02-25T09:57:15Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    9,
                    57,
                    15,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13065v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13065v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Appearance-based gait recognition have achieved strong performance on controlled datasets, yet systematic evaluation of its robustness to real-world corruptions and silhouette variability remains lacking. We present RobustGait, a framework for fine-grained robustness evaluation of appearance-based gait recognition systems. RobustGait evaluation spans four dimensions: the type of perturbation (digital, environmental, temporal, occlusion), the silhouette extraction method (segmentation and parsing networks), the architectural capacities of gait recognition models, and various deployment scenarios. The benchmark introduces 15 corruption types at 5 severity levels across CASIA-B, CCPG, and SUSTech1K, with in-the-wild validation on MEVID, and evaluates six state-of-the-art gait systems. We came across several exciting insights. First, applying noise at the RGB level better reflects real-world degradation, and reveal how distortions propagate through silhouette extraction to the downstream gait recognition systems. Second, gait accuracy is highly sensitive to silhouette extractor biases, revealing an overlooked source of benchmark bias. Third, robustness is dependent on both the type of perturbation and the architectural design. Finally, we explore robustness-enhancing strategies, showing that noise-aware training and knowledge distillation improve performance and move toward deployment-ready systems. Code is available at https://reeshoon.github.io/robustgaitbenchmark",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Appearance-based gait recognition have achieved strong performance on controlled datasets, yet systematic evaluation of its robustness to real-world corruptions and silhouette variability remains lacking. We present RobustGait, a framework for fine-grained robustness evaluation of appearance-based gait recognition systems. RobustGait evaluation spans four dimensions: the type of perturbation (digital, environmental, temporal, occlusion), the silhouette extraction method (segmentation and parsing networks), the architectural capacities of gait recognition models, and various deployment scenarios. The benchmark introduces 15 corruption types at 5 severity levels across CASIA-B, CCPG, and SUSTech1K, with in-the-wild validation on MEVID, and evaluates six state-of-the-art gait systems. We came across several exciting insights. First, applying noise at the RGB level better reflects real-world degradation, and reveal how distortions propagate through silhouette extraction to the downstream gait recognition systems. Second, gait accuracy is highly sensitive to silhouette extractor biases, revealing an overlooked source of benchmark bias. Third, robustness is dependent on both the type of perturbation and the architectural design. Finally, we explore robustness-enhancing strategies, showing that noise-aware training and knowledge distillation improve performance and move toward deployment-ready systems. Code is available at https://reeshoon.github.io/robustgaitbenchmark"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T07:12:06Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    7,
                    12,
                    6,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "IEEE WACV'26 Main Conference",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Reeshoon Sayera"
                    },
                    {
                        "name": "Akash Kumar"
                    },
                    {
                        "name": "Sirshapan Mitra"
                    },
                    {
                        "name": "Prudvi Kamtam"
                    },
                    {
                        "name": "Yogesh S Rawat"
                    }
                ],
                "author_detail": {
                    "name": "Yogesh S Rawat"
                },
                "author": "Yogesh S Rawat"
            },
            {
                "id": "http://arxiv.org/abs/2602.17227v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.17227v3",
                "title": "Quantum key distribution over a metropolitan network using an integrated photonics based prototype",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum key distribution over a metropolitan network using an integrated photonics based prototype"
                },
                "updated": "2026-02-25T09:57:02Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    9,
                    57,
                    2,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.17227v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.17227v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "An industrial-scale adoption of Quantum Key Distribution (QKD) requires the development of practical, stable, resilient and cost-effective hardware that can be manufactured at large scales. In this work we present a high-speed (1.25GHz), field-deployable QKD prototype based on integrated photonics, that is consolidated into standard 19-inch rack compatible units. Through integrated photonics, the system prioritizes autonomous long-term stability in metropolitan settings. The architecture is further simplified by removing the need for chromatic dispersion compensation over metropolitan distances (below 100km). We demonstrate continuous key exchange over more than 4 km of metropolitan optical fiber, where the prototype maintained stable, uninterrupted operation across a measurement spanning more than 12 day-night cycles without manual intervention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An industrial-scale adoption of Quantum Key Distribution (QKD) requires the development of practical, stable, resilient and cost-effective hardware that can be manufactured at large scales. In this work we present a high-speed (1.25GHz), field-deployable QKD prototype based on integrated photonics, that is consolidated into standard 19-inch rack compatible units. Through integrated photonics, the system prioritizes autonomous long-term stability in metropolitan settings. The architecture is further simplified by removing the need for chromatic dispersion compensation over metropolitan distances (below 100km). We demonstrate continuous key exchange over more than 4 km of metropolitan optical fiber, where the prototype maintained stable, uninterrupted operation across a measurement spanning more than 12 day-night cycles without manual intervention."
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-19T10:18:37Z",
                "published_parsed": [
                    2026,
                    2,
                    19,
                    10,
                    18,
                    37,
                    3,
                    50,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph"
                },
                "authors": [
                    {
                        "name": "Maria Ana Pereira"
                    },
                    {
                        "name": "Giulio Gualandi"
                    },
                    {
                        "name": "Rebecka Sax"
                    },
                    {
                        "name": "Alberto Boaron"
                    },
                    {
                        "name": "Raphal Houlmann"
                    },
                    {
                        "name": "Roberto Osellame"
                    },
                    {
                        "name": "Rob Thew"
                    },
                    {
                        "name": "Hugo Zbinden"
                    }
                ],
                "author_detail": {
                    "name": "Hugo Zbinden"
                },
                "author": "Hugo Zbinden"
            },
            {
                "id": "http://arxiv.org/abs/2602.21744v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21744v1",
                "title": "Dual-Hop Joint Visible Light and Backscatter Communication Relaying under Finite Blocklength",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual-Hop Joint Visible Light and Backscatter Communication Relaying under Finite Blocklength"
                },
                "updated": "2026-02-25T09:55:16Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    9,
                    55,
                    16,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21744v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper investigates a dual-hop joint visible light communication (VLC) and backscatter communication (BC) relaying framework under the finite blocklength (FBL) constraint, aiming at energy-neutral Ambient Internet of Things (A-IoT) deployments. In the proposed system, indoor LED access points are used to simultaneously provide illumination and transmit information over light to a backscatter device (BD), which harvests optical energy and backscatters the received messages to user equipments (UEs) equipped with radio frequency (RF) front ends. This forwarding of the information from VLC to RF channels is implemented without the need for carrier synthesizers and power amplifiers at the IoT node. By modeling the end-to-end communication link with short-packet IoT traffic and realistic levels of interference between adjacent VLC coverage areas, we analyze the outage performance and achievable data rate of the proposed system. Simulation results demonstrate that key factors, such as placement and orientation of the BD, as well as the selected code rate of the system affect reliability and data rate that can be achieved for communication purposes. The insights gained from this study pave the way for ambient power-enabled IoT solutions and future hybrid VLC/RF network designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates a dual-hop joint visible light communication (VLC) and backscatter communication (BC) relaying framework under the finite blocklength (FBL) constraint, aiming at energy-neutral Ambient Internet of Things (A-IoT) deployments. In the proposed system, indoor LED access points are used to simultaneously provide illumination and transmit information over light to a backscatter device (BD), which harvests optical energy and backscatters the received messages to user equipments (UEs) equipped with radio frequency (RF) front ends. This forwarding of the information from VLC to RF channels is implemented without the need for carrier synthesizers and power amplifiers at the IoT node. By modeling the end-to-end communication link with short-packet IoT traffic and realistic levels of interference between adjacent VLC coverage areas, we analyze the outage performance and achievable data rate of the proposed system. Simulation results demonstrate that key factors, such as placement and orientation of the BD, as well as the selected code rate of the system affect reliability and data rate that can be achieved for communication purposes. The insights gained from this study pave the way for ambient power-enabled IoT solutions and future hybrid VLC/RF network designs."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T09:55:16Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    9,
                    55,
                    16,
                    2,
                    56,
                    0
                ],
                "arxiv_comment": "6 pages, 10 figures, 1 table",
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "authors": [
                    {
                        "name": "Boxuan Xie"
                    },
                    {
                        "name": "Lauri Mela"
                    },
                    {
                        "name": "Alexis A. Dowhuszko"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Kalle Ruttik"
                    },
                    {
                        "name": "Riku Jntti"
                    }
                ],
                "author_detail": {
                    "name": "Riku Jntti"
                },
                "author": "Riku Jntti"
            },
            {
                "id": "http://arxiv.org/abs/2602.21743v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2602.21743v1",
                "title": "Enhancing Multi-Modal LLMs Reasoning via Difficulty-Aware Group Normalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Multi-Modal LLMs Reasoning via Difficulty-Aware Group Normalization"
                },
                "updated": "2026-02-25T09:52:50Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    9,
                    52,
                    50,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2602.21743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2602.21743v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) and Group Relative Policy Optimization (GRPO) have significantly advanced the reasoning capabilities of large language models. Extending these methods to multimodal settings, however, faces a critical challenge: the instability of std-based normalization, which is easily distorted by extreme samples with nearly positive or negative rewards. Unlike pure-text LLMs, multimodal models are particularly sensitive to such distortions, as both perceptual and reasoning errors influence their responses. To address this, we characterize each sample by its difficulty, defined through perceptual complexity (measured via visual entropy) and reasoning uncertainty (captured by model confidence). Building on this characterization, we propose difficulty-aware group normalization (Durian), which re-groups samples by difficulty levels and shares the std within each group. Our approach preserves GRPO's intra-group distinctions while eliminating sensitivity to extreme cases, yielding significant performance gains across multiple multimodal reasoning benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) and Group Relative Policy Optimization (GRPO) have significantly advanced the reasoning capabilities of large language models. Extending these methods to multimodal settings, however, faces a critical challenge: the instability of std-based normalization, which is easily distorted by extreme samples with nearly positive or negative rewards. Unlike pure-text LLMs, multimodal models are particularly sensitive to such distortions, as both perceptual and reasoning errors influence their responses. To address this, we characterize each sample by its difficulty, defined through perceptual complexity (measured via visual entropy) and reasoning uncertainty (captured by model confidence). Building on this characterization, we propose difficulty-aware group normalization (Durian), which re-groups samples by difficulty levels and shares the std within each group. Our approach preserves GRPO's intra-group distinctions while eliminating sensitivity to extreme cases, yielding significant performance gains across multiple multimodal reasoning benchmarks."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-02-25T09:52:50Z",
                "published_parsed": [
                    2026,
                    2,
                    25,
                    9,
                    52,
                    50,
                    2,
                    56,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jinghan Li"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Xiaoyan Guo"
                    },
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He"
            },
            {
                "id": "http://arxiv.org/abs/2509.01552v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.01552v2",
                "title": "Variation-aware Vision Token Dropping for Faster Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variation-aware Vision Token Dropping for Faster Large Vision-Language Models"
                },
                "updated": "2026-02-25T09:41:37Z",
                "updated_parsed": [
                    2026,
                    2,
                    25,
                    9,
                    41,
                    37,
                    2,
                    56,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.01552v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.01552v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large vision-language models (LVLMs) have demonstrated remarkable capabilities in multimodal understanding tasks. However, the increasing demand for high-resolution image and long-video understanding results in substantial token counts, consequently leading to reduced inference efficiency. Token compression offers a direct solution by reducing the number of tokens to be processed, thereby improving computational efficiency without architectural changes. Through extensive analysis, we identify two critical limitations in existing inner-LLM token compression methods: positional bias and incompatibility with efficient operators, which critically hinder their practical deployment for LVLM acceleration. This paper presents the first approach from a dynamic token variation perspective, revealing that visual token variations within LLMs exhibit task-agnostic properties. We propose Variation-aware Vision Token Dropping (\\textit{i.e.}, \\textbf{V$^2$Drop}), which progressively removes visual tokens with minimal variation during LVLM inference, thereby enhancing computational efficiency. Extensive experiments across multiple models and benchmarks consistently demonstrate that V$^2$Drop maintains \\textbf{94.0\\%} and \\textbf{98.6\\%} of the original performance for image and video understanding tasks respectively, while reducing LLM generation latency by \\textbf{31.5\\%} and \\textbf{74.2\\%}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (LVLMs) have demonstrated remarkable capabilities in multimodal understanding tasks. However, the increasing demand for high-resolution image and long-video understanding results in substantial token counts, consequently leading to reduced inference efficiency. Token compression offers a direct solution by reducing the number of tokens to be processed, thereby improving computational efficiency without architectural changes. Through extensive analysis, we identify two critical limitations in existing inner-LLM token compression methods: positional bias and incompatibility with efficient operators, which critically hinder their practical deployment for LVLM acceleration. This paper presents the first approach from a dynamic token variation perspective, revealing that visual token variations within LLMs exhibit task-agnostic properties. We propose Variation-aware Vision Token Dropping (\\textit{i.e.}, \\textbf{V$^2$Drop}), which progressively removes visual tokens with minimal variation during LVLM inference, thereby enhancing computational efficiency. Extensive experiments across multiple models and benchmarks consistently demonstrate that V$^2$Drop maintains \\textbf{94.0\\%} and \\textbf{98.6\\%} of the original performance for image and video understanding tasks respectively, while reducing LLM generation latency by \\textbf{31.5\\%} and \\textbf{74.2\\%}."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-01T15:28:44Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    15,
                    28,
                    44,
                    0,
                    244,
                    0
                ],
                "arxiv_comment": "Accepted by CVPR 2026. Code is available at \\url{https://github.com/xuyang-liu16/V2Drop}",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Yiyu Wang"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Honggang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Honggang Chen"
                },
                "author": "Honggang Chen"
            }
        ]
    }
]