[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.01723v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v5",
                "updated": "2025-05-13T17:43:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    43,
                    47,
                    1,
                    133,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08587v1",
                "updated": "2025-05-13T13:58:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    58,
                    22,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T13:58:22Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    58,
                    22,
                    1,
                    133,
                    0
                ],
                "title": "Two-Level Sketching Alternating Anderson acceleration for Complex\n  Physics Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Level Sketching Alternating Anderson acceleration for Complex\n  Physics Applications"
                },
                "summary": "We present a novel two-level sketching extension of the Alternating\nAnderson-Picard (AAP) method for accelerating fixed-point iterations in\nchallenging single- and multi-physics simulations governed by discretized\npartial differential equations. Our approach combines a static, physics-based\nprojection that reduces the least-squares problem to the most informative field\n(e.g., via Schur-complement insight) with a dynamic, algebraic sketching stage\ndriven by a backward stability analysis under Lipschitz continuity. We\nintroduce inexpensive estimators for stability thresholds and cache-aware\nrandomized selection strategies to balance computational cost against\nmemory-access overhead. The resulting algorithm solves reduced least-squares\nsystems in place, minimizes memory footprints, and seamlessly alternates\nbetween low-cost Picard updates and Anderson mixing. Implemented in Julia, our\ntwo-level sketching AAP achieves up to 50% time-to-solution reductions compared\nto standard Anderson acceleration-without degrading convergence rates-on\nbenchmark problems including Stokes, p-Laplacian, Bidomain, and Navier-Stokes\nformulations at varying problem sizes. These results demonstrate the method's\nrobustness, scalability, and potential for integration into high-performance\nscientific computing frameworks. Our implementation is available open-source in\nthe AAP.jl library.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel two-level sketching extension of the Alternating\nAnderson-Picard (AAP) method for accelerating fixed-point iterations in\nchallenging single- and multi-physics simulations governed by discretized\npartial differential equations. Our approach combines a static, physics-based\nprojection that reduces the least-squares problem to the most informative field\n(e.g., via Schur-complement insight) with a dynamic, algebraic sketching stage\ndriven by a backward stability analysis under Lipschitz continuity. We\nintroduce inexpensive estimators for stability thresholds and cache-aware\nrandomized selection strategies to balance computational cost against\nmemory-access overhead. The resulting algorithm solves reduced least-squares\nsystems in place, minimizes memory footprints, and seamlessly alternates\nbetween low-cost Picard updates and Anderson mixing. Implemented in Julia, our\ntwo-level sketching AAP achieves up to 50% time-to-solution reductions compared\nto standard Anderson acceleration-without degrading convergence rates-on\nbenchmark problems including Stokes, p-Laplacian, Bidomain, and Navier-Stokes\nformulations at varying problem sizes. These results demonstrate the method's\nrobustness, scalability, and potential for integration into high-performance\nscientific computing frameworks. Our implementation is available open-source in\nthe AAP.jl library."
                },
                "authors": [
                    {
                        "name": "Nicolás A. Barnafi"
                    },
                    {
                        "name": "Massimiliano Lupo Pasini"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Lupo Pasini"
                },
                "author": "Massimiliano Lupo Pasini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65N12, 65N22, 65K10, 65F10, 65F99, 65B99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13989v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13989v2",
                "updated": "2025-05-13T09:36:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    36,
                    3,
                    1,
                    133,
                    0
                ],
                "published": "2025-04-18T13:46:58Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "title": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs"
                },
                "summary": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40% increase in\naccuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40% increase in\naccuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization."
                },
                "authors": [
                    {
                        "name": "Lucas Maisonnave"
                    },
                    {
                        "name": "Cyril Moineau"
                    },
                    {
                        "name": "Olivier Bichler"
                    },
                    {
                        "name": "Fabrice Rastello"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice Rastello"
                },
                "author": "Fabrice Rastello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13989v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13989v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08261v1",
                "updated": "2025-05-13T06:24:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    24,
                    48,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T06:24:48Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    24,
                    48,
                    1,
                    133,
                    0
                ],
                "title": "Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual\n  Compression for Scalable Knowledge Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual\n  Compression for Scalable Knowledge Integration"
                },
                "summary": "The rapid progress in large language models (LLMs) has paved the way for\nnovel approaches in knowledge-intensive tasks. Among these, Cache-Augmented\nGeneration (CAG) has emerged as a promising alternative to Retrieval-Augmented\nGeneration (RAG). CAG minimizes retrieval latency and simplifies system design\nby preloading knowledge into the model's context. However, challenges persist\nin scaling CAG to accommodate large and dynamic knowledge bases effectively.\nThis paper introduces Adaptive Contextual Compression (ACC), an innovative\ntechnique designed to dynamically compress and manage context inputs, enabling\nefficient utilization of the extended memory capabilities of modern LLMs. To\nfurther address the limitations of standalone CAG, we propose a Hybrid CAG-RAG\nFramework, which integrates selective retrieval to augment preloaded contexts\nin scenarios requiring additional information. Comprehensive evaluations on\ndiverse datasets highlight the proposed methods' ability to enhance\nscalability, optimize efficiency, and improve multi-hop reasoning performance,\noffering practical solutions for real-world knowledge integration challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress in large language models (LLMs) has paved the way for\nnovel approaches in knowledge-intensive tasks. Among these, Cache-Augmented\nGeneration (CAG) has emerged as a promising alternative to Retrieval-Augmented\nGeneration (RAG). CAG minimizes retrieval latency and simplifies system design\nby preloading knowledge into the model's context. However, challenges persist\nin scaling CAG to accommodate large and dynamic knowledge bases effectively.\nThis paper introduces Adaptive Contextual Compression (ACC), an innovative\ntechnique designed to dynamically compress and manage context inputs, enabling\nefficient utilization of the extended memory capabilities of modern LLMs. To\nfurther address the limitations of standalone CAG, we propose a Hybrid CAG-RAG\nFramework, which integrates selective retrieval to augment preloaded contexts\nin scenarios requiring additional information. Comprehensive evaluations on\ndiverse datasets highlight the proposed methods' ability to enhance\nscalability, optimize efficiency, and improve multi-hop reasoning performance,\noffering practical solutions for real-world knowledge integration challenges."
                },
                "authors": [
                    {
                        "name": "Rishabh Agrawal"
                    },
                    {
                        "name": "Himanshu Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Himanshu Kumar"
                },
                "author": "Himanshu Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07692v1",
                "updated": "2025-05-12T15:58:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    58,
                    39,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T15:58:39Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    58,
                    39,
                    0,
                    132,
                    0
                ],
                "title": "ABase: the Multi-Tenant NoSQL Serverless Database for Diverse and\n  Dynamic Workloads in Large-scale Cloud Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ABase: the Multi-Tenant NoSQL Serverless Database for Diverse and\n  Dynamic Workloads in Large-scale Cloud Environments"
                },
                "summary": "Multi-tenant architectures enhance the elasticity and resource utilization of\nNoSQL databases by allowing multiple tenants to co-locate and share resources.\nHowever, in large-scale cloud environments, the diverse and dynamic nature of\nworkloads poses significant challenges for multi-tenant NoSQL databases. Based\non our practical observations, we have identified three crucial challenges: (1)\nthe impact of caching on performance isolation, as cache hits alter request\nexecution and resource consumption, leading to inaccurate traffic control; (2)\nthe dynamic changes in traffic, with changes in tenant traffic trends causing\nthrottling or resource wastage, and changes in access distribution causing hot\nkey pressure or cache hit ratio drops; and (3) the imbalanced layout of data\nnodes due to tenants' diverse resource requirements, leading to low resource\nutilization. To address these challenges, we introduce ABase, a multi-tenant\nNoSQL serverless database developed at ByteDance. ABase introduces a two-layer\ncaching mechanism with a cache-aware isolation mechanism to ensure accurate\nresource consumption estimates. Furthermore, ABase employs a predictive\nautoscaling policy to dynamically adjust resources in response to tenant\ntraffic changes and a multi-resource rescheduling algorithm to balance resource\nutilization across data nodes. With these innovations, ABase has successfully\nserved ByteDance's large-scale cloud environment, supporting a total workload\nthat has achieved a peak QPS of over 13 billion and total storage exceeding 1\nEB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-tenant architectures enhance the elasticity and resource utilization of\nNoSQL databases by allowing multiple tenants to co-locate and share resources.\nHowever, in large-scale cloud environments, the diverse and dynamic nature of\nworkloads poses significant challenges for multi-tenant NoSQL databases. Based\non our practical observations, we have identified three crucial challenges: (1)\nthe impact of caching on performance isolation, as cache hits alter request\nexecution and resource consumption, leading to inaccurate traffic control; (2)\nthe dynamic changes in traffic, with changes in tenant traffic trends causing\nthrottling or resource wastage, and changes in access distribution causing hot\nkey pressure or cache hit ratio drops; and (3) the imbalanced layout of data\nnodes due to tenants' diverse resource requirements, leading to low resource\nutilization. To address these challenges, we introduce ABase, a multi-tenant\nNoSQL serverless database developed at ByteDance. ABase introduces a two-layer\ncaching mechanism with a cache-aware isolation mechanism to ensure accurate\nresource consumption estimates. Furthermore, ABase employs a predictive\nautoscaling policy to dynamically adjust resources in response to tenant\ntraffic changes and a multi-resource rescheduling algorithm to balance resource\nutilization across data nodes. With these innovations, ABase has successfully\nserved ByteDance's large-scale cloud environment, supporting a total workload\nthat has achieved a peak QPS of over 13 billion and total storage exceeding 1\nEB."
                },
                "authors": [
                    {
                        "name": "Rong Kang"
                    },
                    {
                        "name": "Yanbin Chen"
                    },
                    {
                        "name": "Ye Liu"
                    },
                    {
                        "name": "Fuxin Jiang"
                    },
                    {
                        "name": "Qingshuo Li"
                    },
                    {
                        "name": "Miao Ma"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Guangliang Zhao"
                    },
                    {
                        "name": "Tieying Zhang"
                    },
                    {
                        "name": "Jianjun Chen"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "SIGMOD 2025 accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07680v1",
                "updated": "2025-05-12T15:46:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    46,
                    28,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T15:46:28Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    46,
                    28,
                    0,
                    132,
                    0
                ],
                "title": "SpecRouter: Adaptive Routing for Multi-Level Speculative Decoding in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecRouter: Adaptive Routing for Multi-Level Speculative Decoding in\n  Large Language Models"
                },
                "summary": "Large Language Models (LLMs) present a critical trade-off between inference\nquality and computational cost: larger models offer superior capabilities but\nincur significant latency, while smaller models are faster but less powerful.\nExisting serving strategies often employ fixed model scales or static two-stage\nspeculative decoding, failing to dynamically adapt to the varying complexities\nof user requests or fluctuations in system performance. This paper introduces\n\\systemname{}, a novel framework that reimagines LLM inference as an adaptive\nrouting problem solved through multi-level speculative decoding. \\systemname{}\ndynamically constructs and optimizes inference \"paths\" (chains of models) based\non real-time feedback, addressing the limitations of static approaches. Our\ncontributions are threefold: (1) An \\textbf{adaptive model chain scheduling}\nmechanism that leverages performance profiling (execution times) and predictive\nsimilarity metrics (derived from token distribution divergence) to continuously\nselect the optimal sequence of draft and verifier models, minimizing predicted\nlatency per generated token. (2) A \\textbf{multi-level collaborative\nverification} framework where intermediate models within the selected chain can\nvalidate speculative tokens, reducing the verification burden on the final,\nmost powerful target model. (3) A \\textbf{synchronized state management} system\nproviding efficient, consistent KV cache handling across heterogeneous models\nin the chain, including precise, low-overhead rollbacks tailored for\nasynchronous batch processing inherent in multi-level speculation. Preliminary\nexperiments demonstrate the validity of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) present a critical trade-off between inference\nquality and computational cost: larger models offer superior capabilities but\nincur significant latency, while smaller models are faster but less powerful.\nExisting serving strategies often employ fixed model scales or static two-stage\nspeculative decoding, failing to dynamically adapt to the varying complexities\nof user requests or fluctuations in system performance. This paper introduces\n\\systemname{}, a novel framework that reimagines LLM inference as an adaptive\nrouting problem solved through multi-level speculative decoding. \\systemname{}\ndynamically constructs and optimizes inference \"paths\" (chains of models) based\non real-time feedback, addressing the limitations of static approaches. Our\ncontributions are threefold: (1) An \\textbf{adaptive model chain scheduling}\nmechanism that leverages performance profiling (execution times) and predictive\nsimilarity metrics (derived from token distribution divergence) to continuously\nselect the optimal sequence of draft and verifier models, minimizing predicted\nlatency per generated token. (2) A \\textbf{multi-level collaborative\nverification} framework where intermediate models within the selected chain can\nvalidate speculative tokens, reducing the verification burden on the final,\nmost powerful target model. (3) A \\textbf{synchronized state management} system\nproviding efficient, consistent KV cache handling across heterogeneous models\nin the chain, including precise, low-overhead rollbacks tailored for\nasynchronous batch processing inherent in multi-level speculation. Preliminary\nexperiments demonstrate the validity of our method."
                },
                "authors": [
                    {
                        "name": "Hang Wu"
                    },
                    {
                        "name": "Jianian Zhu"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Haojie Wang"
                    },
                    {
                        "name": "Biao Hou"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07350v1",
                "updated": "2025-05-12T08:44:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    8,
                    44,
                    10,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T08:44:10Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    8,
                    44,
                    10,
                    0,
                    132,
                    0
                ],
                "title": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films"
                },
                "summary": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems."
                },
                "authors": [
                    {
                        "name": "Roy Styles"
                    },
                    {
                        "name": "Mengke Han"
                    },
                    {
                        "name": "Toon Goris"
                    },
                    {
                        "name": "James Partridge"
                    },
                    {
                        "name": "Brett C. Johnson"
                    },
                    {
                        "name": "Blanca del Rosal"
                    },
                    {
                        "name": "Amanda N. Abraham"
                    },
                    {
                        "name": "Heike Ebendorff-Heidepriem"
                    },
                    {
                        "name": "Brant C. Gibson"
                    },
                    {
                        "name": "Nikolai Dontschuk"
                    },
                    {
                        "name": "Jean-Philippe Tetienne"
                    },
                    {
                        "name": "Philipp Reineck"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Reineck"
                },
                "author": "Philipp Reineck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07274v1",
                "updated": "2025-05-12T06:53:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    6,
                    53,
                    24,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T06:53:24Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    6,
                    53,
                    24,
                    0,
                    132,
                    0
                ],
                "title": "Cache-Efficient Posterior Sampling for Reinforcement Learning with\n  LLM-Derived Priors Across Discrete and Continuous Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Efficient Posterior Sampling for Reinforcement Learning with\n  LLM-Derived Priors Across Discrete and Continuous Domains"
                },
                "summary": "Integrating large language models (LLMs) as priors in reinforcement learning\n(RL) offers significant advantages but comes with substantial computational\ncosts. We present a principled cache-efficient framework for posterior sampling\nwith LLM-derived priors that dramatically reduces these costs while maintaining\nhigh performance. At the core of our approach is an adaptive caching mechanism,\nwhere cache parameters are meta-optimized using surrogate gradients derived\nfrom policy performance. This design enables efficient inference across both\ndiscrete text environments (e.g., TextWorld, ALFWorld) and continuous control\ndomains (e.g., MuJoCo), achieving a 3.8--4.7$\\times$ reduction in LLM queries\nand 4.0--12.0$\\times$ lower median latencies (85--93\\,ms on a consumer GPU)\nwhile retaining 96--98\\% of uncached performance. Our theoretical analysis\nprovides KL divergence bounds on approximation quality, validated empirically.\nThe framework extends to offline RL, where our CQL-Prior variant improves\nperformance by 14--29\\% and reduces training time by 38--40\\%. Extensive\nevaluations across a diverse suite of eight tasks demonstrate the\ngeneralizability and practical viability of LLM-guided RL in\nresource-constrained settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating large language models (LLMs) as priors in reinforcement learning\n(RL) offers significant advantages but comes with substantial computational\ncosts. We present a principled cache-efficient framework for posterior sampling\nwith LLM-derived priors that dramatically reduces these costs while maintaining\nhigh performance. At the core of our approach is an adaptive caching mechanism,\nwhere cache parameters are meta-optimized using surrogate gradients derived\nfrom policy performance. This design enables efficient inference across both\ndiscrete text environments (e.g., TextWorld, ALFWorld) and continuous control\ndomains (e.g., MuJoCo), achieving a 3.8--4.7$\\times$ reduction in LLM queries\nand 4.0--12.0$\\times$ lower median latencies (85--93\\,ms on a consumer GPU)\nwhile retaining 96--98\\% of uncached performance. Our theoretical analysis\nprovides KL divergence bounds on approximation quality, validated empirically.\nThe framework extends to offline RL, where our CQL-Prior variant improves\nperformance by 14--29\\% and reduces training time by 38--40\\%. Extensive\nevaluations across a diverse suite of eight tasks demonstrate the\ngeneralizability and practical viability of LLM-guided RL in\nresource-constrained settings."
                },
                "authors": [
                    {
                        "name": "Ibne Farabi Shihab"
                    },
                    {
                        "name": "Sanjeda Akter"
                    },
                    {
                        "name": "Anuj Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Sharma"
                },
                "author": "Anuj Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07239v1",
                "updated": "2025-05-12T05:29:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    5,
                    29,
                    30,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T05:29:30Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    5,
                    29,
                    30,
                    0,
                    132,
                    0
                ],
                "title": "Comet: Accelerating Private Inference for Large Language Model by\n  Predicting Activation Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comet: Accelerating Private Inference for Large Language Model by\n  Predicting Activation Sparsity"
                },
                "summary": "With the growing use of large language models (LLMs) hosted on cloud\nplatforms to offer inference services, privacy concerns about the potential\nleakage of sensitive information are escalating. Secure multi-party computation\n(MPC) is a promising solution to protect the privacy in LLM inference. However,\nMPC requires frequent inter-server communication, causing high performance\noverhead.\n  Inspired by the prevalent activation sparsity of LLMs, where most neuron are\nnot activated after non-linear activation functions, we propose an efficient\nprivate inference system, Comet. This system employs an accurate and fast\npredictor to predict the sparsity distribution of activation function output.\nAdditionally, we introduce a new private inference protocol. It efficiently and\nsecurely avoids computations involving zero values by exploiting the spatial\nlocality of the predicted sparse distribution. While this computation-avoidance\napproach impacts the spatiotemporal continuity of KV cache entries, we address\nthis challenge with a low-communication overhead cache refilling strategy that\nmerges miss requests and incorporates a prefetching mechanism. Finally, we\nevaluate Comet on four common LLMs and compare it with six state-of-the-art\nprivate inference systems. Comet achieves a 1.87x-2.63x speedup and a\n1.94x-2.64x communication reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing use of large language models (LLMs) hosted on cloud\nplatforms to offer inference services, privacy concerns about the potential\nleakage of sensitive information are escalating. Secure multi-party computation\n(MPC) is a promising solution to protect the privacy in LLM inference. However,\nMPC requires frequent inter-server communication, causing high performance\noverhead.\n  Inspired by the prevalent activation sparsity of LLMs, where most neuron are\nnot activated after non-linear activation functions, we propose an efficient\nprivate inference system, Comet. This system employs an accurate and fast\npredictor to predict the sparsity distribution of activation function output.\nAdditionally, we introduce a new private inference protocol. It efficiently and\nsecurely avoids computations involving zero values by exploiting the spatial\nlocality of the predicted sparse distribution. While this computation-avoidance\napproach impacts the spatiotemporal continuity of KV cache entries, we address\nthis challenge with a low-communication overhead cache refilling strategy that\nmerges miss requests and incorporates a prefetching mechanism. Finally, we\nevaluate Comet on four common LLMs and compare it with six state-of-the-art\nprivate inference systems. Comet achieves a 1.87x-2.63x speedup and a\n1.94x-2.64x communication reduction."
                },
                "authors": [
                    {
                        "name": "Guang Yan"
                    },
                    {
                        "name": "Yuhui Zhang"
                    },
                    {
                        "name": "Zimu Guo"
                    },
                    {
                        "name": "Lutan Zhao"
                    },
                    {
                        "name": "Xiaojun Chen"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_doi": "10.1109/SP61157.2025.00182",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/SP61157.2025.00182",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.07239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to SP 2025",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07203v1",
                "updated": "2025-05-12T03:22:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    3,
                    22,
                    29,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T03:22:29Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    3,
                    22,
                    29,
                    0,
                    132,
                    0
                ],
                "title": "PrefillOnly: An Inference Engine for Prefill-only Workloads in Large\n  Language Model Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefillOnly: An Inference Engine for Prefill-only Workloads in Large\n  Language Model Applications"
                },
                "summary": "Besides typical generative applications, like ChatGPT, GitHub Copilot, and\nCursor, we observe an emerging trend that LLMs are increasingly used in\ntraditional discriminative tasks, such as recommendation, credit verification,\nand data labeling. The key characteristic of these emerging use cases is that\nthe LLM generates only a single output token, rather than an arbitrarily long\nsequence of tokens. We call this prefill-only workload. However, since existing\nLLM engines assume arbitrary output lengths, they fail to leverage the unique\nproperties of prefill-only workloads. In this paper, we present PrefillOnly,\nthe first LLM inference engine that improves the inference throughput and\nlatency by fully embracing the properties of prefill-only workloads. First,\nsince it generates only one token, PrefillOnly only needs to store the KV cache\nof only the last computed layer, rather than of all layers. This drastically\nreduces the GPU memory footprint of LLM inference and allows handling long\ninputs without using solutions that reduces throughput, such as cross-GPU KV\ncache parallelization. Second, because the output length is fixed, rather than\narbitrary, PrefillOnly can precisely determine the job completion time (JCT) of\neach prefill-only request before it starts. This enables efficient JCT-aware\nscheduling policies such as shortest remaining job first. PrefillOnly can\nprocess upto 4x larger queries per second without inflating average and P99\nlatency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Besides typical generative applications, like ChatGPT, GitHub Copilot, and\nCursor, we observe an emerging trend that LLMs are increasingly used in\ntraditional discriminative tasks, such as recommendation, credit verification,\nand data labeling. The key characteristic of these emerging use cases is that\nthe LLM generates only a single output token, rather than an arbitrarily long\nsequence of tokens. We call this prefill-only workload. However, since existing\nLLM engines assume arbitrary output lengths, they fail to leverage the unique\nproperties of prefill-only workloads. In this paper, we present PrefillOnly,\nthe first LLM inference engine that improves the inference throughput and\nlatency by fully embracing the properties of prefill-only workloads. First,\nsince it generates only one token, PrefillOnly only needs to store the KV cache\nof only the last computed layer, rather than of all layers. This drastically\nreduces the GPU memory footprint of LLM inference and allows handling long\ninputs without using solutions that reduces throughput, such as cross-GPU KV\ncache parallelization. Second, because the output length is fixed, rather than\narbitrary, PrefillOnly can precisely determine the job completion time (JCT) of\neach prefill-only request before it starts. This enables efficient JCT-aware\nscheduling policies such as shortest remaining job first. PrefillOnly can\nprocess upto 4x larger queries per second without inflating average and P99\nlatency."
                },
                "authors": [
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Bowen Wang"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yiming Cheng"
                    },
                    {
                        "name": "Qing Lan"
                    },
                    {
                        "name": "Hejian Sang"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Xiaoxuan Liu"
                    },
                    {
                        "name": "Yifan Qiao"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06901v1",
                "updated": "2025-05-11T08:44:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    11,
                    8,
                    44,
                    31,
                    6,
                    131,
                    0
                ],
                "published": "2025-05-11T08:44:31Z",
                "published_parsed": [
                    2025,
                    5,
                    11,
                    8,
                    44,
                    31,
                    6,
                    131,
                    0
                ],
                "title": "Ecco: Improving Memory Bandwidth and Capacity for LLMs via Entropy-aware\n  Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ecco: Improving Memory Bandwidth and Capacity for LLMs via Entropy-aware\n  Cache Compression"
                },
                "summary": "Large language models (LLMs) have demonstrated transformative capabilities\nacross diverse artificial intelligence applications, yet their deployment is\nhindered by substantial memory and computational demands, especially in\nresource-constrained environments. Quantization techniques have emerged as a\ncritical solution, reducing data precision to enhance memory and computational\nefficiency. However, existing methods often suffer from high runtime overheads\nand potential accuracy degradation. To address these challenges, we propose\nEcco, an entropy-based cache compression technique tailored for LLMs. Ecco\ncombines group-wise and non-uniform quantization with pre-defined shared\nk-means patterns and Huffman coding to exploit the inherent entropy\ncharacteristics of LLM cache data. Recognizing the inefficiencies of\ntraditional Huffman coding in terms of parallelism and latency, we introduce a\nnovel parallel Huffman-based decoding process with a multi-stage pipeline\ndesign, reducing latency by two orders of magnitude and achieving throughput\ncomparable to GPU L2 caches. Comprehensive evaluations demonstrate that Ecco\nachieves an up to 2.9$\\times$ and 1.9$\\times$ speedup over the state-of-the-art\nAWQ and SmoothQuant framework, 2.4$\\times$ over the Olive accelerator, all\nwhile increasing memory capacity by nearly 4$\\times$ and maintaining\nstate-of-the-art LLM accuracy. These results underscore the effectiveness of\nour entropy-based cache compression in enhancing LLM performance and\nefficiency, paving the way for more deployable large-scale AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated transformative capabilities\nacross diverse artificial intelligence applications, yet their deployment is\nhindered by substantial memory and computational demands, especially in\nresource-constrained environments. Quantization techniques have emerged as a\ncritical solution, reducing data precision to enhance memory and computational\nefficiency. However, existing methods often suffer from high runtime overheads\nand potential accuracy degradation. To address these challenges, we propose\nEcco, an entropy-based cache compression technique tailored for LLMs. Ecco\ncombines group-wise and non-uniform quantization with pre-defined shared\nk-means patterns and Huffman coding to exploit the inherent entropy\ncharacteristics of LLM cache data. Recognizing the inefficiencies of\ntraditional Huffman coding in terms of parallelism and latency, we introduce a\nnovel parallel Huffman-based decoding process with a multi-stage pipeline\ndesign, reducing latency by two orders of magnitude and achieving throughput\ncomparable to GPU L2 caches. Comprehensive evaluations demonstrate that Ecco\nachieves an up to 2.9$\\times$ and 1.9$\\times$ speedup over the state-of-the-art\nAWQ and SmoothQuant framework, 2.4$\\times$ over the Olive accelerator, all\nwhile increasing memory capacity by nearly 4$\\times$ and maintaining\nstate-of-the-art LLM accuracy. These results underscore the effectiveness of\nour entropy-based cache compression in enhancing LLM performance and\nefficiency, paving the way for more deployable large-scale AI models."
                },
                "authors": [
                    {
                        "name": "Feng Cheng"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Chiyue Wei"
                    },
                    {
                        "name": "Junyao Zhang"
                    },
                    {
                        "name": "Changchun Zhou"
                    },
                    {
                        "name": "Edward Hanson"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Xiaoxiao Liu"
                    },
                    {
                        "name": "Hai \"Helen\" Li"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "arxiv_comment": "ISCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06738v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06738v1",
                "updated": "2025-05-10T19:06:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    10,
                    19,
                    6,
                    37,
                    5,
                    130,
                    0
                ],
                "published": "2025-05-10T19:06:37Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    19,
                    6,
                    37,
                    5,
                    130,
                    0
                ],
                "title": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference"
                },
                "summary": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output)."
                },
                "authors": [
                    {
                        "name": "Zibo Gao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Feng Guo"
                    },
                    {
                        "name": "Yixin Zhang"
                    },
                    {
                        "name": "Yinglong Han"
                    },
                    {
                        "name": "Siyuan Liu"
                    },
                    {
                        "name": "Haiyang Li"
                    },
                    {
                        "name": "Zhiqiang Lv"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Lv"
                },
                "author": "Zhiqiang Lv",
                "arxiv_comment": "Submitted to USENIX Security '25 Cycle 2 in Wednesday, January 22,\n  2025. Under Shepherding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06738v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06738v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06625v1",
                "updated": "2025-05-10T12:16:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    10,
                    12,
                    16,
                    50,
                    5,
                    130,
                    0
                ],
                "published": "2025-05-10T12:16:50Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    12,
                    16,
                    50,
                    5,
                    130,
                    0
                ],
                "title": "CaMDN: Enhancing Cache Efficiency for Multi-tenant DNNs on Integrated\n  NPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaMDN: Enhancing Cache Efficiency for Multi-tenant DNNs on Integrated\n  NPUs"
                },
                "summary": "With the rapid development of DNN applications, multi-tenant execution, where\nmultiple DNNs are co-located on a single SoC, is becoming a prevailing trend.\nAlthough many methods are proposed in prior works to improve multi-tenant\nperformance, the impact of shared cache is not well studied. This paper\nproposes CaMDN, an architecture-scheduling co-design to enhance cache\nefficiency for multi-tenant DNNs on integrated NPUs. Specifically, a\nlightweight architecture is proposed to support model-exclusive, NPU-controlled\nregions inside shared cache to eliminate unexpected cache contention. Moreover,\na cache scheduling method is proposed to improve shared cache utilization. In\nparticular, it includes a cache-aware mapping method for adaptability to the\nvarying available cache capacity and a dynamic allocation algorithm to adjust\nthe usage among co-located DNNs at runtime. Compared to prior works, CaMDN\nreduces the memory access by 33.4% on average and achieves a model speedup of\nup to 2.56$\\times$ (1.88$\\times$ on average).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of DNN applications, multi-tenant execution, where\nmultiple DNNs are co-located on a single SoC, is becoming a prevailing trend.\nAlthough many methods are proposed in prior works to improve multi-tenant\nperformance, the impact of shared cache is not well studied. This paper\nproposes CaMDN, an architecture-scheduling co-design to enhance cache\nefficiency for multi-tenant DNNs on integrated NPUs. Specifically, a\nlightweight architecture is proposed to support model-exclusive, NPU-controlled\nregions inside shared cache to eliminate unexpected cache contention. Moreover,\na cache scheduling method is proposed to improve shared cache utilization. In\nparticular, it includes a cache-aware mapping method for adaptability to the\nvarying available cache capacity and a dynamic allocation algorithm to adjust\nthe usage among co-located DNNs at runtime. Compared to prior works, CaMDN\nreduces the memory access by 33.4% on average and achieves a model speedup of\nup to 2.56$\\times$ (1.88$\\times$ on average)."
                },
                "authors": [
                    {
                        "name": "Tianhao Cai"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Limin Xiao"
                    },
                    {
                        "name": "Meng Han"
                    },
                    {
                        "name": "Zeyu Wang"
                    },
                    {
                        "name": "Lin Sun"
                    },
                    {
                        "name": "Xiaojian Liao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojian Liao"
                },
                "author": "Xiaojian Liao",
                "arxiv_comment": "7 pages, 9 figures. This paper has been accepted to the 2025 Design\n  Automation Conference (DAC)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06556v1",
                "updated": "2025-05-10T07:57:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    10,
                    7,
                    57,
                    2,
                    5,
                    130,
                    0
                ],
                "published": "2025-05-10T07:57:02Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    7,
                    57,
                    2,
                    5,
                    130,
                    0
                ],
                "title": "TierBase: A Workload-Driven Cost-Optimized Key-Value Store",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TierBase: A Workload-Driven Cost-Optimized Key-Value Store"
                },
                "summary": "In the current era of data-intensive applications, the demand for\nhigh-performance, cost-effective storage solutions is paramount. This paper\nintroduces a Space-Performance Cost Model for key-value store, designed to\nguide cost-effective storage configuration decisions. The model quantifies the\ntrade-offs between performance and storage costs, providing a framework for\noptimizing resource allocation in large-scale data serving environments. Guided\nby this cost model, we present TierBase, a distributed key-value store\ndeveloped by Ant Group that optimizes total cost by strategically synchronizing\ndata between cache and storage tiers, maximizing resource utilization and\neffectively handling skewed workloads. To enhance cost-efficiency, TierBase\nincorporates several optimization techniques, including pre-trained data\ncompression, elastic threading mechanisms, and the utilization of persistent\nmemory. We detail TierBase's architecture, key components, and the\nimplementation of cost optimization strategies. Extensive evaluations using\nboth synthetic benchmarks and real-world workloads demonstrate TierBase's\nsuperior cost-effectiveness compared to existing solutions. Furthermore, case\nstudies from Ant Group's production environments showcase TierBase's ability to\nachieve up to 62% cost reduction in primary scenarios, highlighting its\npractical impact in large-scale online data serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the current era of data-intensive applications, the demand for\nhigh-performance, cost-effective storage solutions is paramount. This paper\nintroduces a Space-Performance Cost Model for key-value store, designed to\nguide cost-effective storage configuration decisions. The model quantifies the\ntrade-offs between performance and storage costs, providing a framework for\noptimizing resource allocation in large-scale data serving environments. Guided\nby this cost model, we present TierBase, a distributed key-value store\ndeveloped by Ant Group that optimizes total cost by strategically synchronizing\ndata between cache and storage tiers, maximizing resource utilization and\neffectively handling skewed workloads. To enhance cost-efficiency, TierBase\nincorporates several optimization techniques, including pre-trained data\ncompression, elastic threading mechanisms, and the utilization of persistent\nmemory. We detail TierBase's architecture, key components, and the\nimplementation of cost optimization strategies. Extensive evaluations using\nboth synthetic benchmarks and real-world workloads demonstrate TierBase's\nsuperior cost-effectiveness compared to existing solutions. Furthermore, case\nstudies from Ant Group's production environments showcase TierBase's ability to\nachieve up to 62% cost reduction in primary scenarios, highlighting its\npractical impact in large-scale online data serving."
                },
                "authors": [
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Shiyu Yang"
                    },
                    {
                        "name": "Weibo Chen"
                    },
                    {
                        "name": "Kunming Wang"
                    },
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Junwei Chen"
                    },
                    {
                        "name": "Yuan Su"
                    },
                    {
                        "name": "Xiaoxia Duan"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Jie Song"
                    },
                    {
                        "name": "Ruoyi Ruan"
                    },
                    {
                        "name": "Xuemin Lin"
                    }
                ],
                "author_detail": {
                    "name": "Xuemin Lin"
                },
                "author": "Xuemin Lin",
                "arxiv_comment": "Accepted by ICDE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07872v1",
                "updated": "2025-05-09T21:05:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    21,
                    5,
                    20,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T21:05:20Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    21,
                    5,
                    20,
                    4,
                    129,
                    0
                ],
                "title": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions"
                },
                "summary": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion."
                },
                "authors": [
                    {
                        "name": "Yijing Zhang"
                    },
                    {
                        "name": "Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06095v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06095v3",
                "updated": "2025-05-09T07:26:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    7,
                    26,
                    29,
                    4,
                    129,
                    0
                ],
                "published": "2024-06-10T08:26:27Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    8,
                    26,
                    27,
                    0,
                    162,
                    0
                ],
                "title": "An extension of C++ with memory-centric specifications for HPC to reduce\n  memory footprints and streamline MPI development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An extension of C++ with memory-centric specifications for HPC to reduce\n  memory footprints and streamline MPI development"
                },
                "summary": "The C++ programming language and its cousins lean towards a\nmemory-inefficient storage of structs: The compiler inserts helper bits such\nthat individual instance variables fit to byte or cache boundaries, while it is\nnot able to exploit knowledge about the range of integers, enums or bitsets.\nFurthermore, the language provides neither support for data exchange via MPI\nnor for arbitrary floating-point precisions. We propose C++ attributes through\nwhich developers can guide the compiler what memory arrangements would be\nbeneficial: Can multiple booleans or integers with limited range be squeezed\ninto one bit field, do floating point numbers hold fewer significant bits than\nin the IEEE standard, or does the code benefit from a MPI datatype for subsets\nof attributes? The extension offers the opportunity to fall back to normal\nalignment via plain C++ assignments, no dependencies upon external libraries\nare introduced, and the resulting code remains standard C++ subject to some\nweakened guarantees on addresses and pointer arithmetics. Our work implements\nthe language annotations within LLVM and demonstrates their potential impact,\nboth upon the runtime and the memory footprint, through smoothed particle\nhydrodynamics (SPH) benchmarks. They uncover the potential gains in terms of\nperformance and development productivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The C++ programming language and its cousins lean towards a\nmemory-inefficient storage of structs: The compiler inserts helper bits such\nthat individual instance variables fit to byte or cache boundaries, while it is\nnot able to exploit knowledge about the range of integers, enums or bitsets.\nFurthermore, the language provides neither support for data exchange via MPI\nnor for arbitrary floating-point precisions. We propose C++ attributes through\nwhich developers can guide the compiler what memory arrangements would be\nbeneficial: Can multiple booleans or integers with limited range be squeezed\ninto one bit field, do floating point numbers hold fewer significant bits than\nin the IEEE standard, or does the code benefit from a MPI datatype for subsets\nof attributes? The extension offers the opportunity to fall back to normal\nalignment via plain C++ assignments, no dependencies upon external libraries\nare introduced, and the resulting code remains standard C++ subject to some\nweakened guarantees on addresses and pointer arithmetics. Our work implements\nthe language annotations within LLVM and demonstrates their potential impact,\nboth upon the runtime and the memory footprint, through smoothed particle\nhydrodynamics (SPH) benchmarks. They uncover the potential gains in terms of\nperformance and development productivity."
                },
                "authors": [
                    {
                        "name": "Pawel K. Radtke"
                    },
                    {
                        "name": "Cristian G. Barrera-Hinojosa"
                    },
                    {
                        "name": "Mladen Ivkovic"
                    },
                    {
                        "name": "Tobias Weinzierl"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Weinzierl"
                },
                "author": "Tobias Weinzierl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06095v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06095v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05829v1",
                "updated": "2025-05-09T06:56:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    6,
                    56,
                    17,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T06:56:17Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    6,
                    56,
                    17,
                    4,
                    129,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Increment-Calibrated Caching with\n  Channel-Aware Singular Value Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Increment-Calibrated Caching with\n  Channel-Aware Singular Value Decomposition"
                },
                "summary": "Diffusion transformer (DiT) models have achieved remarkable success in image\ngeneration, thanks for their exceptional generative capabilities and\nscalability. Nonetheless, the iterative nature of diffusion models (DMs)\nresults in high computation complexity, posing challenges for deployment.\nAlthough existing cache-based acceleration methods try to utilize the inherent\ntemporal similarity to skip redundant computations of DiT, the lack of\ncorrection may induce potential quality degradation. In this paper, we propose\nincrement-calibrated caching, a training-free method for DiT acceleration,\nwhere the calibration parameters are generated from the pre-trained model\nitself with low-rank approximation. To deal with the possible correction\nfailure arising from outlier activations, we introduce channel-aware Singular\nValue Decomposition (SVD), which further strengthens the calibration effect.\nExperimental results show that our method always achieve better performance\nthan existing naive caching methods with a similar computation resource budget.\nWhen compared with 35-step DDIM, our method eliminates more than 45%\ncomputation and improves IS by 12 at the cost of less than 0.06 FID increase.\nCode is available at https://github.com/ccccczzy/icc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformer (DiT) models have achieved remarkable success in image\ngeneration, thanks for their exceptional generative capabilities and\nscalability. Nonetheless, the iterative nature of diffusion models (DMs)\nresults in high computation complexity, posing challenges for deployment.\nAlthough existing cache-based acceleration methods try to utilize the inherent\ntemporal similarity to skip redundant computations of DiT, the lack of\ncorrection may induce potential quality degradation. In this paper, we propose\nincrement-calibrated caching, a training-free method for DiT acceleration,\nwhere the calibration parameters are generated from the pre-trained model\nitself with low-rank approximation. To deal with the possible correction\nfailure arising from outlier activations, we introduce channel-aware Singular\nValue Decomposition (SVD), which further strengthens the calibration effect.\nExperimental results show that our method always achieve better performance\nthan existing naive caching methods with a similar computation resource budget.\nWhen compared with 35-step DDIM, our method eliminates more than 45%\ncomputation and improves IS by 12 at the cost of less than 0.06 FID increase.\nCode is available at https://github.com/ccccczzy/icc."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Chen"
                    },
                    {
                        "name": "Keyi Li"
                    },
                    {
                        "name": "Yifan Jia"
                    },
                    {
                        "name": "Le Ye"
                    },
                    {
                        "name": "Yufei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yufei Ma"
                },
                "author": "Yufei Ma",
                "arxiv_comment": "accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05772v1",
                "updated": "2025-05-09T04:17:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    4,
                    17,
                    5,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T04:17:05Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    4,
                    17,
                    5,
                    4,
                    129,
                    0
                ],
                "title": "Sparse Attention Remapping with Clustering for Efficient LLM Decoding on\n  PIM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Attention Remapping with Clustering for Efficient LLM Decoding on\n  PIM"
                },
                "summary": "Transformer-based models are the foundation of modern machine learning, but\ntheir execution, particularly during autoregressive decoding in large language\nmodels (LLMs), places significant pressure on memory systems due to frequent\nmemory accesses and growing key-value (KV) caches. This creates a bottleneck in\nmemory bandwidth, especially as context lengths increase. Processing-in-memory\n(PIM) architectures are a promising solution, offering high internal bandwidth\nand compute parallelism near memory. However, current PIM designs are primarily\noptimized for dense attention and struggle with the dynamic, irregular access\npatterns introduced by modern KV cache sparsity techniques. Consequently, they\nsuffer from workload imbalance, reducing throughput and resource utilization.\nIn this work, we propose STARC, a novel sparsity-optimized data mapping scheme\ntailored specifically for efficient LLM decoding on PIM architectures. STARC\nclusters KV pairs by semantic similarity and maps them to contiguous memory\nregions aligned with PIM bank structures. During decoding, queries retrieve\nrelevant tokens at cluster granularity by matching against precomputed\ncentroids, enabling selective attention and parallel processing without\nfrequent reclustering or data movement overhead. Experiments on the HBM-PIM\nsystem show that, compared to common token-wise sparsity methods, STARC reduces\nattention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a\nKV cache budget of 1024, it achieves up to 54%--74% latency reduction and\n45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC\nmaintains model accuracy comparable to state-of-the-art sparse attention\nmethods, demonstrating its effectiveness in enabling efficient and\nhardware-friendly long-context LLM inference on PIM architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models are the foundation of modern machine learning, but\ntheir execution, particularly during autoregressive decoding in large language\nmodels (LLMs), places significant pressure on memory systems due to frequent\nmemory accesses and growing key-value (KV) caches. This creates a bottleneck in\nmemory bandwidth, especially as context lengths increase. Processing-in-memory\n(PIM) architectures are a promising solution, offering high internal bandwidth\nand compute parallelism near memory. However, current PIM designs are primarily\noptimized for dense attention and struggle with the dynamic, irregular access\npatterns introduced by modern KV cache sparsity techniques. Consequently, they\nsuffer from workload imbalance, reducing throughput and resource utilization.\nIn this work, we propose STARC, a novel sparsity-optimized data mapping scheme\ntailored specifically for efficient LLM decoding on PIM architectures. STARC\nclusters KV pairs by semantic similarity and maps them to contiguous memory\nregions aligned with PIM bank structures. During decoding, queries retrieve\nrelevant tokens at cluster granularity by matching against precomputed\ncentroids, enabling selective attention and parallel processing without\nfrequent reclustering or data movement overhead. Experiments on the HBM-PIM\nsystem show that, compared to common token-wise sparsity methods, STARC reduces\nattention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a\nKV cache budget of 1024, it achieves up to 54%--74% latency reduction and\n45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC\nmaintains model accuracy comparable to state-of-the-art sparse attention\nmethods, demonstrating its effectiveness in enabling efficient and\nhardware-friendly long-context LLM inference on PIM architectures."
                },
                "authors": [
                    {
                        "name": "Zehao Fan"
                    },
                    {
                        "name": "Garrett Gagnon"
                    },
                    {
                        "name": "Zhenyu Liu"
                    },
                    {
                        "name": "Liu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Liu Liu"
                },
                "author": "Liu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v3",
                "updated": "2025-05-09T00:31:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    0,
                    31,
                    24,
                    4,
                    129,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Rayyan Shahid"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05251v1",
                "updated": "2025-05-08T13:56:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    56,
                    20,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T13:56:20Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    56,
                    20,
                    3,
                    128,
                    0
                ],
                "title": "High Altitude Platform-Based Caching and Multicasting for Rural\n  Connectivity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Altitude Platform-Based Caching and Multicasting for Rural\n  Connectivity"
                },
                "summary": "Providing efficient and reliable content delivery in rural areas remains a\nsignificant challenge due to the lack of communication infrastructure. To\nbridge the digital divide, this paper investigates the potential of leveraging\nmultiple high-altitude platforms (HAPs) for energy-efficient content delivery\nin wide rural regions. Each caching-enabled HAP is equipped with both\nFree-Space Optical (FSO) transceivers for backhaul links and Radio Frequency\n(RF) antenna arrays for access links. To further enhance network efficiency, we\nconsider a network coding-based multicasting scheme, where different types of\ncontent are treated as distinct multicast sessions. With the objective of\nminimizing long-term power cost, we propose a hierarchical framework that\nintegrates deep reinforcement learn-ing (DRL) and convex optimization to\njointly optimize dynamic caching strategies and resource allocation across the\nnetwork. Simulation results demonstrate that our approach significantly reduces\npower cost compared to several baseline approaches, providing a practical\nsolution for improving rural connectivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Providing efficient and reliable content delivery in rural areas remains a\nsignificant challenge due to the lack of communication infrastructure. To\nbridge the digital divide, this paper investigates the potential of leveraging\nmultiple high-altitude platforms (HAPs) for energy-efficient content delivery\nin wide rural regions. Each caching-enabled HAP is equipped with both\nFree-Space Optical (FSO) transceivers for backhaul links and Radio Frequency\n(RF) antenna arrays for access links. To further enhance network efficiency, we\nconsider a network coding-based multicasting scheme, where different types of\ncontent are treated as distinct multicast sessions. With the objective of\nminimizing long-term power cost, we propose a hierarchical framework that\nintegrates deep reinforcement learn-ing (DRL) and convex optimization to\njointly optimize dynamic caching strategies and resource allocation across the\nnetwork. Simulation results demonstrate that our approach significantly reduces\npower cost compared to several baseline approaches, providing a practical\nsolution for improving rural connectivity."
                },
                "authors": [
                    {
                        "name": "Yongqiang Zhang"
                    },
                    {
                        "name": "Mustafa A. Kishk"
                    },
                    {
                        "name": "Mohamed-Slim Alouini"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed-Slim Alouini"
                },
                "author": "Mohamed-Slim Alouini",
                "arxiv_comment": "13 pages, 8 figures, submitted to IEEE journals for possible\n  publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "49",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.4.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05130v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05130v1",
                "updated": "2025-05-08T11:07:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    7,
                    35,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T11:07:35Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    7,
                    35,
                    3,
                    128,
                    0
                ],
                "title": "CacheFL: Efficient Federated Cache Model Fine-Tuning for Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFL: Efficient Federated Cache Model Fine-Tuning for Vision-Language\n  Models"
                },
                "summary": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation."
                },
                "authors": [
                    {
                        "name": "Mengjun Yi"
                    },
                    {
                        "name": "Hanwen Zhang"
                    },
                    {
                        "name": "Hui Dou"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "Furao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Furao Shen"
                },
                "author": "Furao Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05130v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05130v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03762v2",
                "updated": "2025-05-08T09:05:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    5,
                    51,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-20T17:48:54Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    17,
                    48,
                    54,
                    6,
                    110,
                    0
                ],
                "title": "CVA6S+: A Superscalar RISC-V Core with High-Throughput Memory\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CVA6S+: A Superscalar RISC-V Core with High-Throughput Memory\n  Architecture"
                },
                "summary": "Open-source RISC-V cores are increasingly adopted in high-end embedded\ndomains such as automotive, where maximizing instructions per cycle (IPC) is\nbecoming critical. Building on the industry-supported open-source CVA6 core and\nits superscalar variant, CVA6S, we introduce CVA6S+, an enhanced version\nincorporating improved branch prediction, register renaming and enhanced\noperand forwarding. These optimizations enable CVA6S+ to achieve a 43.5%\nperformance improvement over the scalar configuration and 10.9% over CVA6S,\nwith an area overhead of just 9.30% over the scalar core (CVA6). Furthermore,\nwe integrate CVA6S+ with the OpenHW Core-V High-Performance L1 Dcache\n(HPDCache) and report a 74.1% bandwidth improvement over the legacy CVA6 cache\nsubsystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source RISC-V cores are increasingly adopted in high-end embedded\ndomains such as automotive, where maximizing instructions per cycle (IPC) is\nbecoming critical. Building on the industry-supported open-source CVA6 core and\nits superscalar variant, CVA6S, we introduce CVA6S+, an enhanced version\nincorporating improved branch prediction, register renaming and enhanced\noperand forwarding. These optimizations enable CVA6S+ to achieve a 43.5%\nperformance improvement over the scalar configuration and 10.9% over CVA6S,\nwith an area overhead of just 9.30% over the scalar core (CVA6). Furthermore,\nwe integrate CVA6S+ with the OpenHW Core-V High-Performance L1 Dcache\n(HPDCache) and report a 74.1% bandwidth improvement over the legacy CVA6 cache\nsubsystem."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Côme Allart"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Zexin Fu"
                    },
                    {
                        "name": "Filippo Grillotti"
                    },
                    {
                        "name": "Fabio De Ambroggi"
                    },
                    {
                        "name": "Elio Guidetti"
                    },
                    {
                        "name": "Jean-Baptiste Rigaud"
                    },
                    {
                        "name": "Olivier Potin"
                    },
                    {
                        "name": "Jean Roch Coulon"
                    },
                    {
                        "name": "César Fuguet"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "3 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12110v2",
                "updated": "2025-05-08T07:55:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    55,
                    38,
                    3,
                    128,
                    0
                ],
                "published": "2024-06-17T21:43:39Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    21,
                    43,
                    39,
                    0,
                    169,
                    0
                ],
                "title": "CacheSquash: Making caches speculation-aware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheSquash: Making caches speculation-aware"
                },
                "summary": "Speculation is key to achieving high CPU performance, yet it enables risks\nlike Spectre attacks which remain a significant challenge to mitigate without\nincurring substantial performance overheads. These attacks typically unfold in\nthree stages: access, transmit, and receive. Typically, they exploit a cache\ntiming side channel during the transmit and receive phases: speculatively\naccessing sensitive data (access), altering cache state (transmit), and then\nutilizing a cache timing attack (e.g., Flush+Reload) to extract the secret\n(receive). Our key observation is that Spectre attacks only require the\ntransmit instruction to execute and dispatch a request to the cache hierarchy.\nIt need not complete before a misprediction is detected (and mis-speculated\ninstructions squashed) because responses from memory that arrive at the cache\nafter squashing still alter cache state. We propose a novel mitigation,\nCacheSquash, that cancels mis-speculated memory accesses. Immediately upon\nsquashing, a cancellation is sent to the cache hierarchy, propagating\ndownstream and preventing any changes to caches that have not yet received a\nresponse. This minimizes cache state changes, thereby reducing the likelihood\nof Spectre attacks succeeding. We implement CacheSquash on gem5 and show that\nit thwarts practical Spectre attacks, with near-zero performance overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculation is key to achieving high CPU performance, yet it enables risks\nlike Spectre attacks which remain a significant challenge to mitigate without\nincurring substantial performance overheads. These attacks typically unfold in\nthree stages: access, transmit, and receive. Typically, they exploit a cache\ntiming side channel during the transmit and receive phases: speculatively\naccessing sensitive data (access), altering cache state (transmit), and then\nutilizing a cache timing attack (e.g., Flush+Reload) to extract the secret\n(receive). Our key observation is that Spectre attacks only require the\ntransmit instruction to execute and dispatch a request to the cache hierarchy.\nIt need not complete before a misprediction is detected (and mis-speculated\ninstructions squashed) because responses from memory that arrive at the cache\nafter squashing still alter cache state. We propose a novel mitigation,\nCacheSquash, that cancels mis-speculated memory accesses. Immediately upon\nsquashing, a cancellation is sent to the cache hierarchy, propagating\ndownstream and preventing any changes to caches that have not yet received a\nresponse. This minimizes cache state changes, thereby reducing the likelihood\nof Spectre attacks succeeding. We implement CacheSquash on gem5 and show that\nit thwarts practical Spectre attacks, with near-zero performance overheads."
                },
                "authors": [
                    {
                        "name": "Hossam ElAtali"
                    },
                    {
                        "name": "N. Asokan"
                    }
                ],
                "author_detail": {
                    "name": "N. Asokan"
                },
                "author": "N. Asokan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01658v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01658v2",
                "updated": "2025-05-08T07:08:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    8,
                    40,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-03T02:47:43Z",
                "published_parsed": [
                    2025,
                    5,
                    3,
                    2,
                    47,
                    43,
                    5,
                    123,
                    0
                ],
                "title": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency"
                },
                "summary": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine"
                },
                "authors": [
                    {
                        "name": "Sihyeong Park"
                    },
                    {
                        "name": "Sungryeol Jeon"
                    },
                    {
                        "name": "Chaelyn Lee"
                    },
                    {
                        "name": "Seokhun Jeon"
                    },
                    {
                        "name": "Byung-Soo Kim"
                    },
                    {
                        "name": "Jemin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jemin Lee"
                },
                "author": "Jemin Lee",
                "arxiv_comment": "Under review; 65 pages; 27 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01658v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01658v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04896v1",
                "updated": "2025-05-08T02:16:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    2,
                    16,
                    8,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T02:16:08Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    2,
                    16,
                    8,
                    3,
                    128,
                    0
                ],
                "title": "Memory Under Siege: A Comprehensive Survey of Side-Channel Attacks on\n  Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Under Siege: A Comprehensive Survey of Side-Channel Attacks on\n  Memory"
                },
                "summary": "Side-channel attacks on memory (SCAM) exploit unintended data leaks from\nmemory subsystems to infer sensitive information, posing significant threats to\nsystem security. These attacks exploit vulnerabilities in memory access\npatterns, cache behaviors, and other microarchitectural features to bypass\ntraditional security measures. The purpose of this research is to examine SCAM,\nclassify various attack techniques, and evaluate existing defense mechanisms.\nIt guides researchers and industry professionals in improving memory security\nand mitigating emerging threats. We begin by identifying the major\nvulnerabilities in the memory system that are frequently exploited in SCAM,\nsuch as cache timing, speculative execution, \\textit{Rowhammer}, and other\nsophisticated approaches. Next, we outline a comprehensive taxonomy that\nsystematically classifies these attacks based on their types, target systems,\nattack vectors, and adversarial capabilities required to execute them. In\naddition, we review the current landscape of mitigation strategies, emphasizing\ntheir strengths and limitations. This work aims to provide a comprehensive\noverview of memory-based side-channel attacks with the goal of providing\nsignificant insights for researchers and practitioners to better understand,\ndetect, and mitigate SCAM risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Side-channel attacks on memory (SCAM) exploit unintended data leaks from\nmemory subsystems to infer sensitive information, posing significant threats to\nsystem security. These attacks exploit vulnerabilities in memory access\npatterns, cache behaviors, and other microarchitectural features to bypass\ntraditional security measures. The purpose of this research is to examine SCAM,\nclassify various attack techniques, and evaluate existing defense mechanisms.\nIt guides researchers and industry professionals in improving memory security\nand mitigating emerging threats. We begin by identifying the major\nvulnerabilities in the memory system that are frequently exploited in SCAM,\nsuch as cache timing, speculative execution, \\textit{Rowhammer}, and other\nsophisticated approaches. Next, we outline a comprehensive taxonomy that\nsystematically classifies these attacks based on their types, target systems,\nattack vectors, and adversarial capabilities required to execute them. In\naddition, we review the current landscape of mitigation strategies, emphasizing\ntheir strengths and limitations. This work aims to provide a comprehensive\noverview of memory-based side-channel attacks with the goal of providing\nsignificant insights for researchers and practitioners to better understand,\ndetect, and mitigate SCAM risks."
                },
                "authors": [
                    {
                        "name": "MD Mahady Hassan"
                    },
                    {
                        "name": "Shanto Roy"
                    },
                    {
                        "name": "Reza Rahaeimehr"
                    }
                ],
                "author_detail": {
                    "name": "Reza Rahaeimehr"
                },
                "author": "Reza Rahaeimehr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04556v1",
                "updated": "2025-05-07T16:44:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    44,
                    21,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T16:44:21Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    44,
                    21,
                    2,
                    127,
                    0
                ],
                "title": "Comparing CPU and GPU compute of PERMANOVA on MI300A",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing CPU and GPU compute of PERMANOVA on MI300A"
                },
                "summary": "Comparing the tradeoffs of CPU and GPU compute for memory-heavy algorithms is\noften challenging, due to the drastically different memory subsystems on host\nCPUs and discrete GPUs. The AMD MI300A is an exception, since it sports both\nCPU and GPU cores in a single package, all backed by the same type of HBM\nmemory. In this paper we analyze the performance of Permutational Multivariate\nAnalysis of Variance (PERMANOVA), a non-parametric method that tests whether\ntwo or more groups of objects are significantly different based on a\ncategorical factor. This method is memory-bound and has been recently optimized\nfor CPU cache locality. Our tests show that GPU cores on the MI300A prefer the\nbrute force approach instead, significantly outperforming the CPU-based\nimplementation. The significant benefit of Simultaneous Multithreading (SMT)\nwas also a pleasant surprise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing the tradeoffs of CPU and GPU compute for memory-heavy algorithms is\noften challenging, due to the drastically different memory subsystems on host\nCPUs and discrete GPUs. The AMD MI300A is an exception, since it sports both\nCPU and GPU cores in a single package, all backed by the same type of HBM\nmemory. In this paper we analyze the performance of Permutational Multivariate\nAnalysis of Variance (PERMANOVA), a non-parametric method that tests whether\ntwo or more groups of objects are significantly different based on a\ncategorical factor. This method is memory-bound and has been recently optimized\nfor CPU cache locality. Our tests show that GPU cores on the MI300A prefer the\nbrute force approach instead, significantly outperforming the CPU-based\nimplementation. The significant benefit of Simultaneous Multithreading (SMT)\nwas also a pleasant surprise."
                },
                "authors": [
                    {
                        "name": "Igor Sfiligoi"
                    }
                ],
                "author_detail": {
                    "name": "Igor Sfiligoi"
                },
                "author": "Igor Sfiligoi",
                "arxiv_comment": "7 pages, 1 figure, Accepted at PEARC25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04466v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04466v1",
                "updated": "2025-05-07T14:37:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    37,
                    13,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T14:37:13Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    37,
                    13,
                    2,
                    127,
                    0
                ],
                "title": "Securing Immersive 360 Video Streams through Attribute-Based Selective\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Securing Immersive 360 Video Streams through Attribute-Based Selective\n  Encryption"
                },
                "summary": "Delivering high-quality, secure 360{\\deg} video content introduces unique\nchallenges, primarily due to the high bitrates and interactive demands of\nimmersive media. Traditional HTTPS-based methods, although widely used, face\nlimitations in computational efficiency and scalability when securing these\nhigh-resolution streams. To address these issues, this paper proposes a novel\nframework integrating Attribute-Based Encryption (ABE) with selective\nencryption techniques tailored specifically for tiled 360{\\deg} video\nstreaming. Our approach employs selective encryption of frames at varying\nlevels to reduce computational overhead while ensuring robust protection\nagainst unauthorized access.\n  Moreover, we explore viewport-adaptive encryption, dynamically encrypting\nmore frames within tiles occupying larger portions of the viewer's field of\nview. This targeted method significantly enhances security in critical viewing\nareas without unnecessary overhead in peripheral regions. We deploy and\nevaluate our proposed approach using the CloudLab testbed, comparing its\nperformance against traditional HTTPS streaming. Experimental results\ndemonstrate that our ABE-based model achieves reduced computational load on\nintermediate caches, improves cache hit rates, and maintains comparable visual\nquality to HTTPS, as assessed by Video Multimethod Assessment Fusion (VMAF).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delivering high-quality, secure 360{\\deg} video content introduces unique\nchallenges, primarily due to the high bitrates and interactive demands of\nimmersive media. Traditional HTTPS-based methods, although widely used, face\nlimitations in computational efficiency and scalability when securing these\nhigh-resolution streams. To address these issues, this paper proposes a novel\nframework integrating Attribute-Based Encryption (ABE) with selective\nencryption techniques tailored specifically for tiled 360{\\deg} video\nstreaming. Our approach employs selective encryption of frames at varying\nlevels to reduce computational overhead while ensuring robust protection\nagainst unauthorized access.\n  Moreover, we explore viewport-adaptive encryption, dynamically encrypting\nmore frames within tiles occupying larger portions of the viewer's field of\nview. This targeted method significantly enhances security in critical viewing\nareas without unnecessary overhead in peripheral regions. We deploy and\nevaluate our proposed approach using the CloudLab testbed, comparing its\nperformance against traditional HTTPS streaming. Experimental results\ndemonstrate that our ABE-based model achieves reduced computational load on\nintermediate caches, improves cache hit rates, and maintains comparable visual\nquality to HTTPS, as assessed by Video Multimethod Assessment Fusion (VMAF)."
                },
                "authors": [
                    {
                        "name": "Mohammad Waquas Usmani"
                    },
                    {
                        "name": "Susmit Shannigrahi"
                    },
                    {
                        "name": "Michael Zink"
                    }
                ],
                "author_detail": {
                    "name": "Michael Zink"
                },
                "author": "Michael Zink",
                "arxiv_comment": "8 pages plus references, 10 figures, some with subfigures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04466v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04466v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04421v1",
                "updated": "2025-05-07T13:54:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    54,
                    26,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T13:54:26Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    54,
                    26,
                    2,
                    127,
                    0
                ],
                "title": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders"
                },
                "summary": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users."
                },
                "authors": [
                    {
                        "name": "Zheng Chai"
                    },
                    {
                        "name": "Qin Ren"
                    },
                    {
                        "name": "Xijun Xiao"
                    },
                    {
                        "name": "Huizhi Yang"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Di Chen"
                    },
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Wenlin Zhao"
                    },
                    {
                        "name": "Lele Yu"
                    },
                    {
                        "name": "Xionghang Xie"
                    },
                    {
                        "name": "Shiru Ren"
                    },
                    {
                        "name": "Xiang Sun"
                    },
                    {
                        "name": "Yaocheng Tan"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Yuchao Zheng"
                    },
                    {
                        "name": "Di Wu"
                    }
                ],
                "author_detail": {
                    "name": "Di Wu"
                },
                "author": "Di Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13779v2",
                "updated": "2025-05-07T13:07:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    7,
                    25,
                    2,
                    127,
                    0
                ],
                "published": "2024-12-18T12:16:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Rehearsal-Free Continual Federated Learning with Synergistic Synaptic\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsal-Free Continual Federated Learning with Synergistic Synaptic\n  Intelligence"
                },
                "summary": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2403.05890",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04326v1",
                "updated": "2025-05-07T11:21:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    11,
                    21,
                    12,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T11:21:12Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    11,
                    21,
                    12,
                    2,
                    127,
                    0
                ],
                "title": "Design and Evaluation of an NDN-Based Network for Distributed Digital\n  Twins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Evaluation of an NDN-Based Network for Distributed Digital\n  Twins"
                },
                "summary": "Digital twins (DT) have received significant attention due to their numerous\nbenefits, such as real-time data analytics and cost reduction in production. DT\nserves as a fundamental component of many applications, encompassing smart\nmanufacturing, intelligent vehicles, and smart cities. By using Machine\nLearning (ML) and Artificial Intelligence (AI) techniques, DTs can efficiently\nfacilitate decision-making and productivity by simulating the status and\nchanges of a physical entity. To handle the massive amount of data brought by\nDTs, it is challenging to achieve low response latency for data fetching over\nexisting IP-based networks. IP-based networks use host addresses for end-to-end\ncommunication, making data distribution between DTs inefficient. Thus, we\npropose to use DTs in a distributed manner over Named Data Networking (NDN)\nnetworks. NDN is data-centric where data is routed based on content names,\ndynamically adjusting paths to optimize latency. Popular data is cached in\nnetwork nodes, reducing data transmission and network congestion. Since data is\nfetched by content names, users and mobile devices can move freely without IP\naddress reassignment. By using in-network caching and adaptive routing, we\nreckon NDN is an ideal fit for Future G Networks in the context of Digital\nTwins. We compared DTs in edge scenarios with cloud scenarios over NDN and\nIP-based networks to validate our insights. Extensive simulation results show\nthat using DT in the edge reduces response latency by 10.2x. This position\npaper represents an initial investigation into the gap in distributed DTs over\nNDN, serving as an early-stage study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital twins (DT) have received significant attention due to their numerous\nbenefits, such as real-time data analytics and cost reduction in production. DT\nserves as a fundamental component of many applications, encompassing smart\nmanufacturing, intelligent vehicles, and smart cities. By using Machine\nLearning (ML) and Artificial Intelligence (AI) techniques, DTs can efficiently\nfacilitate decision-making and productivity by simulating the status and\nchanges of a physical entity. To handle the massive amount of data brought by\nDTs, it is challenging to achieve low response latency for data fetching over\nexisting IP-based networks. IP-based networks use host addresses for end-to-end\ncommunication, making data distribution between DTs inefficient. Thus, we\npropose to use DTs in a distributed manner over Named Data Networking (NDN)\nnetworks. NDN is data-centric where data is routed based on content names,\ndynamically adjusting paths to optimize latency. Popular data is cached in\nnetwork nodes, reducing data transmission and network congestion. Since data is\nfetched by content names, users and mobile devices can move freely without IP\naddress reassignment. By using in-network caching and adaptive routing, we\nreckon NDN is an ideal fit for Future G Networks in the context of Digital\nTwins. We compared DTs in edge scenarios with cloud scenarios over NDN and\nIP-based networks to validate our insights. Extensive simulation results show\nthat using DT in the edge reduces response latency by 10.2x. This position\npaper represents an initial investigation into the gap in distributed DTs over\nNDN, serving as an early-stage study."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Zihan Jia"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Lin Cui"
                    },
                    {
                        "name": "Fung Po Tso"
                    }
                ],
                "author_detail": {
                    "name": "Fung Po Tso"
                },
                "author": "Fung Po Tso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04216v1",
                "updated": "2025-05-07T08:10:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    10,
                    39,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T08:10:39Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    10,
                    39,
                    2,
                    127,
                    0
                ],
                "title": "Computational Model for Photoionization in Pure SF6 Streamer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Model for Photoionization in Pure SF6 Streamer"
                },
                "summary": "Photoionization plays a crucial role in achieving spatial numerical\nconvergence and accurate quantitative predictions in SF6 streamer simulations,\nbut accurate models for SF6 photoionization remains limited, motivating this\npaper. First, we develop a computational model for SF6 photoionization and\nprovide the detailed modeling process. Then, we perform comparative studies\nagainst simplified approaches. The results demonstrate that the proposed model\neffectively captures the non-local effects of SF6 photoionization, enhancing\nboth the spatial numerical convergence and the accuracy of the streamer\nstructure. Finally, we perform comparative studies by artificially increasing\nthe photoionization intensity through multiplying the photoionization source\nterm Sph by a factor of 10 (10*Sph) relative to the baseline intensity.\nRegarding breakdown voltage prediction, 10*Sph leads to a significant\nunderestimation of the breakdown voltage for positive streamers, introducing\nerrors greater than 0.5 kV, while exerting a relatively small impact on\nnegative streamers. Regarding streamer propagation dynamics, 10*Sph reduces the\ncontraction at the positive streamer head and significantly lowers the local\nfield by more than 700 Td, thereby slowing down its speed. In contrast, 10*Sph\nhas little impact on the morphology of the negative streamers and slightly\nenhances the local field by less than 200 Td, thereby consistently accelerating\nits propagation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photoionization plays a crucial role in achieving spatial numerical\nconvergence and accurate quantitative predictions in SF6 streamer simulations,\nbut accurate models for SF6 photoionization remains limited, motivating this\npaper. First, we develop a computational model for SF6 photoionization and\nprovide the detailed modeling process. Then, we perform comparative studies\nagainst simplified approaches. The results demonstrate that the proposed model\neffectively captures the non-local effects of SF6 photoionization, enhancing\nboth the spatial numerical convergence and the accuracy of the streamer\nstructure. Finally, we perform comparative studies by artificially increasing\nthe photoionization intensity through multiplying the photoionization source\nterm Sph by a factor of 10 (10*Sph) relative to the baseline intensity.\nRegarding breakdown voltage prediction, 10*Sph leads to a significant\nunderestimation of the breakdown voltage for positive streamers, introducing\nerrors greater than 0.5 kV, while exerting a relatively small impact on\nnegative streamers. Regarding streamer propagation dynamics, 10*Sph reduces the\ncontraction at the positive streamer head and significantly lowers the local\nfield by more than 700 Td, thereby slowing down its speed. In contrast, 10*Sph\nhas little impact on the morphology of the negative streamers and slightly\nenhances the local field by less than 200 Td, thereby consistently accelerating\nits propagation."
                },
                "authors": [
                    {
                        "name": "Zihao Feng"
                    }
                ],
                "author_detail": {
                    "name": "Zihao Feng"
                },
                "author": "Zihao Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12224v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12224v2",
                "updated": "2025-05-07T07:57:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    57,
                    21,
                    2,
                    127,
                    0
                ],
                "published": "2025-02-17T14:54:14Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    54,
                    14,
                    0,
                    48,
                    0
                ],
                "title": "Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer\n  Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer\n  Gate"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Fang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Yuegui Huang"
                    },
                    {
                        "name": "Yufeng Lyu"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Fan Yu"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12224v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12224v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04129v1",
                "updated": "2025-05-07T05:00:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    0,
                    10,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T05:00:10Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    0,
                    10,
                    2,
                    127,
                    0
                ],
                "title": "Maxing Out the SVM: Performance Impact of Memory and Program Cache Sizes\n  in the Agave Validator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maxing Out the SVM: Performance Impact of Memory and Program Cache Sizes\n  in the Agave Validator"
                },
                "summary": "In this paper we analyze some of the bottlenecks in the execution pipeline of\nSolana's Agave validator client, focusing on RAM and program cache usage under\nmainnet conditions. Through a series of controlled experiments, we measure the\nvalidator's throughput and resource efficiency as RAM availability ranges\nbetween 128 GB to 1,536 GB (1.5 TB). We discover that the validator performance\ndegrades significantly below 256 GB, with transaction processing falling behind\nreal-time block production. Additionally, we study the program cache behavior,\nidentifying inefficiencies in program eviction and load latency. Our results\nprovide practical guidance for hardware provisioning and suggest improvements\nto the Solana execution and caching strategy, reducing latency due to the\nprogram cache by 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we analyze some of the bottlenecks in the execution pipeline of\nSolana's Agave validator client, focusing on RAM and program cache usage under\nmainnet conditions. Through a series of controlled experiments, we measure the\nvalidator's throughput and resource efficiency as RAM availability ranges\nbetween 128 GB to 1,536 GB (1.5 TB). We discover that the validator performance\ndegrades significantly below 256 GB, with transaction processing falling behind\nreal-time block production. Additionally, we study the program cache behavior,\nidentifying inefficiencies in program eviction and load latency. Our results\nprovide practical guidance for hardware provisioning and suggest improvements\nto the Solana execution and caching strategy, reducing latency due to the\nprogram cache by 90%."
                },
                "authors": [
                    {
                        "name": "Turan Vural"
                    },
                    {
                        "name": "Yuki Yuminaga"
                    },
                    {
                        "name": "Alex Petrosyan"
                    },
                    {
                        "name": "Ben Livshits"
                    }
                ],
                "author_detail": {
                    "name": "Ben Livshits"
                },
                "author": "Ben Livshits",
                "arxiv_comment": "15 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v7",
                "updated": "2025-05-14T04:38:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    38,
                    42,
                    2,
                    134,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v6",
                "updated": "2025-05-06T19:28:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    19,
                    28,
                    56,
                    1,
                    126,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12240v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12240v3",
                "updated": "2025-05-06T15:23:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    15,
                    23,
                    12,
                    1,
                    126,
                    0
                ],
                "published": "2025-04-16T16:45:19Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cobra: Efficient Line Art COlorization with BRoAder References"
                },
                "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/."
                },
                "authors": [
                    {
                        "name": "Junhao Zhuang"
                    },
                    {
                        "name": "Lingen Li"
                    },
                    {
                        "name": "Xuan Ju"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project page with code: https://zhuang2002.github.io/Cobra/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12240v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12240v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02922v1",
                "updated": "2025-05-05T18:01:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    18,
                    1,
                    17,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T18:01:17Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    18,
                    1,
                    17,
                    0,
                    125,
                    0
                ],
                "title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference"
                },
                "summary": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy."
                },
                "authors": [
                    {
                        "name": "Yaoqi Chen"
                    },
                    {
                        "name": "Jinkai Zhang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Jingjia Luo"
                    },
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Jiawei Jiang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02533v1",
                "updated": "2025-05-05T10:16:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    16,
                    16,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T10:16:16Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    16,
                    16,
                    0,
                    125,
                    0
                ],
                "title": "Large Language Model Partitioning for Low-Latency Inference at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Partitioning for Low-Latency Inference at the Edge"
                },
                "summary": "Large Language Models (LLMs) based on autoregressive, decoder-only\nTransformers generate text one token at a time, where a token represents a\ndiscrete unit of text. As each newly produced token is appended to the partial\noutput sequence, the length grows and so does the memory and compute load, due\nto the expanding key-value caches, which store intermediate representations of\nall previously generated tokens in the multi-head attention (MHA) layer. As\nthis iterative process steadily increases memory and compute demands,\nlayer-based partitioning in resource-constrained edge environments often\nresults in memory overload or high inference latency. To address this and\nreduce inference latency, we propose a resource-aware Transformer architecture\npartitioning algorithm, where the partitioning decision is updated at regular\nintervals during token generation. The approach is myopic in that it is based\non instantaneous information about device resource availability and network\nlink bandwidths. When first executed, the algorithm places blocks on devices,\nand in later executions, it migrates these blocks among devices so that the sum\nof migration delay and inference delay remains low. Our approach partitions the\ndecoder at the attention head level, co-locating each attention head with its\nkey-value cache and allowing dynamic migrations whenever resources become\ntight. By allocating different attention heads to different devices, we exploit\nparallel execution of attention heads and thus achieve substantial reductions\nin inference delays. Our experiments show that in small-scale settings (3-5\ndevices), the proposed method achieves within 15 to 20 percent of an exact\noptimal solver's latency, while in larger-scale tests it achieves notable\nimprovements in inference speed and memory usage compared to state-of-the-art\nlayer-based partitioning approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) based on autoregressive, decoder-only\nTransformers generate text one token at a time, where a token represents a\ndiscrete unit of text. As each newly produced token is appended to the partial\noutput sequence, the length grows and so does the memory and compute load, due\nto the expanding key-value caches, which store intermediate representations of\nall previously generated tokens in the multi-head attention (MHA) layer. As\nthis iterative process steadily increases memory and compute demands,\nlayer-based partitioning in resource-constrained edge environments often\nresults in memory overload or high inference latency. To address this and\nreduce inference latency, we propose a resource-aware Transformer architecture\npartitioning algorithm, where the partitioning decision is updated at regular\nintervals during token generation. The approach is myopic in that it is based\non instantaneous information about device resource availability and network\nlink bandwidths. When first executed, the algorithm places blocks on devices,\nand in later executions, it migrates these blocks among devices so that the sum\nof migration delay and inference delay remains low. Our approach partitions the\ndecoder at the attention head level, co-locating each attention head with its\nkey-value cache and allowing dynamic migrations whenever resources become\ntight. By allocating different attention heads to different devices, we exploit\nparallel execution of attention heads and thus achieve substantial reductions\nin inference delays. Our experiments show that in small-scale settings (3-5\ndevices), the proposed method achieves within 15 to 20 percent of an exact\noptimal solver's latency, while in larger-scale tests it achieves notable\nimprovements in inference speed and memory usage compared to state-of-the-art\nlayer-based partitioning approaches."
                },
                "authors": [
                    {
                        "name": "Dimitrios Kafetzis"
                    },
                    {
                        "name": "Ramin Khalili"
                    },
                    {
                        "name": "Iordanis Koutsopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Iordanis Koutsopoulos"
                },
                "author": "Iordanis Koutsopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02346v1",
                "updated": "2025-05-05T04:01:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    4,
                    1,
                    56,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T04:01:56Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    4,
                    1,
                    56,
                    0,
                    125,
                    0
                ],
                "title": "An Empirical Study on the Performance and Energy Usage of Compiled\n  Python Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on the Performance and Energy Usage of Compiled\n  Python Code"
                },
                "summary": "Python is a popular programming language known for its ease of learning and\nextensive libraries. However, concerns about performance and energy consumption\nhave led to the development of compilers to enhance Python code efficiency.\nDespite the proven benefits of existing compilers on the efficiency of Python\ncode, there is limited analysis comparing their performance and energy\nefficiency, particularly considering code characteristics and factors like CPU\nfrequency and core count. Our study investigates how compilation impacts the\nperformance and energy consumption of Python code, using seven benchmarks\ncompiled with eight different tools: PyPy, Numba, Nuitka, Mypyc, Codon, Cython,\nPyston-lite, and the experimental Python 3.13 version, compared to CPython. The\nbenchmarks are single-threaded and executed on an NUC and a server, measuring\nenergy usage, execution time, memory usage, and Last-Level Cache (LLC) miss\nrates at a fixed frequency and on a single core. The results show that\ncompilation can significantly enhance execution time, energy and memory usage,\nwith Codon, PyPy, and Numba achieving over 90\\% speed and energy improvements.\nNuitka optimizes memory usage consistently on both testbeds. The impact of\ncompilation on LLC miss rate is not clear since it varies considerably across\nbenchmarks for each compiler. Our study is important for researchers and\npractitioners focused on improving Python code performance and energy\nefficiency. We outline future research directions, such as exploring caching\neffects on energy usage. Our findings help practitioners choose the best\ncompiler based on their efficiency benefits and accessibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Python is a popular programming language known for its ease of learning and\nextensive libraries. However, concerns about performance and energy consumption\nhave led to the development of compilers to enhance Python code efficiency.\nDespite the proven benefits of existing compilers on the efficiency of Python\ncode, there is limited analysis comparing their performance and energy\nefficiency, particularly considering code characteristics and factors like CPU\nfrequency and core count. Our study investigates how compilation impacts the\nperformance and energy consumption of Python code, using seven benchmarks\ncompiled with eight different tools: PyPy, Numba, Nuitka, Mypyc, Codon, Cython,\nPyston-lite, and the experimental Python 3.13 version, compared to CPython. The\nbenchmarks are single-threaded and executed on an NUC and a server, measuring\nenergy usage, execution time, memory usage, and Last-Level Cache (LLC) miss\nrates at a fixed frequency and on a single core. The results show that\ncompilation can significantly enhance execution time, energy and memory usage,\nwith Codon, PyPy, and Numba achieving over 90\\% speed and energy improvements.\nNuitka optimizes memory usage consistently on both testbeds. The impact of\ncompilation on LLC miss rate is not clear since it varies considerably across\nbenchmarks for each compiler. Our study is important for researchers and\npractitioners focused on improving Python code performance and energy\nefficiency. We outline future research directions, such as exploring caching\neffects on energy usage. Our findings help practitioners choose the best\ncompiler based on their efficiency benefits and accessibility."
                },
                "authors": [
                    {
                        "name": "Vincenzo Stoico"
                    },
                    {
                        "name": "Andrei Calin Dragomir"
                    },
                    {
                        "name": "Patricia Lago"
                    }
                ],
                "author_detail": {
                    "name": "Patricia Lago"
                },
                "author": "Patricia Lago",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10375v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10375v2",
                "updated": "2025-05-04T09:49:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    9,
                    49,
                    42,
                    6,
                    124,
                    0
                ],
                "published": "2024-12-16T07:59:21Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    7,
                    59,
                    21,
                    0,
                    351,
                    0
                ],
                "title": "DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient\n  MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient\n  MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models, though highly effective for various machine\nlearning tasks, face significant deployment challenges on memory-constrained\ndevices. While GPUs offer fast inference, their limited memory compared to CPUs\nmeans not all experts can be stored on the GPU simultaneously, necessitating\nfrequent, costly data transfers from CPU memory, often negating GPU speed\nadvantages. To address this, we present DAOP, an on-device MoE inference engine\nto optimize parallel GPU-CPU execution. DAOP dynamically allocates experts\nbetween CPU and GPU based on per-sequence activation patterns, and selectively\npre-calculates predicted experts on CPUs to minimize transfer latency. This\napproach enables efficient resource utilization across various expert cache\nratios while maintaining model accuracy through a novel graceful degradation\nmechanism. Comprehensive evaluations across various datasets show that DAOP\noutperforms traditional expert caching and prefetching methods by up to 8.20x\nand offloading techniques by 1.35x while maintaining accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models, though highly effective for various machine\nlearning tasks, face significant deployment challenges on memory-constrained\ndevices. While GPUs offer fast inference, their limited memory compared to CPUs\nmeans not all experts can be stored on the GPU simultaneously, necessitating\nfrequent, costly data transfers from CPU memory, often negating GPU speed\nadvantages. To address this, we present DAOP, an on-device MoE inference engine\nto optimize parallel GPU-CPU execution. DAOP dynamically allocates experts\nbetween CPU and GPU based on per-sequence activation patterns, and selectively\npre-calculates predicted experts on CPUs to minimize transfer latency. This\napproach enables efficient resource utilization across various expert cache\nratios while maintaining model accuracy through a novel graceful degradation\nmechanism. Comprehensive evaluations across various datasets show that DAOP\noutperforms traditional expert caching and prefetching methods by up to 8.20x\nand offloading techniques by 1.35x while maintaining accuracy."
                },
                "authors": [
                    {
                        "name": "Yujie Zhang"
                    },
                    {
                        "name": "Shivam Aggarwal"
                    },
                    {
                        "name": "Tulika Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Tulika Mitra"
                },
                "author": "Tulika Mitra",
                "arxiv_comment": "7 pages, 10 figures, Accepted by DATE Conference 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10375v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02027v1",
                "updated": "2025-05-04T08:30:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    8,
                    30,
                    0,
                    6,
                    124,
                    0
                ],
                "published": "2025-05-04T08:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    8,
                    30,
                    0,
                    6,
                    124,
                    0
                ],
                "title": "GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph\n  In-Context Learning"
                },
                "summary": "Graph In-Context Learning, with the ability to adapt pre-trained graph models\nto novel and diverse downstream graphs without updating any parameters, has\ngained much attention in the community. The key to graph in-context learning is\nto perform downstream graphs conditioned on chosen prompt examples. Existing\nmethods randomly select subgraphs or edges as prompts, leading to noisy graph\nprompts and inferior model performance. Additionally, due to the gap between\npre-training and testing graphs, when the number of classes in the testing\ngraphs is much greater than that in the training, the in-context learning\nability will also significantly deteriorate. To tackle the aforementioned\nchallenges, we develop a multi-stage adaptive prompt optimization method\nGraphPrompter, which optimizes the entire process of generating, selecting, and\nusing graph prompts for better in-context learning capabilities. Firstly,\nPrompt Generator introduces a reconstruction layer to highlight the most\ninformative edges and reduce irrelevant noise for graph prompt construction.\nFurthermore, in the selection stage, Prompt Selector employs the $k$-nearest\nneighbors algorithm and pre-trained selection layers to dynamically choose\nappropriate samples and minimize the influence of irrelevant prompts. Finally,\nwe leverage a Prompt Augmenter with a cache replacement strategy to enhance the\ngeneralization capability of the pre-trained model on new datasets. Extensive\nexperiments show that GraphPrompter effectively enhances the in-context\nlearning ability of graph models. On average across all the settings, our\napproach surpasses the state-of-the-art baselines by over 8%. Our code is\nreleased at https://github.com/karin0018/GraphPrompter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph In-Context Learning, with the ability to adapt pre-trained graph models\nto novel and diverse downstream graphs without updating any parameters, has\ngained much attention in the community. The key to graph in-context learning is\nto perform downstream graphs conditioned on chosen prompt examples. Existing\nmethods randomly select subgraphs or edges as prompts, leading to noisy graph\nprompts and inferior model performance. Additionally, due to the gap between\npre-training and testing graphs, when the number of classes in the testing\ngraphs is much greater than that in the training, the in-context learning\nability will also significantly deteriorate. To tackle the aforementioned\nchallenges, we develop a multi-stage adaptive prompt optimization method\nGraphPrompter, which optimizes the entire process of generating, selecting, and\nusing graph prompts for better in-context learning capabilities. Firstly,\nPrompt Generator introduces a reconstruction layer to highlight the most\ninformative edges and reduce irrelevant noise for graph prompt construction.\nFurthermore, in the selection stage, Prompt Selector employs the $k$-nearest\nneighbors algorithm and pre-trained selection layers to dynamically choose\nappropriate samples and minimize the influence of irrelevant prompts. Finally,\nwe leverage a Prompt Augmenter with a cache replacement strategy to enhance the\ngeneralization capability of the pre-trained model on new datasets. Extensive\nexperiments show that GraphPrompter effectively enhances the in-context\nlearning ability of graph models. On average across all the settings, our\napproach surpasses the state-of-the-art baselines by over 8%. Our code is\nreleased at https://github.com/karin0018/GraphPrompter."
                },
                "authors": [
                    {
                        "name": "Rui Lv"
                    },
                    {
                        "name": "Zaixi Zhang"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Weibo Gao"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Jiaxia Yan"
                    },
                    {
                        "name": "Linan Yue"
                    },
                    {
                        "name": "Fangzhou Yao"
                    }
                ],
                "author_detail": {
                    "name": "Fangzhou Yao"
                },
                "author": "Fangzhou Yao",
                "arxiv_comment": "14 pages. IEEE International Conference on Data Engineering\n  (ICDE'2025), accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v3",
                "updated": "2025-05-03T04:07:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    3,
                    4,
                    7,
                    7,
                    5,
                    123,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.9$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.9$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20335v2",
                "updated": "2025-05-03T01:10:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    3,
                    1,
                    10,
                    30,
                    5,
                    123,
                    0
                ],
                "published": "2025-04-29T00:58:59Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    0,
                    58,
                    59,
                    1,
                    119,
                    0
                ],
                "title": "VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with\n  Delayed Hits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with\n  Delayed Hits"
                },
                "summary": "Caches are fundamental to latency-sensitive systems like Content Delivery\nNetworks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit\nphenomenon where multiple requests for an object occur during its fetch from\nthe remote server after a miss significantly inflates user-perceived latency.\nWhile recent algorithms acknowledge delayed hits by estimating the resulting\naggregate delay, they predominantly focus on its mean value. We identify and\ndemonstrate that such approaches are insufficient, as the real aggregate delay\nfrequently exhibits substantial variance in the true production system, leading\nto suboptimal latency performance when ignored. Thus, we propose VA-CDH, a\nvariance-aware method to optimize latency for caching with delayed hits. It\nemploys a novel ranking function that explicitly incorporates both the\nempirically estimated mean and standard deviation of aggregate delay, allowing\ncaching decisions to account for its variation. We derive the analytical\ndistribution of aggregate delay under Poisson arrivals as a theoretical\ncontribution, offering more statistical insight beyond the mean value. Through\nthe simulations conducted on synthetic and real-world datasets, we show that\nVA-CDH reduces the total latency by 1%-6% approximately compared to\nstate-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches are fundamental to latency-sensitive systems like Content Delivery\nNetworks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit\nphenomenon where multiple requests for an object occur during its fetch from\nthe remote server after a miss significantly inflates user-perceived latency.\nWhile recent algorithms acknowledge delayed hits by estimating the resulting\naggregate delay, they predominantly focus on its mean value. We identify and\ndemonstrate that such approaches are insufficient, as the real aggregate delay\nfrequently exhibits substantial variance in the true production system, leading\nto suboptimal latency performance when ignored. Thus, we propose VA-CDH, a\nvariance-aware method to optimize latency for caching with delayed hits. It\nemploys a novel ranking function that explicitly incorporates both the\nempirically estimated mean and standard deviation of aggregate delay, allowing\ncaching decisions to account for its variation. We derive the analytical\ndistribution of aggregate delay under Poisson arrivals as a theoretical\ncontribution, offering more statistical insight beyond the mean value. Through\nthe simulations conducted on synthetic and real-world datasets, we show that\nVA-CDH reduces the total latency by 1%-6% approximately compared to\nstate-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Bowen Jiang"
                    },
                    {
                        "name": "Chaofan Ma"
                    },
                    {
                        "name": "Duo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Duo Wang"
                },
                "author": "Duo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13298v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13298v3",
                "updated": "2025-05-02T13:55:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    55,
                    21,
                    4,
                    122,
                    0
                ],
                "published": "2025-01-23T00:57:01Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    0,
                    57,
                    1,
                    3,
                    23,
                    0
                ],
                "title": "Collaborative Coded Caching for Partially Connected Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Coded Caching for Partially Connected Networks"
                },
                "summary": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed\nmultiple-input-multiple-output (MIMO) Gaussian broadcast channel. We propose a\nnovel delivery scheme consisting of two phases: partitioning and transmission.\nIn the partitioning phase, users with identical cache profiles are partitioned\ninto the minimum number of sets, such that users within each set can\nsuccessfully decode their desired message from a joint transmission enabled by\nMIMO precoding. To optimally partition the users, we employ the branch and\nbound method. In the transmission phase, each partition is treated as a single\nentity, and codewords are multicast to partitions with distinct cache profiles.\nThe proposed delivery scheme is applicable to any partially connected network,\nand while the partitioning is optimal, the overall delivery scheme, including\ntransmission, is heuristic. Interestingly, simulation results show that its\nperformance closely approximates that of the fully connected optimal solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed\nmultiple-input-multiple-output (MIMO) Gaussian broadcast channel. We propose a\nnovel delivery scheme consisting of two phases: partitioning and transmission.\nIn the partitioning phase, users with identical cache profiles are partitioned\ninto the minimum number of sets, such that users within each set can\nsuccessfully decode their desired message from a joint transmission enabled by\nMIMO precoding. To optimally partition the users, we employ the branch and\nbound method. In the transmission phase, each partition is treated as a single\nentity, and codewords are multicast to partitions with distinct cache profiles.\nThe proposed delivery scheme is applicable to any partially connected network,\nand while the partitioning is optimal, the overall delivery scheme, including\ntransmission, is heuristic. Interestingly, simulation results show that its\nperformance closely approximates that of the fully connected optimal solution."
                },
                "authors": [
                    {
                        "name": "Kagan Akcay"
                    },
                    {
                        "name": "Eleftherios Lampiris"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13298v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13298v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01164v1",
                "updated": "2025-05-02T10:13:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    13,
                    12,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T10:13:12Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    13,
                    12,
                    4,
                    122,
                    0
                ],
                "title": "CaGR-RAG: Context-aware Query Grouping for Disk-based Vector Search in\n  RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaGR-RAG: Context-aware Query Grouping for Disk-based Vector Search in\n  RAG Systems"
                },
                "summary": "Modern embedding models capture both semantic and syntactic structures of\nqueries, often mapping different queries to similar regions in vector space.\nThis results in non-uniform cluster access patterns in disk-based vector search\nsystems, particularly in Retrieval Augmented Generation (RAG) framework. While\nexisting approaches optimize individual queries, they overlook the impact of\ncluster access patterns, failing to account for the locality effects of queries\nthat access similar clusters. This oversight reduces cache efficiency and\nincreases search latency due to excessive disk I/O. To address this, we\nintroduce CaGR-RAG, a context-aware query grouping mechanism that organizes\nqueries based on shared cluster access patterns. Additionally, it incorporates\nopportunistic cluster prefetching to minimize cache misses during transitions\nbetween query groups, further optimizing retrieval performance. Experimental\nresults show that CaGR-RAG reduces 99th percentile tail latency by up to 51.55%\nwhile consistently maintaining a higher cache hit ratio than the baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern embedding models capture both semantic and syntactic structures of\nqueries, often mapping different queries to similar regions in vector space.\nThis results in non-uniform cluster access patterns in disk-based vector search\nsystems, particularly in Retrieval Augmented Generation (RAG) framework. While\nexisting approaches optimize individual queries, they overlook the impact of\ncluster access patterns, failing to account for the locality effects of queries\nthat access similar clusters. This oversight reduces cache efficiency and\nincreases search latency due to excessive disk I/O. To address this, we\nintroduce CaGR-RAG, a context-aware query grouping mechanism that organizes\nqueries based on shared cluster access patterns. Additionally, it incorporates\nopportunistic cluster prefetching to minimize cache misses during transitions\nbetween query groups, further optimizing retrieval performance. Experimental\nresults show that CaGR-RAG reduces 99th percentile tail latency by up to 51.55%\nwhile consistently maintaining a higher cache hit ratio than the baseline."
                },
                "authors": [
                    {
                        "name": "Yeonwoo Jeong"
                    },
                    {
                        "name": "Kyuli Park"
                    },
                    {
                        "name": "Hyunji Cho"
                    },
                    {
                        "name": "Sungyong Park"
                    }
                ],
                "author_detail": {
                    "name": "Sungyong Park"
                },
                "author": "Sungyong Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01002v1",
                "updated": "2025-05-02T04:57:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T04:57:06Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "title": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber"
                },
                "summary": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses."
                },
                "authors": [
                    {
                        "name": "NEXT Collaboration"
                    },
                    {
                        "name": "C. Adams"
                    },
                    {
                        "name": "H. Almazán"
                    },
                    {
                        "name": "V. Álvarez"
                    },
                    {
                        "name": "K. Bailey"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "B. J. P. Jones"
                    },
                    {
                        "name": "S. Johnston"
                    },
                    {
                        "name": "K. Mistry"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "D. R. Nygren"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "L. Rogers"
                    },
                    {
                        "name": "J. Waldschmidt"
                    },
                    {
                        "name": "B. Aparicio"
                    },
                    {
                        "name": "A. I. Aranburu"
                    },
                    {
                        "name": "L. Arazi"
                    },
                    {
                        "name": "I. J. Arnquist"
                    },
                    {
                        "name": "F. Auria-Luna"
                    },
                    {
                        "name": "S. Ayet"
                    },
                    {
                        "name": "C. D. R. Azevedo"
                    },
                    {
                        "name": "F. Ballester"
                    },
                    {
                        "name": "M. del Barrio-Torregrosa"
                    },
                    {
                        "name": "A. Bayo"
                    },
                    {
                        "name": "J. M. Benlloch-Rodríguez"
                    },
                    {
                        "name": "F. I. G. M. Borges"
                    },
                    {
                        "name": "A. Brodolin"
                    },
                    {
                        "name": "S. Cárcel"
                    },
                    {
                        "name": "A. Castillo"
                    },
                    {
                        "name": "L. Cid"
                    },
                    {
                        "name": "C. A. N. Conde"
                    },
                    {
                        "name": "T. Contreras"
                    },
                    {
                        "name": "F. P. Cossío"
                    },
                    {
                        "name": "R. Coupe"
                    },
                    {
                        "name": "E. Dey"
                    },
                    {
                        "name": "G. Díaz"
                    },
                    {
                        "name": "C. Echevarria"
                    },
                    {
                        "name": "M. Elorza"
                    },
                    {
                        "name": "J. Escada"
                    },
                    {
                        "name": "R. Esteve"
                    },
                    {
                        "name": "R. Felkai"
                    },
                    {
                        "name": "L. M. P. Fernandes"
                    },
                    {
                        "name": "P. Ferrario"
                    },
                    {
                        "name": "A. L. Ferreira"
                    },
                    {
                        "name": "F. W. Foss"
                    },
                    {
                        "name": "Z. Freixa"
                    },
                    {
                        "name": "J. García-Barrena"
                    },
                    {
                        "name": "J. J. Gómez-Cadenas"
                    },
                    {
                        "name": "J. W. R. Grocott"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "J. Hauptman"
                    },
                    {
                        "name": "C. A. O. Henriques"
                    },
                    {
                        "name": "J. A. Hernando Morata"
                    },
                    {
                        "name": "P. Herrero-Gómez"
                    },
                    {
                        "name": "V. Herrero"
                    },
                    {
                        "name": "C. Hervés Carrete"
                    },
                    {
                        "name": "Y. Ifergan"
                    },
                    {
                        "name": "F. Kellerer"
                    },
                    {
                        "name": "L. Larizgoitia"
                    },
                    {
                        "name": "A. Larumbe"
                    },
                    {
                        "name": "P. Lebrun"
                    },
                    {
                        "name": "F. Lopez"
                    },
                    {
                        "name": "N. López-March"
                    },
                    {
                        "name": "R. Madigan"
                    },
                    {
                        "name": "R. D. P. Mano"
                    },
                    {
                        "name": "A. P. Marques"
                    },
                    {
                        "name": "J. Martín-Albo"
                    },
                    {
                        "name": "G. Martínez-Lema"
                    },
                    {
                        "name": "M. Martínez-Vara"
                    },
                    {
                        "name": "R. L. Miller"
                    },
                    {
                        "name": "J. Molina-Canteras"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "C. M. B. Monteiro"
                    },
                    {
                        "name": "F. J. Mora"
                    },
                    {
                        "name": "P. Novella"
                    },
                    {
                        "name": "A. Nuñez"
                    },
                    {
                        "name": "E. Oblak"
                    },
                    {
                        "name": "J. Palacio"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "A. Para"
                    },
                    {
                        "name": "A. Pazos"
                    },
                    {
                        "name": "J. Pelegrin"
                    },
                    {
                        "name": "M. Pérez Maneiro"
                    },
                    {
                        "name": "M. Querol"
                    },
                    {
                        "name": "J. Renner"
                    },
                    {
                        "name": "I. Rivilla"
                    },
                    {
                        "name": "C. Rogero"
                    },
                    {
                        "name": "B. Romeo"
                    },
                    {
                        "name": "C. Romo-Luque"
                    },
                    {
                        "name": "V. San Nacienciano"
                    },
                    {
                        "name": "F. P. Santos"
                    },
                    {
                        "name": "J. M. F. dos Santos"
                    },
                    {
                        "name": "M. Seemann"
                    },
                    {
                        "name": "I. Shomroni"
                    },
                    {
                        "name": "P. A. O. C. Silva"
                    },
                    {
                        "name": "A. Simón"
                    },
                    {
                        "name": "S. R. Soleti"
                    },
                    {
                        "name": "M. Sorel"
                    },
                    {
                        "name": "J. Soto-Oton"
                    },
                    {
                        "name": "J. M. R. Teixeira"
                    },
                    {
                        "name": "S. Teruel-Pardo"
                    },
                    {
                        "name": "J. F. Toledo"
                    },
                    {
                        "name": "C. Tonnelé"
                    },
                    {
                        "name": "S. Torelli"
                    },
                    {
                        "name": "J. Torrent"
                    },
                    {
                        "name": "A. Trettin"
                    },
                    {
                        "name": "A. Usón"
                    },
                    {
                        "name": "P. R. G. Valle"
                    },
                    {
                        "name": "J. F. C. A. Veloso"
                    },
                    {
                        "name": "J. Waiton"
                    },
                    {
                        "name": "A. Yubero-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "A. Yubero-Navarro"
                },
                "author": "A. Yubero-Navarro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00962v1",
                "updated": "2025-05-02T02:36:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    2,
                    36,
                    23,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T02:36:23Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    2,
                    36,
                    23,
                    4,
                    122,
                    0
                ],
                "title": "The Open-Source BlackParrot-BedRock Cache Coherence System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Open-Source BlackParrot-BedRock Cache Coherence System"
                },
                "summary": "This dissertation revisits the topic of programmable cache coherence engines\nin the context of modern shared-memory multicore processors. First, the\nopen-source BedRock cache coherence protocol is described. BedRock employs the\ncanonical MOESIF coherence states and reduces implementation burden by\neliminating transient coherence states from the protocol. The protocol's design\ncomplexity, concurrency, and verification effort are analyzed and compared to a\ncanonical directory-based invalidate coherence protocol. Second, the\narchitecture and microarchitecture of three separate cache coherence\ndirectories implementing the BedRock protocol within the BlackParrot 64-bit\nRISC-V multicore processor, collectively called BlackParrot-BedRock\n(BP-BedRock), are described. A fixed-function coherence directory engine\nimplementation provides a baseline design for performance and area comparisons.\nA microcode-programmable coherence directory implementation demonstrates the\nfeasibility of implementing a programmable coherence engine capable of\nmaintaining sufficient protocol processing performance. A hybrid fixed-function\nand programmable coherence directory blends the protocol processing performance\nof the fixed-function design with the programmable flexibility of the\nmicrocode-programmable design. Collectively, the BedRock coherence protocol and\nits three BP-BedRock implementations demonstrate the feasibility and challenges\nof including programmable logic within the coherence system of modern\nshared-memory multicore processors, paving the way for future research into the\napplication- and system-level benefits of programmable coherence engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This dissertation revisits the topic of programmable cache coherence engines\nin the context of modern shared-memory multicore processors. First, the\nopen-source BedRock cache coherence protocol is described. BedRock employs the\ncanonical MOESIF coherence states and reduces implementation burden by\neliminating transient coherence states from the protocol. The protocol's design\ncomplexity, concurrency, and verification effort are analyzed and compared to a\ncanonical directory-based invalidate coherence protocol. Second, the\narchitecture and microarchitecture of three separate cache coherence\ndirectories implementing the BedRock protocol within the BlackParrot 64-bit\nRISC-V multicore processor, collectively called BlackParrot-BedRock\n(BP-BedRock), are described. A fixed-function coherence directory engine\nimplementation provides a baseline design for performance and area comparisons.\nA microcode-programmable coherence directory implementation demonstrates the\nfeasibility of implementing a programmable coherence engine capable of\nmaintaining sufficient protocol processing performance. A hybrid fixed-function\nand programmable coherence directory blends the protocol processing performance\nof the fixed-function design with the programmable flexibility of the\nmicrocode-programmable design. Collectively, the BedRock coherence protocol and\nits three BP-BedRock implementations demonstrate the feasibility and challenges\nof including programmable logic within the coherence system of modern\nshared-memory multicore processors, paving the way for future research into the\napplication- and system-level benefits of programmable coherence engines."
                },
                "authors": [
                    {
                        "name": "Mark Unruh Wyse"
                    }
                ],
                "author_detail": {
                    "name": "Mark Unruh Wyse"
                },
                "author": "Mark Unruh Wyse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00901v1",
                "updated": "2025-05-01T22:32:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T22:32:29Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "title": "Heterogeneous Memory Benchmarking Toolkit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Memory Benchmarking Toolkit"
                },
                "summary": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems that enables users\nto understand and precisely characterize the temporal behavior of all available\nmemory modules under configurable contention stress scenarios. Since\nkernel-level provides a high degree of control over allocation, cache\nmaintenance, $CPUs$, interrupts, and I/O device activity, seeking the most\naccurate way to benchmark heterogeneous memory subsystems, would be achieved by\nimplementing it in the kernel. This gives us the privilege to directly map\npieces of contiguous physical memory and instantiate allocators, allowing us to\nfinely control cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU_FPGA platform, demonstrates its capability\nto precisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems that enables users\nto understand and precisely characterize the temporal behavior of all available\nmemory modules under configurable contention stress scenarios. Since\nkernel-level provides a high degree of control over allocation, cache\nmaintenance, $CPUs$, interrupts, and I/O device activity, seeking the most\naccurate way to benchmark heterogeneous memory subsystems, would be achieved by\nimplementing it in the kernel. This gives us the privilege to directly map\npieces of contiguous physical memory and instantiate allocators, allowing us to\nfinely control cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU_FPGA platform, demonstrates its capability\nto precisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system."
                },
                "authors": [
                    {
                        "name": "Golsana Ghaemi"
                    },
                    {
                        "name": "Kazem Taram"
                    },
                    {
                        "name": "Renato Mancuso"
                    }
                ],
                "author_detail": {
                    "name": "Renato Mancuso"
                },
                "author": "Renato Mancuso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00817v1",
                "updated": "2025-05-01T19:18:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    18,
                    56,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T19:18:56Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    18,
                    56,
                    3,
                    121,
                    0
                ],
                "title": "Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from\n  Large Language Models"
                },
                "summary": "Side-channel attacks on shared hardware resources increasingly threaten\nconfidentiality, especially with the rise of Large Language Models (LLMs). In\nthis work, we introduce Spill The Beans, a novel application of cache\nside-channels to leak tokens generated by an LLM. By co-locating an attack\nprocess on the same hardware as the victim model, we flush and reload embedding\nvectors from the embedding layer, where each token corresponds to a unique\nembedding vector. When accessed during token generation, it results in a cache\nhit detectable by our attack on shared lower-level caches.\n  A significant challenge is the massive size of LLMs, which, by nature of\ntheir compute intensive operation, quickly evicts embedding vectors from the\ncache. We address this by balancing the number of tokens monitored against the\namount of information leaked. Monitoring more tokens increases potential\nvocabulary leakage but raises the chance of missing cache hits due to eviction;\nmonitoring fewer tokens improves detection reliability but limits vocabulary\ncoverage.\n  Through extensive experimentation, we demonstrate the feasibility of leaking\ntokens from LLMs via cache side-channels. Our findings reveal a new\nvulnerability in LLM deployments, highlighting that even sophisticated models\nare susceptible to traditional side-channel attacks. We discuss the\nimplications for privacy and security in LLM-serving infrastructures and\nsuggest considerations for mitigating such threats. For proof of concept we\nconsider two concrete attack scenarios: Our experiments show that an attacker\ncan recover as much as 80%-90% of a high entropy API key with single shot\nmonitoring. As for English text we can reach a 40% recovery rate with a single\nshot. We should note that the rate highly depends on the monitored token set\nand these rates can be improved by targeting more specialized output domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Side-channel attacks on shared hardware resources increasingly threaten\nconfidentiality, especially with the rise of Large Language Models (LLMs). In\nthis work, we introduce Spill The Beans, a novel application of cache\nside-channels to leak tokens generated by an LLM. By co-locating an attack\nprocess on the same hardware as the victim model, we flush and reload embedding\nvectors from the embedding layer, where each token corresponds to a unique\nembedding vector. When accessed during token generation, it results in a cache\nhit detectable by our attack on shared lower-level caches.\n  A significant challenge is the massive size of LLMs, which, by nature of\ntheir compute intensive operation, quickly evicts embedding vectors from the\ncache. We address this by balancing the number of tokens monitored against the\namount of information leaked. Monitoring more tokens increases potential\nvocabulary leakage but raises the chance of missing cache hits due to eviction;\nmonitoring fewer tokens improves detection reliability but limits vocabulary\ncoverage.\n  Through extensive experimentation, we demonstrate the feasibility of leaking\ntokens from LLMs via cache side-channels. Our findings reveal a new\nvulnerability in LLM deployments, highlighting that even sophisticated models\nare susceptible to traditional side-channel attacks. We discuss the\nimplications for privacy and security in LLM-serving infrastructures and\nsuggest considerations for mitigating such threats. For proof of concept we\nconsider two concrete attack scenarios: Our experiments show that an attacker\ncan recover as much as 80%-90% of a high entropy API key with single shot\nmonitoring. As for English text we can reach a 40% recovery rate with a single\nshot. We should note that the rate highly depends on the monitored token set\nand these rates can be improved by targeting more specialized output domains."
                },
                "authors": [
                    {
                        "name": "Andrew Adiletta"
                    },
                    {
                        "name": "Berk Sunar"
                    }
                ],
                "author_detail": {
                    "name": "Berk Sunar"
                },
                "author": "Berk Sunar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00768v1",
                "updated": "2025-05-01T18:00:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    40,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T18:00:40Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    40,
                    3,
                    121,
                    0
                ],
                "title": "Optomechanical resource for fault-tolerant quantum computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optomechanical resource for fault-tolerant quantum computing"
                },
                "summary": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive."
                },
                "authors": [
                    {
                        "name": "Margaret Pavlovich"
                    },
                    {
                        "name": "Peter Rakich"
                    },
                    {
                        "name": "Shruti Puri"
                    }
                ],
                "author_detail": {
                    "name": "Shruti Puri"
                },
                "author": "Shruti Puri",
                "arxiv_comment": "19 pages, 9 figures. Supplement 29 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00570v1",
                "updated": "2025-05-01T14:53:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    14,
                    53,
                    12,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T14:53:12Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    14,
                    53,
                    12,
                    3,
                    121,
                    0
                ],
                "title": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context\n  Window Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context\n  Window Extension"
                },
                "summary": "Extending the context window in large language models (LLMs) is essential for\napplications involving long-form content generation. However, the linear\nincrease in key-value (KV) cache memory requirements and the quadratic\ncomplexity of self-attention with respect to sequence length present\nsignificant challenges during fine-tuning and inference. Existing methods\nsuffer from performance degradation when extending to longer contexts. In this\nwork, we introduce a novel context extension method that optimizes both\nfine-tuning and inference efficiency. Our method exploits a key observation: in\nthe frequency domain, the energy distribution of the KV cache is primarily\nconcentrated in low-frequency components. By filtering out the high-frequency\ncomponents, the KV cache can be effectively compressed with minimal information\nloss. Building on this insight, we propose an efficient compression technique,\nFreqKV, that iteratively compresses the increasing KV cache to a fixed size in\nthe frequency domain, applicable to both fine-tuning and inference. FreqKV\nintroduces no additional parameters or architectural modifications. With\nminimal fine-tuning, LLMs can learn to leverage the limited cache that is\ncompressed in the frequency domain and extend the context window efficiently.\nExperiments on various long context language modeling and understanding tasks\ndemonstrate the efficiency and efficacy of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending the context window in large language models (LLMs) is essential for\napplications involving long-form content generation. However, the linear\nincrease in key-value (KV) cache memory requirements and the quadratic\ncomplexity of self-attention with respect to sequence length present\nsignificant challenges during fine-tuning and inference. Existing methods\nsuffer from performance degradation when extending to longer contexts. In this\nwork, we introduce a novel context extension method that optimizes both\nfine-tuning and inference efficiency. Our method exploits a key observation: in\nthe frequency domain, the energy distribution of the KV cache is primarily\nconcentrated in low-frequency components. By filtering out the high-frequency\ncomponents, the KV cache can be effectively compressed with minimal information\nloss. Building on this insight, we propose an efficient compression technique,\nFreqKV, that iteratively compresses the increasing KV cache to a fixed size in\nthe frequency domain, applicable to both fine-tuning and inference. FreqKV\nintroduces no additional parameters or architectural modifications. With\nminimal fine-tuning, LLMs can learn to leverage the limited cache that is\ncompressed in the frequency domain and extend the context window efficiently.\nExperiments on various long context language modeling and understanding tasks\ndemonstrate the efficiency and efficacy of the proposed method."
                },
                "authors": [
                    {
                        "name": "Jushi Kai"
                    },
                    {
                        "name": "Boyi Zeng"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Zhouhan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouhan Lin"
                },
                "author": "Zhouhan Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00315v1",
                "updated": "2025-05-01T05:22:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    5,
                    22,
                    11,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T05:22:11Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    5,
                    22,
                    11,
                    3,
                    121,
                    0
                ],
                "title": "Mixture of Sparse Attention: Content-Based Learnable Sparse Attention\n  via Expert-Choice Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Sparse Attention: Content-Based Learnable Sparse Attention\n  via Expert-Choice Routing"
                },
                "summary": "Recent advances in large language models highlighted the excessive quadratic\ncost of self-attention. Despite the significant research efforts, subquadratic\nattention methods still suffer from inferior performance in practice. We\nhypothesize that dynamic, learned content-based sparsity can lead to more\nefficient attention mechanisms. We present Mixture of Sparse Attention (MoSA),\na novel approach inspired by Mixture of Experts (MoE) with expert choice\nrouting. MoSA dynamically selects tokens for each attention head, allowing\narbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of\nlength $T$, MoSA reduces the computational complexity of each attention head\nfrom $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same\ncomputational budget, allowing higher specialization. We show that among the\ntested sparse attention variants, MoSA is the only one that can outperform the\ndense baseline, sometimes with up to 27% better perplexity for an identical\ncompute budget. MoSA can also reduce the resource usage compared to dense\nself-attention. Despite using torch implementation without an optimized kernel,\nperplexity-matched MoSA models are simultaneously faster in wall-clock time,\nrequire less memory for training, and drastically reduce the size of the\nKV-cache compared to the dense transformer baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models highlighted the excessive quadratic\ncost of self-attention. Despite the significant research efforts, subquadratic\nattention methods still suffer from inferior performance in practice. We\nhypothesize that dynamic, learned content-based sparsity can lead to more\nefficient attention mechanisms. We present Mixture of Sparse Attention (MoSA),\na novel approach inspired by Mixture of Experts (MoE) with expert choice\nrouting. MoSA dynamically selects tokens for each attention head, allowing\narbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of\nlength $T$, MoSA reduces the computational complexity of each attention head\nfrom $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same\ncomputational budget, allowing higher specialization. We show that among the\ntested sparse attention variants, MoSA is the only one that can outperform the\ndense baseline, sometimes with up to 27% better perplexity for an identical\ncompute budget. MoSA can also reduce the resource usage compared to dense\nself-attention. Despite using torch implementation without an optimized kernel,\nperplexity-matched MoSA models are simultaneously faster in wall-clock time,\nrequire less memory for training, and drastically reduce the size of the\nKV-cache compared to the dense transformer baselines."
                },
                "authors": [
                    {
                        "name": "Piotr Piękos"
                    },
                    {
                        "name": "Róbert Csordás"
                    },
                    {
                        "name": "Jürgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Schmidhuber"
                },
                "author": "Jürgen Schmidhuber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04532v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04532v3",
                "updated": "2025-05-01T02:14:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    2,
                    14,
                    5,
                    3,
                    121,
                    0
                ],
                "published": "2024-05-07T17:59:30Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    17,
                    59,
                    30,
                    1,
                    128,
                    0
                ],
                "title": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM\n  Serving"
                },
                "summary": "Quantization can accelerate large language model (LLM) inference. Going\nbeyond INT8 quantization, the research community is actively exploring even\nlower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization\ntechniques only accelerate low-batch, edge LLM inference, failing to deliver\nperformance gains in large-batch, cloud-based LLM serving. We uncover a\ncritical issue: existing INT4 quantization methods suffer from significant\nruntime overhead (20-90%) when dequantizing either weights or partial sums on\nGPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization\nalgorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands\nfor quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented\nby the QServe inference library that achieves measured speedup. The key insight\ndriving QServe is that the efficiency of LLM serving on GPUs is critically\ninfluenced by operations on low-throughput CUDA cores. Building upon this\ninsight, in QoQ algorithm, we introduce progressive quantization that can allow\nlow dequantization overhead in W4A8 GEMM. Additionally, we develop\nSmoothAttention to effectively mitigate the accuracy degradation incurred by\n4-bit KV quantization. In the QServe system, we perform compute-aware weight\nreordering and take advantage of register-level parallelism to reduce\ndequantization latency. We also make fused attention memory-bound, harnessing\nthe performance gain brought by KV4 quantization. As a result, QServe improves\nthe maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x\non L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to\nTensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput\nthan TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of\nLLM serving by 3x. Code is available at\nhttps://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization can accelerate large language model (LLM) inference. Going\nbeyond INT8 quantization, the research community is actively exploring even\nlower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization\ntechniques only accelerate low-batch, edge LLM inference, failing to deliver\nperformance gains in large-batch, cloud-based LLM serving. We uncover a\ncritical issue: existing INT4 quantization methods suffer from significant\nruntime overhead (20-90%) when dequantizing either weights or partial sums on\nGPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization\nalgorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands\nfor quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented\nby the QServe inference library that achieves measured speedup. The key insight\ndriving QServe is that the efficiency of LLM serving on GPUs is critically\ninfluenced by operations on low-throughput CUDA cores. Building upon this\ninsight, in QoQ algorithm, we introduce progressive quantization that can allow\nlow dequantization overhead in W4A8 GEMM. Additionally, we develop\nSmoothAttention to effectively mitigate the accuracy degradation incurred by\n4-bit KV quantization. In the QServe system, we perform compute-aware weight\nreordering and take advantage of register-level parallelism to reduce\ndequantization latency. We also make fused attention memory-bound, harnessing\nthe performance gain brought by KV4 quantization. As a result, QServe improves\nthe maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x\non L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to\nTensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput\nthan TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of\nLLM serving by 3x. Code is available at\nhttps://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Zhekai Zhang"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Chuang Gan"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "The first three authors contribute equally to this project and are\n  listed in the alphabetical order. Yujun Lin leads the quantization algorithm,\n  Haotian Tang and Shang Yang lead the GPU kernels and the serving system. Code\n  is available at https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04532v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04532v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19602v2",
                "updated": "2025-05-01T00:13:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    0,
                    13,
                    6,
                    3,
                    121,
                    0
                ],
                "published": "2025-04-28T09:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    4,
                    30,
                    0,
                    118,
                    0
                ],
                "title": "Soft-Label Caching and Sharpening for Communication-Efficient Federated\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft-Label Caching and Sharpening for Communication-Efficient Federated\n  Distillation"
                },
                "summary": "Federated Learning (FL) enables collaborative model training across\ndecentralized clients, enhancing privacy by keeping data local. Yet\nconventional FL, relying on frequent parameter-sharing, suffers from high\ncommunication overhead and limited model heterogeneity. Distillation-based FL\napproaches address these issues by sharing predictions (soft-labels) instead,\nbut they often involve redundant transmissions across communication rounds,\nreducing efficiency. We propose SCARLET, a novel framework integrating\nsynchronized soft-label caching and an enhanced Entropy Reduction Aggregation\n(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing\ncached soft-labels, achieving up to 50% reduction in communication costs\ncompared to existing methods while maintaining accuracy. Enhanced ERA can be\ntuned to adapt to non-IID data variations, ensuring robust aggregation and\nperformance in diverse client scenarios. Experimental evaluations demonstrate\nthat SCARLET consistently outperforms state-of-the-art distillation-based FL\nmethods in terms of accuracy and communication efficiency. The implementation\nof SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables collaborative model training across\ndecentralized clients, enhancing privacy by keeping data local. Yet\nconventional FL, relying on frequent parameter-sharing, suffers from high\ncommunication overhead and limited model heterogeneity. Distillation-based FL\napproaches address these issues by sharing predictions (soft-labels) instead,\nbut they often involve redundant transmissions across communication rounds,\nreducing efficiency. We propose SCARLET, a novel framework integrating\nsynchronized soft-label caching and an enhanced Entropy Reduction Aggregation\n(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing\ncached soft-labels, achieving up to 50% reduction in communication costs\ncompared to existing methods while maintaining accuracy. Enhanced ERA can be\ntuned to adapt to non-IID data variations, ensuring robust aggregation and\nperformance in diverse client scenarios. Experimental evaluations demonstrate\nthat SCARLET consistently outperforms state-of-the-art distillation-based FL\nmethods in terms of accuracy and communication efficiency. The implementation\nof SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET."
                },
                "authors": [
                    {
                        "name": "Kitsuya Azuma"
                    },
                    {
                        "name": "Takayuki Nishio"
                    },
                    {
                        "name": "Yuichi Kitagawa"
                    },
                    {
                        "name": "Wakako Nakano"
                    },
                    {
                        "name": "Takahito Tanimura"
                    }
                ],
                "author_detail": {
                    "name": "Takahito Tanimura"
                },
                "author": "Takahito Tanimura",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v2",
                "updated": "2025-04-30T19:48:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    19,
                    48,
                    41,
                    2,
                    120,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of 75%, 50%, and 25%, and\nthe training-based model Learning-to-cache has a caching level of 22%.\nSpecifically, the FID values change from 30.454 to 21.690 (28.8%), from 6.857\nto 5.821 (15.1%), from 3.870 to 3.692 (4.6%), and from 3.539 to 3.451 (2.5%)\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of 75%, 50%, and 25%, and\nthe training-based model Learning-to-cache has a caching level of 22%.\nSpecifically, the FID values change from 30.454 to 21.690 (28.8%), from 6.857\nto 5.821 (15.1%), from 3.870 to 3.692 (4.6%), and from 3.539 to 3.451 (2.5%)\nrespectively."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Xingyu Zhu"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00074v1",
                "updated": "2025-04-30T18:00:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    18,
                    0,
                    2,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T18:00:02Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    18,
                    0,
                    2,
                    2,
                    120,
                    0
                ],
                "title": "SDW driven \"magnetic breakdown\" in a d-wave altermagnet KV$_2$Se$_2$O",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SDW driven \"magnetic breakdown\" in a d-wave altermagnet KV$_2$Se$_2$O"
                },
                "summary": "Altermagnets, combining zero net magnetization with intrinsic spin splitting,\ndemonstrate unique quantum phenomena crucial for spintronic applications.\nKV$_2$Se$_2$O is proven to be a d-wave altermagnet with phase transition from a\ncheckerboard-type (C-type) antiferromagnetic (AFM) state to a spin density wave\n(SDW) state as the temperature decreases. After phase transition, the apparent\nparadox emerges where angle-resolved photoemission spectroscopy (ARPES) reveals\nnegligible Fermi surface modifications, while physical property measurement\nsystem (PPMS) measurements uncover substantial changes in transport properties.\nOur study explores the microscopic mechanisms governing phase-dependent\ntransport properties of KV$_2$Se$_2$O base on first-principles calculations.\nThe spin canting driven by periodic spin modulation in the SDW phase reduces\nthe magnetic symmetry of KV$_2$Se$_2$O. The resultant band degeneracy lifting\nand Fermi surface reconstruction induce the ``magnetic breakdown\" phenomenon,\nwhich alters carrier trajectories, modifies carrier concentration, strengthens\nelectron-hole compensation, and ultimately accounts for the contrasting\nmagnetic-field-dependent Hall resistivity relative to the C-type AFM state. Our\nwork proposes an innovative method for identifying the electronic structure\nevolution across phase transitions from transport signatures, providing a novel\nparadigm for altermagnets research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnets, combining zero net magnetization with intrinsic spin splitting,\ndemonstrate unique quantum phenomena crucial for spintronic applications.\nKV$_2$Se$_2$O is proven to be a d-wave altermagnet with phase transition from a\ncheckerboard-type (C-type) antiferromagnetic (AFM) state to a spin density wave\n(SDW) state as the temperature decreases. After phase transition, the apparent\nparadox emerges where angle-resolved photoemission spectroscopy (ARPES) reveals\nnegligible Fermi surface modifications, while physical property measurement\nsystem (PPMS) measurements uncover substantial changes in transport properties.\nOur study explores the microscopic mechanisms governing phase-dependent\ntransport properties of KV$_2$Se$_2$O base on first-principles calculations.\nThe spin canting driven by periodic spin modulation in the SDW phase reduces\nthe magnetic symmetry of KV$_2$Se$_2$O. The resultant band degeneracy lifting\nand Fermi surface reconstruction induce the ``magnetic breakdown\" phenomenon,\nwhich alters carrier trajectories, modifies carrier concentration, strengthens\nelectron-hole compensation, and ultimately accounts for the contrasting\nmagnetic-field-dependent Hall resistivity relative to the C-type AFM state. Our\nwork proposes an innovative method for identifying the electronic structure\nevolution across phase transitions from transport signatures, providing a novel\nparadigm for altermagnets research."
                },
                "authors": [
                    {
                        "name": "Xu Yan"
                    },
                    {
                        "name": "Ziyin Song"
                    },
                    {
                        "name": "Juntao Song"
                    },
                    {
                        "name": "Zhong Fang"
                    },
                    {
                        "name": "Hongming Weng"
                    },
                    {
                        "name": "Quansheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Quansheng Wu"
                },
                "author": "Quansheng Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21594v1",
                "updated": "2025-04-30T12:51:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    51,
                    59,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T12:51:59Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    51,
                    59,
                    2,
                    120,
                    0
                ],
                "title": "Switching Transients in Constrained Transformer-Line/Cable\n  Configurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Switching Transients in Constrained Transformer-Line/Cable\n  Configurations"
                },
                "summary": "This paper investigates the transient phenomena that occur in two special\ncases in the Netherlands: (A) during the energization of a power transformer\nvia a cable feeder and (B) the energization of a power transformer together\nwith an overhead line (OHL). In Case A a 7 km long 150 kV cable and a 150/50 kV\ntransformer are connected and energized at the same time. In Case B a 150/50 kV\ntransformer and a short 50 kV OHL are connected and energized simultaneously.\nThe reason behind this kind of situations is related to space restrictions and\ncost efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the transient phenomena that occur in two special\ncases in the Netherlands: (A) during the energization of a power transformer\nvia a cable feeder and (B) the energization of a power transformer together\nwith an overhead line (OHL). In Case A a 7 km long 150 kV cable and a 150/50 kV\ntransformer are connected and energized at the same time. In Case B a 150/50 kV\ntransformer and a short 50 kV OHL are connected and energized simultaneously.\nThe reason behind this kind of situations is related to space restrictions and\ncost efficiency."
                },
                "authors": [
                    {
                        "name": "Y. Xiang"
                    },
                    {
                        "name": "L. Wu"
                    },
                    {
                        "name": "K. Velitsikakis"
                    },
                    {
                        "name": "A. L. J. Janssen"
                    }
                ],
                "author_detail": {
                    "name": "A. L. J. Janssen"
                },
                "author": "A. L. J. Janssen",
                "arxiv_comment": "11 pages, 17 figures, CIGRE conference 2016",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00745v1",
                "updated": "2025-04-30T08:08:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    8,
                    15,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T08:08:15Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    8,
                    15,
                    2,
                    120,
                    0
                ],
                "title": "Responsive DNN Adaptation for Video Analytics against Environment Shift\n  via Hierarchical Mobile-Cloud Collaborations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Responsive DNN Adaptation for Video Analytics against Environment Shift\n  via Hierarchical Mobile-Cloud Collaborations"
                },
                "summary": "Mobile video analysis systems often encounter various deploying environments,\nwhere environment shifts present greater demands for responsiveness in\nadaptations of deployed \"expert DNN models\". Existing model adaptation\nframeworks primarily operate in a cloud-centric way, exhibiting degraded\nperformance during adaptation and delayed reactions to environment shifts.\nInstead, this paper proposes MOCHA, a novel framework optimizing the\nresponsiveness of continuous model adaptation through hierarchical\ncollaborations between mobile and cloud resources. Specifically, MOCHA (1)\nreduces adaptation response delays by performing on-device model reuse and fast\nfine-tuning before requesting cloud model retrieval and end-to-end retraining;\n(2) accelerates history expert model retrieval by organizing them into a\nstructured taxonomy utilizing domain semantics analyzed by a cloud foundation\nmodel as indices; (3) enables efficient local model reuse by maintaining\nonboard expert model caches for frequent scenes, which proactively prefetch\nmodel weights from the cloud model database. Extensive evaluations with\nreal-world videos on three DNN tasks show MOCHA improves the model accuracy\nduring adaptation by up to 6.8% while saving the response delay and retraining\ntime by up to 35.5x and 3.0x respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile video analysis systems often encounter various deploying environments,\nwhere environment shifts present greater demands for responsiveness in\nadaptations of deployed \"expert DNN models\". Existing model adaptation\nframeworks primarily operate in a cloud-centric way, exhibiting degraded\nperformance during adaptation and delayed reactions to environment shifts.\nInstead, this paper proposes MOCHA, a novel framework optimizing the\nresponsiveness of continuous model adaptation through hierarchical\ncollaborations between mobile and cloud resources. Specifically, MOCHA (1)\nreduces adaptation response delays by performing on-device model reuse and fast\nfine-tuning before requesting cloud model retrieval and end-to-end retraining;\n(2) accelerates history expert model retrieval by organizing them into a\nstructured taxonomy utilizing domain semantics analyzed by a cloud foundation\nmodel as indices; (3) enables efficient local model reuse by maintaining\nonboard expert model caches for frequent scenes, which proactively prefetch\nmodel weights from the cloud model database. Extensive evaluations with\nreal-world videos on three DNN tasks show MOCHA improves the model accuracy\nduring adaptation by up to 6.8% while saving the response delay and retraining\ntime by up to 35.5x and 3.0x respectively."
                },
                "authors": [
                    {
                        "name": "Maozhe Zhao"
                    },
                    {
                        "name": "Shengzhong Liu"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "Sensys 2025 final version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21230v1",
                "updated": "2025-04-29T23:43:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    43,
                    59,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T23:43:59Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    43,
                    59,
                    1,
                    119,
                    0
                ],
                "title": "Kimina Lean Server: Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimina Lean Server: Technical Report"
                },
                "summary": "We introduce the Kimina Lean Server, an open-source project that enables fast\nand scalable interaction with Lean 4 via a unified REST API, designed as a\nsimple verifier for reinforcement learning pipelines. Built on top of the Lean\nFRO's LeanREPL, it combines server-side parallelization by managing multiple\nLean REPL processes in parallel, with an LRU caching strategy that reuses Lean\nimports across multiple requests. These features help reduce initialization\noverhead and allow large-scale batch processing of Lean code. The client-side\ninterface allows users to submit batches of proofs and receive Lean feedback,\nincluding extracted tactics and tactic states via infotree processing. These\nfeatures enable a high-performance, scalable workflow for both interaction and\nextraction of proofs, tactics, and tactic states. We open source our\nimplementation on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Kimina Lean Server, an open-source project that enables fast\nand scalable interaction with Lean 4 via a unified REST API, designed as a\nsimple verifier for reinforcement learning pipelines. Built on top of the Lean\nFRO's LeanREPL, it combines server-side parallelization by managing multiple\nLean REPL processes in parallel, with an LRU caching strategy that reuses Lean\nimports across multiple requests. These features help reduce initialization\noverhead and allow large-scale batch processing of Lean code. The client-side\ninterface allows users to submit batches of proofs and receive Lean feedback,\nincluding extracted tactics and tactic states via infotree processing. These\nfeatures enable a high-performance, scalable workflow for both interaction and\nextraction of proofs, tactics, and tactic states. We open source our\nimplementation on GitHub."
                },
                "authors": [
                    {
                        "name": "Marco Dos Santos"
                    },
                    {
                        "name": "Haiming Wang"
                    },
                    {
                        "name": "Hugues de Saxcé"
                    },
                    {
                        "name": "Ran Wang"
                    },
                    {
                        "name": "Mantas Baksys"
                    },
                    {
                        "name": "Mert Unsal"
                    },
                    {
                        "name": "Junqi Liu"
                    },
                    {
                        "name": "Zhengying Liu"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21228v1",
                "updated": "2025-04-29T23:42:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    42,
                    21,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T23:42:21Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    42,
                    21,
                    1,
                    119,
                    0
                ],
                "title": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt\n  Injection Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt\n  Injection Attacks"
                },
                "summary": "Large Language Models (LLMs) are identified as being susceptible to indirect\nprompt injection attack, where the model undesirably deviates from\nuser-provided instructions by executing tasks injected in the prompt context.\nThis vulnerability stems from LLMs' inability to distinguish between data and\ninstructions within a prompt. In this paper, we propose CachePrune that defends\nagainst this attack by identifying and pruning task-triggering neurons from the\nKV cache of the input prompt context. By pruning such neurons, we encourage the\nLLM to treat the text spans of input prompt context as only pure data, instead\nof any indicator of instruction following. These neurons are identified via\nfeature attribution with a loss function induced from an upperbound of the\nDirect Preference Optimization (DPO) objective. We show that such a loss\nfunction enables effective feature attribution with only a few samples. We\nfurther improve on the quality of feature attribution, by exploiting an\nobserved triggering effect in instruction following. Our approach does not\nimpose any formatting on the original prompt or introduce extra test-time LLM\ncalls. Experiments show that CachePrune significantly reduces attack success\nrates without compromising the response quality. Note: This paper aims to\ndefend against indirect prompt injection attacks, with the goal of developing\nmore secure and robust AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are identified as being susceptible to indirect\nprompt injection attack, where the model undesirably deviates from\nuser-provided instructions by executing tasks injected in the prompt context.\nThis vulnerability stems from LLMs' inability to distinguish between data and\ninstructions within a prompt. In this paper, we propose CachePrune that defends\nagainst this attack by identifying and pruning task-triggering neurons from the\nKV cache of the input prompt context. By pruning such neurons, we encourage the\nLLM to treat the text spans of input prompt context as only pure data, instead\nof any indicator of instruction following. These neurons are identified via\nfeature attribution with a loss function induced from an upperbound of the\nDirect Preference Optimization (DPO) objective. We show that such a loss\nfunction enables effective feature attribution with only a few samples. We\nfurther improve on the quality of feature attribution, by exploiting an\nobserved triggering effect in instruction following. Our approach does not\nimpose any formatting on the original prompt or introduce extra test-time LLM\ncalls. Experiments show that CachePrune significantly reduces attack success\nrates without compromising the response quality. Note: This paper aims to\ndefend against indirect prompt injection attacks, with the goal of developing\nmore secure and robust AI systems."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Junda Wu"
                    },
                    {
                        "name": "Yu Xia"
                    },
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Lina Yao"
                    },
                    {
                        "name": "Julian McAuley"
                    }
                ],
                "author_detail": {
                    "name": "Julian McAuley"
                },
                "author": "Julian McAuley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12322v2",
                "updated": "2025-04-29T17:54:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    54,
                    42,
                    1,
                    119,
                    0
                ],
                "published": "2025-01-21T17:41:54Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    17,
                    41,
                    54,
                    1,
                    21,
                    0
                ],
                "title": "An Achievable Scheme for the K-user Linear Computation Broadcast Channel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Achievable Scheme for the K-user Linear Computation Broadcast Channel"
                },
                "summary": "This paper presents a new achievable scheme for the K-user Linear Computation\nBroadcast Channel (K-LCBC). A K-LCBC comprises data stored on a server and K\nusers, each aiming to retrieve a desired linear function of the data by\nleveraging their prior locally available side information in the form of\nanother linear function of the data. The proposed scheme is based on a subspace\ndecomposition derived from representable polymatroid spaces. This decomposition\nenables the server to effectively design multicast messages that simultaneously\nbenefit multiple users and allow users to eliminate interference using their\navailable side information. This work extends existing results for the 3-LCBC\nby introducing a linear programming framework to optimize multicast\nopportunities across an arbitrary number of users. The proposed approach can be\nused to derive achievable scheme for the K-user coded caching problem with\nlinear coded placement and scalar linear function retrieval, which was our\noriginal motivation to investigate the K-LCBC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a new achievable scheme for the K-user Linear Computation\nBroadcast Channel (K-LCBC). A K-LCBC comprises data stored on a server and K\nusers, each aiming to retrieve a desired linear function of the data by\nleveraging their prior locally available side information in the form of\nanother linear function of the data. The proposed scheme is based on a subspace\ndecomposition derived from representable polymatroid spaces. This decomposition\nenables the server to effectively design multicast messages that simultaneously\nbenefit multiple users and allow users to eliminate interference using their\navailable side information. This work extends existing results for the 3-LCBC\nby introducing a linear programming framework to optimize multicast\nopportunities across an arbitrary number of users. The proposed approach can be\nused to derive achievable scheme for the K-user coded caching problem with\nlinear coded placement and scalar linear function retrieval, which was our\noriginal motivation to investigate the K-LCBC."
                },
                "authors": [
                    {
                        "name": "Yinbin Ma"
                    },
                    {
                        "name": "Daniela Tuninetti"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Tuninetti"
                },
                "author": "Daniela Tuninetti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v2",
                "updated": "2025-04-29T14:25:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    25,
                    8,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2504.11704",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20246v1",
                "updated": "2025-04-28T20:30:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    20,
                    30,
                    59,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T20:30:59Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    20,
                    30,
                    59,
                    0,
                    118,
                    0
                ],
                "title": "Tree embedding based mapping system for low-latency mobile applications\n  in multi-access networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree embedding based mapping system for low-latency mobile applications\n  in multi-access networks"
                },
                "summary": "Low-latency applications like AR/VR and online gaming need fast, stable\nconnections. New technologies such as V2X, LEO satellites, and 6G bring unique\nchallenges in mobility management. Traditional solutions based on centralized\nor distributed anchors often fall short in supporting rapid mobility due to\ninefficient routing, low versatility, and insufficient multi-access support. In\nthis paper, we design a new end-to-end system for tracking multi-connected\nmobile devices at scale and optimizing performance for latency-sensitive,\nhighly dynamic applications. Our system, based on the locator/ID separation\nprinciple, extends to multi-access networks without requiring specialized\nrouters or caching. Using a novel tree embedding-based overlay, we enable fast\nsession setup while allowing endpoints to directly handle mobility between\nthem. Evaluation with real network data shows our solution cuts connection\nlatency to 7.42% inflation over the shortest path, compared to LISP's 359\\% due\nto cache misses. It also significantly reduces location update overhead and\ndisruption time during mobility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-latency applications like AR/VR and online gaming need fast, stable\nconnections. New technologies such as V2X, LEO satellites, and 6G bring unique\nchallenges in mobility management. Traditional solutions based on centralized\nor distributed anchors often fall short in supporting rapid mobility due to\ninefficient routing, low versatility, and insufficient multi-access support. In\nthis paper, we design a new end-to-end system for tracking multi-connected\nmobile devices at scale and optimizing performance for latency-sensitive,\nhighly dynamic applications. Our system, based on the locator/ID separation\nprinciple, extends to multi-access networks without requiring specialized\nrouters or caching. Using a novel tree embedding-based overlay, we enable fast\nsession setup while allowing endpoints to directly handle mobility between\nthem. Evaluation with real network data shows our solution cuts connection\nlatency to 7.42% inflation over the shortest path, compared to LISP's 359\\% due\nto cache misses. It also significantly reduces location update overhead and\ndisruption time during mobility."
                },
                "authors": [
                    {
                        "name": "Yu Mi"
                    },
                    {
                        "name": "Randeep Bhatia"
                    },
                    {
                        "name": "Fang Hao"
                    },
                    {
                        "name": "An Wang"
                    },
                    {
                        "name": "Steve Benno"
                    },
                    {
                        "name": "Tv Lakshman"
                    }
                ],
                "author_detail": {
                    "name": "Tv Lakshman"
                },
                "author": "Tv Lakshman",
                "arxiv_comment": "Accepted by IEEE INFOCOM 2025-IEEE Conference on Computer\n  Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v3",
                "updated": "2025-04-28T17:17:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    17,
                    53,
                    0,
                    118,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay\n  using Combinatorial t-Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay\n  using Combinatorial t-Designs"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "IEEE Internet of Things Journal (Accepted for publication). The\n  Hierarchical coded caching scheme in this updated version unifies the scheme\n  in the previous version and the schemes in arxiv:2402.07188. This version\n  includes a more comprehensive performance analysis. To reflect these the\n  title has been updated",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19984v1",
                "updated": "2025-04-28T16:59:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    59,
                    13,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T16:59:13Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    59,
                    13,
                    0,
                    118,
                    0
                ],
                "title": "3D MPSoC with On-Chip Cache Support -- Design and Exploitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D MPSoC with On-Chip Cache Support -- Design and Exploitation"
                },
                "summary": "The increasing density of transistors in Integrated Circuits (ICs) has\nenabled the development of highly integrated Systems-on-Chip (SoCs) and, more\nrecently, Multiprocessor Systems-on-Chip (MPSoCs). To address scalability\nchallenges in communication and memory performance, three-dimensional (3D)\nNetwork-on-Chip (NoC) architectures have emerged, offering improvements in\ncommunication latency and throughput. However, memory system efficiency remains\na critical bottleneck in NoC-based designs. This work proposes the design and\nexperimental exploration of 3D MPSoCs with on-chip cache support by employing\ndistinct communication infrastructures for inter-processor and memory\ninteractions. Specifically, packet-based NoCs are adopted for inter-processor\ncommunication, while a crossbar-based infrastructure supports a cache coherence\nhierarchy for memory access. A two-layer system architecture is introduced,\ncombining a Uniform Memory Access (UMA) model within clusters and a No Remote\nMemory Access (NORMA) model between clusters, aiming to balance scalability and\ncoherence requirements. Emerging memory technologies such as PCRAM and MRAM are\nexplored to optimize performance, energy consumption, and area usage.\nExperimental evaluations are conducted using the Gem5 simulator, targeting a\nmodel based on the ARM Versatile Express platform. The outcomes of this study\naim to enhance MPSoC scalability while meeting the stringent demands of\nmemory-centric applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing density of transistors in Integrated Circuits (ICs) has\nenabled the development of highly integrated Systems-on-Chip (SoCs) and, more\nrecently, Multiprocessor Systems-on-Chip (MPSoCs). To address scalability\nchallenges in communication and memory performance, three-dimensional (3D)\nNetwork-on-Chip (NoC) architectures have emerged, offering improvements in\ncommunication latency and throughput. However, memory system efficiency remains\na critical bottleneck in NoC-based designs. This work proposes the design and\nexperimental exploration of 3D MPSoCs with on-chip cache support by employing\ndistinct communication infrastructures for inter-processor and memory\ninteractions. Specifically, packet-based NoCs are adopted for inter-processor\ncommunication, while a crossbar-based infrastructure supports a cache coherence\nhierarchy for memory access. A two-layer system architecture is introduced,\ncombining a Uniform Memory Access (UMA) model within clusters and a No Remote\nMemory Access (NORMA) model between clusters, aiming to balance scalability and\ncoherence requirements. Emerging memory technologies such as PCRAM and MRAM are\nexplored to optimize performance, energy consumption, and area usage.\nExperimental evaluations are conducted using the Gem5 simulator, targeting a\nmodel based on the ARM Versatile Express platform. The outcomes of this study\naim to enhance MPSoC scalability while meeting the stringent demands of\nmemory-centric applications."
                },
                "authors": [
                    {
                        "name": "Rodrigo Cataldo"
                    },
                    {
                        "name": "Cesar Marcon"
                    },
                    {
                        "name": "Debora Matos"
                    }
                ],
                "author_detail": {
                    "name": "Debora Matos"
                },
                "author": "Debora Matos",
                "arxiv_comment": "Progress Seminar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19874v1",
                "updated": "2025-04-28T15:05:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    5,
                    35,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:05:35Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    5,
                    35,
                    0,
                    118,
                    0
                ],
                "title": "TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate"
                },
                "summary": "Vector quantization, a problem rooted in Shannon's source coding theory, aims\nto quantize high-dimensional Euclidean vectors while minimizing distortion in\ntheir geometric structure. We propose TurboQuant to address both mean-squared\nerror (MSE) and inner product distortion, overcoming limitations of existing\nmethods that fail to achieve optimal distortion rates. Our data-oblivious\nalgorithms, suitable for online applications, achieve near-optimal distortion\nrates (within a small constant factor) across all bit-widths and dimensions.\nTurboQuant achieves this by randomly rotating input vectors, inducing a\nconcentrated Beta distribution on coordinates, and leveraging the\nnear-independence property of distinct coordinates in high dimensions to simply\napply optimal scalar quantizers per each coordinate. Recognizing that\nMSE-optimal quantizers introduce bias in inner product estimation, we propose a\ntwo-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL\n(QJL) transform on the residual, resulting in an unbiased inner product\nquantizer. We also provide a formal proof of the information-theoretic lower\nbounds on best achievable distortion rate by any vector quantizer,\ndemonstrating that TurboQuant closely matches these bounds, differing only by a\nsmall constant ($\\approx 2.7$) factor. Experimental results validate our\ntheoretical findings, showing that for KV cache quantization, we achieve\nabsolute quality neutrality with 3.5 bits per channel and marginal quality\ndegradation with 2.5 bits per channel. Furthermore, in nearest neighbor search\ntasks, our method outperforms existing product quantization techniques in\nrecall while reducing indexing time to virtually zero.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector quantization, a problem rooted in Shannon's source coding theory, aims\nto quantize high-dimensional Euclidean vectors while minimizing distortion in\ntheir geometric structure. We propose TurboQuant to address both mean-squared\nerror (MSE) and inner product distortion, overcoming limitations of existing\nmethods that fail to achieve optimal distortion rates. Our data-oblivious\nalgorithms, suitable for online applications, achieve near-optimal distortion\nrates (within a small constant factor) across all bit-widths and dimensions.\nTurboQuant achieves this by randomly rotating input vectors, inducing a\nconcentrated Beta distribution on coordinates, and leveraging the\nnear-independence property of distinct coordinates in high dimensions to simply\napply optimal scalar quantizers per each coordinate. Recognizing that\nMSE-optimal quantizers introduce bias in inner product estimation, we propose a\ntwo-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL\n(QJL) transform on the residual, resulting in an unbiased inner product\nquantizer. We also provide a formal proof of the information-theoretic lower\nbounds on best achievable distortion rate by any vector quantizer,\ndemonstrating that TurboQuant closely matches these bounds, differing only by a\nsmall constant ($\\approx 2.7$) factor. Experimental results validate our\ntheoretical findings, showing that for KV cache quantization, we achieve\nabsolute quality neutrality with 3.5 bits per channel and marginal quality\ndegradation with 2.5 bits per channel. Furthermore, in nearest neighbor search\ntasks, our method outperforms existing product quantization techniques in\nrecall while reducing indexing time to virtually zero."
                },
                "authors": [
                    {
                        "name": "Amir Zandieh"
                    },
                    {
                        "name": "Majid Daliri"
                    },
                    {
                        "name": "Majid Hadian"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    }
                ],
                "author_detail": {
                    "name": "Vahab Mirrokni"
                },
                "author": "Vahab Mirrokni",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19867v1",
                "updated": "2025-04-28T15:00:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    0,
                    3,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:00:03Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    0,
                    3,
                    0,
                    118,
                    0
                ],
                "title": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\n  Computation and Unified Storage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\n  Computation and Unified Storage"
                },
                "summary": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models."
                },
                "authors": [
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Lufang Chen"
                    },
                    {
                        "name": "Zhong Wang"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Qiuli Mao"
                    },
                    {
                        "name": "Jianping Ma"
                    },
                    {
                        "name": "Chao Xiong"
                    },
                    {
                        "name": "Guanyu Wu"
                    },
                    {
                        "name": "Buhe Han"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yun Liang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "18 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19601v1",
                "updated": "2025-04-28T09:03:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    3,
                    45,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T09:03:45Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    3,
                    45,
                    0,
                    118,
                    0
                ],
                "title": "Characterizing the Optimal Memory-Rate Tradeoff in Secure Coded Caching\n  for Small Buffer or Small Rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing the Optimal Memory-Rate Tradeoff in Secure Coded Caching\n  for Small Buffer or Small Rate"
                },
                "summary": "We consider the secure coded caching problem proposed by Ravindrakumar et. al\nwhere no user can obtain information about files other than the one requested.\nWe first propose three new schemes for the three cases of cache size $M=1$,\n$N=2$ files and arbitrary $K$ users, delivery rate $ R=1$, arbitrary $N$ files\nand $K$ users, and the general case for arbitrary $N$ files and $K$ users,\nrespectively. Then we derive converse results by characterizing new properties\nof secure coded caching schemes. As a result, we characterize the two\nend-points of the optimal memory-rate tradeoff curve for arbitrary number of\nusers and files. Furthermore, for the case of $N=2$ files and arbitrary number\nof users, we also characterize a segment of the optimal memory-rate tradeoff\ncurve, where the cache size is relatively small.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the secure coded caching problem proposed by Ravindrakumar et. al\nwhere no user can obtain information about files other than the one requested.\nWe first propose three new schemes for the three cases of cache size $M=1$,\n$N=2$ files and arbitrary $K$ users, delivery rate $ R=1$, arbitrary $N$ files\nand $K$ users, and the general case for arbitrary $N$ files and $K$ users,\nrespectively. Then we derive converse results by characterizing new properties\nof secure coded caching schemes. As a result, we characterize the two\nend-points of the optimal memory-rate tradeoff curve for arbitrary number of\nusers and files. Furthermore, for the case of $N=2$ files and arbitrary number\nof users, we also characterize a segment of the optimal memory-rate tradeoff\ncurve, where the cache size is relatively small."
                },
                "authors": [
                    {
                        "name": "Han Fang"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Wei Kang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Kang"
                },
                "author": "Wei Kang",
                "arxiv_comment": "Submitted to IEEE Transactions on Information Theory",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19561v1",
                "updated": "2025-04-28T08:12:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    12,
                    30,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T08:12:30Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    12,
                    30,
                    0,
                    118,
                    0
                ],
                "title": "Quantifying Memory Utilization with Effective State-Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Memory Utilization with Effective State-Size"
                },
                "summary": "The need to develop a general framework for architecture analysis is becoming\nincreasingly important, given the expanding design space of sequence models. To\nthis end, we draw insights from classical signal processing and control theory,\nto develop a quantitative measure of \\textit{memory utilization}: the internal\nmechanisms through which a model stores past information to produce future\noutputs. This metric, which we call \\textbf{\\textit{effective state-size}}\n(ESS), is tailored to the fundamental class of systems with\n\\textit{input-invariant} and \\textit{input-varying linear operators},\nencompassing a variety of computational units such as variants of attention,\nconvolutions, and recurrences. Unlike prior work on memory utilization, which\neither relies on raw operator visualizations (e.g. attention maps), or simply\nthe total \\textit{memory capacity} (i.e. cache size) of a model, our metrics\nprovide highly interpretable and actionable measurements. In particular, we\nshow how ESS can be leveraged to improve initialization strategies, inform\nnovel regularizers and advance the performance-efficiency frontier through\nmodel distillation. Furthermore, we demonstrate that the effect of context\ndelimiters (such as end-of-speech tokens) on ESS highlights cross-architectural\ndifferences in how large language models utilize their available memory to\nrecall information. Overall, we find that ESS provides valuable insights into\nthe dynamics that dictate memory utilization, enabling the design of more\nefficient and effective sequence models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need to develop a general framework for architecture analysis is becoming\nincreasingly important, given the expanding design space of sequence models. To\nthis end, we draw insights from classical signal processing and control theory,\nto develop a quantitative measure of \\textit{memory utilization}: the internal\nmechanisms through which a model stores past information to produce future\noutputs. This metric, which we call \\textbf{\\textit{effective state-size}}\n(ESS), is tailored to the fundamental class of systems with\n\\textit{input-invariant} and \\textit{input-varying linear operators},\nencompassing a variety of computational units such as variants of attention,\nconvolutions, and recurrences. Unlike prior work on memory utilization, which\neither relies on raw operator visualizations (e.g. attention maps), or simply\nthe total \\textit{memory capacity} (i.e. cache size) of a model, our metrics\nprovide highly interpretable and actionable measurements. In particular, we\nshow how ESS can be leveraged to improve initialization strategies, inform\nnovel regularizers and advance the performance-efficiency frontier through\nmodel distillation. Furthermore, we demonstrate that the effect of context\ndelimiters (such as end-of-speech tokens) on ESS highlights cross-architectural\ndifferences in how large language models utilize their available memory to\nrecall information. Overall, we find that ESS provides valuable insights into\nthe dynamics that dictate memory utilization, enabling the design of more\nefficient and effective sequence models."
                },
                "authors": [
                    {
                        "name": "Rom N. Parnichkun"
                    },
                    {
                        "name": "Neehal Tumma"
                    },
                    {
                        "name": "Armin W. Thomas"
                    },
                    {
                        "name": "Alessandro Moro"
                    },
                    {
                        "name": "Qi An"
                    },
                    {
                        "name": "Taiji Suzuki"
                    },
                    {
                        "name": "Atsushi Yamashita"
                    },
                    {
                        "name": "Michael Poli"
                    },
                    {
                        "name": "Stefano Massaroli"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Massaroli"
                },
                "author": "Stefano Massaroli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19475v1",
                "updated": "2025-04-28T04:31:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T04:31:24Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "title": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video"
                },
                "summary": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field."
                },
                "authors": [
                    {
                        "name": "Sonia Joseph"
                    },
                    {
                        "name": "Praneet Suresh"
                    },
                    {
                        "name": "Lorenz Hufe"
                    },
                    {
                        "name": "Edward Stevinson"
                    },
                    {
                        "name": "Robert Graham"
                    },
                    {
                        "name": "Yash Vadi"
                    },
                    {
                        "name": "Danilo Bzdok"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    },
                    {
                        "name": "Lee Sharkey"
                    },
                    {
                        "name": "Blake Aaron Richards"
                    }
                ],
                "author_detail": {
                    "name": "Blake Aaron Richards"
                },
                "author": "Blake Aaron Richards",
                "arxiv_comment": "4 pages, 3 figures, 9 tables. Oral and Tutorial at the CVPR\n  Mechanistic Interpretability for Vision (MIV) Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18001v2",
                "updated": "2025-04-28T04:02:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    2,
                    30,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-25T01:10:49Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    1,
                    10,
                    49,
                    4,
                    115,
                    0
                ],
                "title": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data"
                },
                "summary": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc."
                },
                "authors": [
                    {
                        "name": "Daniel Zavorotny"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "arxiv_comment": "11 pages, 11 figures, EGPGV25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v3",
                "updated": "2025-04-28T02:58:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    2,
                    58,
                    27,
                    0,
                    118,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19365v1",
                "updated": "2025-04-27T22:05:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T22:05:14Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "title": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration"
                },
                "summary": "Graphics Processing Units (GPUs) have become essential for computationally\nintensive applications. However, emerging workloads such as recommender\nsystems, graph analytics, and data analytics often involve processing data\nexceeding GPU on-chip memory capacity. To mitigate this issue, existing\nsolutions enable GPUs to use CPU DRAM or SSDs as external memory. Among them,\nthe GPU-centric approach lets GPU threads directly initiate NVMe requests,\neliminating CPU intervention overhead over traditional methods. However, the\nSOTA GPU-centric approach adopts a synchronous IO model, and threads must\ntolerate the long latency in communication before starting any tasks.\n  In this work, we propose AGILE, a lightweight and efficient asynchronous\nlibrary allowing GPU threads to access SSDs asynchronously while eliminating\ndeadlock risks. AGILE also integrates a flexible software cache using GPU\nHigh-Bandwidth Mamory (HBM). We demonstrate that the asynchronous GPU-centric\nIO achieves up to 1.88$\\times$ improvement in workloads with different\ncomputation-to-communication (CTC) ratios. We also compare AGILE with the SOTA\nwork BaM on Deep Learning Recommendation Models (DLRM) with various settings,\nand the results show that AGILE achieves 1.75$\\times$ performance improvement\ndue to its efficient design and the overlapping strategy enabled by an\nasynchronous IO model. We further evaluate AGILE's API overhead on graph\napplications, and the results demonstrate AGILE reduces software cache overhead\nby up to 3.12$\\times$ and overhead in NVMe IO requests by up to 2.85$\\times$.\nCompared with BaM, AGILE consumes fewer registers and exhibits up to\n1.32$\\times$ reduction in the usage of registers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphics Processing Units (GPUs) have become essential for computationally\nintensive applications. However, emerging workloads such as recommender\nsystems, graph analytics, and data analytics often involve processing data\nexceeding GPU on-chip memory capacity. To mitigate this issue, existing\nsolutions enable GPUs to use CPU DRAM or SSDs as external memory. Among them,\nthe GPU-centric approach lets GPU threads directly initiate NVMe requests,\neliminating CPU intervention overhead over traditional methods. However, the\nSOTA GPU-centric approach adopts a synchronous IO model, and threads must\ntolerate the long latency in communication before starting any tasks.\n  In this work, we propose AGILE, a lightweight and efficient asynchronous\nlibrary allowing GPU threads to access SSDs asynchronously while eliminating\ndeadlock risks. AGILE also integrates a flexible software cache using GPU\nHigh-Bandwidth Mamory (HBM). We demonstrate that the asynchronous GPU-centric\nIO achieves up to 1.88$\\times$ improvement in workloads with different\ncomputation-to-communication (CTC) ratios. We also compare AGILE with the SOTA\nwork BaM on Deep Learning Recommendation Models (DLRM) with various settings,\nand the results show that AGILE achieves 1.75$\\times$ performance improvement\ndue to its efficient design and the overlapping strategy enabled by an\nasynchronous IO model. We further evaluate AGILE's API overhead on graph\napplications, and the results demonstrate AGILE reduces software cache overhead\nby up to 3.12$\\times$ and overhead in NVMe IO requests by up to 2.85$\\times$.\nCompared with BaM, AGILE consumes fewer registers and exhibits up to\n1.32$\\times$ reduction in the usage of registers."
                },
                "authors": [
                    {
                        "name": "Zhuoping Yang"
                    },
                    {
                        "name": "Jinming Zhuang"
                    },
                    {
                        "name": "Xingzhen Chen"
                    },
                    {
                        "name": "Alex K. Jones"
                    },
                    {
                        "name": "Peipei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Peipei Zhou"
                },
                "author": "Peipei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19266v1",
                "updated": "2025-04-27T14:46:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    46,
                    43,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T14:46:43Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    46,
                    43,
                    6,
                    117,
                    0
                ],
                "title": "OpenFusion++: An Open-vocabulary Real-time Scene Understanding System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenFusion++: An Open-vocabulary Real-time Scene Understanding System"
                },
                "summary": "Real-time open-vocabulary scene understanding is essential for efficient 3D\nperception in applications such as vision-language navigation, embodied\nintelligence, and augmented reality. However, existing methods suffer from\nimprecise instance segmentation, static semantic updates, and limited handling\nof complex queries. To address these issues, we present OpenFusion++, a\nTSDF-based real-time 3D semantic-geometric reconstruction system. Our approach\nrefines 3D point clouds by fusing confidence maps from foundational models,\ndynamically updates global semantic labels via an adaptive cache based on\ninstance area, and employs a dual-path encoding framework that integrates\nobject attributes with environmental context for precise query responses.\nExperiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate\nthat OpenFusion++ significantly outperforms the baseline in both semantic\naccuracy and query responsiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time open-vocabulary scene understanding is essential for efficient 3D\nperception in applications such as vision-language navigation, embodied\nintelligence, and augmented reality. However, existing methods suffer from\nimprecise instance segmentation, static semantic updates, and limited handling\nof complex queries. To address these issues, we present OpenFusion++, a\nTSDF-based real-time 3D semantic-geometric reconstruction system. Our approach\nrefines 3D point clouds by fusing confidence maps from foundational models,\ndynamically updates global semantic labels via an adaptive cache based on\ninstance area, and employs a dual-path encoding framework that integrates\nobject attributes with environmental context for precise query responses.\nExperiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate\nthat OpenFusion++ significantly outperforms the baseline in both semantic\naccuracy and query responsiveness."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Jin"
                    },
                    {
                        "name": "Matteo Frosi"
                    },
                    {
                        "name": "Matteo Matteucci"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Matteucci"
                },
                "author": "Matteo Matteucci",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45, 68U05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.4.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19191v1",
                "updated": "2025-04-27T10:48:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    10,
                    48,
                    56,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T10:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    10,
                    48,
                    56,
                    6,
                    117,
                    0
                ],
                "title": "WuNeng: Hybrid State with Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WuNeng: Hybrid State with Attention"
                },
                "summary": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures."
                },
                "authors": [
                    {
                        "name": "Liu Xiao"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Lin Yueyu"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yueyu"
                },
                "author": "Lin Yueyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10883v2",
                "updated": "2025-04-26T12:07:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    26,
                    12,
                    7,
                    35,
                    5,
                    116,
                    0
                ],
                "published": "2024-11-16T20:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "title": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs"
                },
                "summary": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack."
                },
                "authors": [
                    {
                        "name": "Cheng Gu"
                    },
                    {
                        "name": "Yicheng Zhang"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "arxiv_comment": "Accepted to IEEE S&P 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v3",
                "updated": "2025-04-25T19:40:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    19,
                    40,
                    54,
                    4,
                    115,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18434v1",
                "updated": "2025-04-25T15:45:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    45,
                    36,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:45:36Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    45,
                    36,
                    4,
                    115,
                    0
                ],
                "title": "Constructing Hamiltonian Decompositions of Complete $k$-Uniform\n  Hypergraphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing Hamiltonian Decompositions of Complete $k$-Uniform\n  Hypergraphs"
                },
                "summary": "Motivated by the wide-ranging applications of Hamiltonian decompositions in\ndistributed computing, coded caching, routing, resource allocation, load\nbalancing, and fault tolerance, our work presents a comprehensive design for\nHamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$.\nBuilding upon the resolution of the long-standing conjecture of the existence\nof Hamiltonian decompositions of complete hypergraphs, a problem that was\nresolved using existence-based methods, our contribution goes beyond the\nprevious explicit designs, which were confined to the specific cases of $k=2$\nand $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing\nfor a broad applicability of Hamiltonian decompositions in various settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the wide-ranging applications of Hamiltonian decompositions in\ndistributed computing, coded caching, routing, resource allocation, load\nbalancing, and fault tolerance, our work presents a comprehensive design for\nHamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$.\nBuilding upon the resolution of the long-standing conjecture of the existence\nof Hamiltonian decompositions of complete hypergraphs, a problem that was\nresolved using existence-based methods, our contribution goes beyond the\nprevious explicit designs, which were confined to the specific cases of $k=2$\nand $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing\nfor a broad applicability of Hamiltonian decompositions in various settings."
                },
                "authors": [
                    {
                        "name": "Javad Maheri"
                    },
                    {
                        "name": "Petros Elia"
                    }
                ],
                "author_detail": {
                    "name": "Petros Elia"
                },
                "author": "Petros Elia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18432v1",
                "updated": "2025-04-25T15:44:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    44,
                    38,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:44:38Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    44,
                    38,
                    4,
                    115,
                    0
                ],
                "title": "FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack"
                },
                "summary": "As the gap between network and CPU speeds rapidly increases, the CPU-centric\nnetwork stack proves inadequate due to excessive CPU and memory overhead. While\nhardware-offloaded network stacks alleviate these issues, they suffer from\nlimited flexibility in both control and data planes. Offloading network stack\nto off-path SmartNIC seems promising to provide high flexibility; however,\nthroughput remains constrained by inherent SmartNIC architectural limitations.\n  To this end, we design FlexiNS, a SmartNIC-centric network stack with\nsoftware transport programmability and line-rate packet processing\ncapabilities. To grapple with the limitation of SmartNIC-induced challenges,\nFlexiNS introduces: (a) a header-only offloading TX path; (b) an\nunlimited-working-set in-cache processing RX path; (c) a high-performance\nDMA-only notification pipe; and (d) a programmable offloading engine. We\nprototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box\nRDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\\times$ higher\nthroughput than the microkernel-based baseline in block storage disaggregation\nand 1.3$\\times$ higher throughput than the hardware-offloaded baseline in\nKVCache transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the gap between network and CPU speeds rapidly increases, the CPU-centric\nnetwork stack proves inadequate due to excessive CPU and memory overhead. While\nhardware-offloaded network stacks alleviate these issues, they suffer from\nlimited flexibility in both control and data planes. Offloading network stack\nto off-path SmartNIC seems promising to provide high flexibility; however,\nthroughput remains constrained by inherent SmartNIC architectural limitations.\n  To this end, we design FlexiNS, a SmartNIC-centric network stack with\nsoftware transport programmability and line-rate packet processing\ncapabilities. To grapple with the limitation of SmartNIC-induced challenges,\nFlexiNS introduces: (a) a header-only offloading TX path; (b) an\nunlimited-working-set in-cache processing RX path; (c) a high-performance\nDMA-only notification pipe; and (d) a programmable offloading engine. We\nprototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box\nRDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\\times$ higher\nthroughput than the microkernel-based baseline in block storage disaggregation\nand 1.3$\\times$ higher throughput than the hardware-offloaded baseline in\nKVCache transfer."
                },
                "authors": [
                    {
                        "name": "Xuzheng Chen"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Baolin Zhu"
                    },
                    {
                        "name": "Xueying Zhu"
                    },
                    {
                        "name": "Zhongqing Chen"
                    },
                    {
                        "name": "Shu Ma"
                    },
                    {
                        "name": "Lingjun Zhu"
                    },
                    {
                        "name": "Chao Shi"
                    },
                    {
                        "name": "Yin Zhang"
                    },
                    {
                        "name": "Zeke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zeke Wang"
                },
                "author": "Zeke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18242v1",
                "updated": "2025-04-25T10:43:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    43,
                    23,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T10:43:23Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    43,
                    23,
                    4,
                    115,
                    0
                ],
                "title": "Demand Private Coded Caching: Small Cache Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demand Private Coded Caching: Small Cache Size"
                },
                "summary": "We investigate the demand private coded caching problem, which is an $(N,K)$\ncoded caching problem with $N$ files, $K$ users, each equipped with a cache of\nsize $M$, and an additional privacy constraint on user demands, i.e., each user\ncan not gain any information about the demands of other users. We focus on\nscenarios where the size of users' caches is small, aiming to further\ncharacterize the fundamental limits of this problem. We first present a new\nvirtual-user-based achievable scheme for arbitrary number of users and files,\nand two MDS-code-based achievable schemes for the case $N \\le K$. With a newly\nderived converse bound for the case $N \\le K$, these proposed schemes lead to\nthe optimal memory-rate tradeoff of the demand private coded caching problem\nfor $M \\in \\big[0, \\frac{N}{(K+1)(N-1)} \\big] $ where $N \\le K \\le 2N-2$, and\nthe optimal memory-rate tradeoff for $M \\in \\big[0, \\frac{1}{K+1} \\big] $ where\n$ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users,\nby deriving another new converse bound, the optimal memory-rate tradeoff is\ncharacterized for $M\\in \\big[0,\\frac{2}{K}\\big] \\cup\n\\big[\\frac{2(K-1)}{K+1},2\\big]$. Finally, we provide the optimal memory-rate\ntradeoff of the demand private coded caching problem for 2 files and 3 users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the demand private coded caching problem, which is an $(N,K)$\ncoded caching problem with $N$ files, $K$ users, each equipped with a cache of\nsize $M$, and an additional privacy constraint on user demands, i.e., each user\ncan not gain any information about the demands of other users. We focus on\nscenarios where the size of users' caches is small, aiming to further\ncharacterize the fundamental limits of this problem. We first present a new\nvirtual-user-based achievable scheme for arbitrary number of users and files,\nand two MDS-code-based achievable schemes for the case $N \\le K$. With a newly\nderived converse bound for the case $N \\le K$, these proposed schemes lead to\nthe optimal memory-rate tradeoff of the demand private coded caching problem\nfor $M \\in \\big[0, \\frac{N}{(K+1)(N-1)} \\big] $ where $N \\le K \\le 2N-2$, and\nthe optimal memory-rate tradeoff for $M \\in \\big[0, \\frac{1}{K+1} \\big] $ where\n$ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users,\nby deriving another new converse bound, the optimal memory-rate tradeoff is\ncharacterized for $M\\in \\big[0,\\frac{2}{K}\\big] \\cup\n\\big[\\frac{2(K-1)}{K+1},2\\big]$. Finally, we provide the optimal memory-rate\ntradeoff of the demand private coded caching problem for 2 files and 3 users."
                },
                "authors": [
                    {
                        "name": "Qinyi Lu"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Wei Kang"
                    },
                    {
                        "name": "Chunguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Chunguo Li"
                },
                "author": "Chunguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18082v1",
                "updated": "2025-04-25T05:16:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    16,
                    53,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T05:16:53Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    16,
                    53,
                    4,
                    115,
                    0
                ],
                "title": "Efficient GNN Training Through Structure-Aware Randomized Mini-Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient GNN Training Through Structure-Aware Randomized Mini-Batching"
                },
                "summary": "Graph Neural Networks (GNNs) enable learning on realworld graphs and\nmini-batch training has emerged as the de facto standard for training GNNs\nbecause it can scale to very large graphs and improve convergence. Current\nmini-batch construction policies largely ignore efficiency considerations of\nGNN training. Specifically, existing mini-batching techniques employ\nrandomization schemes to improve accuracy and convergence. However, these\nrandomization schemes are often agnostic to the structural properties of the\ngraph (for eg. community structure), resulting in highly irregular memory\naccess patterns during GNN training that make suboptimal use of on-chip GPU\ncaches. On the other hand, while deterministic mini-batching based solely on\ngraph structure delivers fast runtime performance, the lack of randomness\ncompromises both the final model accuracy and training convergence speed. In\nthis paper, we present Community-structure-aware Randomized Mini-batching\n(COMM-RAND), a novel methodology that bridges the gap between the above\nextremes. COMM-RAND allows practitioners to explore the space between pure\nrandomness and pure graph structural awareness during mini-batch construction,\nleading to significantly more efficient GNN training with similar accuracy. We\nevaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND\ncuts down GNN training time by up to 2.76x (1.8x on average) while achieving an\naccuracy that is within 1.79% points (0.42% on average) compared to popular\nrandom mini-batching approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) enable learning on realworld graphs and\nmini-batch training has emerged as the de facto standard for training GNNs\nbecause it can scale to very large graphs and improve convergence. Current\nmini-batch construction policies largely ignore efficiency considerations of\nGNN training. Specifically, existing mini-batching techniques employ\nrandomization schemes to improve accuracy and convergence. However, these\nrandomization schemes are often agnostic to the structural properties of the\ngraph (for eg. community structure), resulting in highly irregular memory\naccess patterns during GNN training that make suboptimal use of on-chip GPU\ncaches. On the other hand, while deterministic mini-batching based solely on\ngraph structure delivers fast runtime performance, the lack of randomness\ncompromises both the final model accuracy and training convergence speed. In\nthis paper, we present Community-structure-aware Randomized Mini-batching\n(COMM-RAND), a novel methodology that bridges the gap between the above\nextremes. COMM-RAND allows practitioners to explore the space between pure\nrandomness and pure graph structural awareness during mini-batch construction,\nleading to significantly more efficient GNN training with similar accuracy. We\nevaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND\ncuts down GNN training time by up to 2.76x (1.8x on average) while achieving an\naccuracy that is within 1.79% points (0.42% on average) compared to popular\nrandom mini-batching approaches."
                },
                "authors": [
                    {
                        "name": "Vignesh Balaji"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    },
                    {
                        "name": "Gal Chechik"
                    },
                    {
                        "name": "Haggai Maron"
                    }
                ],
                "author_detail": {
                    "name": "Haggai Maron"
                },
                "author": "Haggai Maron",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v2",
                "updated": "2025-04-25T05:08:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    8,
                    45,
                    4,
                    115,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). That is, C3 on average achieves only 21% of ideal\nspeedup. This is so, due to known challenges of compute and memory interference\nbetween concurrent GPU kernels (that is, sharing of GPU's compute units, caches\nand HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build concurrent communication collectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). That is, C3 on average achieves only 21% of ideal\nspeedup. This is so, due to known challenges of compute and memory interference\nbetween concurrent GPU kernels (that is, sharing of GPU's compute units, caches\nand HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build concurrent communication collectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16620v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16620v2",
                "updated": "2025-04-25T05:05:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    5,
                    49,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-23T11:18:34Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    18,
                    34,
                    2,
                    113,
                    0
                ],
                "title": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV$_3$Sb$_5$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV$_3$Sb$_5$"
                },
                "summary": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, F$_c$, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of T$_c$ = 78 K. Notably, the pump-induced band shifts\nat F$_c$ were comparable to those caused by thermal effects at T$_c$. These\nfindings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane\nCDW emerges above 150 K, with out-of-plane electronic correlations leading to\nthe $2\\times2 \\times 2$ CDW near T$_c$, offering key insights into the\ninterplay between the electronic and structural dynamics in AV$_3$Sb$_5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, F$_c$, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of T$_c$ = 78 K. Notably, the pump-induced band shifts\nat F$_c$ were comparable to those caused by thermal effects at T$_c$. These\nfindings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane\nCDW emerges above 150 K, with out-of-plane electronic correlations leading to\nthe $2\\times2 \\times 2$ CDW near T$_c$, offering key insights into the\ninterplay between the electronic and structural dynamics in AV$_3$Sb$_5$."
                },
                "authors": [
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Shaofeng Duan"
                    },
                    {
                        "name": "Xiangqi Liu"
                    },
                    {
                        "name": "Zhihua Liu"
                    },
                    {
                        "name": "Shichong Wang"
                    },
                    {
                        "name": "Lingxiao Gu"
                    },
                    {
                        "name": "Jiongyu Huang"
                    },
                    {
                        "name": "Wenxuan Yang"
                    },
                    {
                        "name": "Jianzhe Liu"
                    },
                    {
                        "name": "Dong Qian"
                    },
                    {
                        "name": "Yanfeng Guo"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_doi": "10.1016/j.scib.2025.02.018",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.scib.2025.02.018",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.16620v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16620v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 4 figures",
                "arxiv_journal_ref": "Science Bulletin 70, 1211-1214 (2025)",
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17995v1",
                "updated": "2025-04-25T00:41:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T00:41:43Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "title": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study"
                },
                "summary": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal electronic correlations. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal electronic correlations. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials."
                },
                "authors": [
                    {
                        "name": "Indukuru Ramesh Reddy"
                    },
                    {
                        "name": "Sayandeep Ghosh"
                    },
                    {
                        "name": "Bongjae Kim"
                    },
                    {
                        "name": "Chang-Jong Kang"
                    }
                ],
                "author_detail": {
                    "name": "Chang-Jong Kang"
                },
                "author": "Chang-Jong Kang",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17866v1",
                "updated": "2025-04-24T18:09:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    18,
                    9,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T18:09:25Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    18,
                    9,
                    25,
                    3,
                    114,
                    0
                ],
                "title": "Updated parameters of the LArQL model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Updated parameters of the LArQL model"
                },
                "summary": "The need for a microscopic description of scintillation light generation in\nliquid argon becomes increasingly desirable with the upcoming operation of\nlarge scale LArTPCs in the next decade. While a detailed mathematical account\nof the process is still to be achieved, a phenomenological model for\nsimultaneously treating ionization and scintillation, LArQL, has been\nsuccessfully employed to describe the range of electric fields from 0 to 0.75\nkV/cm and dE/dx from 2 to 40 MeV/cm providing the anti-correlation between the\nfree ionization charge and scintillation light. A reanalysis of the original\nmodel parameter values has been performed within a global fit procedure and is\npresented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need for a microscopic description of scintillation light generation in\nliquid argon becomes increasingly desirable with the upcoming operation of\nlarge scale LArTPCs in the next decade. While a detailed mathematical account\nof the process is still to be achieved, a phenomenological model for\nsimultaneously treating ionization and scintillation, LArQL, has been\nsuccessfully employed to describe the range of electric fields from 0 to 0.75\nkV/cm and dE/dx from 2 to 40 MeV/cm providing the anti-correlation between the\nfree ionization charge and scintillation light. A reanalysis of the original\nmodel parameter values has been performed within a global fit procedure and is\npresented."
                },
                "authors": [
                    {
                        "name": "L. Paulucci"
                    },
                    {
                        "name": "F. Cavanna"
                    },
                    {
                        "name": "V. Vale"
                    },
                    {
                        "name": "F. Marinho"
                    }
                ],
                "author_detail": {
                    "name": "F. Marinho"
                },
                "author": "F. Marinho",
                "arxiv_comment": "Part of the proceedings of LIDINE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17584v1",
                "updated": "2025-04-24T14:14:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T14:14:07Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "title": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes."
                },
                "authors": [
                    {
                        "name": "Qingyuan Liu"
                    },
                    {
                        "name": "Liyan Chen"
                    },
                    {
                        "name": "Yanning Yang"
                    },
                    {
                        "name": "Haocheng Wang"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Zhigang Mao"
                    },
                    {
                        "name": "Naifeng Jing"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17554v1",
                "updated": "2025-04-24T13:47:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    47,
                    35,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:47:35Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    47,
                    35,
                    3,
                    114,
                    0
                ],
                "title": "Rethinking PM Crash Consistency in the CXL Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking PM Crash Consistency in the CXL Era"
                },
                "summary": "Persistent Memory (PM) introduces new opportunities for designing\ncrash-consistent applications without the traditional storage overheads.\nHowever, ensuring crash consistency in PM demands intricate knowledge of CPU,\ncache, and memory interactions. Hardware and software mechanisms have been\nproposed to ease this burden, but neither proved sufficient, prompting a\nvariety of bug detection tools.\n  With the sunset of Intel Optane comes the rise of Compute Express Link (CXL)\nfor PM. In this position paper, we discuss the impact of CXL's disaggregated\nand heterogeneous nature in the development of crash-consistent PM\napplications, and outline three research directions: hardware primitives,\npersistency frameworks, and bug detection tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Memory (PM) introduces new opportunities for designing\ncrash-consistent applications without the traditional storage overheads.\nHowever, ensuring crash consistency in PM demands intricate knowledge of CPU,\ncache, and memory interactions. Hardware and software mechanisms have been\nproposed to ease this burden, but neither proved sufficient, prompting a\nvariety of bug detection tools.\n  With the sunset of Intel Optane comes the rise of Compute Express Link (CXL)\nfor PM. In this position paper, we discuss the impact of CXL's disaggregated\nand heterogeneous nature in the development of crash-consistent PM\napplications, and outline three research directions: hardware primitives,\npersistency frameworks, and bug detection tools."
                },
                "authors": [
                    {
                        "name": "João Oliveira"
                    },
                    {
                        "name": "João Gonçalves"
                    },
                    {
                        "name": "Miguel Matos"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Matos"
                },
                "author": "Miguel Matos",
                "arxiv_comment": "5 pages (2 extra pages for references), 1 figure, 2 algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v3",
                "updated": "2025-04-24T08:39:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    39,
                    13,
                    3,
                    114,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA)\nto offload data transfer, descriptor rings for buffering and queuing, and\ninterrupts for asynchrony between cores and device.\n  In this paper we question this wisdom in the light of two trends: modern and\nemerging cache-coherent interconnects like CXL3.0, and workloads, particularly\nmicroservices and serverless computing. Like some others before us, we argue\nthat the assumptions of the DMA-based model are obsolete, and in many use-cases\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, delivers a more efficient system.\n  However, we push this idea much further. We show, in a real hardware\nimplementation, the gains in latency for fine-grained communication achievable\nusing an open cache-coherence protocol which exposes cache transitions to a\nsmart device, and that throughput is competitive with DMA over modern\ninterconnects. We also demonstrate three use-cases: fine-grained RPC-style\ninvocation of functions on an accelerator, offloading of operators in a\nstreaming dataflow engine, and a network interface targeting serverless\nfunctions, comparing our use of coherence with both traditional DMA-style\ninteraction and a highly-optimized implementation using memory-mapped\nprogrammed I/O over PCIe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA)\nto offload data transfer, descriptor rings for buffering and queuing, and\ninterrupts for asynchrony between cores and device.\n  In this paper we question this wisdom in the light of two trends: modern and\nemerging cache-coherent interconnects like CXL3.0, and workloads, particularly\nmicroservices and serverless computing. Like some others before us, we argue\nthat the assumptions of the DMA-based model are obsolete, and in many use-cases\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, delivers a more efficient system.\n  However, we push this idea much further. We show, in a real hardware\nimplementation, the gains in latency for fine-grained communication achievable\nusing an open cache-coherence protocol which exposes cache transitions to a\nsmart device, and that throughput is competitive with DMA over modern\ninterconnects. We also demonstrate three use-cases: fine-grained RPC-style\ninvocation of functions on an accelerator, offloading of operators in a\nstreaming dataflow engine, and a network interface targeting serverless\nfunctions, comparing our use of coherence with both traditional DMA-style\ninteraction and a highly-optimized implementation using memory-mapped\nprogrammed I/O over PCIe."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15192v2",
                "updated": "2025-04-24T04:36:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    36,
                    20,
                    3,
                    114,
                    0
                ],
                "published": "2025-02-21T04:07:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "SPAARC: Spatial Proximity and Association based prefetching for\n  Augmented Reality in edge Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPAARC: Spatial Proximity and Association based prefetching for\n  Augmented Reality in edge Cache"
                },
                "summary": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects."
                },
                "authors": [
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Abhishek Chandra"
                    },
                    {
                        "name": "Jon Weissman"
                    }
                ],
                "author_detail": {
                    "name": "Jon Weissman"
                },
                "author": "Jon Weissman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14992v2",
                "updated": "2025-04-24T04:13:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    13,
                    49,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-21T09:41:26Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    41,
                    26,
                    0,
                    111,
                    0
                ],
                "title": "Efficient Pretraining Length Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Pretraining Length Scaling"
                },
                "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Xun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xun Zhou"
                },
                "author": "Xun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02441v2",
                "updated": "2025-04-24T01:47:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    1,
                    47,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-03T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Cognitive Memory in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Memory in Large Language Models"
                },
                "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."
                },
                "authors": [
                    {
                        "name": "Lianlei Shan"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Zezhou Zhu"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "arxiv_comment": "37 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15364v2",
                "updated": "2025-04-23T18:02:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    18,
                    2,
                    55,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T18:12:46Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    18,
                    12,
                    46,
                    0,
                    111,
                    0
                ],
                "title": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments"
                },
                "summary": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B."
                },
                "authors": [
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matt J Morse"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "8 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15437v2",
                "updated": "2025-04-23T15:02:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    2,
                    16,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T21:01:57Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    21,
                    1,
                    57,
                    0,
                    111,
                    0
                ],
                "title": "Iris: A Next Generation Digital Pathology Rendering Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iris: A Next Generation Digital Pathology Rendering Engine"
                },
                "summary": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology. Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 us per\nslide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology. Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 us per\nslide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms."
                },
                "authors": [
                    {
                        "name": "Ryan Erik Landvater"
                    },
                    {
                        "name": "Ulysses Balis"
                    }
                ],
                "author_detail": {
                    "name": "Ulysses Balis"
                },
                "author": "Ulysses Balis",
                "arxiv_doi": "10.1016/j.jpi.2024.100414",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jpi.2024.100414",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.15437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 8 figures",
                "arxiv_journal_ref": "Journal of Pathology Informatics, 16, 100414 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10138v2",
                "updated": "2025-04-23T10:48:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    48,
                    52,
                    2,
                    113,
                    0
                ],
                "published": "2025-01-17T12:01:28Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "title": "The NIC should be part of the OS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NIC should be part of the OS"
                },
                "summary": "The network interface adapter (NIC) is a critical component of a cloud server\noccupying a unique position. Not only is network performance vital to efficient\noperation of the machine, but unlike compute accelerators like GPUs, the\nnetwork subsystem must react to unpredictable events like the arrival of a\nnetwork packet and communicate with the appropriate application end point with\nminimal latency.\n  Current approaches to server stacks navigate a trade-off between flexibility,\nefficiency, and performance: the fastest kernel-bypass approaches dedicate\ncores to applications, busy-wait on receive queues, etc. while more flexible\napproaches appropriate to more dynamic workload mixes incur much greater\nsoftware overhead on the data path.\n  However, we reject this trade-off, which we ascribe to an arbitrary (and\nsub-optimal) split in system state between the OS and the NIC. Instead, by\nexploiting the properties of cache-coherent interconnects and integrating the\nNIC closely with the OS kernel, we can achieve something surprising:\nperformance for RPC workloads better than the fastest kernelbypass approaches\nwithout sacrificing the robustness and dynamic adaptation of kernel-based\nnetwork subsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The network interface adapter (NIC) is a critical component of a cloud server\noccupying a unique position. Not only is network performance vital to efficient\noperation of the machine, but unlike compute accelerators like GPUs, the\nnetwork subsystem must react to unpredictable events like the arrival of a\nnetwork packet and communicate with the appropriate application end point with\nminimal latency.\n  Current approaches to server stacks navigate a trade-off between flexibility,\nefficiency, and performance: the fastest kernel-bypass approaches dedicate\ncores to applications, busy-wait on receive queues, etc. while more flexible\napproaches appropriate to more dynamic workload mixes incur much greater\nsoftware overhead on the data path.\n  However, we reject this trade-off, which we ascribe to an arbitrary (and\nsub-optimal) split in system state between the OS and the NIC. Instead, by\nexploiting the properties of cache-coherent interconnects and integrating the\nNIC closely with the OS kernel, we can achieve something surprising:\nperformance for RPC workloads better than the fastest kernelbypass approaches\nwithout sacrificing the robustness and dynamic adaptation of kernel-based\nnetwork subsystems."
                },
                "authors": [
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "arxiv_doi": "10.1145/3713082.3730388",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713082.3730388",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.10138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Camera ready for HotOS'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v2",
                "updated": "2025-04-23T05:04:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    5,
                    4,
                    58,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Caching through Attention Output Error based Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Caching through Attention Output Error based Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "14 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06015v2",
                "updated": "2025-04-23T04:21:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    4,
                    21,
                    49,
                    2,
                    113,
                    0
                ],
                "published": "2025-03-08T02:35:16Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "title": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems"
                },
                "summary": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e they do not adapt to changing cache access\npatterns. Newer developments such as the High-Luminosity - Large Hadron\nCollider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move\ntoward streaming readout based Data Acquisition systems (DAQs) will increase\nthe data production exponentially and hence burden the storage, compute &\nnetwork infrastructures. Moreover, existing caching frameworks are optimized to\nreduce latency, but not optimized for storage. This, in combination with\nlimited cache capacities relative to total data, makes it difficult to achieve\ndata locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, we first present a Long Short-Term Memory-based (LSTM) hourly and\nmulti-step cache usage prediction. Second, we present an hourly file-level\naccess prediction model based on CatboostRegressor. To date, most ML-based\ncache prediction strategies in HEP have focused on daily cache usage and\nlimited works tackled hourly cache usage and even fewer strategies addressed\nhourly file-level access prediction. File-level access prediction allows for\nthe design of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending the\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e they do not adapt to changing cache access\npatterns. Newer developments such as the High-Luminosity - Large Hadron\nCollider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move\ntoward streaming readout based Data Acquisition systems (DAQs) will increase\nthe data production exponentially and hence burden the storage, compute &\nnetwork infrastructures. Moreover, existing caching frameworks are optimized to\nreduce latency, but not optimized for storage. This, in combination with\nlimited cache capacities relative to total data, makes it difficult to achieve\ndata locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, we first present a Long Short-Term Memory-based (LSTM) hourly and\nmulti-step cache usage prediction. Second, we present an hourly file-level\naccess prediction model based on CatboostRegressor. To date, most ML-based\ncache prediction strategies in HEP have focused on daily cache usage and\nlimited works tackled hourly cache usage and even fewer strategies addressed\nhourly file-level access prediction. File-level access prediction allows for\nthe design of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending the\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations."
                },
                "authors": [
                    {
                        "name": "Venkat Sai Suman Lamba Karanam"
                    },
                    {
                        "name": "Sarat Sasank Barla"
                    },
                    {
                        "name": "Byrav Ramamurthy"
                    },
                    {
                        "name": "Derek Weitzel"
                    }
                ],
                "author_detail": {
                    "name": "Derek Weitzel"
                },
                "author": "Derek Weitzel",
                "arxiv_comment": "Submitted as a contribution to the CHEP 2024 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16324v1",
                "updated": "2025-04-22T23:52:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    23,
                    52,
                    13,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T23:52:13Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    23,
                    52,
                    13,
                    1,
                    112,
                    0
                ],
                "title": "The Dawn of Disaggregation and the Coherence Conundrum: A Call for\n  Federated Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dawn of Disaggregation and the Coherence Conundrum: A Call for\n  Federated Coherence"
                },
                "summary": "Disaggregated memory is an upcoming data center technology that will allow\nnodes (servers) to share data efficiently. Sharing data creates a debate on the\nlevel of cache coherence the system should provide. While current proposals aim\nto provide coherence for all or parts of the disaggregated memory, we argue\nthat this approach is problematic, because of scalability limitations and\nhardware complexity. Instead, we propose and formally define federated\ncoherence, a model that provides coherence only within nodes, not across nodes.\nFederated coherence can use current intra-node coherence provided by processors\nwithout requiring expensive mechanisms for inter-node coherence. Developers can\nuse federated coherence with a few simple programming paradigms and a\nsynchronization library. We sketch some potential applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory is an upcoming data center technology that will allow\nnodes (servers) to share data efficiently. Sharing data creates a debate on the\nlevel of cache coherence the system should provide. While current proposals aim\nto provide coherence for all or parts of the disaggregated memory, we argue\nthat this approach is problematic, because of scalability limitations and\nhardware complexity. Instead, we propose and formally define federated\ncoherence, a model that provides coherence only within nodes, not across nodes.\nFederated coherence can use current intra-node coherence provided by processors\nwithout requiring expensive mechanisms for inter-node coherence. Developers can\nuse federated coherence with a few simple programming paradigms and a\nsynchronization library. We sketch some potential applications."
                },
                "authors": [
                    {
                        "name": "Jaewan Hong"
                    },
                    {
                        "name": "Marcos K. Aguilera"
                    },
                    {
                        "name": "Emmanuel Amaro"
                    },
                    {
                        "name": "Vincent Liu"
                    },
                    {
                        "name": "Aurojit Panda"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v3",
                "updated": "2025-04-22T17:34:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    34,
                    34,
                    1,
                    112,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Presented at IEEE Custom Integrated Circuits Conference (CICC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v2",
                "updated": "2025-04-22T17:23:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    23,
                    28,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators"
                },
                "summary": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hoßfeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "15 pages, 10 figures. Updated references and author name presentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14489v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14489v2",
                "updated": "2025-04-22T15:19:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    19,
                    48,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-20T04:46:34Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    4,
                    46,
                    34,
                    6,
                    110,
                    0
                ],
                "title": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing"
                },
                "summary": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads."
                },
                "authors": [
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14489v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14489v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.10888v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10888v2",
                "updated": "2025-05-13T17:58:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    58,
                    50,
                    1,
                    133,
                    0
                ],
                "published": "2025-02-15T19:44:50Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    19,
                    44,
                    50,
                    5,
                    46,
                    0
                ],
                "title": "Tensor parametric Hamiltonian operator inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor parametric Hamiltonian operator inference"
                },
                "summary": "This work presents a tensorial approach to constructing data-driven\nreduced-order models corresponding to semi-discrete partial differential\nequations with canonical Hamiltonian structure. By expressing parameter-varying\noperators with affine dependence as contractions of a generalized parameter\nvector against a constant tensor, this method leverages the operator inference\nframework to capture parametric dependence in the learned reduced-order model\nvia the solution to a convex, least-squares optimization problem. This leads to\na concise and straightforward implementation which compactifies previous\nparametric operator inference approaches and directly extends to learning\nparametric operators with symmetry constraints, a key feature required for\nconstructing structure-preserving surrogates of Hamiltonian systems. The\nproposed approach is demonstrated on both a (non-Hamiltonian) heat equation\nwith variable diffusion coefficient as well as a Hamiltonian wave equation with\nvariable wave speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a tensorial approach to constructing data-driven\nreduced-order models corresponding to semi-discrete partial differential\nequations with canonical Hamiltonian structure. By expressing parameter-varying\noperators with affine dependence as contractions of a generalized parameter\nvector against a constant tensor, this method leverages the operator inference\nframework to capture parametric dependence in the learned reduced-order model\nvia the solution to a convex, least-squares optimization problem. This leads to\na concise and straightforward implementation which compactifies previous\nparametric operator inference approaches and directly extends to learning\nparametric operators with symmetry constraints, a key feature required for\nconstructing structure-preserving surrogates of Hamiltonian systems. The\nproposed approach is demonstrated on both a (non-Hamiltonian) heat equation\nwith variable diffusion coefficient as well as a Hamiltonian wave equation with\nvariable wave speed."
                },
                "authors": [
                    {
                        "name": "Arjun Vijaywargiya"
                    },
                    {
                        "name": "Shane A. McQuarrie"
                    },
                    {
                        "name": "Anthony Gruber"
                    }
                ],
                "author_detail": {
                    "name": "Anthony Gruber"
                },
                "author": "Anthony Gruber",
                "arxiv_comment": "https://github.com/sandialabs/HamiltonianOpInf/tree/parametric",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10888v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10888v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "15A69, 35B30, 65P10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08784v1",
                "updated": "2025-05-13T17:58:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    58,
                    16,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T17:58:16Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    58,
                    16,
                    1,
                    133,
                    0
                ],
                "title": "PCS-UQ: Uncertainty Quantification via the\n  Predictability-Computability-Stability Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PCS-UQ: Uncertainty Quantification via the\n  Predictability-Computability-Stability Framework"
                },
                "summary": "As machine learning (ML) models are increasingly deployed in high-stakes\ndomains, trustworthy uncertainty quantification (UQ) is critical for ensuring\nthe safety and reliability of these models. Traditional UQ methods rely on\nspecifying a true generative model and are not robust to misspecification. On\nthe other hand, conformal inference allows for arbitrary ML models but does not\nconsider model selection, which leads to large interval sizes. We tackle these\ndrawbacks by proposing a UQ method based on the predictability, computability,\nand stability (PCS) framework for veridical data science proposed by Yu and\nKumbier. Specifically, PCS-UQ addresses model selection by using a prediction\ncheck to screen out unsuitable models. PCS-UQ then fits these screened\nalgorithms across multiple bootstraps to assess inter-sample variability and\nalgorithmic instability, enabling more reliable uncertainty estimates. Further,\nwe propose a novel calibration scheme that improves local adaptivity of our\nprediction sets. Experiments across $17$ regression and $6$ classification\ndatasets show that PCS-UQ achieves the desired coverage and reduces width over\nconformal approaches by $\\approx 20\\%$. Further, our local analysis shows\nPCS-UQ often achieves target coverage across subgroups while conformal methods\nfail to do so. For large deep-learning models, we propose computationally\nefficient approximation schemes that avoid the expensive multiple bootstrap\ntrainings of PCS-UQ. Across three computer vision benchmarks, PCS-UQ reduces\nprediction set size over conformal methods by $20\\%$. Theoretically, we show a\nmodified PCS-UQ algorithm is a form of split conformal inference and achieves\nthe desired coverage with exchangeable data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As machine learning (ML) models are increasingly deployed in high-stakes\ndomains, trustworthy uncertainty quantification (UQ) is critical for ensuring\nthe safety and reliability of these models. Traditional UQ methods rely on\nspecifying a true generative model and are not robust to misspecification. On\nthe other hand, conformal inference allows for arbitrary ML models but does not\nconsider model selection, which leads to large interval sizes. We tackle these\ndrawbacks by proposing a UQ method based on the predictability, computability,\nand stability (PCS) framework for veridical data science proposed by Yu and\nKumbier. Specifically, PCS-UQ addresses model selection by using a prediction\ncheck to screen out unsuitable models. PCS-UQ then fits these screened\nalgorithms across multiple bootstraps to assess inter-sample variability and\nalgorithmic instability, enabling more reliable uncertainty estimates. Further,\nwe propose a novel calibration scheme that improves local adaptivity of our\nprediction sets. Experiments across $17$ regression and $6$ classification\ndatasets show that PCS-UQ achieves the desired coverage and reduces width over\nconformal approaches by $\\approx 20\\%$. Further, our local analysis shows\nPCS-UQ often achieves target coverage across subgroups while conformal methods\nfail to do so. For large deep-learning models, we propose computationally\nefficient approximation schemes that avoid the expensive multiple bootstrap\ntrainings of PCS-UQ. Across three computer vision benchmarks, PCS-UQ reduces\nprediction set size over conformal methods by $20\\%$. Theoretically, we show a\nmodified PCS-UQ algorithm is a form of split conformal inference and achieves\nthe desired coverage with exchangeable data."
                },
                "authors": [
                    {
                        "name": "Abhineet Agarwal"
                    },
                    {
                        "name": "Michael Xiao"
                    },
                    {
                        "name": "Rebecca Barter"
                    },
                    {
                        "name": "Omer Ronen"
                    },
                    {
                        "name": "Boyu Fan"
                    },
                    {
                        "name": "Bin Yu"
                    }
                ],
                "author_detail": {
                    "name": "Bin Yu"
                },
                "author": "Bin Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08783v1",
                "updated": "2025-05-13T17:58:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    58,
                    8,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T17:58:08Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    58,
                    8,
                    1,
                    133,
                    0
                ],
                "title": "CodePDE: An Inference Framework for LLM-driven PDE Solver Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodePDE: An Inference Framework for LLM-driven PDE Solver Generation"
                },
                "summary": "Partial differential equations (PDEs) are fundamental to modeling physical\nsystems, yet solving them remains a complex challenge. Traditional numerical\nsolvers rely on expert knowledge to implement and are computationally\nexpensive, while neural-network-based solvers require large training datasets\nand often lack interpretability. In this work, we frame PDE solving as a code\ngeneration task and introduce CodePDE, the first inference framework for\ngenerating PDE solvers using large language models (LLMs). Leveraging advanced\ninference-time algorithms and scaling strategies, CodePDE unlocks critical\ncapacities of LLM for PDE solving: reasoning, debugging, selfrefinement, and\ntest-time scaling -- all without task-specific tuning. CodePDE achieves\nsuperhuman performance across a range of representative PDE problems. We also\npresent a systematic empirical analysis of LLM generated solvers, analyzing\ntheir accuracy, efficiency, and numerical scheme choices. Our findings\nhighlight the promise and the current limitations of LLMs in PDE solving,\noffering a new perspective on solver design and opportunities for future model\ndevelopment. Our code is available at https://github.com/LithiumDA/CodePDE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partial differential equations (PDEs) are fundamental to modeling physical\nsystems, yet solving them remains a complex challenge. Traditional numerical\nsolvers rely on expert knowledge to implement and are computationally\nexpensive, while neural-network-based solvers require large training datasets\nand often lack interpretability. In this work, we frame PDE solving as a code\ngeneration task and introduce CodePDE, the first inference framework for\ngenerating PDE solvers using large language models (LLMs). Leveraging advanced\ninference-time algorithms and scaling strategies, CodePDE unlocks critical\ncapacities of LLM for PDE solving: reasoning, debugging, selfrefinement, and\ntest-time scaling -- all without task-specific tuning. CodePDE achieves\nsuperhuman performance across a range of representative PDE problems. We also\npresent a systematic empirical analysis of LLM generated solvers, analyzing\ntheir accuracy, efficiency, and numerical scheme choices. Our findings\nhighlight the promise and the current limitations of LLMs in PDE solving,\noffering a new perspective on solver design and opportunities for future model\ndevelopment. Our code is available at https://github.com/LithiumDA/CodePDE."
                },
                "authors": [
                    {
                        "name": "Shanda Li"
                    },
                    {
                        "name": "Tanya Marwah"
                    },
                    {
                        "name": "Junhong Shen"
                    },
                    {
                        "name": "Weiwei Sun"
                    },
                    {
                        "name": "Andrej Risteski"
                    },
                    {
                        "name": "Yiming Yang"
                    },
                    {
                        "name": "Ameet Talwalkar"
                    }
                ],
                "author_detail": {
                    "name": "Ameet Talwalkar"
                },
                "author": "Ameet Talwalkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08780v1",
                "updated": "2025-05-13T17:56:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    56,
                    2,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T17:56:02Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    56,
                    2,
                    1,
                    133,
                    0
                ],
                "title": "Polar motion of Venus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polar motion of Venus"
                },
                "summary": "Five Venus missions are under development to study the planet in the next\ndecade, with both NASA's VERITAS and ESA's EnVision featuring a geophysical\ninvestigation among their objectives. Their radar and gravity experiments will\ndetermine Venus's orientation, enabling spin dynamics analyses to infer\ngeophysical and atmospheric properties. This work aims to characterize Venus's\npolar motion -- the motion of its spin axis in a body-fixed frame-focusing on\nsignatures from its interior and atmosphere to support its potential detection\nby future orbiters. We develop a polar motion model for a triaxial planet\naccounting for solar torque, centrifugal and tidal deformations of a\nviscoelastic mantle, and atmospheric dynamics. Core-mantle coupling effects are\nanalyzed separately considering a simplified spherical core. We compute the\nperiod and damping time of the free motion -- called the Chandler wobble -- and\ndetermine the frequencies and amplitudes of the forced motion. We revisit the\nChandler frequency expression. Solar torque is the dominant phenomenon\naffecting Venus's Chandler frequency, increasing it by a factor of 2.75. Our\nmodel predicts a Chandler period in the range [12900 ; 18900] years. The\nChandler wobble appears as a linear polar drift of about 90 meters on Venus's\nsurface during EnVision's 4-year primary mission, at the limit of its\nresolution. We also predict forced polar motion oscillations with an amplitude\nof about 20 meters, driven by the atmosphere and the solar torque. Compared to\nthe 240-meter spin axis precession occurring in inertial space over this\nduration, these results suggest that Venus's polar motion could also be\ndetectable by future orbiters. It should be incorporated into rotation models\nwhen anticipating these missions, providing additional constraints on Venus's\ninterior structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Five Venus missions are under development to study the planet in the next\ndecade, with both NASA's VERITAS and ESA's EnVision featuring a geophysical\ninvestigation among their objectives. Their radar and gravity experiments will\ndetermine Venus's orientation, enabling spin dynamics analyses to infer\ngeophysical and atmospheric properties. This work aims to characterize Venus's\npolar motion -- the motion of its spin axis in a body-fixed frame-focusing on\nsignatures from its interior and atmosphere to support its potential detection\nby future orbiters. We develop a polar motion model for a triaxial planet\naccounting for solar torque, centrifugal and tidal deformations of a\nviscoelastic mantle, and atmospheric dynamics. Core-mantle coupling effects are\nanalyzed separately considering a simplified spherical core. We compute the\nperiod and damping time of the free motion -- called the Chandler wobble -- and\ndetermine the frequencies and amplitudes of the forced motion. We revisit the\nChandler frequency expression. Solar torque is the dominant phenomenon\naffecting Venus's Chandler frequency, increasing it by a factor of 2.75. Our\nmodel predicts a Chandler period in the range [12900 ; 18900] years. The\nChandler wobble appears as a linear polar drift of about 90 meters on Venus's\nsurface during EnVision's 4-year primary mission, at the limit of its\nresolution. We also predict forced polar motion oscillations with an amplitude\nof about 20 meters, driven by the atmosphere and the solar torque. Compared to\nthe 240-meter spin axis precession occurring in inertial space over this\nduration, these results suggest that Venus's polar motion could also be\ndetectable by future orbiters. It should be incorporated into rotation models\nwhen anticipating these missions, providing additional constraints on Venus's\ninterior structure."
                },
                "authors": [
                    {
                        "name": "PL. Phan"
                    },
                    {
                        "name": "N. Rambaux"
                    }
                ],
                "author_detail": {
                    "name": "N. Rambaux"
                },
                "author": "N. Rambaux",
                "arxiv_comment": "16 pages, 7 figures, 6 tables, accepted for publication in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v5",
                "updated": "2025-05-13T17:43:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    43,
                    47,
                    1,
                    133,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08768v1",
                "updated": "2025-05-13T17:39:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    39,
                    31,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T17:39:31Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    39,
                    31,
                    1,
                    133,
                    0
                ],
                "title": "SPAT: Sensitivity-based Multihead-attention Pruning on Time Series\n  Forecasting Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPAT: Sensitivity-based Multihead-attention Pruning on Time Series\n  Forecasting Models"
                },
                "summary": "Attention-based architectures have achieved superior performance in\nmultivariate time series forecasting but are computationally expensive.\nTechniques such as patching and adaptive masking have been developed to reduce\ntheir sizes and latencies. In this work, we propose a structured pruning\nmethod, SPAT ($\\textbf{S}$ensitivity $\\textbf{P}$runer for\n$\\textbf{At}$tention), which selectively removes redundant attention mechanisms\nand yields highly effective models. Different from previous approaches, SPAT\naims to remove the entire attention module, which reduces the risk of\noverfitting and enables speed-up without demanding specialized hardware. We\npropose a dynamic sensitivity metric, $\\textbf{S}$ensitivity\n$\\textbf{E}$nhanced $\\textbf{N}$ormalized $\\textbf{D}$ispersion (SEND) that\nmeasures the importance of each attention module during the pre-training phase.\nExperiments on multivariate datasets demonstrate that SPAT-pruned models\nachieve reductions of 2.842% in MSE, 1.996% in MAE, and 35.274% in FLOPs.\nFurthermore, SPAT-pruned models outperform existing lightweight, Mamba-based\nand LLM-based SOTA methods in both standard and zero-shot inference,\nhighlighting the importance of retaining only the most effective attention\nmechanisms. We have made our code publicly available\nhttps://anonymous.4open.science/r/SPAT-6042.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-based architectures have achieved superior performance in\nmultivariate time series forecasting but are computationally expensive.\nTechniques such as patching and adaptive masking have been developed to reduce\ntheir sizes and latencies. In this work, we propose a structured pruning\nmethod, SPAT ($\\textbf{S}$ensitivity $\\textbf{P}$runer for\n$\\textbf{At}$tention), which selectively removes redundant attention mechanisms\nand yields highly effective models. Different from previous approaches, SPAT\naims to remove the entire attention module, which reduces the risk of\noverfitting and enables speed-up without demanding specialized hardware. We\npropose a dynamic sensitivity metric, $\\textbf{S}$ensitivity\n$\\textbf{E}$nhanced $\\textbf{N}$ormalized $\\textbf{D}$ispersion (SEND) that\nmeasures the importance of each attention module during the pre-training phase.\nExperiments on multivariate datasets demonstrate that SPAT-pruned models\nachieve reductions of 2.842% in MSE, 1.996% in MAE, and 35.274% in FLOPs.\nFurthermore, SPAT-pruned models outperform existing lightweight, Mamba-based\nand LLM-based SOTA methods in both standard and zero-shot inference,\nhighlighting the importance of retaining only the most effective attention\nmechanisms. We have made our code publicly available\nhttps://anonymous.4open.science/r/SPAT-6042."
                },
                "authors": [
                    {
                        "name": "Suhan Guo"
                    },
                    {
                        "name": "Jiahong Deng"
                    },
                    {
                        "name": "Mengjun Yi"
                    },
                    {
                        "name": "Furao Shen"
                    },
                    {
                        "name": "Jian Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhao"
                },
                "author": "Jian Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14917v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14917v5",
                "updated": "2025-05-13T17:06:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    6,
                    22,
                    1,
                    133,
                    0
                ],
                "published": "2025-01-24T20:54:29Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    20,
                    54,
                    29,
                    4,
                    24,
                    0
                ],
                "title": "Self-reflecting Large Language Models: A Hegelian Dialectical Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-reflecting Large Language Models: A Hegelian Dialectical Approach"
                },
                "summary": "Investigating NLP through a philosophical lens has recently caught\nresearcher's eyes as it connects computational methods with classical schools\nof philosophy. This paper introduces a philosophical approach inspired by the\n\\textit{Hegelian Dialectic} for LLMs' \\textit{self-reflection}, utilizing a\nself-dialectical approach to emulate internal critiques and then synthesize new\nideas by resolving the opposing points of view. Moreover, this paper\ninvestigates the effect of LLMs' temperature for generation by establishing a\ndynamic annealing approach, which promotes the creativity in the early stages\nand gradually refines it by focusing on the nuances, as well as a\nfixed-temperature strategy for generation. We assess the effectiveness of our\nproposed method in generating novel ideas and in improving the reasoning\nabilities of LLMs during problem-solving. Moreover, we implement a Multi-Agent\nMajority Voting (MAMV) strategy to assess the validity and novelty of the\ngenerated ideas, which proves useful in the absence of domain experts. Our\nexperiments demonstrate promising results in generating ideas and enhancing\nproblem-solving performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating NLP through a philosophical lens has recently caught\nresearcher's eyes as it connects computational methods with classical schools\nof philosophy. This paper introduces a philosophical approach inspired by the\n\\textit{Hegelian Dialectic} for LLMs' \\textit{self-reflection}, utilizing a\nself-dialectical approach to emulate internal critiques and then synthesize new\nideas by resolving the opposing points of view. Moreover, this paper\ninvestigates the effect of LLMs' temperature for generation by establishing a\ndynamic annealing approach, which promotes the creativity in the early stages\nand gradually refines it by focusing on the nuances, as well as a\nfixed-temperature strategy for generation. We assess the effectiveness of our\nproposed method in generating novel ideas and in improving the reasoning\nabilities of LLMs during problem-solving. Moreover, we implement a Multi-Agent\nMajority Voting (MAMV) strategy to assess the validity and novelty of the\ngenerated ideas, which proves useful in the absence of domain experts. Our\nexperiments demonstrate promising results in generating ideas and enhancing\nproblem-solving performance."
                },
                "authors": [
                    {
                        "name": "Sara Abdali"
                    },
                    {
                        "name": "Can Goksen"
                    },
                    {
                        "name": "Saeed Amizadeh"
                    },
                    {
                        "name": "Julie E. Maybee"
                    },
                    {
                        "name": "Kazuhito Koishida"
                    }
                ],
                "author_detail": {
                    "name": "Kazuhito Koishida"
                },
                "author": "Kazuhito Koishida",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14917v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14917v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08750v1",
                "updated": "2025-05-13T17:02:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    2,
                    33,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T17:02:33Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    2,
                    33,
                    1,
                    133,
                    0
                ],
                "title": "AC-Reason: Towards Theory-Guided Actual Causality Reasoning with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AC-Reason: Towards Theory-Guided Actual Causality Reasoning with Large\n  Language Models"
                },
                "summary": "Actual causality (AC), a fundamental aspect of causal reasoning (CR), is\nresponsible for attribution and responsibility assignment in real-world\nscenarios. However, existing LLM-based methods lack grounding in formal AC\ntheory, resulting in limited interpretability. Therefore, we propose AC-Reason,\na semi-formal reasoning framework that identifies causally relevant events\nwithin an AC scenario, infers the values of their formal causal factors (e.g.,\nsufficiency, necessity, and normality), and answers AC queries via a\ntheory-guided algorithm with explanations. While AC-Reason does not explicitly\nconstruct a causal graph, it operates over variables in the underlying causal\nstructure to support principled reasoning. To enable comprehensive evaluation,\nwe introduce AC-Bench, a new benchmark built upon and substantially extending\nBig-Bench Hard Causal Judgment (BBH-CJ). AC-Bench comprises ~1K carefully\nannotated samples, each with detailed reasoning steps and focuses solely on\nactual causation. The case study shows that synthesized samples in AC-Bench\npresent greater challenges for LLMs. Extensive experiments on BBH-CJ and\nAC-Bench show that AC-Reason consistently improves LLM performance over\nbaselines. On BBH-CJ, all tested LLMs surpass the average human rater accuracy\nof 69.60%, with GPT-4 + AC-Reason achieving 75.04%. On AC-Bench, GPT-4 +\nAC-Reason again achieves the highest accuracy of 71.82%. AC-Bench further\nenables fine-grained analysis of reasoning faithfulness, revealing that only\nQwen-2.5-72B-Instruct, Claude-3.5-Sonnet, and GPT-4o exhibit faithful\nreasoning, whereas GPT-4 tends to exploit shortcuts. Finally, our ablation\nstudy proves that integrating AC theory into LLMs is highly effective, with the\nproposed algorithm contributing the most significant performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Actual causality (AC), a fundamental aspect of causal reasoning (CR), is\nresponsible for attribution and responsibility assignment in real-world\nscenarios. However, existing LLM-based methods lack grounding in formal AC\ntheory, resulting in limited interpretability. Therefore, we propose AC-Reason,\na semi-formal reasoning framework that identifies causally relevant events\nwithin an AC scenario, infers the values of their formal causal factors (e.g.,\nsufficiency, necessity, and normality), and answers AC queries via a\ntheory-guided algorithm with explanations. While AC-Reason does not explicitly\nconstruct a causal graph, it operates over variables in the underlying causal\nstructure to support principled reasoning. To enable comprehensive evaluation,\nwe introduce AC-Bench, a new benchmark built upon and substantially extending\nBig-Bench Hard Causal Judgment (BBH-CJ). AC-Bench comprises ~1K carefully\nannotated samples, each with detailed reasoning steps and focuses solely on\nactual causation. The case study shows that synthesized samples in AC-Bench\npresent greater challenges for LLMs. Extensive experiments on BBH-CJ and\nAC-Bench show that AC-Reason consistently improves LLM performance over\nbaselines. On BBH-CJ, all tested LLMs surpass the average human rater accuracy\nof 69.60%, with GPT-4 + AC-Reason achieving 75.04%. On AC-Bench, GPT-4 +\nAC-Reason again achieves the highest accuracy of 71.82%. AC-Bench further\nenables fine-grained analysis of reasoning faithfulness, revealing that only\nQwen-2.5-72B-Instruct, Claude-3.5-Sonnet, and GPT-4o exhibit faithful\nreasoning, whereas GPT-4 tends to exploit shortcuts. Finally, our ablation\nstudy proves that integrating AC theory into LLMs is highly effective, with the\nproposed algorithm contributing the most significant performance gains."
                },
                "authors": [
                    {
                        "name": "Yanxi Zhang"
                    },
                    {
                        "name": "Xin Cong"
                    },
                    {
                        "name": "Zhong Zhang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Dongyan Zhao"
                    },
                    {
                        "name": "Yesai Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yesai Wu"
                },
                "author": "Yesai Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08744v1",
                "updated": "2025-05-13T16:58:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    58,
                    5,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T16:58:05Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    58,
                    5,
                    1,
                    133,
                    0
                ],
                "title": "DeepMath-Creative: A Benchmark for Evaluating Mathematical Creativity of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepMath-Creative: A Benchmark for Evaluating Mathematical Creativity of\n  Large Language Models"
                },
                "summary": "To advance the mathematical proficiency of large language models (LLMs), the\nDeepMath team has launched an open-source initiative aimed at developing an\nopen mathematical LLM and systematically evaluating its mathematical\ncreativity. This paper represents the initial contribution of this initiative.\nWhile recent developments in mathematical LLMs have predominantly emphasized\nreasoning skills, as evidenced by benchmarks on elementary to\nundergraduate-level mathematical tasks, the creative capabilities of these\nmodels have received comparatively little attention, and evaluation datasets\nremain scarce. To address this gap, we propose an evaluation criteria for\nmathematical creativity and introduce DeepMath-Creative, a novel, high-quality\nbenchmark comprising constructive problems across algebra, geometry, analysis,\nand other domains. We conduct a systematic evaluation of mainstream LLMs'\ncreative problem-solving abilities using this dataset. Experimental results\nshow that even under lenient scoring criteria -- emphasizing core solution\ncomponents and disregarding minor inaccuracies, such as small logical gaps,\nincomplete justifications, or redundant explanations -- the best-performing\nmodel, O3 Mini, achieves merely 70% accuracy, primarily on basic\nundergraduate-level constructive tasks. Performance declines sharply on more\ncomplex problems, with models failing to provide substantive strategies for\nopen problems. These findings suggest that, although current LLMs display a\ndegree of constructive proficiency on familiar and lower-difficulty problems,\nsuch performance is likely attributable to the recombination of memorized\npatterns rather than authentic creative insight or novel synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To advance the mathematical proficiency of large language models (LLMs), the\nDeepMath team has launched an open-source initiative aimed at developing an\nopen mathematical LLM and systematically evaluating its mathematical\ncreativity. This paper represents the initial contribution of this initiative.\nWhile recent developments in mathematical LLMs have predominantly emphasized\nreasoning skills, as evidenced by benchmarks on elementary to\nundergraduate-level mathematical tasks, the creative capabilities of these\nmodels have received comparatively little attention, and evaluation datasets\nremain scarce. To address this gap, we propose an evaluation criteria for\nmathematical creativity and introduce DeepMath-Creative, a novel, high-quality\nbenchmark comprising constructive problems across algebra, geometry, analysis,\nand other domains. We conduct a systematic evaluation of mainstream LLMs'\ncreative problem-solving abilities using this dataset. Experimental results\nshow that even under lenient scoring criteria -- emphasizing core solution\ncomponents and disregarding minor inaccuracies, such as small logical gaps,\nincomplete justifications, or redundant explanations -- the best-performing\nmodel, O3 Mini, achieves merely 70% accuracy, primarily on basic\nundergraduate-level constructive tasks. Performance declines sharply on more\ncomplex problems, with models failing to provide substantive strategies for\nopen problems. These findings suggest that, although current LLMs display a\ndegree of constructive proficiency on familiar and lower-difficulty problems,\nsuch performance is likely attributable to the recombination of memorized\npatterns rather than authentic creative insight or novel synthesis."
                },
                "authors": [
                    {
                        "name": "Xiaoyang Chen"
                    },
                    {
                        "name": "Xinan Dai"
                    },
                    {
                        "name": "Yu Du"
                    },
                    {
                        "name": "Qian Feng"
                    },
                    {
                        "name": "Naixu Guo"
                    },
                    {
                        "name": "Tingshuo Gu"
                    },
                    {
                        "name": "Yuting Gao"
                    },
                    {
                        "name": "Yingyi Gao"
                    },
                    {
                        "name": "Xudong Han"
                    },
                    {
                        "name": "Xiang Jiang"
                    },
                    {
                        "name": "Yilin Jin"
                    },
                    {
                        "name": "Hongyi Lin"
                    },
                    {
                        "name": "Shisheng Lin"
                    },
                    {
                        "name": "Xiangnan Li"
                    },
                    {
                        "name": "Yuante Li"
                    },
                    {
                        "name": "Yixing Li"
                    },
                    {
                        "name": "Zhentao Lai"
                    },
                    {
                        "name": "Zilu Ma"
                    },
                    {
                        "name": "Yingrong Peng"
                    },
                    {
                        "name": "Jiacheng Qian"
                    },
                    {
                        "name": "Hao-Yu Sun"
                    },
                    {
                        "name": "Jianbo Sun"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Siwei Wu"
                    },
                    {
                        "name": "Zian Wang"
                    },
                    {
                        "name": "Bin Xu"
                    },
                    {
                        "name": "Jianghao Xu"
                    },
                    {
                        "name": "Yiyang Yu"
                    },
                    {
                        "name": "Zichuan Yang"
                    },
                    {
                        "name": "Hongji Zha"
                    },
                    {
                        "name": "Ruichong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruichong Zhang"
                },
                "author": "Ruichong Zhang",
                "arxiv_comment": "14 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08739v1",
                "updated": "2025-05-13T16:52:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    52,
                    19,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T16:52:19Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    52,
                    19,
                    1,
                    133,
                    0
                ],
                "title": "Probability Consistency in Large Language Models: Theoretical\n  Foundations Meet Empirical Discrepancies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probability Consistency in Large Language Models: Theoretical\n  Foundations Meet Empirical Discrepancies"
                },
                "summary": "Can autoregressive large language models (LLMs) learn consistent probability\ndistributions when trained on sequences in different token orders? We prove\nformally that for any well-defined probability distribution, sequence\nperplexity is invariant under any factorization, including forward, backward,\nor arbitrary permutations. This result establishes a rigorous theoretical\nfoundation for studying how LLMs learn from data and defines principled\nprotocols for empirical evaluation. Applying these protocols, we show that\nprior studies examining ordering effects suffer from critical methodological\nflaws. We retrain GPT-2 models across forward, backward, and arbitrary permuted\norders on scientific text. We find systematic deviations from theoretical\ninvariance across all orderings with arbitrary permutations strongly deviating\nfrom both forward and backward models, which largely (but not completely)\nagreed with one another. Deviations were traceable to differences in\nself-attention, reflecting positional and locality biases in processing. Our\ntheoretical and empirical results provide novel avenues for understanding\npositional biases in LLMs and suggest methods for detecting when LLMs'\nprobability distributions are inconsistent and therefore untrustworthy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can autoregressive large language models (LLMs) learn consistent probability\ndistributions when trained on sequences in different token orders? We prove\nformally that for any well-defined probability distribution, sequence\nperplexity is invariant under any factorization, including forward, backward,\nor arbitrary permutations. This result establishes a rigorous theoretical\nfoundation for studying how LLMs learn from data and defines principled\nprotocols for empirical evaluation. Applying these protocols, we show that\nprior studies examining ordering effects suffer from critical methodological\nflaws. We retrain GPT-2 models across forward, backward, and arbitrary permuted\norders on scientific text. We find systematic deviations from theoretical\ninvariance across all orderings with arbitrary permutations strongly deviating\nfrom both forward and backward models, which largely (but not completely)\nagreed with one another. Deviations were traceable to differences in\nself-attention, reflecting positional and locality biases in processing. Our\ntheoretical and empirical results provide novel avenues for understanding\npositional biases in LLMs and suggest methods for detecting when LLMs'\nprobability distributions are inconsistent and therefore untrustworthy."
                },
                "authors": [
                    {
                        "name": "Xiaoliang Luo"
                    },
                    {
                        "name": "Xinyi Xu"
                    },
                    {
                        "name": "Michael Ramscar"
                    },
                    {
                        "name": "Bradley C. Love"
                    }
                ],
                "author_detail": {
                    "name": "Bradley C. Love"
                },
                "author": "Bradley C. Love",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08734v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08734v1",
                "updated": "2025-05-13T16:46:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    46,
                    25,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T16:46:25Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    46,
                    25,
                    1,
                    133,
                    0
                ],
                "title": "NurValues: Real-World Nursing Values Evaluation for Large Language\n  Models in Clinical Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NurValues: Real-World Nursing Values Evaluation for Large Language\n  Models in Clinical Context"
                },
                "summary": "This work introduces the first benchmark for nursing value alignment,\nconsisting of five core value dimensions distilled from international nursing\ncodes: Altruism, Human Dignity, Integrity, Justice, and Professionalism. The\nbenchmark comprises 1,100 real-world nursing behavior instances collected\nthrough a five-month longitudinal field study across three hospitals of varying\ntiers. These instances are annotated by five clinical nurses and then augmented\nwith LLM-generated counterfactuals with reversed ethic polarity. Each original\ncase is paired with a value-aligned and a value-violating version, resulting in\n2,200 labeled instances that constitute the Easy-Level dataset. To increase\nadversarial complexity, each instance is further transformed into a\ndialogue-based format that embeds contextual cues and subtle misleading\nsignals, yielding a Hard-Level dataset. We evaluate 23 state-of-the-art (SoTA)\nLLMs on their alignment with nursing values. Our findings reveal three key\ninsights: (1) DeepSeek-V3 achieves the highest performance on the Easy-Level\ndataset (94.55), where Claude 3.5 Sonnet outperforms other models on the\nHard-Level dataset (89.43), significantly surpassing the medical LLMs; (2)\nJustice is consistently the most difficult nursing value dimension to evaluate;\nand (3) in-context learning significantly improves alignment. This work aims to\nprovide a foundation for value-sensitive LLMs development in clinical settings.\nThe dataset and the code are available at\nhttps://huggingface.co/datasets/Ben012345/NurValues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces the first benchmark for nursing value alignment,\nconsisting of five core value dimensions distilled from international nursing\ncodes: Altruism, Human Dignity, Integrity, Justice, and Professionalism. The\nbenchmark comprises 1,100 real-world nursing behavior instances collected\nthrough a five-month longitudinal field study across three hospitals of varying\ntiers. These instances are annotated by five clinical nurses and then augmented\nwith LLM-generated counterfactuals with reversed ethic polarity. Each original\ncase is paired with a value-aligned and a value-violating version, resulting in\n2,200 labeled instances that constitute the Easy-Level dataset. To increase\nadversarial complexity, each instance is further transformed into a\ndialogue-based format that embeds contextual cues and subtle misleading\nsignals, yielding a Hard-Level dataset. We evaluate 23 state-of-the-art (SoTA)\nLLMs on their alignment with nursing values. Our findings reveal three key\ninsights: (1) DeepSeek-V3 achieves the highest performance on the Easy-Level\ndataset (94.55), where Claude 3.5 Sonnet outperforms other models on the\nHard-Level dataset (89.43), significantly surpassing the medical LLMs; (2)\nJustice is consistently the most difficult nursing value dimension to evaluate;\nand (3) in-context learning significantly improves alignment. This work aims to\nprovide a foundation for value-sensitive LLMs development in clinical settings.\nThe dataset and the code are available at\nhttps://huggingface.co/datasets/Ben012345/NurValues."
                },
                "authors": [
                    {
                        "name": "Ben Yao"
                    },
                    {
                        "name": "Qiuchi Li"
                    },
                    {
                        "name": "Yazhou Zhang"
                    },
                    {
                        "name": "Siyu Yang"
                    },
                    {
                        "name": "Bohan Zhang"
                    },
                    {
                        "name": "Prayag Tiwari"
                    },
                    {
                        "name": "Jing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Jing Qin"
                },
                "author": "Jing Qin",
                "arxiv_comment": "25 pages, 10 figures, 16 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08734v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08733v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08733v1",
                "updated": "2025-05-13T16:46:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    46,
                    15,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T16:46:15Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    46,
                    15,
                    1,
                    133,
                    0
                ],
                "title": "Absolute measurement of the exchange interaction in an InSb quantum well\n  using Landau-level tunnelling spectroscopy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Absolute measurement of the exchange interaction in an InSb quantum well\n  using Landau-level tunnelling spectroscopy"
                },
                "summary": "We studied InSb quantum well devices using Landau level tunneling\nspectroscopy through a three-terminal differential conductance technique. This\nmethod is similar to filled state scanning tunneling microscopy but uses a\nstationary contact instead of a mobile tip to analyze the two-dimensional\nelectron system. Applying magnetic fields up to 15~T, we identified clear peaks\nin the differential current-voltage profiles, indicative of Landau level\nformation. By examining deviations from the expected Landau fan diagram, we\nextract an absolute value for the exchange-induced energy shift. Through an\nempirical analysis, we derive a formula describing the exchange shift as a\nfunction of both magnetic field strength and electron filling. Our findings\nindicate that the emptying of the $\\nu=2$ and $\\nu=3$ Landau levels causes an\nexchange interaction energy shift in the $\\nu=1$ level. Unlike prior studies\nthat infer level energies relative to one another and report oscillatory\ng-factor behavior, our method references the energy of the Landau levels above\nthe filled states of the contact under a bias voltage, revealing that only the\nground state Landau level experiences a measurable exchange shift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We studied InSb quantum well devices using Landau level tunneling\nspectroscopy through a three-terminal differential conductance technique. This\nmethod is similar to filled state scanning tunneling microscopy but uses a\nstationary contact instead of a mobile tip to analyze the two-dimensional\nelectron system. Applying magnetic fields up to 15~T, we identified clear peaks\nin the differential current-voltage profiles, indicative of Landau level\nformation. By examining deviations from the expected Landau fan diagram, we\nextract an absolute value for the exchange-induced energy shift. Through an\nempirical analysis, we derive a formula describing the exchange shift as a\nfunction of both magnetic field strength and electron filling. Our findings\nindicate that the emptying of the $\\nu=2$ and $\\nu=3$ Landau levels causes an\nexchange interaction energy shift in the $\\nu=1$ level. Unlike prior studies\nthat infer level energies relative to one another and report oscillatory\ng-factor behavior, our method references the energy of the Landau levels above\nthe filled states of the contact under a bias voltage, revealing that only the\nground state Landau level experiences a measurable exchange shift."
                },
                "authors": [
                    {
                        "name": "S. K. Clowes"
                    },
                    {
                        "name": "C. P. Allford"
                    },
                    {
                        "name": "D. Shearer"
                    },
                    {
                        "name": "G. V. Smith"
                    },
                    {
                        "name": "R. Simmons"
                    },
                    {
                        "name": "B. N. Murdin"
                    },
                    {
                        "name": "U. Zeitler"
                    },
                    {
                        "name": "P. D. Buckle"
                    }
                ],
                "author_detail": {
                    "name": "P. D. Buckle"
                },
                "author": "P. D. Buckle",
                "arxiv_comment": "7 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08733v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08733v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02870v2",
                "updated": "2025-05-13T16:41:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    41,
                    54,
                    1,
                    133,
                    0
                ],
                "published": "2025-04-01T12:56:39Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    56,
                    39,
                    1,
                    91,
                    0
                ],
                "title": "AI Hiring with LLMs: A Context-Aware and Explainable Multi-Agent\n  Framework for Resume Screening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Hiring with LLMs: A Context-Aware and Explainable Multi-Agent\n  Framework for Resume Screening"
                },
                "summary": "Resume screening is a critical yet time-intensive process in talent\nacquisition, requiring recruiters to analyze vast volume of job applications\nwhile remaining objective, accurate, and fair. With the advancements in Large\nLanguage Models (LLMs), their reasoning capabilities and extensive knowledge\nbases demonstrate new opportunities to streamline and automate recruitment\nworkflows. In this work, we propose a multi-agent framework for resume\nscreening using LLMs to systematically process and evaluate resumes. The\nframework consists of four core agents, including a resume extractor, an\nevaluator, a summarizer, and a score formatter. To enhance the contextual\nrelevance of candidate assessments, we integrate Retrieval-Augmented Generation\n(RAG) within the resume evaluator, allowing incorporation of external knowledge\nsources, such as industry-specific expertise, professional certifications,\nuniversity rankings, and company-specific hiring criteria. This dynamic\nadaptation enables personalized recruitment, bridging the gap between AI\nautomation and talent acquisition. We assess the effectiveness of our approach\nby comparing AI-generated scores with ratings provided by HR professionals on a\ndataset of anonymized online resumes. The findings highlight the potential of\nmulti-agent RAG-LLM systems in automating resume screening, enabling more\nefficient and scalable hiring workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resume screening is a critical yet time-intensive process in talent\nacquisition, requiring recruiters to analyze vast volume of job applications\nwhile remaining objective, accurate, and fair. With the advancements in Large\nLanguage Models (LLMs), their reasoning capabilities and extensive knowledge\nbases demonstrate new opportunities to streamline and automate recruitment\nworkflows. In this work, we propose a multi-agent framework for resume\nscreening using LLMs to systematically process and evaluate resumes. The\nframework consists of four core agents, including a resume extractor, an\nevaluator, a summarizer, and a score formatter. To enhance the contextual\nrelevance of candidate assessments, we integrate Retrieval-Augmented Generation\n(RAG) within the resume evaluator, allowing incorporation of external knowledge\nsources, such as industry-specific expertise, professional certifications,\nuniversity rankings, and company-specific hiring criteria. This dynamic\nadaptation enables personalized recruitment, bridging the gap between AI\nautomation and talent acquisition. We assess the effectiveness of our approach\nby comparing AI-generated scores with ratings provided by HR professionals on a\ndataset of anonymized online resumes. The findings highlight the potential of\nmulti-agent RAG-LLM systems in automating resume screening, enabling more\nefficient and scalable hiring workflows."
                },
                "authors": [
                    {
                        "name": "Frank P. -W. Lo"
                    },
                    {
                        "name": "Jianing Qiu"
                    },
                    {
                        "name": "Zeyu Wang"
                    },
                    {
                        "name": "Haibao Yu"
                    },
                    {
                        "name": "Yeming Chen"
                    },
                    {
                        "name": "Gao Zhang"
                    },
                    {
                        "name": "Benny Lo"
                    }
                ],
                "author_detail": {
                    "name": "Benny Lo"
                },
                "author": "Benny Lo",
                "arxiv_comment": "Accepted by CVPR 2025 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08729v1",
                "updated": "2025-05-13T16:39:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    39,
                    19,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T16:39:19Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    39,
                    19,
                    1,
                    133,
                    0
                ],
                "title": "Assumption-robust Causal Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assumption-robust Causal Inference"
                },
                "summary": "In observational causal inference, it is common to encounter multiple\nadjustment sets that appear equally plausible. It is often untestable which of\nthese adjustment sets are valid to adjust for (i.e., satisfies ignorability).\nThis discrepancy can pose practical challenges as it is typically unclear how\nto reconcile multiple, possibly conflicting estimates of the average treatment\neffect (ATE). A naive approach is to report the whole range (convex hull of the\nunion) of the resulting confidence intervals. However, the width of this\ninterval might not shrink to zero in large samples and can be unnecessarily\nwide in real applications. To address this issue, we propose a summary\nprocedure that generates a single estimate, one confidence interval, and\nidentifies a set of units for which the causal effect estimate remains valid,\nprovided at least one adjustment set is valid. The width of our proposed\nconfidence interval shrinks to zero with sample size at $n^{-1/2}$ rate, unlike\nthe original range which is of constant order. Thus, our assumption-robust\napproach enables reliable causal inference on the ATE even in scenarios where\nmost of the adjustment sets are invalid. Admittedly, this robustness comes at a\ncost: our inferential guarantees apply to a target population close to, but\ndifferent from, the one originally intended. We use synthetic and real-data\nexamples to demonstrate that our proposed procedure provides substantially\ntighter confidence intervals for the ATE as compared to the whole range. In\nparticular, for a real-world dataset on 401(k) retirement plans our method\nproduces a confidence interval 50\\% shorter than the whole range of confidence\nintervals based on multiple adjustment sets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In observational causal inference, it is common to encounter multiple\nadjustment sets that appear equally plausible. It is often untestable which of\nthese adjustment sets are valid to adjust for (i.e., satisfies ignorability).\nThis discrepancy can pose practical challenges as it is typically unclear how\nto reconcile multiple, possibly conflicting estimates of the average treatment\neffect (ATE). A naive approach is to report the whole range (convex hull of the\nunion) of the resulting confidence intervals. However, the width of this\ninterval might not shrink to zero in large samples and can be unnecessarily\nwide in real applications. To address this issue, we propose a summary\nprocedure that generates a single estimate, one confidence interval, and\nidentifies a set of units for which the causal effect estimate remains valid,\nprovided at least one adjustment set is valid. The width of our proposed\nconfidence interval shrinks to zero with sample size at $n^{-1/2}$ rate, unlike\nthe original range which is of constant order. Thus, our assumption-robust\napproach enables reliable causal inference on the ATE even in scenarios where\nmost of the adjustment sets are invalid. Admittedly, this robustness comes at a\ncost: our inferential guarantees apply to a target population close to, but\ndifferent from, the one originally intended. We use synthetic and real-data\nexamples to demonstrate that our proposed procedure provides substantially\ntighter confidence intervals for the ATE as compared to the whole range. In\nparticular, for a real-world dataset on 401(k) retirement plans our method\nproduces a confidence interval 50\\% shorter than the whole range of confidence\nintervals based on multiple adjustment sets."
                },
                "authors": [
                    {
                        "name": "Aditya Ghosh"
                    },
                    {
                        "name": "Dominik Rothenhäusler"
                    }
                ],
                "author_detail": {
                    "name": "Dominik Rothenhäusler"
                },
                "author": "Dominik Rothenhäusler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08728v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08728v1",
                "updated": "2025-05-13T16:39:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    39,
                    0,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T16:39:00Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    39,
                    0,
                    1,
                    133,
                    0
                ],
                "title": "Securing RAG: A Risk Assessment and Mitigation Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Securing RAG: A Risk Assessment and Mitigation Framework"
                },
                "summary": "Retrieval Augmented Generation (RAG) has emerged as the de facto industry\nstandard for user-facing NLP applications, offering the ability to integrate\ndata without re-training or fine-tuning Large Language Models (LLMs). This\ncapability enhances the quality and accuracy of responses but also introduces\nnovel security and privacy challenges, particularly when sensitive data is\nintegrated. With the rapid adoption of RAG, securing data and services has\nbecome a critical priority. This paper first reviews the vulnerabilities of RAG\npipelines, and outlines the attack surface from data pre-processing and data\nstorage management to integration with LLMs. The identified risks are then\npaired with corresponding mitigations in a structured overview. In a second\nstep, the paper develops a framework that combines RAG-specific security\nconsiderations, with existing general security guidelines, industry standards,\nand best practices. The proposed framework aims to guide the implementation of\nrobust, compliant, secure, and trustworthy RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) has emerged as the de facto industry\nstandard for user-facing NLP applications, offering the ability to integrate\ndata without re-training or fine-tuning Large Language Models (LLMs). This\ncapability enhances the quality and accuracy of responses but also introduces\nnovel security and privacy challenges, particularly when sensitive data is\nintegrated. With the rapid adoption of RAG, securing data and services has\nbecome a critical priority. This paper first reviews the vulnerabilities of RAG\npipelines, and outlines the attack surface from data pre-processing and data\nstorage management to integration with LLMs. The identified risks are then\npaired with corresponding mitigations in a structured overview. In a second\nstep, the paper develops a framework that combines RAG-specific security\nconsiderations, with existing general security guidelines, industry standards,\nand best practices. The proposed framework aims to guide the implementation of\nrobust, compliant, secure, and trustworthy RAG systems."
                },
                "authors": [
                    {
                        "name": "Lukas Ammann"
                    },
                    {
                        "name": "Sara Ott"
                    },
                    {
                        "name": "Christoph R. Landolt"
                    },
                    {
                        "name": "Marco P. Lehmann"
                    }
                ],
                "author_detail": {
                    "name": "Marco P. Lehmann"
                },
                "author": "Marco P. Lehmann",
                "arxiv_comment": "8 pages, 3 figures, Sara Ott and Lukas Ammann contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08728v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08728v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02732v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02732v3",
                "updated": "2025-05-13T16:38:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    38,
                    34,
                    1,
                    133,
                    0
                ],
                "published": "2025-04-03T16:17:55Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    17,
                    55,
                    3,
                    93,
                    0
                ],
                "title": "Why do LLMs attend to the first token?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why do LLMs attend to the first token?"
                },
                "summary": "Large Language Models (LLMs) tend to attend heavily to the first token in the\nsequence -- creating a so-called attention sink. Many works have studied this\nphenomenon in detail, proposing various ways to either leverage or alleviate\nit. Attention sinks have been connected to quantisation difficulties, security\nissues, and streaming attention. Yet, while many works have provided conditions\nin which they occur or not, a critical question remains shallowly answered: Why\ndo LLMs learn such patterns and how are they being used? In this work, we argue\ntheoretically and empirically that this mechanism provides a method for LLMs to\navoid over-mixing, connecting this to existing lines of work that study\nmathematically how information propagates in Transformers. We conduct\nexperiments to validate our theoretical intuitions and show how choices such as\ncontext length, depth, and data packing influence the sink behaviour. We hope\nthat this study provides a new practical perspective on why attention sinks are\nuseful in LLMs, leading to a better understanding of the attention patterns\nthat form during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) tend to attend heavily to the first token in the\nsequence -- creating a so-called attention sink. Many works have studied this\nphenomenon in detail, proposing various ways to either leverage or alleviate\nit. Attention sinks have been connected to quantisation difficulties, security\nissues, and streaming attention. Yet, while many works have provided conditions\nin which they occur or not, a critical question remains shallowly answered: Why\ndo LLMs learn such patterns and how are they being used? In this work, we argue\ntheoretically and empirically that this mechanism provides a method for LLMs to\navoid over-mixing, connecting this to existing lines of work that study\nmathematically how information propagates in Transformers. We conduct\nexperiments to validate our theoretical intuitions and show how choices such as\ncontext length, depth, and data packing influence the sink behaviour. We hope\nthat this study provides a new practical perspective on why attention sinks are\nuseful in LLMs, leading to a better understanding of the attention patterns\nthat form during training."
                },
                "authors": [
                    {
                        "name": "Federico Barbero"
                    },
                    {
                        "name": "Álvaro Arroyo"
                    },
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Christos Perivolaropoulos"
                    },
                    {
                        "name": "Michael Bronstein"
                    },
                    {
                        "name": "Petar Veličković"
                    },
                    {
                        "name": "Razvan Pascanu"
                    }
                ],
                "author_detail": {
                    "name": "Razvan Pascanu"
                },
                "author": "Razvan Pascanu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02732v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02732v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08727v1",
                "updated": "2025-05-13T16:37:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    37,
                    54,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T16:37:54Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    37,
                    54,
                    1,
                    133,
                    0
                ],
                "title": "Memorization-Compression Cycles Improve Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memorization-Compression Cycles Improve Generalization"
                },
                "summary": "We prove theoretically that generalization improves not only through data\nscaling but also by compressing internal representations. To operationalize\nthis insight, we introduce the Information Bottleneck Language Modeling (IBLM)\nobjective, which reframes language modeling as a constrained optimization\nproblem: minimizing representation entropy subject to optimal prediction\nperformance. Empirically, we observe an emergent memorization-compression cycle\nduring LLM pretraining, evidenced by oscillation positive/negative gradient\nalignment between cross-entropy and Matrix-Based Entropy (MBE), a measure of\nrepresentation entropy. This pattern closely mirrors the predictive-compressive\ntrade-off prescribed by IBLM and also parallels the biological alternation\nbetween awake learning and sleep consolidation. Motivated by this observation,\nwe propose Gated Phase Transition (GAPT), a training algorithm that adaptively\nswitches between memorization and compression phases. When applied to GPT-2\npretraining on FineWeb dataset, GAPT reduces MBE by 50% and improves\ncross-entropy by 4.8%. GAPT improves OOD generalizatino by 35% in a pretraining\ntask on arithmetic multiplication. In a setting designed to simulate\ncatastrophic forgetting, GAPT reduces interference by compressing and\nseparating representations, achieving a 97% improvement in separation -\nparalleling the functional role of sleep consolidation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We prove theoretically that generalization improves not only through data\nscaling but also by compressing internal representations. To operationalize\nthis insight, we introduce the Information Bottleneck Language Modeling (IBLM)\nobjective, which reframes language modeling as a constrained optimization\nproblem: minimizing representation entropy subject to optimal prediction\nperformance. Empirically, we observe an emergent memorization-compression cycle\nduring LLM pretraining, evidenced by oscillation positive/negative gradient\nalignment between cross-entropy and Matrix-Based Entropy (MBE), a measure of\nrepresentation entropy. This pattern closely mirrors the predictive-compressive\ntrade-off prescribed by IBLM and also parallels the biological alternation\nbetween awake learning and sleep consolidation. Motivated by this observation,\nwe propose Gated Phase Transition (GAPT), a training algorithm that adaptively\nswitches between memorization and compression phases. When applied to GPT-2\npretraining on FineWeb dataset, GAPT reduces MBE by 50% and improves\ncross-entropy by 4.8%. GAPT improves OOD generalizatino by 35% in a pretraining\ntask on arithmetic multiplication. In a setting designed to simulate\ncatastrophic forgetting, GAPT reduces interference by compressing and\nseparating representations, achieving a 97% improvement in separation -\nparalleling the functional role of sleep consolidation."
                },
                "authors": [
                    {
                        "name": "Fangyuan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fangyuan Yu"
                },
                "author": "Fangyuan Yu",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00958v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00958v4",
                "updated": "2025-05-13T16:29:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    29,
                    8,
                    1,
                    133,
                    0
                ],
                "published": "2025-01-01T21:29:37Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    21,
                    29,
                    37,
                    2,
                    1,
                    0
                ],
                "title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2.5 Years in Class: A Multimodal Textbook for Vision-Language\n  Pretraining"
                },
                "summary": "Compared to image-text pair data, interleaved corpora enable Vision-Language\nModels (VLMs) to understand the world more naturally like humans. However, such\nexisting datasets are crawled from webpage, facing challenges like low\nknowledge density, loose image-text relations, and poor logical coherence\nbetween images. On the other hand, the internet hosts vast instructional videos\n(e.g., online geometry courses) that are widely used by humans to learn\nfoundational subjects, yet these valuable resources remain underexplored in VLM\ntraining. In this paper, we introduce a high-quality \\textbf{multimodal\ntextbook} corpus with richer foundational knowledge for VLM pretraining. It\ncollects over 2.5 years of instructional videos, totaling 22,000 class hours.\nWe first use an LLM-proposed taxonomy to systematically gather instructional\nvideos. Then we progressively extract and refine visual (keyframes), audio\n(ASR), and textual knowledge (OCR) from the videos, and organize as an\nimage-text interleaved corpus based on temporal order. Compared to its\ncounterparts, our video-centric textbook offers more coherent context, richer\nknowledge, and better image-text alignment. Experiments demonstrate its superb\npretraining performance, particularly in knowledge- and reasoning-intensive\ntasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook\nexhibit outstanding interleaved context awareness, leveraging visual and\ntextual cues in their few-shot context for task solving. Our code are available\nat https://github.com/DAMO-NLP-SG/multimodal_textbook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compared to image-text pair data, interleaved corpora enable Vision-Language\nModels (VLMs) to understand the world more naturally like humans. However, such\nexisting datasets are crawled from webpage, facing challenges like low\nknowledge density, loose image-text relations, and poor logical coherence\nbetween images. On the other hand, the internet hosts vast instructional videos\n(e.g., online geometry courses) that are widely used by humans to learn\nfoundational subjects, yet these valuable resources remain underexplored in VLM\ntraining. In this paper, we introduce a high-quality \\textbf{multimodal\ntextbook} corpus with richer foundational knowledge for VLM pretraining. It\ncollects over 2.5 years of instructional videos, totaling 22,000 class hours.\nWe first use an LLM-proposed taxonomy to systematically gather instructional\nvideos. Then we progressively extract and refine visual (keyframes), audio\n(ASR), and textual knowledge (OCR) from the videos, and organize as an\nimage-text interleaved corpus based on temporal order. Compared to its\ncounterparts, our video-centric textbook offers more coherent context, richer\nknowledge, and better image-text alignment. Experiments demonstrate its superb\npretraining performance, particularly in knowledge- and reasoning-intensive\ntasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook\nexhibit outstanding interleaved context awareness, leveraging visual and\ntextual cues in their few-shot context for task solving. Our code are available\nat https://github.com/DAMO-NLP-SG/multimodal_textbook."
                },
                "authors": [
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Jiashuo Sun"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Deli Zhao"
                    },
                    {
                        "name": "Yueting Zhuang"
                    },
                    {
                        "name": "Lidong Bing"
                    }
                ],
                "author_detail": {
                    "name": "Lidong Bing"
                },
                "author": "Lidong Bing",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00958v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00958v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08719v1",
                "updated": "2025-05-13T16:27:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    27,
                    7,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T16:27:07Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    27,
                    7,
                    1,
                    133,
                    0
                ],
                "title": "PWC-MoE: Privacy-Aware Wireless Collaborative Mixture of Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PWC-MoE: Privacy-Aware Wireless Collaborative Mixture of Experts"
                },
                "summary": "Large language models (LLMs) hosted on cloud servers alleviate the\ncomputational and storage burdens on local devices but raise privacy concerns\ndue to sensitive data transmission and require substantial communication\nbandwidth, which is challenging in constrained environments. In contrast, small\nlanguage models (SLMs) running locally enhance privacy but suffer from limited\nperformance on complex tasks. To balance computational cost, performance, and\nprivacy protection under bandwidth constraints, we propose a privacy-aware\nwireless collaborative mixture of experts (PWC-MoE) framework. Specifically,\nPWC-MoE employs a sparse privacy-aware gating network to dynamically route\nsensitive tokens to privacy experts located on local clients, while\nnon-sensitive tokens are routed to non-privacy experts located at the remote\nbase station. To achieve computational efficiency, the gating network ensures\nthat each token is dynamically routed to and processed by only one expert. To\nenhance scalability and prevent overloading of specific experts, we introduce a\ngroup-wise load-balancing mechanism for the gating network that evenly\ndistributes sensitive tokens among privacy experts and non-sensitive tokens\namong non-privacy experts. To adapt to bandwidth constraints while preserving\nmodel performance, we propose a bandwidth-adaptive and importance-aware token\noffloading scheme. This scheme incorporates an importance predictor to evaluate\nthe importance scores of non-sensitive tokens, prioritizing the most important\ntokens for transmission to the base station based on their predicted importance\nand the available bandwidth. Experiments demonstrate that the PWC-MoE framework\neffectively preserves privacy and maintains high performance even in\nbandwidth-constrained environments, offering a practical solution for deploying\nLLMs in privacy-sensitive and bandwidth-limited scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) hosted on cloud servers alleviate the\ncomputational and storage burdens on local devices but raise privacy concerns\ndue to sensitive data transmission and require substantial communication\nbandwidth, which is challenging in constrained environments. In contrast, small\nlanguage models (SLMs) running locally enhance privacy but suffer from limited\nperformance on complex tasks. To balance computational cost, performance, and\nprivacy protection under bandwidth constraints, we propose a privacy-aware\nwireless collaborative mixture of experts (PWC-MoE) framework. Specifically,\nPWC-MoE employs a sparse privacy-aware gating network to dynamically route\nsensitive tokens to privacy experts located on local clients, while\nnon-sensitive tokens are routed to non-privacy experts located at the remote\nbase station. To achieve computational efficiency, the gating network ensures\nthat each token is dynamically routed to and processed by only one expert. To\nenhance scalability and prevent overloading of specific experts, we introduce a\ngroup-wise load-balancing mechanism for the gating network that evenly\ndistributes sensitive tokens among privacy experts and non-sensitive tokens\namong non-privacy experts. To adapt to bandwidth constraints while preserving\nmodel performance, we propose a bandwidth-adaptive and importance-aware token\noffloading scheme. This scheme incorporates an importance predictor to evaluate\nthe importance scores of non-sensitive tokens, prioritizing the most important\ntokens for transmission to the base station based on their predicted importance\nand the available bandwidth. Experiments demonstrate that the PWC-MoE framework\neffectively preserves privacy and maintains high performance even in\nbandwidth-constrained environments, offering a practical solution for deploying\nLLMs in privacy-sensitive and bandwidth-limited scenarios."
                },
                "authors": [
                    {
                        "name": "Yang Su"
                    },
                    {
                        "name": "Na Yan"
                    },
                    {
                        "name": "Yansha Deng"
                    },
                    {
                        "name": "Robert Schober"
                    }
                ],
                "author_detail": {
                    "name": "Robert Schober"
                },
                "author": "Robert Schober",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.01233v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.01233v2",
                "updated": "2025-05-13T16:24:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    24,
                    57,
                    1,
                    133,
                    0
                ],
                "published": "2023-02-02T17:14:54Z",
                "published_parsed": [
                    2023,
                    2,
                    2,
                    17,
                    14,
                    54,
                    3,
                    33,
                    0
                ],
                "title": "Sparse High-Dimensional Vector Autoregressive Bootstrap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse High-Dimensional Vector Autoregressive Bootstrap"
                },
                "summary": "We introduce a high-dimensional multiplier bootstrap for time series data\nbased on capturing dependence through a sparsely estimated vector\nautoregressive model. We prove its consistency for inference on\nhigh-dimensional means under two different moment assumptions on the errors,\nnamely sub-gaussian moments and a finite number of absolute moments. In\nestablishing these results, we derive a Gaussian approximation for the maximum\nmean of a linear process, which may be of independent interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a high-dimensional multiplier bootstrap for time series data\nbased on capturing dependence through a sparsely estimated vector\nautoregressive model. We prove its consistency for inference on\nhigh-dimensional means under two different moment assumptions on the errors,\nnamely sub-gaussian moments and a finite number of absolute moments. In\nestablishing these results, we derive a Gaussian approximation for the maximum\nmean of a linear process, which may be of independent interest."
                },
                "authors": [
                    {
                        "name": "Robert Adamek"
                    },
                    {
                        "name": "Stephan Smeekes"
                    },
                    {
                        "name": "Ines Wilms"
                    }
                ],
                "author_detail": {
                    "name": "Ines Wilms"
                },
                "author": "Ines Wilms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.01233v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.01233v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00651v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00651v2",
                "updated": "2025-05-13T16:24:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    24,
                    54,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-01T16:54:21Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    16,
                    54,
                    21,
                    3,
                    121,
                    0
                ],
                "title": "Open-Source LLM-Driven Federated Transformer for Predictive IoV\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-Source LLM-Driven Federated Transformer for Predictive IoV\n  Management"
                },
                "summary": "The proliferation of connected vehicles within the Internet of Vehicles (IoV)\necosystem presents critical challenges in ensuring scalable, real-time, and\nprivacy-preserving traffic management. Existing centralized IoV solutions often\nsuffer from high latency, limited scalability, and reliance on proprietary\nArtificial Intelligence (AI) models, creating significant barriers to\nwidespread deployment, particularly in dynamic and privacy-sensitive\nenvironments. Meanwhile, integrating Large Language Models (LLMs) in vehicular\nsystems remains underexplored, especially concerning prompt optimization and\neffective utilization in federated contexts. To address these challenges, we\npropose the Federated Prompt-Optimized Traffic Transformer (FPoTT), a novel\nframework that leverages open-source LLMs for predictive IoV management. FPoTT\nintroduces a dynamic prompt optimization mechanism that iteratively refines\ntextual prompts to enhance trajectory prediction. The architecture employs a\ndual-layer federated learning paradigm, combining lightweight edge models for\nreal-time inference with cloud-based LLMs to retain global intelligence. A\nTransformer-driven synthetic data generator is incorporated to augment training\nwith diverse, high-fidelity traffic scenarios in the Next Generation Simulation\n(NGSIM) format. Extensive evaluations demonstrate that FPoTT, utilizing\nEleutherAI Pythia-1B, achieves 99.86% prediction accuracy on real-world data\nwhile maintaining high performance on synthetic datasets. These results\nunderscore the potential of open-source LLMs in enabling secure, adaptive, and\nscalable IoV management, offering a promising alternative to proprietary\nsolutions in smart mobility ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of connected vehicles within the Internet of Vehicles (IoV)\necosystem presents critical challenges in ensuring scalable, real-time, and\nprivacy-preserving traffic management. Existing centralized IoV solutions often\nsuffer from high latency, limited scalability, and reliance on proprietary\nArtificial Intelligence (AI) models, creating significant barriers to\nwidespread deployment, particularly in dynamic and privacy-sensitive\nenvironments. Meanwhile, integrating Large Language Models (LLMs) in vehicular\nsystems remains underexplored, especially concerning prompt optimization and\neffective utilization in federated contexts. To address these challenges, we\npropose the Federated Prompt-Optimized Traffic Transformer (FPoTT), a novel\nframework that leverages open-source LLMs for predictive IoV management. FPoTT\nintroduces a dynamic prompt optimization mechanism that iteratively refines\ntextual prompts to enhance trajectory prediction. The architecture employs a\ndual-layer federated learning paradigm, combining lightweight edge models for\nreal-time inference with cloud-based LLMs to retain global intelligence. A\nTransformer-driven synthetic data generator is incorporated to augment training\nwith diverse, high-fidelity traffic scenarios in the Next Generation Simulation\n(NGSIM) format. Extensive evaluations demonstrate that FPoTT, utilizing\nEleutherAI Pythia-1B, achieves 99.86% prediction accuracy on real-world data\nwhile maintaining high performance on synthetic datasets. These results\nunderscore the potential of open-source LLMs in enabling secure, adaptive, and\nscalable IoV management, offering a promising alternative to proprietary\nsolutions in smart mobility ecosystems."
                },
                "authors": [
                    {
                        "name": "Yazan Otoum"
                    },
                    {
                        "name": "Arghavan Asad"
                    },
                    {
                        "name": "Ishtiaq Ahmad"
                    }
                ],
                "author_detail": {
                    "name": "Ishtiaq Ahmad"
                },
                "author": "Ishtiaq Ahmad",
                "arxiv_comment": "Preprint version; submitted for academic peer review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00651v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00651v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01555v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01555v3",
                "updated": "2025-05-13T16:15:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    15,
                    43,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-02T19:47:30Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    19,
                    47,
                    30,
                    4,
                    122,
                    0
                ],
                "title": "Structured dataset of reported cloud seeding activities in the United\n  States (2000 to 2025) using a large language model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured dataset of reported cloud seeding activities in the United\n  States (2000 to 2025) using a large language model"
                },
                "summary": "Cloud seeding, a weather modification technique used to increase\nprecipitation, has been employed in the western United States since the 1940s.\nHowever, structured datasets are not currently available to analyze these\nefforts. To address this gap, we present a structured dataset of reported cloud\nseeding activities in the U.S. from 2000 to 2025, including the year, season,\nstate, seeding agent, apparatus used for deployment, and purpose. Using\nOpenAI's o4-mini large language model (LLM), combined with multi-stage\nPDF-to-text conversion and response-parsing code, we processed 836 historical\nreports from the National Oceanic and Atmospheric Administration (NOAA) to\nextract the data. The resulting dataset achieved 94.72% human-verified accuracy\nacross all fields and is publicly available on Zenodo. Our results help fill\nthe gap in structured cloud seeding data and demonstrate the potential for LLMs\nto extract structured environmental data from historical documents. More\nbroadly, this work provides a scalable framework for unlocking historical data\nfrom scanned documents across scientific domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud seeding, a weather modification technique used to increase\nprecipitation, has been employed in the western United States since the 1940s.\nHowever, structured datasets are not currently available to analyze these\nefforts. To address this gap, we present a structured dataset of reported cloud\nseeding activities in the U.S. from 2000 to 2025, including the year, season,\nstate, seeding agent, apparatus used for deployment, and purpose. Using\nOpenAI's o4-mini large language model (LLM), combined with multi-stage\nPDF-to-text conversion and response-parsing code, we processed 836 historical\nreports from the National Oceanic and Atmospheric Administration (NOAA) to\nextract the data. The resulting dataset achieved 94.72% human-verified accuracy\nacross all fields and is publicly available on Zenodo. Our results help fill\nthe gap in structured cloud seeding data and demonstrate the potential for LLMs\nto extract structured environmental data from historical documents. More\nbroadly, this work provides a scalable framework for unlocking historical data\nfrom scanned documents across scientific domains."
                },
                "authors": [
                    {
                        "name": "Jared Joseph Donohue"
                    },
                    {
                        "name": "Kara D. Lamb"
                    }
                ],
                "author_detail": {
                    "name": "Kara D. Lamb"
                },
                "author": "Kara D. Lamb",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01555v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01555v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08704v1",
                "updated": "2025-05-13T16:11:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    11,
                    29,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T16:11:29Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    11,
                    29,
                    1,
                    133,
                    0
                ],
                "title": "LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from\n  EHRs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from\n  EHRs"
                },
                "summary": "Electronic Health Records (EHRs) are digital records of patient information,\noften containing unstructured clinical text. Named Entity Recognition (NER) is\nessential in EHRs for extracting key medical entities like problems, tests, and\ntreatments to support downstream clinical applications. This paper explores\nprompt-based medical entity recognition using large language models (LLMs),\nspecifically GPT-4o and DeepSeek-R1, guided by various prompt engineering\ntechniques, including zero-shot, few-shot, and an ensemble approach. Among all\nstrategies, GPT-4o with prompt ensemble achieved the highest classification\nperformance with an F1-score of 0.95 and recall of 0.98, outperforming\nDeepSeek-R1 on the task. The ensemble method improved reliability by\naggregating outputs through embedding-based similarity and majority voting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic Health Records (EHRs) are digital records of patient information,\noften containing unstructured clinical text. Named Entity Recognition (NER) is\nessential in EHRs for extracting key medical entities like problems, tests, and\ntreatments to support downstream clinical applications. This paper explores\nprompt-based medical entity recognition using large language models (LLMs),\nspecifically GPT-4o and DeepSeek-R1, guided by various prompt engineering\ntechniques, including zero-shot, few-shot, and an ensemble approach. Among all\nstrategies, GPT-4o with prompt ensemble achieved the highest classification\nperformance with an F1-score of 0.95 and recall of 0.98, outperforming\nDeepSeek-R1 on the task. The ensemble method improved reliability by\naggregating outputs through embedding-based similarity and majority voting."
                },
                "authors": [
                    {
                        "name": "K M Sajjadul Islam"
                    },
                    {
                        "name": "Ayesha Siddika Nipu"
                    },
                    {
                        "name": "Jiawei Wu"
                    },
                    {
                        "name": "Praveen Madiraju"
                    }
                ],
                "author_detail": {
                    "name": "Praveen Madiraju"
                },
                "author": "Praveen Madiraju",
                "arxiv_comment": "IEEE 26th International Conference on Information Reuse and\n  Integration for Data Science (IRI 2025), San Jose, CA, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08379v2",
                "updated": "2025-05-13T16:08:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    8,
                    10,
                    1,
                    133,
                    0
                ],
                "published": "2024-09-12T19:59:54Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    19,
                    59,
                    54,
                    3,
                    256,
                    0
                ],
                "title": "The Impact of Large Language Models on Open-source Innovation: Evidence\n  from GitHub Copilot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Large Language Models on Open-source Innovation: Evidence\n  from GitHub Copilot"
                },
                "summary": "Large Language Models (LLMs) have been shown to enhance individual\nproductivity in guided settings. Whereas LLMs are likely to also transform\ninnovation processes in a collaborative work setting, it is unclear what\ntrajectory this transformation will follow. Innovation in these contexts\nencompasses both capability innovation that explores new possibilities by\nacquiring new competencies in a project and iterative innovation that exploits\nexisting foundations by enhancing established competencies and improving\nproject quality. Whether LLMs affect these two aspects of collaborative work\nand to what extent is an open empirical question. Open-source development\nprovides an ideal setting to examine LLM impacts on these innovation types, as\nits voluntary and open/collaborative nature of contributions provides the\ngreatest opportunity for technological augmentation. We focus on open-source\nprojects on GitHub by leveraging a natural experiment around the selective\nrollout of GitHub Copilot (a programming-focused LLM) in October 2021, where\nGitHub Copilot selectively supported programming languages like Python or Rust,\nbut not R or Haskell. We observe a significant jump in overall contributions,\nsuggesting that LLMs effectively augment collaborative innovation in an\nunguided setting. Interestingly, Copilot's launch increased iterative\ninnovation focused on maintenance-related or feature-refining contributions\nsignificantly more than it did capability innovation through code-development\nor feature-introducing commits. This disparity was more pronounced after the\nmodel upgrade in June 2022 and was evident in active projects with extensive\ncoding activity, suggesting that as both LLM capabilities and/or available\ncontextual information improve, the gap between capability and iterative\ninnovation may widen. We discuss practical and policy implications to\nincentivize high-value innovative solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been shown to enhance individual\nproductivity in guided settings. Whereas LLMs are likely to also transform\ninnovation processes in a collaborative work setting, it is unclear what\ntrajectory this transformation will follow. Innovation in these contexts\nencompasses both capability innovation that explores new possibilities by\nacquiring new competencies in a project and iterative innovation that exploits\nexisting foundations by enhancing established competencies and improving\nproject quality. Whether LLMs affect these two aspects of collaborative work\nand to what extent is an open empirical question. Open-source development\nprovides an ideal setting to examine LLM impacts on these innovation types, as\nits voluntary and open/collaborative nature of contributions provides the\ngreatest opportunity for technological augmentation. We focus on open-source\nprojects on GitHub by leveraging a natural experiment around the selective\nrollout of GitHub Copilot (a programming-focused LLM) in October 2021, where\nGitHub Copilot selectively supported programming languages like Python or Rust,\nbut not R or Haskell. We observe a significant jump in overall contributions,\nsuggesting that LLMs effectively augment collaborative innovation in an\nunguided setting. Interestingly, Copilot's launch increased iterative\ninnovation focused on maintenance-related or feature-refining contributions\nsignificantly more than it did capability innovation through code-development\nor feature-introducing commits. This disparity was more pronounced after the\nmodel upgrade in June 2022 and was evident in active projects with extensive\ncoding activity, suggesting that as both LLM capabilities and/or available\ncontextual information improve, the gap between capability and iterative\ninnovation may widen. We discuss practical and policy implications to\nincentivize high-value innovative solutions."
                },
                "authors": [
                    {
                        "name": "Doron Yeverechyahu"
                    },
                    {
                        "name": "Raveesh Mayya"
                    },
                    {
                        "name": "Gal Oestreicher-Singer"
                    }
                ],
                "author_detail": {
                    "name": "Gal Oestreicher-Singer"
                },
                "author": "Gal Oestreicher-Singer",
                "arxiv_comment": "JEL Classification: O31, C88, J24, O35, L86",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; D.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17669v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17669v2",
                "updated": "2025-05-13T15:59:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    59,
                    47,
                    1,
                    133,
                    0
                ],
                "published": "2024-05-27T21:47:41Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    21,
                    47,
                    41,
                    0,
                    148,
                    0
                ],
                "title": "Bayesian Nonparametrics for Principal Stratification with Continuous\n  Post-Treatment Variables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Nonparametrics for Principal Stratification with Continuous\n  Post-Treatment Variables"
                },
                "summary": "Principal stratification provides a causal inference framework for\ninvestigating treatment effects in the presence of a post-treatment variable.\nPrincipal strata play a key role in characterizing the treatment effect by\nidentifying groups of units with the same or similar values for the potential\npost-treatment variable under both treatment levels. The literature has focused\nmainly on binary post-treatment variables. Few papers considered continuous\npost-treatment variables. In the presence of a continuous post-treatment, a\nchallenge is how to identify and characterize meaningful coarsening of the\nlatent principal strata that lead to interpretable principal causal effects.\nThis paper introduces the confounders-aware shared-atom Bayesian mixture, a\nnovel approach for principal stratification with binary treatment and\ncontinuous post-treatment variables. Our method leverages Bayesian\nnonparametric priors with an innovative hierarchical structure for the\npotential post-treatment variable that overcomes some of the limitations of\nprevious works. Specifically, the novel features of our method allow for (i)\nidentifying coarsened principal strata through a data-adaptive approach and\n(ii) providing a comprehensive quantification of the uncertainty surrounding\nstratum membership. Through Monte Carlo simulations, we show that the proposed\nmethodology performs better than existing methods in characterizing the\nprincipal strata and estimating principal effects of the treatment. Finally,\nour proposed model is applied to a case study in which we estimate the causal\neffects of US national air quality regulations on pollution levels and health\noutcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Principal stratification provides a causal inference framework for\ninvestigating treatment effects in the presence of a post-treatment variable.\nPrincipal strata play a key role in characterizing the treatment effect by\nidentifying groups of units with the same or similar values for the potential\npost-treatment variable under both treatment levels. The literature has focused\nmainly on binary post-treatment variables. Few papers considered continuous\npost-treatment variables. In the presence of a continuous post-treatment, a\nchallenge is how to identify and characterize meaningful coarsening of the\nlatent principal strata that lead to interpretable principal causal effects.\nThis paper introduces the confounders-aware shared-atom Bayesian mixture, a\nnovel approach for principal stratification with binary treatment and\ncontinuous post-treatment variables. Our method leverages Bayesian\nnonparametric priors with an innovative hierarchical structure for the\npotential post-treatment variable that overcomes some of the limitations of\nprevious works. Specifically, the novel features of our method allow for (i)\nidentifying coarsened principal strata through a data-adaptive approach and\n(ii) providing a comprehensive quantification of the uncertainty surrounding\nstratum membership. Through Monte Carlo simulations, we show that the proposed\nmethodology performs better than existing methods in characterizing the\nprincipal strata and estimating principal effects of the treatment. Finally,\nour proposed model is applied to a case study in which we estimate the causal\neffects of US national air quality regulations on pollution levels and health\noutcomes."
                },
                "authors": [
                    {
                        "name": "Dafne Zorzetto"
                    },
                    {
                        "name": "Antonio Canale"
                    },
                    {
                        "name": "Fabrizia Mealli"
                    },
                    {
                        "name": "Francesca Dominici"
                    },
                    {
                        "name": "Falco J. Bargagli-Stoffi"
                    }
                ],
                "author_detail": {
                    "name": "Falco J. Bargagli-Stoffi"
                },
                "author": "Falco J. Bargagli-Stoffi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17669v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17669v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08699v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08699v2",
                "updated": "2025-05-14T02:10:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    10,
                    29,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-13T15:58:57Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    58,
                    57,
                    1,
                    133,
                    0
                ],
                "title": "Granite-speech: open-source speech-aware LLMs with strong English ASR\n  capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Granite-speech: open-source speech-aware LLMs with strong English ASR\n  capabilities"
                },
                "summary": "Granite-speech LLMs are compact and efficient speech language models\nspecifically designed for English ASR and automatic speech translation (AST).\nThe models were trained by modality aligning the 2B and 8B parameter variants\nof granite-3.3-instruct to speech on publicly available open-source corpora\ncontaining audio inputs and text targets consisting of either human transcripts\nfor ASR or automatically generated translations for AST. Comprehensive\nbenchmarking shows that on English ASR, which was our primary focus, they\noutperform several competitors' models that were trained on orders of magnitude\nmore proprietary data, and they keep pace on English-to-X AST for major\nEuropean languages, Japanese, and Chinese. The speech-specific components are:\na conformer acoustic encoder using block attention and self-conditioning\ntrained with connectionist temporal classification, a windowed\nquery-transformer speech modality adapter used to do temporal downsampling of\nthe acoustic embeddings and map them to the LLM text embedding space, and LoRA\nadapters to further fine-tune the text LLM. Granite-speech-3.3 operates in two\nmodes: in speech mode, it performs ASR and AST by activating the encoder,\nprojector, and LoRA adapters; in text mode, it calls the underlying\ngranite-3.3-instruct model directly (without LoRA), essentially preserving all\nthe text LLM capabilities and safety. Both models are freely available on\nHuggingFace (https://huggingface.co/ibm-granite/granite-speech-3.3-2b and\nhttps://huggingface.co/ibm-granite/granite-speech-3.3-8b) and can be used for\nboth research and commercial purposes under a permissive Apache 2.0 license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Granite-speech LLMs are compact and efficient speech language models\nspecifically designed for English ASR and automatic speech translation (AST).\nThe models were trained by modality aligning the 2B and 8B parameter variants\nof granite-3.3-instruct to speech on publicly available open-source corpora\ncontaining audio inputs and text targets consisting of either human transcripts\nfor ASR or automatically generated translations for AST. Comprehensive\nbenchmarking shows that on English ASR, which was our primary focus, they\noutperform several competitors' models that were trained on orders of magnitude\nmore proprietary data, and they keep pace on English-to-X AST for major\nEuropean languages, Japanese, and Chinese. The speech-specific components are:\na conformer acoustic encoder using block attention and self-conditioning\ntrained with connectionist temporal classification, a windowed\nquery-transformer speech modality adapter used to do temporal downsampling of\nthe acoustic embeddings and map them to the LLM text embedding space, and LoRA\nadapters to further fine-tune the text LLM. Granite-speech-3.3 operates in two\nmodes: in speech mode, it performs ASR and AST by activating the encoder,\nprojector, and LoRA adapters; in text mode, it calls the underlying\ngranite-3.3-instruct model directly (without LoRA), essentially preserving all\nthe text LLM capabilities and safety. Both models are freely available on\nHuggingFace (https://huggingface.co/ibm-granite/granite-speech-3.3-2b and\nhttps://huggingface.co/ibm-granite/granite-speech-3.3-8b) and can be used for\nboth research and commercial purposes under a permissive Apache 2.0 license."
                },
                "authors": [
                    {
                        "name": "George Saon"
                    },
                    {
                        "name": "Avihu Dekel"
                    },
                    {
                        "name": "Alexander Brooks"
                    },
                    {
                        "name": "Tohru Nagano"
                    },
                    {
                        "name": "Abraham Daniels"
                    },
                    {
                        "name": "Aharon Satt"
                    },
                    {
                        "name": "Ashish Mittal"
                    },
                    {
                        "name": "Brian Kingsbury"
                    },
                    {
                        "name": "David Haws"
                    },
                    {
                        "name": "Edmilson Morais"
                    },
                    {
                        "name": "Gakuto Kurata"
                    },
                    {
                        "name": "Hagai Aronowitz"
                    },
                    {
                        "name": "Ibrahim Ibrahim"
                    },
                    {
                        "name": "Jeff Kuo"
                    },
                    {
                        "name": "Kate Soule"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Masayuki Suzuki"
                    },
                    {
                        "name": "Ron Hoory"
                    },
                    {
                        "name": "Samuel Thomas"
                    },
                    {
                        "name": "Sashi Novitasari"
                    },
                    {
                        "name": "Takashi Fukuda"
                    },
                    {
                        "name": "Vishal Sunder"
                    },
                    {
                        "name": "Xiaodong Cui"
                    },
                    {
                        "name": "Zvi Kons"
                    }
                ],
                "author_detail": {
                    "name": "Zvi Kons"
                },
                "author": "Zvi Kons",
                "arxiv_comment": "7 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08699v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08699v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08695v1",
                "updated": "2025-05-13T15:54:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    54,
                    36,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T15:54:36Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    54,
                    36,
                    1,
                    133,
                    0
                ],
                "title": "SPAST: Arbitrary Style Transfer with Style Priors via Pre-trained\n  Large-scale Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPAST: Arbitrary Style Transfer with Style Priors via Pre-trained\n  Large-scale Model"
                },
                "summary": "Given an arbitrary content and style image, arbitrary style transfer aims to\nrender a new stylized\n  image which preserves the content image's structure and possesses the style\nimage's style. Existing\n  arbitrary style transfer methods are based on either small models or\npre-trained large-scale models.\n  The small model-based methods fail to generate high-quality stylized images,\nbringing artifacts and\n  disharmonious patterns. The pre-trained large-scale model-based methods can\ngenerate high-quality\n  stylized images but struggle to preserve the content structure and cost long\ninference time. To this\n  end, we propose a new framework, called SPAST, to generate high-quality\nstylized images with\n  less inference time. Specifically, we design a novel Local-global Window Size\nStylization Module\n  (LGWSSM)tofuse style features into content features. Besides, we introduce a\nnovel style prior loss,\n  which can dig out the style priors from a pre-trained large-scale model into\nthe SPAST and motivate\n  the SPAST to generate high-quality stylized images with short inference\ntime.We conduct abundant\n  experiments to verify that our proposed method can generate high-quality\nstylized images and less\n  inference time compared with the SOTA arbitrary style transfer methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given an arbitrary content and style image, arbitrary style transfer aims to\nrender a new stylized\n  image which preserves the content image's structure and possesses the style\nimage's style. Existing\n  arbitrary style transfer methods are based on either small models or\npre-trained large-scale models.\n  The small model-based methods fail to generate high-quality stylized images,\nbringing artifacts and\n  disharmonious patterns. The pre-trained large-scale model-based methods can\ngenerate high-quality\n  stylized images but struggle to preserve the content structure and cost long\ninference time. To this\n  end, we propose a new framework, called SPAST, to generate high-quality\nstylized images with\n  less inference time. Specifically, we design a novel Local-global Window Size\nStylization Module\n  (LGWSSM)tofuse style features into content features. Besides, we introduce a\nnovel style prior loss,\n  which can dig out the style priors from a pre-trained large-scale model into\nthe SPAST and motivate\n  the SPAST to generate high-quality stylized images with short inference\ntime.We conduct abundant\n  experiments to verify that our proposed method can generate high-quality\nstylized images and less\n  inference time compared with the SOTA arbitrary style transfer methods."
                },
                "authors": [
                    {
                        "name": "Zhanjie Zhang"
                    },
                    {
                        "name": "Quanwei Zhang"
                    },
                    {
                        "name": "Junsheng Luan"
                    },
                    {
                        "name": "Mengyuan Yang"
                    },
                    {
                        "name": "Yun Wang"
                    },
                    {
                        "name": "Lei Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhao"
                },
                "author": "Lei Zhao",
                "arxiv_comment": "Accepted by Neural Networks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08690v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08690v1",
                "updated": "2025-05-13T15:47:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    47,
                    54,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T15:47:54Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    47,
                    54,
                    1,
                    133,
                    0
                ],
                "title": "Adaptive Schema-aware Event Extraction with Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Schema-aware Event Extraction with Retrieval-Augmented\n  Generation"
                },
                "summary": "Event extraction (EE) is a fundamental task in natural language processing\n(NLP) that involves identifying and extracting event information from\nunstructured text. Effective EE in real-world scenarios requires two key steps:\nselecting appropriate schemas from hundreds of candidates and executing the\nextraction process. Existing research exhibits two critical gaps: (1) the rigid\nschema fixation in existing pipeline systems, and (2) the absence of benchmarks\nfor evaluating joint schema matching and extraction. Although large language\nmodels (LLMs) offer potential solutions, their schema hallucination tendencies\nand context window limitations pose challenges for practical deployment. In\nresponse, we propose Adaptive Schema-aware Event Extraction (ASEE), a novel\nparadigm combining schema paraphrasing with schema retrieval-augmented\ngeneration. ASEE adeptly retrieves paraphrased schemas and accurately generates\ntargeted structures. To facilitate rigorous evaluation, we construct the\nMulti-Dimensional Schema-aware Event Extraction (MD-SEE) benchmark, which\nsystematically consolidates 12 datasets across diverse domains, complexity\nlevels, and language settings. Extensive evaluations on MD-SEE show that our\nproposed ASEE demonstrates strong adaptability across various scenarios,\nsignificantly improving the accuracy of event extraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event extraction (EE) is a fundamental task in natural language processing\n(NLP) that involves identifying and extracting event information from\nunstructured text. Effective EE in real-world scenarios requires two key steps:\nselecting appropriate schemas from hundreds of candidates and executing the\nextraction process. Existing research exhibits two critical gaps: (1) the rigid\nschema fixation in existing pipeline systems, and (2) the absence of benchmarks\nfor evaluating joint schema matching and extraction. Although large language\nmodels (LLMs) offer potential solutions, their schema hallucination tendencies\nand context window limitations pose challenges for practical deployment. In\nresponse, we propose Adaptive Schema-aware Event Extraction (ASEE), a novel\nparadigm combining schema paraphrasing with schema retrieval-augmented\ngeneration. ASEE adeptly retrieves paraphrased schemas and accurately generates\ntargeted structures. To facilitate rigorous evaluation, we construct the\nMulti-Dimensional Schema-aware Event Extraction (MD-SEE) benchmark, which\nsystematically consolidates 12 datasets across diverse domains, complexity\nlevels, and language settings. Extensive evaluations on MD-SEE show that our\nproposed ASEE demonstrates strong adaptability across various scenarios,\nsignificantly improving the accuracy of event extraction."
                },
                "authors": [
                    {
                        "name": "Sheng Liang"
                    },
                    {
                        "name": "Hang Lv"
                    },
                    {
                        "name": "Zhihao Wen"
                    },
                    {
                        "name": "Yaxiong Wu"
                    },
                    {
                        "name": "Yongyue Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08690v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18970v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18970v2",
                "updated": "2025-05-13T15:46:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    46,
                    33,
                    1,
                    133,
                    0
                ],
                "published": "2025-03-22T01:55:32Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    55,
                    32,
                    5,
                    81,
                    0
                ],
                "title": "From S4 to Mamba: A Comprehensive Survey on Structured State Space\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From S4 to Mamba: A Comprehensive Survey on Structured State Space\n  Models"
                },
                "summary": "Recent advancements in sequence modeling have led to the emergence of\nStructured State Space Models (SSMs) as an efficient alternative to Recurrent\nNeural Networks (RNNs) and Transformers, addressing challenges in long-range\ndependency modeling and computational efficiency. While RNNs suffer from\nvanishing gradients and sequential inefficiencies, and Transformers face\nquadratic complexity, SSMs leverage structured recurrence and state-space\nrepresentations to achieve superior long-sequence processing with linear or\nnear-linear complexity. This survey provides a comprehensive review of SSMs,\ntracing their evolution from the foundational S4 model to its successors like\nMamba, Simplified Structured State Space Sequence Model (S5), and Jamba,\nhighlighting their improvements in computational efficiency, memory\noptimization, and inference speed. By comparing SSMs with traditional sequence\nmodels across domains such as natural language processing (NLP), speech\nrecognition, vision, and time-series forecasting, we demonstrate their\nadvantages in handling long-range dependencies while reducing computational\noverhead. Despite their potential, challenges remain in areas such as training\noptimization, hybrid modeling, and interpretability. This survey serves as a\nstructured guide for researchers and practitioners, detailing the advancements,\ntrade-offs, and future directions of SSM-based architectures in AI and deep\nlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in sequence modeling have led to the emergence of\nStructured State Space Models (SSMs) as an efficient alternative to Recurrent\nNeural Networks (RNNs) and Transformers, addressing challenges in long-range\ndependency modeling and computational efficiency. While RNNs suffer from\nvanishing gradients and sequential inefficiencies, and Transformers face\nquadratic complexity, SSMs leverage structured recurrence and state-space\nrepresentations to achieve superior long-sequence processing with linear or\nnear-linear complexity. This survey provides a comprehensive review of SSMs,\ntracing their evolution from the foundational S4 model to its successors like\nMamba, Simplified Structured State Space Sequence Model (S5), and Jamba,\nhighlighting their improvements in computational efficiency, memory\noptimization, and inference speed. By comparing SSMs with traditional sequence\nmodels across domains such as natural language processing (NLP), speech\nrecognition, vision, and time-series forecasting, we demonstrate their\nadvantages in handling long-range dependencies while reducing computational\noverhead. Despite their potential, challenges remain in areas such as training\noptimization, hybrid modeling, and interpretability. This survey serves as a\nstructured guide for researchers and practitioners, detailing the advancements,\ntrade-offs, and future directions of SSM-based architectures in AI and deep\nlearning."
                },
                "authors": [
                    {
                        "name": "Shriyank Somvanshi"
                    },
                    {
                        "name": "Md Monzurul Islam"
                    },
                    {
                        "name": "Mahmuda Sultana Mimi"
                    },
                    {
                        "name": "Sazzad Bin Bashar Polock"
                    },
                    {
                        "name": "Gaurab Chhetri"
                    },
                    {
                        "name": "Subasish Das"
                    }
                ],
                "author_detail": {
                    "name": "Subasish Das"
                },
                "author": "Subasish Das",
                "arxiv_comment": "30 pages, 8 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18970v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18970v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08683v1",
                "updated": "2025-05-13T15:44:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    44,
                    10,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T15:44:10Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    44,
                    10,
                    1,
                    133,
                    0
                ],
                "title": "Uncertainty-Aware Surrogate-based Amortized Bayesian Inference for\n  Computationally Expensive Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-Aware Surrogate-based Amortized Bayesian Inference for\n  Computationally Expensive Models"
                },
                "summary": "Bayesian inference typically relies on a large number of model evaluations to\nestimate posterior distributions. Established methods like Markov Chain Monte\nCarlo (MCMC) and Amortized Bayesian Inference (ABI) can become computationally\nchallenging. While ABI enables fast inference after training, generating\nsufficient training data still requires thousands of model simulations, which\nis infeasible for expensive models. Surrogate models offer a solution by\nproviding approximate simulations at a lower computational cost, allowing the\ngeneration of large data sets for training. However, the introduced\napproximation errors and uncertainties can lead to overconfident posterior\nestimates. To address this, we propose Uncertainty-Aware Surrogate-based\nAmortized Bayesian Inference (UA-SABI) - a framework that combines surrogate\nmodeling and ABI while explicitly quantifying and propagating surrogate\nuncertainties through the inference pipeline. Our experiments show that this\napproach enables reliable, fast, and repeated Bayesian inference for\ncomputationally expensive models, even under tight time constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference typically relies on a large number of model evaluations to\nestimate posterior distributions. Established methods like Markov Chain Monte\nCarlo (MCMC) and Amortized Bayesian Inference (ABI) can become computationally\nchallenging. While ABI enables fast inference after training, generating\nsufficient training data still requires thousands of model simulations, which\nis infeasible for expensive models. Surrogate models offer a solution by\nproviding approximate simulations at a lower computational cost, allowing the\ngeneration of large data sets for training. However, the introduced\napproximation errors and uncertainties can lead to overconfident posterior\nestimates. To address this, we propose Uncertainty-Aware Surrogate-based\nAmortized Bayesian Inference (UA-SABI) - a framework that combines surrogate\nmodeling and ABI while explicitly quantifying and propagating surrogate\nuncertainties through the inference pipeline. Our experiments show that this\napproach enables reliable, fast, and repeated Bayesian inference for\ncomputationally expensive models, even under tight time constraints."
                },
                "authors": [
                    {
                        "name": "Stefania Scheurer"
                    },
                    {
                        "name": "Philipp Reiser"
                    },
                    {
                        "name": "Tim Brünnette"
                    },
                    {
                        "name": "Wolfgang Nowak"
                    },
                    {
                        "name": "Anneli Guthke"
                    },
                    {
                        "name": "Paul-Christian Bürkner"
                    }
                ],
                "author_detail": {
                    "name": "Paul-Christian Bürkner"
                },
                "author": "Paul-Christian Bürkner",
                "arxiv_comment": "16 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08681v1",
                "updated": "2025-05-13T15:43:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    43,
                    35,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T15:43:35Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    43,
                    35,
                    1,
                    133,
                    0
                ],
                "title": "A Mamba-based Network for Semi-supervised Singing Melody Extraction\n  Using Confidence Binary Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Mamba-based Network for Semi-supervised Singing Melody Extraction\n  Using Confidence Binary Regularization"
                },
                "summary": "Singing melody extraction (SME) is a key task in the field of music\ninformation retrieval. However, existing methods are facing several\nlimitations: firstly, prior models use transformers to capture the contextual\ndependencies, which requires quadratic computation resulting in low efficiency\nin the inference stage. Secondly, prior works typically rely on\nfrequencysupervised methods to estimate the fundamental frequency (f0), which\nignores that the musical performance is actually based on notes. Thirdly,\ntransformers typically require large amounts of labeled data to achieve optimal\nperformances, but the SME task lacks of sufficient annotated data. To address\nthese issues, in this paper, we propose a mamba-based network, called\nSpectMamba, for semi-supervised singing melody extraction using confidence\nbinary regularization. In particular, we begin by introducing vision mamba to\nachieve computational linear complexity. Then, we propose a novel note-f0\ndecoder that allows the model to better mimic the musical performance. Further,\nto alleviate the scarcity of the labeled data, we introduce a confidence binary\nregularization (CBR) module to leverage the unlabeled data by maximizing the\nprobability of the correct classes. The proposed method is evaluated on several\npublic datasets and the conducted experiments demonstrate the effectiveness of\nour proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Singing melody extraction (SME) is a key task in the field of music\ninformation retrieval. However, existing methods are facing several\nlimitations: firstly, prior models use transformers to capture the contextual\ndependencies, which requires quadratic computation resulting in low efficiency\nin the inference stage. Secondly, prior works typically rely on\nfrequencysupervised methods to estimate the fundamental frequency (f0), which\nignores that the musical performance is actually based on notes. Thirdly,\ntransformers typically require large amounts of labeled data to achieve optimal\nperformances, but the SME task lacks of sufficient annotated data. To address\nthese issues, in this paper, we propose a mamba-based network, called\nSpectMamba, for semi-supervised singing melody extraction using confidence\nbinary regularization. In particular, we begin by introducing vision mamba to\nachieve computational linear complexity. Then, we propose a novel note-f0\ndecoder that allows the model to better mimic the musical performance. Further,\nto alleviate the scarcity of the labeled data, we introduce a confidence binary\nregularization (CBR) module to leverage the unlabeled data by maximizing the\nprobability of the correct classes. The proposed method is evaluated on several\npublic datasets and the conducted experiments demonstrate the effectiveness of\nour proposed method."
                },
                "authors": [
                    {
                        "name": "Xiaoliang He"
                    },
                    {
                        "name": "Kangjie Dong"
                    },
                    {
                        "name": "Jingkai Cao"
                    },
                    {
                        "name": "Shuai Yu"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Yi Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yu"
                },
                "author": "Yi Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10686v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10686v4",
                "updated": "2025-05-13T15:38:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    38,
                    9,
                    1,
                    133,
                    0
                ],
                "published": "2024-02-16T13:41:18Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    13,
                    41,
                    18,
                    4,
                    47,
                    0
                ],
                "title": "On the Impact of Uncertainty and Calibration on Likelihood-Ratio\n  Membership Inference Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Impact of Uncertainty and Calibration on Likelihood-Ratio\n  Membership Inference Attacks"
                },
                "summary": "In a membership inference attack (MIA), an attacker exploits the\noverconfidence exhibited by typical machine learning models to determine\nwhether a specific data point was used to train a target model. In this paper,\nwe analyze the performance of the likelihood ratio attack (LiRA) within an\ninformation-theoretical framework that allows the investigation of the impact\nof the aleatoric uncertainty in the true data generation process, of the\nepistemic uncertainty caused by a limited training data set, and of the\ncalibration level of the target model. We compare three different settings, in\nwhich the attacker receives decreasingly informative feedback from the target\nmodel: confidence vector (CV) disclosure, in which the output probability\nvector is released; true label confidence (TLC) disclosure, in which only the\nprobability assigned to the true label is made available by the model; and\ndecision set (DS) disclosure, in which an adaptive prediction set is produced\nas in conformal prediction. We derive bounds on the advantage of an MIA\nadversary with the aim of offering insights into the impact of uncertainty and\ncalibration on the effectiveness of MIAs. Simulation results demonstrate that\nthe derived analytical bounds predict well the effectiveness of MIAs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a membership inference attack (MIA), an attacker exploits the\noverconfidence exhibited by typical machine learning models to determine\nwhether a specific data point was used to train a target model. In this paper,\nwe analyze the performance of the likelihood ratio attack (LiRA) within an\ninformation-theoretical framework that allows the investigation of the impact\nof the aleatoric uncertainty in the true data generation process, of the\nepistemic uncertainty caused by a limited training data set, and of the\ncalibration level of the target model. We compare three different settings, in\nwhich the attacker receives decreasingly informative feedback from the target\nmodel: confidence vector (CV) disclosure, in which the output probability\nvector is released; true label confidence (TLC) disclosure, in which only the\nprobability assigned to the true label is made available by the model; and\ndecision set (DS) disclosure, in which an adaptive prediction set is produced\nas in conformal prediction. We derive bounds on the advantage of an MIA\nadversary with the aim of offering insights into the impact of uncertainty and\ncalibration on the effectiveness of MIAs. Simulation results demonstrate that\nthe derived analytical bounds predict well the effectiveness of MIAs."
                },
                "authors": [
                    {
                        "name": "Meiyi Zhu"
                    },
                    {
                        "name": "Caili Guo"
                    },
                    {
                        "name": "Chunyan Feng"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "arxiv_comment": "16 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10686v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10686v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08662v1",
                "updated": "2025-05-13T15:24:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    24,
                    8,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T15:24:08Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    24,
                    8,
                    1,
                    133,
                    0
                ],
                "title": "Revealing economic facts: LLMs know more than they say",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing economic facts: LLMs know more than they say"
                },
                "summary": "We investigate whether the hidden states of large language models (LLMs) can\nbe used to estimate and impute economic and financial statistics. Focusing on\ncounty-level (e.g. unemployment) and firm-level (e.g. total assets) variables,\nwe show that a simple linear model trained on the hidden states of open-source\nLLMs outperforms the models' text outputs. This suggests that hidden states\ncapture richer economic information than the responses of the LLMs reveal\ndirectly. A learning curve analysis indicates that only a few dozen labelled\nexamples are sufficient for training. We also propose a transfer learning\nmethod that improves estimation accuracy without requiring any labelled data\nfor the target variable. Finally, we demonstrate the practical utility of\nhidden-state representations in super-resolution and data imputation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate whether the hidden states of large language models (LLMs) can\nbe used to estimate and impute economic and financial statistics. Focusing on\ncounty-level (e.g. unemployment) and firm-level (e.g. total assets) variables,\nwe show that a simple linear model trained on the hidden states of open-source\nLLMs outperforms the models' text outputs. This suggests that hidden states\ncapture richer economic information than the responses of the LLMs reveal\ndirectly. A learning curve analysis indicates that only a few dozen labelled\nexamples are sufficient for training. We also propose a transfer learning\nmethod that improves estimation accuracy without requiring any labelled data\nfor the target variable. Finally, we demonstrate the practical utility of\nhidden-state representations in super-resolution and data imputation tasks."
                },
                "authors": [
                    {
                        "name": "Marcus Buckmann"
                    },
                    {
                        "name": "Quynh Anh Nguyen"
                    },
                    {
                        "name": "Edward Hill"
                    }
                ],
                "author_detail": {
                    "name": "Edward Hill"
                },
                "author": "Edward Hill",
                "arxiv_comment": "34 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02283v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02283v5",
                "updated": "2025-05-13T15:14:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    14,
                    37,
                    1,
                    133,
                    0
                ],
                "published": "2025-02-04T12:50:16Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    12,
                    50,
                    16,
                    1,
                    35,
                    0
                ],
                "title": "GP-GS: Gaussian Processes for Enhanced Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GP-GS: Gaussian Processes for Enhanced Gaussian Splatting"
                },
                "summary": "3D Gaussian Splatting has emerged as an efficient photorealistic novel view\nsynthesis method. However, its reliance on sparse Structure-from-Motion (SfM)\npoint clouds often limits scene reconstruction quality. To address the\nlimitation, this paper proposes a novel 3D reconstruction framework, Gaussian\nProcesses enhanced Gaussian Splatting (GP-GS), in which a multi-output Gaussian\nProcess model is developed to enable adaptive and uncertainty-guided\ndensification of sparse SfM point clouds. Specifically, we propose a dynamic\nsampling and filtering pipeline that adaptively expands the SfM point clouds by\nleveraging GP-based predictions to infer new candidate points from the input 2D\npixels and depth maps. The pipeline utilizes uncertainty estimates to guide the\npruning of high-variance predictions, ensuring geometric consistency and\nenabling the generation of dense point clouds. These densified point clouds\nprovide high-quality initial 3D Gaussians, enhancing reconstruction\nperformance. Extensive experiments conducted on synthetic and real-world\ndatasets across various scales validate the effectiveness and practicality of\nthe proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting has emerged as an efficient photorealistic novel view\nsynthesis method. However, its reliance on sparse Structure-from-Motion (SfM)\npoint clouds often limits scene reconstruction quality. To address the\nlimitation, this paper proposes a novel 3D reconstruction framework, Gaussian\nProcesses enhanced Gaussian Splatting (GP-GS), in which a multi-output Gaussian\nProcess model is developed to enable adaptive and uncertainty-guided\ndensification of sparse SfM point clouds. Specifically, we propose a dynamic\nsampling and filtering pipeline that adaptively expands the SfM point clouds by\nleveraging GP-based predictions to infer new candidate points from the input 2D\npixels and depth maps. The pipeline utilizes uncertainty estimates to guide the\npruning of high-variance predictions, ensuring geometric consistency and\nenabling the generation of dense point clouds. These densified point clouds\nprovide high-quality initial 3D Gaussians, enhancing reconstruction\nperformance. Extensive experiments conducted on synthetic and real-world\ndatasets across various scales validate the effectiveness and practicality of\nthe proposed framework."
                },
                "authors": [
                    {
                        "name": "Zhihao Guo"
                    },
                    {
                        "name": "Jingxuan Su"
                    },
                    {
                        "name": "Shenglin Wang"
                    },
                    {
                        "name": "Jinlong Fan"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Hadi Amirpour"
                    },
                    {
                        "name": "Yunlong Zhao"
                    },
                    {
                        "name": "Liangxiu Han"
                    },
                    {
                        "name": "Peng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Wang"
                },
                "author": "Peng Wang",
                "arxiv_comment": "12 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02283v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02283v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08648v1",
                "updated": "2025-05-13T15:08:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    8,
                    55,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T15:08:55Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    8,
                    55,
                    1,
                    133,
                    0
                ],
                "title": "Enhancing Software Development with Context-Aware Conversational Agents:\n  A User Study on Developer Interactions with Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Software Development with Context-Aware Conversational Agents:\n  A User Study on Developer Interactions with Chatbots"
                },
                "summary": "Software development is a cognitively intensive process requiring\nmultitasking, adherence to evolving workflows, and continuous learning. With\nthe rise of large language model (LLM)-based tools, such as conversational\nagents (CAs), there is growing interest in supporting developers through\nnatural language interaction. However, little is known about the specific\nfeatures developers seek in these systems. We conducted a user study with 29\ndevelopers using a prototype text-based chatbot to investigate preferred\nfunctionalities. Our findings reveal strong interest in task automation,\nversion control support, and contextual adaptability, especially the need to\ntailor assistance for both novice and experienced users. We highlight the\nimportance of deep contextual understanding, historical interaction awareness,\nand personalized support in CA design. This study contributes to the\ndevelopment of context-aware chatbots that enhance productivity and\nsatisfaction, and it outlines opportunities for future research on human-AI\ncollaboration in software engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software development is a cognitively intensive process requiring\nmultitasking, adherence to evolving workflows, and continuous learning. With\nthe rise of large language model (LLM)-based tools, such as conversational\nagents (CAs), there is growing interest in supporting developers through\nnatural language interaction. However, little is known about the specific\nfeatures developers seek in these systems. We conducted a user study with 29\ndevelopers using a prototype text-based chatbot to investigate preferred\nfunctionalities. Our findings reveal strong interest in task automation,\nversion control support, and contextual adaptability, especially the need to\ntailor assistance for both novice and experienced users. We highlight the\nimportance of deep contextual understanding, historical interaction awareness,\nand personalized support in CA design. This study contributes to the\ndevelopment of context-aware chatbots that enhance productivity and\nsatisfaction, and it outlines opportunities for future research on human-AI\ncollaboration in software engineering."
                },
                "authors": [
                    {
                        "name": "Glaucia Melo"
                    },
                    {
                        "name": "Paulo Alencar"
                    },
                    {
                        "name": "Donald Cowan"
                    }
                ],
                "author_detail": {
                    "name": "Donald Cowan"
                },
                "author": "Donald Cowan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16978v2",
                "updated": "2025-05-13T15:07:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    7,
                    26,
                    1,
                    133,
                    0
                ],
                "published": "2024-08-30T02:44:26Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    44,
                    26,
                    4,
                    243,
                    0
                ],
                "title": "Training Ultra Long Context Language Model with Fully Pipelined\n  Distributed Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Ultra Long Context Language Model with Fully Pipelined\n  Distributed Transformer"
                },
                "summary": "Large Language Models (LLMs) with long context capabilities are integral to\ncomplex tasks in natural language processing and computational biology, such as\ntext generation and protein sequence analysis. However, training LLMs directly\non extremely long contexts demands considerable GPU resources and increased\nmemory, leading to higher costs and greater complexity. Alternative approaches\nthat introduce long context capabilities via downstream finetuning or\nadaptations impose significant design limitations. In this paper, we propose\nFully Pipelined Distributed Transformer (FPDT) for efficiently training\nlong-context LLMs with extreme hardware efficiency. For GPT and Llama models,\nwe achieve a 16x increase in sequence length that can be trained on the same\nhardware compared to current state-of-the-art solutions. With our dedicated\nsequence chunk pipeline design, we can now train 8B LLM with 2 million sequence\nlength on only 4 GPUs, while also maintaining over 55% of MFU. Our proposed\nFPDT is agnostic to existing training techniques and is proven to work\nefficiently across different LLM models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with long context capabilities are integral to\ncomplex tasks in natural language processing and computational biology, such as\ntext generation and protein sequence analysis. However, training LLMs directly\non extremely long contexts demands considerable GPU resources and increased\nmemory, leading to higher costs and greater complexity. Alternative approaches\nthat introduce long context capabilities via downstream finetuning or\nadaptations impose significant design limitations. In this paper, we propose\nFully Pipelined Distributed Transformer (FPDT) for efficiently training\nlong-context LLMs with extreme hardware efficiency. For GPT and Llama models,\nwe achieve a 16x increase in sequence length that can be trained on the same\nhardware compared to current state-of-the-art solutions. With our dedicated\nsequence chunk pipeline design, we can now train 8B LLM with 2 million sequence\nlength on only 4 GPUs, while also maintaining over 55% of MFU. Our proposed\nFPDT is agnostic to existing training techniques and is proven to work\nefficiently across different LLM models."
                },
                "authors": [
                    {
                        "name": "Jinghan Yao"
                    },
                    {
                        "name": "Sam Ade Jacobs"
                    },
                    {
                        "name": "Masahiro Tanaka"
                    },
                    {
                        "name": "Olatunji Ruwase"
                    },
                    {
                        "name": "Hari Subramoni"
                    },
                    {
                        "name": "Dhabaleswar K. Panda"
                    }
                ],
                "author_detail": {
                    "name": "Dhabaleswar K. Panda"
                },
                "author": "Dhabaleswar K. Panda",
                "arxiv_comment": "The Eighth Annual Conference on Machine Learning and Systems\n  (MLSys'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08643v1",
                "updated": "2025-05-13T15:02:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    2,
                    54,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T15:02:54Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    2,
                    54,
                    1,
                    133,
                    0
                ],
                "title": "WixQA: A Multi-Dataset Benchmark for Enterprise Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WixQA: A Multi-Dataset Benchmark for Enterprise Retrieval-Augmented\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) is a cornerstone of modern question\nanswering (QA) systems, enabling grounded answers based on external knowledge.\nAlthough recent progress has been driven by open-domain datasets, enterprise QA\nsystems need datasets that mirror the concrete, domain-specific issues users\nraise in day-to-day support scenarios. Critically, evaluating end-to-end RAG\nsystems requires benchmarks comprising not only question--answer pairs but also\nthe specific knowledge base (KB) snapshot from which answers were derived. To\naddress this need, we introduce WixQA, a benchmark suite featuring QA datasets\nprecisely grounded in the released KB corpus, enabling holistic evaluation of\nretrieval and generation components. WixQA includes three distinct QA datasets\nderived from Wix.com customer support interactions and grounded in a snapshot\nof the public Wix Help Center KB: (i) WixQA-ExpertWritten, 200 real user\nqueries with expert-authored, multi-step answers; (ii) WixQA-Simulated, 200\nexpert-validated QA pairs distilled from user dialogues; and (iii)\nWixQA-Synthetic, 6,222 LLM-generated QA pairs, with one pair systematically\nderived from each article in the knowledge base. We release the KB snapshot\nalongside the datasets under MIT license and provide comprehensive baseline\nresults, forming a unique benchmark for evaluating enterprise RAG systems in\nrealistic enterprise environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) is a cornerstone of modern question\nanswering (QA) systems, enabling grounded answers based on external knowledge.\nAlthough recent progress has been driven by open-domain datasets, enterprise QA\nsystems need datasets that mirror the concrete, domain-specific issues users\nraise in day-to-day support scenarios. Critically, evaluating end-to-end RAG\nsystems requires benchmarks comprising not only question--answer pairs but also\nthe specific knowledge base (KB) snapshot from which answers were derived. To\naddress this need, we introduce WixQA, a benchmark suite featuring QA datasets\nprecisely grounded in the released KB corpus, enabling holistic evaluation of\nretrieval and generation components. WixQA includes three distinct QA datasets\nderived from Wix.com customer support interactions and grounded in a snapshot\nof the public Wix Help Center KB: (i) WixQA-ExpertWritten, 200 real user\nqueries with expert-authored, multi-step answers; (ii) WixQA-Simulated, 200\nexpert-validated QA pairs distilled from user dialogues; and (iii)\nWixQA-Synthetic, 6,222 LLM-generated QA pairs, with one pair systematically\nderived from each article in the knowledge base. We release the KB snapshot\nalongside the datasets under MIT license and provide comprehensive baseline\nresults, forming a unique benchmark for evaluating enterprise RAG systems in\nrealistic enterprise environments."
                },
                "authors": [
                    {
                        "name": "Dvir Cohen"
                    },
                    {
                        "name": "Lin Burg"
                    },
                    {
                        "name": "Sviatoslav Pykhnivskyi"
                    },
                    {
                        "name": "Hagit Gur"
                    },
                    {
                        "name": "Stanislav Kovynov"
                    },
                    {
                        "name": "Olga Atzmon"
                    },
                    {
                        "name": "Gilad Barkan"
                    }
                ],
                "author_detail": {
                    "name": "Gilad Barkan"
                },
                "author": "Gilad Barkan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08639v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08639v1",
                "updated": "2025-05-13T14:56:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    56,
                    54,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T14:56:54Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    56,
                    54,
                    1,
                    133,
                    0
                ],
                "title": "Robust Indoor Localization via Conformal Methods and Variational\n  Bayesian Adaptive Filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Indoor Localization via Conformal Methods and Variational\n  Bayesian Adaptive Filtering"
                },
                "summary": "Indoor localization is critical for IoT applications, yet challenges such as\nnon-Gaussian noise, environmental interference, and measurement outliers hinder\nthe robustness of traditional methods. Existing approaches, including Kalman\nfiltering and its variants, often rely on Gaussian assumptions or static\nthresholds, limiting adaptability in dynamic environments. This paper proposes\na hierarchical robust framework integrating Variational Bayesian (VB) parameter\nlearning, Huber M-estimation, and Conformal Outlier Detection (COD) to address\nthese limitations. First, VB inference jointly estimates state and noise\nparameters, adapting to time-varying uncertainties. Second, Huber-based robust\nfiltering suppresses mild outliers while preserving Gaussian efficiency. Third,\nCOD provides statistical guarantees for outlier detection via dynamically\ncalibrated thresholds, ensuring a user-controlled false alarm rate.\nTheoretically, we prove the Semi-positive Definiteness of Huber-based Kalman\nfiltering covariance and the coverage of sliding window conformal prediction.\nExperiments on geomagnetic fingerprint datasets demonstrate significant\nimprovements: fingerprint matching accuracy increases from 81.25% to 93.75%,\nand positioning errors decrease from 0.62-6.87 m to 0.03-0.35 m. Comparative\nstudies further validate the framework's robustness, showing consistent\nperformance gains under non-Gaussian noise and outlier conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indoor localization is critical for IoT applications, yet challenges such as\nnon-Gaussian noise, environmental interference, and measurement outliers hinder\nthe robustness of traditional methods. Existing approaches, including Kalman\nfiltering and its variants, often rely on Gaussian assumptions or static\nthresholds, limiting adaptability in dynamic environments. This paper proposes\na hierarchical robust framework integrating Variational Bayesian (VB) parameter\nlearning, Huber M-estimation, and Conformal Outlier Detection (COD) to address\nthese limitations. First, VB inference jointly estimates state and noise\nparameters, adapting to time-varying uncertainties. Second, Huber-based robust\nfiltering suppresses mild outliers while preserving Gaussian efficiency. Third,\nCOD provides statistical guarantees for outlier detection via dynamically\ncalibrated thresholds, ensuring a user-controlled false alarm rate.\nTheoretically, we prove the Semi-positive Definiteness of Huber-based Kalman\nfiltering covariance and the coverage of sliding window conformal prediction.\nExperiments on geomagnetic fingerprint datasets demonstrate significant\nimprovements: fingerprint matching accuracy increases from 81.25% to 93.75%,\nand positioning errors decrease from 0.62-6.87 m to 0.03-0.35 m. Comparative\nstudies further validate the framework's robustness, showing consistent\nperformance gains under non-Gaussian noise and outlier conditions."
                },
                "authors": [
                    {
                        "name": "Zhiyi Zhou"
                    },
                    {
                        "name": "Dongzhuo Liu"
                    },
                    {
                        "name": "Songtao Guo"
                    },
                    {
                        "name": "Yuanyuan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yuanyuan Yang"
                },
                "author": "Yuanyuan Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08639v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08638v1",
                "updated": "2025-05-13T14:55:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    55,
                    31,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T14:55:31Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    55,
                    31,
                    1,
                    133,
                    0
                ],
                "title": "TRAIL: Trace Reasoning and Agentic Issue Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRAIL: Trace Reasoning and Agentic Issue Localization"
                },
                "summary": "The increasing adoption of agentic workflows across diverse domains brings a\ncritical need to scalably and systematically evaluate the complex traces these\nsystems generate. Current evaluation methods depend on manual, domain-specific\nhuman analysis of lengthy workflow traces - an approach that does not scale\nwith the growing complexity and volume of agentic outputs. Error analysis in\nthese settings is further complicated by the interplay of external tool outputs\nand language model reasoning, making it more challenging than traditional\nsoftware debugging. In this work, we (1) articulate the need for robust and\ndynamic evaluation methods for agentic workflow traces, (2) introduce a formal\ntaxonomy of error types encountered in agentic systems, and (3) present a set\nof 148 large human-annotated traces (TRAIL) constructed using this taxonomy and\ngrounded in established agentic benchmarks. To ensure ecological validity, we\ncurate traces from both single and multi-agent systems, focusing on real-world\napplications such as software engineering and open-world information retrieval.\nOur evaluations reveal that modern long context LLMs perform poorly at trace\ndebugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our\ndataset and code are made publicly available to support and accelerate future\nresearch in scalable evaluation for agentic workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of agentic workflows across diverse domains brings a\ncritical need to scalably and systematically evaluate the complex traces these\nsystems generate. Current evaluation methods depend on manual, domain-specific\nhuman analysis of lengthy workflow traces - an approach that does not scale\nwith the growing complexity and volume of agentic outputs. Error analysis in\nthese settings is further complicated by the interplay of external tool outputs\nand language model reasoning, making it more challenging than traditional\nsoftware debugging. In this work, we (1) articulate the need for robust and\ndynamic evaluation methods for agentic workflow traces, (2) introduce a formal\ntaxonomy of error types encountered in agentic systems, and (3) present a set\nof 148 large human-annotated traces (TRAIL) constructed using this taxonomy and\ngrounded in established agentic benchmarks. To ensure ecological validity, we\ncurate traces from both single and multi-agent systems, focusing on real-world\napplications such as software engineering and open-world information retrieval.\nOur evaluations reveal that modern long context LLMs perform poorly at trace\ndebugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our\ndataset and code are made publicly available to support and accelerate future\nresearch in scalable evaluation for agentic workflows."
                },
                "authors": [
                    {
                        "name": "Darshan Deshpande"
                    },
                    {
                        "name": "Varun Gangal"
                    },
                    {
                        "name": "Hersh Mehta"
                    },
                    {
                        "name": "Jitin Krishnan"
                    },
                    {
                        "name": "Anand Kannappan"
                    },
                    {
                        "name": "Rebecca Qian"
                    }
                ],
                "author_detail": {
                    "name": "Rebecca Qian"
                },
                "author": "Rebecca Qian",
                "arxiv_comment": "Dataset link: https://huggingface.co/datasets/PatronusAI/TRAIL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08627v1",
                "updated": "2025-05-13T14:46:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    46,
                    23,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T14:46:23Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    46,
                    23,
                    1,
                    133,
                    0
                ],
                "title": "Augmented Reality for RObots (ARRO): Pointing Visuomotor Policies\n  Towards Visual Robustness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Reality for RObots (ARRO): Pointing Visuomotor Policies\n  Towards Visual Robustness"
                },
                "summary": "Visuomotor policies trained on human expert demonstrations have recently\nshown strong performance across a wide range of robotic manipulation tasks.\nHowever, these policies remain highly sensitive to domain shifts stemming from\nbackground or robot embodiment changes, which limits their generalization\ncapabilities. In this paper, we present ARRO, a novel calibration-free visual\nrepresentation that leverages zero-shot open-vocabulary segmentation and object\ndetection models to efficiently mask out task-irrelevant regions of the scene\nwithout requiring additional training. By filtering visual distractors and\noverlaying virtual guides during both training and inference, ARRO improves\nrobustness to scene variations and reduces the need for additional data\ncollection. We extensively evaluate ARRO with Diffusion Policy on several\ntabletop manipulation tasks in both simulation and real-world environments, and\nfurther demonstrate its compatibility and effectiveness with generalist robot\npolicies, such as Octo and OpenVLA. Across all settings in our evaluation, ARRO\nyields consistent performance gains, allows for selective masking to choose\nbetween different objects, and shows robustness even to challenging\nsegmentation conditions. Videos showcasing our results are available at:\naugmented-reality-for-robots.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visuomotor policies trained on human expert demonstrations have recently\nshown strong performance across a wide range of robotic manipulation tasks.\nHowever, these policies remain highly sensitive to domain shifts stemming from\nbackground or robot embodiment changes, which limits their generalization\ncapabilities. In this paper, we present ARRO, a novel calibration-free visual\nrepresentation that leverages zero-shot open-vocabulary segmentation and object\ndetection models to efficiently mask out task-irrelevant regions of the scene\nwithout requiring additional training. By filtering visual distractors and\noverlaying virtual guides during both training and inference, ARRO improves\nrobustness to scene variations and reduces the need for additional data\ncollection. We extensively evaluate ARRO with Diffusion Policy on several\ntabletop manipulation tasks in both simulation and real-world environments, and\nfurther demonstrate its compatibility and effectiveness with generalist robot\npolicies, such as Octo and OpenVLA. Across all settings in our evaluation, ARRO\nyields consistent performance gains, allows for selective masking to choose\nbetween different objects, and shows robustness even to challenging\nsegmentation conditions. Videos showcasing our results are available at:\naugmented-reality-for-robots.github.io"
                },
                "authors": [
                    {
                        "name": "Reihaneh Mirjalili"
                    },
                    {
                        "name": "Tobias Jülg"
                    },
                    {
                        "name": "Florian Walter"
                    },
                    {
                        "name": "Wolfram Burgard"
                    }
                ],
                "author_detail": {
                    "name": "Wolfram Burgard"
                },
                "author": "Wolfram Burgard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08622v1",
                "updated": "2025-05-13T14:40:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    40,
                    22,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T14:40:22Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    40,
                    22,
                    1,
                    133,
                    0
                ],
                "title": "Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with\n  Language Models"
                },
                "summary": "Text-to-image generative models like DALL-E and Stable Diffusion have\nrevolutionized visual content creation across various applications, including\nadvertising, personalized media, and design prototyping. However, crafting\neffective textual prompts to guide these models remains challenging, often\nrequiring extensive trial and error. Existing prompt inversion approaches, such\nas soft and hard prompt techniques, are not so effective due to the limited\ninterpretability and incoherent prompt generation. To address these issues, we\npropose Visually Guided Decoding (VGD), a gradient-free approach that leverages\nlarge language models (LLMs) and CLIP-based guidance to generate coherent and\nsemantically aligned prompts. In essence, VGD utilizes the robust text\ngeneration capabilities of LLMs to produce human-readable prompts. Further, by\nemploying CLIP scores to ensure alignment with user-specified visual concepts,\nVGD enhances the interpretability, generalization, and flexibility of prompt\ngeneration without the need for additional training. Our experiments\ndemonstrate that VGD outperforms existing prompt inversion techniques in\ngenerating understandable and contextually relevant prompts, facilitating more\nintuitive and controllable interactions with text-to-image models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generative models like DALL-E and Stable Diffusion have\nrevolutionized visual content creation across various applications, including\nadvertising, personalized media, and design prototyping. However, crafting\neffective textual prompts to guide these models remains challenging, often\nrequiring extensive trial and error. Existing prompt inversion approaches, such\nas soft and hard prompt techniques, are not so effective due to the limited\ninterpretability and incoherent prompt generation. To address these issues, we\npropose Visually Guided Decoding (VGD), a gradient-free approach that leverages\nlarge language models (LLMs) and CLIP-based guidance to generate coherent and\nsemantically aligned prompts. In essence, VGD utilizes the robust text\ngeneration capabilities of LLMs to produce human-readable prompts. Further, by\nemploying CLIP scores to ensure alignment with user-specified visual concepts,\nVGD enhances the interpretability, generalization, and flexibility of prompt\ngeneration without the need for additional training. Our experiments\ndemonstrate that VGD outperforms existing prompt inversion techniques in\ngenerating understandable and contextually relevant prompts, facilitating more\nintuitive and controllable interactions with text-to-image models."
                },
                "authors": [
                    {
                        "name": "Donghoon Kim"
                    },
                    {
                        "name": "Minji Bae"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Byonghyo Shim"
                    }
                ],
                "author_detail": {
                    "name": "Byonghyo Shim"
                },
                "author": "Byonghyo Shim",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08620v1",
                "updated": "2025-05-13T14:39:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    39,
                    33,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T14:39:33Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    39,
                    33,
                    1,
                    133,
                    0
                ],
                "title": "Resource-Efficient Language Models: Quantization for Fast and Accessible\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Efficient Language Models: Quantization for Fast and Accessible\n  Inference"
                },
                "summary": "Large language models have significantly advanced natural language\nprocessing, yet their heavy resource demands pose severe challenges regarding\nhardware accessibility and energy consumption. This paper presents a focused\nand high-level review of post-training quantization (PTQ) techniques designed\nto optimize the inference efficiency of LLMs by the end-user, including details\non various quantization schemes, granularities, and trade-offs. The aim is to\nprovide a balanced overview between the theory and applications of\npost-training quantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have significantly advanced natural language\nprocessing, yet their heavy resource demands pose severe challenges regarding\nhardware accessibility and energy consumption. This paper presents a focused\nand high-level review of post-training quantization (PTQ) techniques designed\nto optimize the inference efficiency of LLMs by the end-user, including details\non various quantization schemes, granularities, and trade-offs. The aim is to\nprovide a balanced overview between the theory and applications of\npost-training quantization."
                },
                "authors": [
                    {
                        "name": "Tollef Emil Jørgensen"
                    }
                ],
                "author_detail": {
                    "name": "Tollef Emil Jørgensen"
                },
                "author": "Tollef Emil Jørgensen",
                "arxiv_comment": "17 pages, 9 figures, preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08619v1",
                "updated": "2025-05-13T14:38:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    38,
                    25,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T14:38:25Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    38,
                    25,
                    1,
                    133,
                    0
                ],
                "title": "Cost Function Estimation Using Inverse Reinforcement Learning with\n  Minimal Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost Function Estimation Using Inverse Reinforcement Learning with\n  Minimal Observations"
                },
                "summary": "We present an iterative inverse reinforcement learning algorithm to infer\noptimal cost functions in continuous spaces. Based on a popular maximum entropy\ncriteria, our approach iteratively finds a weight improvement step and proposes\na method to find an appropriate step size that ensures learned cost function\nfeatures remain similar to the demonstrated trajectory features. In contrast to\nsimilar approaches, our algorithm can individually tune the effectiveness of\neach observation for the partition function and does not need a large sample\nset, enabling faster learning. We generate sample trajectories by solving an\noptimal control problem instead of random sampling, leading to more informative\ntrajectories. The performance of our method is compared to two state of the art\nalgorithms to demonstrate its benefits in several simulated environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an iterative inverse reinforcement learning algorithm to infer\noptimal cost functions in continuous spaces. Based on a popular maximum entropy\ncriteria, our approach iteratively finds a weight improvement step and proposes\na method to find an appropriate step size that ensures learned cost function\nfeatures remain similar to the demonstrated trajectory features. In contrast to\nsimilar approaches, our algorithm can individually tune the effectiveness of\neach observation for the partition function and does not need a large sample\nset, enabling faster learning. We generate sample trajectories by solving an\noptimal control problem instead of random sampling, leading to more informative\ntrajectories. The performance of our method is compared to two state of the art\nalgorithms to demonstrate its benefits in several simulated environments."
                },
                "authors": [
                    {
                        "name": "Sarmad Mehrdad"
                    },
                    {
                        "name": "Avadesh Meduri"
                    },
                    {
                        "name": "Ludovic Righetti"
                    }
                ],
                "author_detail": {
                    "name": "Ludovic Righetti"
                },
                "author": "Ludovic Righetti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08600v1",
                "updated": "2025-05-13T14:16:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    16,
                    12,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T14:16:12Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    16,
                    12,
                    1,
                    133,
                    0
                ],
                "title": "Automatic Task Detection and Heterogeneous LLM Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Task Detection and Heterogeneous LLM Speculative Decoding"
                },
                "summary": "Speculative decoding, which combines a draft model with a target model, has\nemerged as an effective approach to accelerate large language model (LLM)\ninference. However, existing methods often face a trade-off between the\nacceptance rate and decoding speed in downstream tasks due to the limited\ncapacity of the draft model, making it difficult to ensure efficiency across\ndiverse tasks. To address this problem, we propose a speculative decoding\nalgorithm tailored for downstream task optimization. It includes an automatic\ntask partitioning and assigning method, which automatically categorizes\ndownstream tasks into different sub-tasks and assigns them to a set of\nheterogeneous draft models. Each draft model is aligned with the target model\nusing task-specific data, thereby enhancing the consistency of inference\nresults. In addition, our proposed method incorporates an online lightweight\nprompt classifier to dynamically route prompts to the appropriate draft model.\nExperimental results demonstrate that the proposed method improves draft\naccuracy by 6% to 50% over vanilla speculative decoding, while achieving a\nspeedup of 1.10x to 2.64x in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding, which combines a draft model with a target model, has\nemerged as an effective approach to accelerate large language model (LLM)\ninference. However, existing methods often face a trade-off between the\nacceptance rate and decoding speed in downstream tasks due to the limited\ncapacity of the draft model, making it difficult to ensure efficiency across\ndiverse tasks. To address this problem, we propose a speculative decoding\nalgorithm tailored for downstream task optimization. It includes an automatic\ntask partitioning and assigning method, which automatically categorizes\ndownstream tasks into different sub-tasks and assigns them to a set of\nheterogeneous draft models. Each draft model is aligned with the target model\nusing task-specific data, thereby enhancing the consistency of inference\nresults. In addition, our proposed method incorporates an online lightweight\nprompt classifier to dynamically route prompts to the appropriate draft model.\nExperimental results demonstrate that the proposed method improves draft\naccuracy by 6% to 50% over vanilla speculative decoding, while achieving a\nspeedup of 1.10x to 2.64x in LLM inference."
                },
                "authors": [
                    {
                        "name": "Danying Ge"
                    },
                    {
                        "name": "Jianhua Gao"
                    },
                    {
                        "name": "Qizhi Jiang"
                    },
                    {
                        "name": "Yifei Feng"
                    },
                    {
                        "name": "Weixing Ji"
                    }
                ],
                "author_detail": {
                    "name": "Weixing Ji"
                },
                "author": "Weixing Ji",
                "arxiv_comment": "10 pages, 10 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06205v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06205v3",
                "updated": "2025-05-13T14:11:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    11,
                    59,
                    1,
                    133,
                    0
                ],
                "published": "2024-10-08T17:07:01Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    17,
                    7,
                    1,
                    1,
                    282,
                    0
                ],
                "title": "Round and Round We Go! What makes Rotary Positional Encodings useful?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Round and Round We Go! What makes Rotary Positional Encodings useful?"
                },
                "summary": "Positional Encodings (PEs) are a critical component of Transformer-based\nLarge Language Models (LLMs), providing the attention mechanism with important\nsequence-position information. One of the most popular types of encoding used\ntoday in LLMs are Rotary Positional Encodings (RoPE), that rotate the queries\nand keys based on their relative distance. A common belief is that RoPE is\nuseful because it helps to decay token dependency as relative distance\nincreases. In this work, we argue that this is unlikely to be the core reason.\nWe study the internals of a trained Gemma 7B model to understand how RoPE is\nbeing used at a mechanical level. We find that Gemma learns to use RoPE to\nconstruct robust \"positional\" attention patterns by exploiting the highest\nfrequencies. We also find that, in general, Gemma greatly prefers to use the\nlowest frequencies of RoPE, which we suspect are used to carry semantic\ninformation. We mathematically prove interesting behaviours of RoPE and conduct\nexperiments to verify our findings, proposing a modification of RoPE that fixes\nsome highlighted issues and improves performance. We believe that this work\nrepresents an interesting step in better understanding PEs in LLMs, which we\nbelieve holds crucial value for scaling LLMs to large sizes and context\nlengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Positional Encodings (PEs) are a critical component of Transformer-based\nLarge Language Models (LLMs), providing the attention mechanism with important\nsequence-position information. One of the most popular types of encoding used\ntoday in LLMs are Rotary Positional Encodings (RoPE), that rotate the queries\nand keys based on their relative distance. A common belief is that RoPE is\nuseful because it helps to decay token dependency as relative distance\nincreases. In this work, we argue that this is unlikely to be the core reason.\nWe study the internals of a trained Gemma 7B model to understand how RoPE is\nbeing used at a mechanical level. We find that Gemma learns to use RoPE to\nconstruct robust \"positional\" attention patterns by exploiting the highest\nfrequencies. We also find that, in general, Gemma greatly prefers to use the\nlowest frequencies of RoPE, which we suspect are used to carry semantic\ninformation. We mathematically prove interesting behaviours of RoPE and conduct\nexperiments to verify our findings, proposing a modification of RoPE that fixes\nsome highlighted issues and improves performance. We believe that this work\nrepresents an interesting step in better understanding PEs in LLMs, which we\nbelieve holds crucial value for scaling LLMs to large sizes and context\nlengths."
                },
                "authors": [
                    {
                        "name": "Federico Barbero"
                    },
                    {
                        "name": "Alex Vitvitskyi"
                    },
                    {
                        "name": "Christos Perivolaropoulos"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Petar Veličković"
                    }
                ],
                "author_detail": {
                    "name": "Petar Veličković"
                },
                "author": "Petar Veličković",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06205v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06205v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07687v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07687v3",
                "updated": "2025-05-13T14:09:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    9,
                    20,
                    1,
                    133,
                    0
                ],
                "published": "2025-04-10T12:16:32Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    12,
                    16,
                    32,
                    3,
                    100,
                    0
                ],
                "title": "FMNV: A Dataset of Media-Published News Videos for Fake News Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FMNV: A Dataset of Media-Published News Videos for Fake News Detection"
                },
                "summary": "News media, particularly video-based platforms, have become deeply embed-ded\nin daily life, concurrently amplifying the risks of misinformation\ndissem-ination. Consequently, multimodal fake news detection has garnered\nsignifi-cant research attention. However, existing datasets predominantly\ncomprise user-generated videos characterized by crude editing and limited\npublic en-gagement, whereas professionally crafted fake news videos\ndisseminated by media outlets-often politically or virally motivated-pose\nsubstantially greater societal harm. To address this gap, we construct FMNV, a\nnovel da-taset exclusively composed of news videos published by media\norganizations. Through empirical analysis of existing datasets and our curated\ncollection, we categorize fake news videos into four distinct types. Building\nupon this taxonomy, we employ Large Language Models (LLMs) to automatically\ngenerate deceptive content by manipulating authentic media-published news\nvideos. Furthermore, we propose FMNVD, a baseline model featuring a dual-stream\narchitecture that integrates spatio-temporal motion features from a 3D\nResNeXt-101 backbone and static visual semantics from CLIP. The two streams are\nfused via an attention-based mechanism, while co-attention modules refine the\nvisual, textual, and audio features for effective multi-modal aggregation.\nComparative experiments demonstrate both the generali-zation capability of FMNV\nacross multiple baselines and the superior detec-tion efficacy of FMNVD. This\nwork establishes critical benchmarks for de-tecting high-impact fake news in\nmedia ecosystems while advancing meth-odologies for cross-modal inconsistency\nanalysis. Our dataset is available in https://github.com/DennisIW/FMNV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "News media, particularly video-based platforms, have become deeply embed-ded\nin daily life, concurrently amplifying the risks of misinformation\ndissem-ination. Consequently, multimodal fake news detection has garnered\nsignifi-cant research attention. However, existing datasets predominantly\ncomprise user-generated videos characterized by crude editing and limited\npublic en-gagement, whereas professionally crafted fake news videos\ndisseminated by media outlets-often politically or virally motivated-pose\nsubstantially greater societal harm. To address this gap, we construct FMNV, a\nnovel da-taset exclusively composed of news videos published by media\norganizations. Through empirical analysis of existing datasets and our curated\ncollection, we categorize fake news videos into four distinct types. Building\nupon this taxonomy, we employ Large Language Models (LLMs) to automatically\ngenerate deceptive content by manipulating authentic media-published news\nvideos. Furthermore, we propose FMNVD, a baseline model featuring a dual-stream\narchitecture that integrates spatio-temporal motion features from a 3D\nResNeXt-101 backbone and static visual semantics from CLIP. The two streams are\nfused via an attention-based mechanism, while co-attention modules refine the\nvisual, textual, and audio features for effective multi-modal aggregation.\nComparative experiments demonstrate both the generali-zation capability of FMNV\nacross multiple baselines and the superior detec-tion efficacy of FMNVD. This\nwork establishes critical benchmarks for de-tecting high-impact fake news in\nmedia ecosystems while advancing meth-odologies for cross-modal inconsistency\nanalysis. Our dataset is available in https://github.com/DennisIW/FMNV."
                },
                "authors": [
                    {
                        "name": "Yihao Wang"
                    },
                    {
                        "name": "Zhong Qian"
                    },
                    {
                        "name": "Peifeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Peifeng Li"
                },
                "author": "Peifeng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07687v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07687v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08594v1",
                "updated": "2025-05-13T14:06:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    6,
                    13,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T14:06:13Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    6,
                    13,
                    1,
                    133,
                    0
                ],
                "title": "Clustering of Incomplete Data via a Bipartite Graph Structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clustering of Incomplete Data via a Bipartite Graph Structure"
                },
                "summary": "There are various approaches to graph learning for data clustering,\nincorporating different spectral and structural constraints through diverse\ngraph structures. Some methods rely on bipartite graph models, where nodes are\ndivided into two classes: centers and members. These models typically require\naccess to data for the center nodes in addition to observations from the member\nnodes. However, such additional data may not always be available in many\npractical scenarios. Moreover, popular Gaussian models for graph learning have\ndemonstrated limited effectiveness in modeling data with heavy-tailed\ndistributions, which are common in financial markets. In this paper, we propose\na clustering method based on a bipartite graph model that addresses these\nchallenges. First, it can infer clusters from incomplete data without requiring\ninformation about the center nodes. Second, it is designed to effectively\nhandle heavy-tailed data. Numerical experiments using real financial data\nvalidate the efficiency of the proposed method for data clustering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There are various approaches to graph learning for data clustering,\nincorporating different spectral and structural constraints through diverse\ngraph structures. Some methods rely on bipartite graph models, where nodes are\ndivided into two classes: centers and members. These models typically require\naccess to data for the center nodes in addition to observations from the member\nnodes. However, such additional data may not always be available in many\npractical scenarios. Moreover, popular Gaussian models for graph learning have\ndemonstrated limited effectiveness in modeling data with heavy-tailed\ndistributions, which are common in financial markets. In this paper, we propose\na clustering method based on a bipartite graph model that addresses these\nchallenges. First, it can infer clusters from incomplete data without requiring\ninformation about the center nodes. Second, it is designed to effectively\nhandle heavy-tailed data. Numerical experiments using real financial data\nvalidate the efficiency of the proposed method for data clustering."
                },
                "authors": [
                    {
                        "name": "Amirhossein Javaheri"
                    },
                    {
                        "name": "Daniel P. Palomar"
                    }
                ],
                "author_detail": {
                    "name": "Daniel P. Palomar"
                },
                "author": "Daniel P. Palomar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08590v1",
                "updated": "2025-05-13T14:01:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    1,
                    35,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T14:01:35Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    1,
                    35,
                    1,
                    133,
                    0
                ],
                "title": "Enhancing Thyroid Cytology Diagnosis with RAG-Optimized LLMs and\n  Pa-thology Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Thyroid Cytology Diagnosis with RAG-Optimized LLMs and\n  Pa-thology Foundation Models"
                },
                "summary": "Advancements in artificial intelligence (AI) are transforming pathology by\nintegrat-ing large language models (LLMs) with retrieval-augmented generation\n(RAG) and domain-specific foundation models. This study explores the\napplication of RAG-enhanced LLMs coupled with pathology foundation models for\nthyroid cytology diagnosis, addressing challenges in cytological\ninterpretation, standardization, and diagnostic accuracy. By leveraging a\ncurated knowledge base, RAG facilitates dy-namic retrieval of relevant case\nstudies, diagnostic criteria, and expert interpreta-tion, improving the\ncontextual understanding of LLMs. Meanwhile, pathology foun-dation models,\ntrained on high-resolution pathology images, refine feature extrac-tion and\nclassification capabilities. The fusion of these AI-driven approaches en-hances\ndiagnostic consistency, reduces variability, and supports pathologists in\ndis-tinguishing benign from malignant thyroid lesions. Our results demonstrate\nthat integrating RAG with pathology-specific LLMs significantly improves\ndiagnostic efficiency and interpretability, paving the way for AI-assisted\nthyroid cytopathology, with foundation model UNI achieving AUC 0.73-0.93 for\ncorrect prediction of surgi-cal pathology diagnosis from thyroid cytology\nsamples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in artificial intelligence (AI) are transforming pathology by\nintegrat-ing large language models (LLMs) with retrieval-augmented generation\n(RAG) and domain-specific foundation models. This study explores the\napplication of RAG-enhanced LLMs coupled with pathology foundation models for\nthyroid cytology diagnosis, addressing challenges in cytological\ninterpretation, standardization, and diagnostic accuracy. By leveraging a\ncurated knowledge base, RAG facilitates dy-namic retrieval of relevant case\nstudies, diagnostic criteria, and expert interpreta-tion, improving the\ncontextual understanding of LLMs. Meanwhile, pathology foun-dation models,\ntrained on high-resolution pathology images, refine feature extrac-tion and\nclassification capabilities. The fusion of these AI-driven approaches en-hances\ndiagnostic consistency, reduces variability, and supports pathologists in\ndis-tinguishing benign from malignant thyroid lesions. Our results demonstrate\nthat integrating RAG with pathology-specific LLMs significantly improves\ndiagnostic efficiency and interpretability, paving the way for AI-assisted\nthyroid cytopathology, with foundation model UNI achieving AUC 0.73-0.93 for\ncorrect prediction of surgi-cal pathology diagnosis from thyroid cytology\nsamples."
                },
                "authors": [
                    {
                        "name": "Hussien Al-Asi"
                    },
                    {
                        "name": "Jordan P Reynolds"
                    },
                    {
                        "name": "Shweta Agarwal"
                    },
                    {
                        "name": "Bryan J Dangott"
                    },
                    {
                        "name": "Aziza Nassar"
                    },
                    {
                        "name": "Zeynettin Akkus"
                    }
                ],
                "author_detail": {
                    "name": "Zeynettin Akkus"
                },
                "author": "Zeynettin Akkus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07352v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07352v2",
                "updated": "2025-05-13T13:58:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    58,
                    53,
                    1,
                    133,
                    0
                ],
                "published": "2024-12-10T09:43:34Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    43,
                    34,
                    1,
                    345,
                    0
                ],
                "title": "Inference after discretizing time-varying unobserved heterogeneity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference after discretizing time-varying unobserved heterogeneity"
                },
                "summary": "Approximating time-varying unobserved heterogeneity by discrete types has\nbecome increasingly popular in economics. Yet, provably valid post-clustering\ninference for target parameters in models that do not impose an exact group\nstructure is still lacking. This paper fills this gap in the leading case of a\nlinear panel data model with nonseparable two-way unobserved heterogeneity.\nBuilding on insights from the double machine learning literature, we propose a\nsimple inference procedure based on a bias-reducing moment. Asymptotic theory\nand simulations suggest excellent performance. In the application on fiscal\npolicy we revisit, the novel approach yields conclusions in line with economic\ntheory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximating time-varying unobserved heterogeneity by discrete types has\nbecome increasingly popular in economics. Yet, provably valid post-clustering\ninference for target parameters in models that do not impose an exact group\nstructure is still lacking. This paper fills this gap in the leading case of a\nlinear panel data model with nonseparable two-way unobserved heterogeneity.\nBuilding on insights from the double machine learning literature, we propose a\nsimple inference procedure based on a bias-reducing moment. Asymptotic theory\nand simulations suggest excellent performance. In the application on fiscal\npolicy we revisit, the novel approach yields conclusions in line with economic\ntheory."
                },
                "authors": [
                    {
                        "name": "Jad Beyhum"
                    },
                    {
                        "name": "Martin Mugnier"
                    }
                ],
                "author_detail": {
                    "name": "Martin Mugnier"
                },
                "author": "Martin Mugnier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07352v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07352v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08588v1",
                "updated": "2025-05-13T13:58:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    58,
                    29,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T13:58:29Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    58,
                    29,
                    1,
                    133,
                    0
                ],
                "title": "Small but Significant: On the Promise of Small Language Models for\n  Accessible AIED",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small but Significant: On the Promise of Small Language Models for\n  Accessible AIED"
                },
                "summary": "GPT has become nearly synonymous with large language models (LLMs), an\nincreasingly popular term in AIED proceedings. A simple keyword-based search\nreveals that 61% of the 76 long and short papers presented at AIED 2024\ndescribe novel solutions using LLMs to address some of the long-standing\nchallenges in education, and 43% specifically mention GPT. Although LLMs\npioneered by GPT create exciting opportunities to strengthen the impact of AI\non education, we argue that the field's predominant focus on GPT and other\nresource-intensive LLMs (with more than 10B parameters) risks neglecting the\npotential impact that small language models (SLMs) can make in providing\nresource-constrained institutions with equitable and affordable access to\nhigh-quality AI tools. Supported by positive results on knowledge component\n(KC) discovery, a critical challenge in AIED, we demonstrate that SLMs such as\nPhi-2 can produce an effective solution without elaborate prompting strategies.\nHence, we call for more attention to developing SLM-based AIED approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT has become nearly synonymous with large language models (LLMs), an\nincreasingly popular term in AIED proceedings. A simple keyword-based search\nreveals that 61% of the 76 long and short papers presented at AIED 2024\ndescribe novel solutions using LLMs to address some of the long-standing\nchallenges in education, and 43% specifically mention GPT. Although LLMs\npioneered by GPT create exciting opportunities to strengthen the impact of AI\non education, we argue that the field's predominant focus on GPT and other\nresource-intensive LLMs (with more than 10B parameters) risks neglecting the\npotential impact that small language models (SLMs) can make in providing\nresource-constrained institutions with equitable and affordable access to\nhigh-quality AI tools. Supported by positive results on knowledge component\n(KC) discovery, a critical challenge in AIED, we demonstrate that SLMs such as\nPhi-2 can produce an effective solution without elaborate prompting strategies.\nHence, we call for more attention to developing SLM-based AIED approaches."
                },
                "authors": [
                    {
                        "name": "Yumou Wei"
                    },
                    {
                        "name": "Paulo Carvalho"
                    },
                    {
                        "name": "John Stamper"
                    }
                ],
                "author_detail": {
                    "name": "John Stamper"
                },
                "author": "John Stamper",
                "arxiv_comment": "This vision paper advocates using small language models (e.g., Phi-2)\n  in AI for education (AIED)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17362v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17362v2",
                "updated": "2025-05-13T13:57:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    57,
                    59,
                    1,
                    133,
                    0
                ],
                "published": "2024-10-22T18:55:26Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    18,
                    55,
                    26,
                    1,
                    296,
                    0
                ],
                "title": "Sealing Europa's vents by vapor deposition: An order of magnitude study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sealing Europa's vents by vapor deposition: An order of magnitude study"
                },
                "summary": "Fractures and vents in the ice crust of Europa, exposing the sub-surface\nocean or liquid-water inclusions to the vacuum, might be responsible for the\ngeneration of water-vapor plumes. During its passage through the ice, the plume\nvapor is expected to partially condense on the cold ice walls. Together with\nother effects (water spillage, compression forces, etc.) this mechanism likely\ncontributes to sealing the vent. In this work, we develop a simple\nlumped-parameter model that can quantify how quickly a hypothetical vent of\nprescribed width would be sealed via water-vapor deposition. As an example, we\napply our model to the vent size and density conditions inferred from the 2012\nHubble Space Telescope plume detection, predicting a sealing time of about 30\nminutes. This suggests that the actual ice fracture might have been larger than\noriginally proposed and/or the plume density at the vent might have been lower.\nWhile many other effects could have been present and responsible for sealing\nthe vent, our estimates indicate that vapor deposition might have played a\nmajor role in eventually shutting off the observed plume. A map of sealing\ntimes vs. plume density, mass flow rate and aperture areas is given. Plume\nquantities from the literature are analyzed and compared to our results. For a\ngiven plume density/mass flow rate, small apertures would be sealed quickly by\nvapor deposition and are thus incompatible with observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fractures and vents in the ice crust of Europa, exposing the sub-surface\nocean or liquid-water inclusions to the vacuum, might be responsible for the\ngeneration of water-vapor plumes. During its passage through the ice, the plume\nvapor is expected to partially condense on the cold ice walls. Together with\nother effects (water spillage, compression forces, etc.) this mechanism likely\ncontributes to sealing the vent. In this work, we develop a simple\nlumped-parameter model that can quantify how quickly a hypothetical vent of\nprescribed width would be sealed via water-vapor deposition. As an example, we\napply our model to the vent size and density conditions inferred from the 2012\nHubble Space Telescope plume detection, predicting a sealing time of about 30\nminutes. This suggests that the actual ice fracture might have been larger than\noriginally proposed and/or the plume density at the vent might have been lower.\nWhile many other effects could have been present and responsible for sealing\nthe vent, our estimates indicate that vapor deposition might have played a\nmajor role in eventually shutting off the observed plume. A map of sealing\ntimes vs. plume density, mass flow rate and aperture areas is given. Plume\nquantities from the literature are analyzed and compared to our results. For a\ngiven plume density/mass flow rate, small apertures would be sealed quickly by\nvapor deposition and are thus incompatible with observations."
                },
                "authors": [
                    {
                        "name": "Stefano Boccelli"
                    },
                    {
                        "name": "Shane R. Carberry Mogan"
                    },
                    {
                        "name": "Robert E. Johnson"
                    },
                    {
                        "name": "Orenthal J. Tucker"
                    }
                ],
                "author_detail": {
                    "name": "Orenthal J. Tucker"
                },
                "author": "Orenthal J. Tucker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17362v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17362v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "85A20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08566v1",
                "updated": "2025-05-13T13:41:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    41,
                    4,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T13:41:04Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    41,
                    4,
                    1,
                    133,
                    0
                ],
                "title": "Extract the Best, Discard the Rest: CSI Feedback with Offline Large AI\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extract the Best, Discard the Rest: CSI Feedback with Offline Large AI\n  Models"
                },
                "summary": "Large AI models (LAMs) have shown strong potential in wireless communication\ntasks, but their practical deployment remains hindered by latency and\ncomputational constraints. In this work, we focus on the challenge of\nintegrating LAMs into channel state information (CSI) feedback for\nfrequency-division duplex (FDD) massive multiple-intput multiple-output (MIMO)\nsystems. To this end, we propose two offline frameworks, namely site-specific\nLAM-enhanced CSI feedback (SSLCF) and multi-scenario LAM-enhanced CSI feedback\n(MSLCF), that incorporate LAMs into the codebook-based CSI feedback paradigm\nwithout requiring real-time inference. Specifically, SSLCF generates a\nsite-specific enhanced codebook through fine-tuning on locally collected CSI\ndata, while MSLCF improves generalization by pre-generating a set of\nenvironment-aware codebooks. Both of these frameworks build upon the LAM with\nvision-based backbone, which is pre-trained on large-scale image datasets and\nfine-tuned with CSI data to generate customized codebooks. This resulting\nnetwork named LVM4CF captures the structural similarity between CSI and image,\nallowing the LAM to refine codewords tailored to the specific environments. To\noptimize the codebook refinement capability of LVM4CF under both single- and\ndual-side deployment modes, we further propose corresponding training and\ninference algorithms. Simulation results show that our frameworks significantly\noutperform existing schemes in both reconstruction accuracy and system\nthroughput, without introducing additional inference latency or computational\noverhead. These results also support the core design methodology of our\nproposed frameworks, extracting the best and discarding the rest, as a\npromising pathway for integrating LAMs into future wireless systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large AI models (LAMs) have shown strong potential in wireless communication\ntasks, but their practical deployment remains hindered by latency and\ncomputational constraints. In this work, we focus on the challenge of\nintegrating LAMs into channel state information (CSI) feedback for\nfrequency-division duplex (FDD) massive multiple-intput multiple-output (MIMO)\nsystems. To this end, we propose two offline frameworks, namely site-specific\nLAM-enhanced CSI feedback (SSLCF) and multi-scenario LAM-enhanced CSI feedback\n(MSLCF), that incorporate LAMs into the codebook-based CSI feedback paradigm\nwithout requiring real-time inference. Specifically, SSLCF generates a\nsite-specific enhanced codebook through fine-tuning on locally collected CSI\ndata, while MSLCF improves generalization by pre-generating a set of\nenvironment-aware codebooks. Both of these frameworks build upon the LAM with\nvision-based backbone, which is pre-trained on large-scale image datasets and\nfine-tuned with CSI data to generate customized codebooks. This resulting\nnetwork named LVM4CF captures the structural similarity between CSI and image,\nallowing the LAM to refine codewords tailored to the specific environments. To\noptimize the codebook refinement capability of LVM4CF under both single- and\ndual-side deployment modes, we further propose corresponding training and\ninference algorithms. Simulation results show that our frameworks significantly\noutperform existing schemes in both reconstruction accuracy and system\nthroughput, without introducing additional inference latency or computational\noverhead. These results also support the core design methodology of our\nproposed frameworks, extracting the best and discarding the rest, as a\npromising pathway for integrating LAMs into future wireless systems."
                },
                "authors": [
                    {
                        "name": "Jialin Zhuang"
                    },
                    {
                        "name": "Yafei Wang"
                    },
                    {
                        "name": "Hongwei Hou"
                    },
                    {
                        "name": "Yu Han"
                    },
                    {
                        "name": "Wenjin Wang"
                    },
                    {
                        "name": "Shi Jin"
                    },
                    {
                        "name": "Jiangzhou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangzhou Wang"
                },
                "author": "Jiangzhou Wang",
                "arxiv_comment": "This work has been submitted to the IEEE for possible\n  publication.Copyright may be transferred without notice, after which this\n  version may no longer be accessible",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02423v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02423v2",
                "updated": "2025-05-13T13:19:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    19,
                    32,
                    1,
                    133,
                    0
                ],
                "published": "2025-01-05T02:30:41Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    2,
                    30,
                    41,
                    6,
                    5,
                    0
                ],
                "title": "Scaling Laws for Floating Point Quantization Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Laws for Floating Point Quantization Training"
                },
                "summary": "Low-precision training is considered an effective strategy for reducing both\ntraining and downstream inference costs. Previous scaling laws for precision\nmainly focus on integer quantization, which pay less attention to the\nconstituents in floating-point (FP) quantization, and thus cannot well fit the\nLLM losses in this scenario. In contrast, while FP quantization training is\nmore commonly implemented in production, it's research has been relatively\nsuperficial. In this paper, we thoroughly explore the effects of FP\nquantization targets, exponent bits, mantissa bits, and the calculation\ngranularity of the scaling factor in FP quantization training performance of\nLLM models. In addition to an accurate FP quantization unified scaling law, we\nalso provide valuable suggestions for the community: (1) Exponent bits\ncontribute slightly more to the model performance than mantissa bits. We\nprovide the optimal exponent-mantissa bit ratio for different bit numbers,\nwhich is available for future reference by hardware manufacturers; (2) We\ndiscover the formation of the critical data size in low-precision LLM training.\nToo much training data exceeding the critical data size will inversely bring in\ndegradation of LLM performance; (3) The optimal FP quantization precision is\ndirectly proportional to the computational power, but within a wide\ncomputational power range. We estimate that the best cost-performance precision\nshould lie between 4-8 bits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-precision training is considered an effective strategy for reducing both\ntraining and downstream inference costs. Previous scaling laws for precision\nmainly focus on integer quantization, which pay less attention to the\nconstituents in floating-point (FP) quantization, and thus cannot well fit the\nLLM losses in this scenario. In contrast, while FP quantization training is\nmore commonly implemented in production, it's research has been relatively\nsuperficial. In this paper, we thoroughly explore the effects of FP\nquantization targets, exponent bits, mantissa bits, and the calculation\ngranularity of the scaling factor in FP quantization training performance of\nLLM models. In addition to an accurate FP quantization unified scaling law, we\nalso provide valuable suggestions for the community: (1) Exponent bits\ncontribute slightly more to the model performance than mantissa bits. We\nprovide the optimal exponent-mantissa bit ratio for different bit numbers,\nwhich is available for future reference by hardware manufacturers; (2) We\ndiscover the formation of the critical data size in low-precision LLM training.\nToo much training data exceeding the critical data size will inversely bring in\ndegradation of LLM performance; (3) The optimal FP quantization precision is\ndirectly proportional to the computational power, but within a wide\ncomputational power range. We estimate that the best cost-performance precision\nshould lie between 4-8 bits."
                },
                "authors": [
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Weidong Han"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Yixing Li"
                    },
                    {
                        "name": "An Wang"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Jie Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Jiang"
                },
                "author": "Jie Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02423v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02423v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07142v2",
                "updated": "2025-05-13T13:15:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    15,
                    18,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-11T22:58:26Z",
                "published_parsed": [
                    2025,
                    5,
                    11,
                    22,
                    58,
                    26,
                    6,
                    131,
                    0
                ],
                "title": "Exploring Anthropomorphism in Conversational Agents for Environmental\n  Sustainability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Anthropomorphism in Conversational Agents for Environmental\n  Sustainability"
                },
                "summary": "The paper investigates the integration of Large Language Models (LLMs) into\nConversational Agents (CAs) to encourage a shift in consumption patterns from a\ndemand-driven to a supply-based paradigm. Specifically, the research examines\nthe role of anthropomorphic design in delivering environmentally conscious\nmessages by comparing two CA designs: a personified agent representing an\nappliance and a traditional, non-personified assistant. A lab study (N=26)\nassessed the impact of these designs on interaction, perceived self-efficacy,\nand engagement. Results indicate that LLM-based CAs significantly enhance\nusers' self-reported eco-friendly behaviors, with participants expressing\ngreater confidence in managing energy consumption. While the anthropomorphic\ndesign did not notably affect self-efficacy, those interacting with the\npersonified agent reported a stronger sense of connection with the system.\nThese findings suggest that although anthropomorphic CAs may improve user\nengagement, both designs hold promise for fostering sustainable behaviors in\nhome energy management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paper investigates the integration of Large Language Models (LLMs) into\nConversational Agents (CAs) to encourage a shift in consumption patterns from a\ndemand-driven to a supply-based paradigm. Specifically, the research examines\nthe role of anthropomorphic design in delivering environmentally conscious\nmessages by comparing two CA designs: a personified agent representing an\nappliance and a traditional, non-personified assistant. A lab study (N=26)\nassessed the impact of these designs on interaction, perceived self-efficacy,\nand engagement. Results indicate that LLM-based CAs significantly enhance\nusers' self-reported eco-friendly behaviors, with participants expressing\ngreater confidence in managing energy consumption. While the anthropomorphic\ndesign did not notably affect self-efficacy, those interacting with the\npersonified agent reported a stronger sense of connection with the system.\nThese findings suggest that although anthropomorphic CAs may improve user\nengagement, both designs hold promise for fostering sustainable behaviors in\nhome energy management."
                },
                "authors": [
                    {
                        "name": "Mathyas Giudici"
                    },
                    {
                        "name": "Samuele Scherini"
                    },
                    {
                        "name": "Pascal Chaussumier"
                    },
                    {
                        "name": "Stefano Ginocchio"
                    },
                    {
                        "name": "Franca Garzotto"
                    }
                ],
                "author_detail": {
                    "name": "Franca Garzotto"
                },
                "author": "Franca Garzotto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04717v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04717v4",
                "updated": "2025-05-14T01:48:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    1,
                    48,
                    30,
                    2,
                    134,
                    0
                ],
                "published": "2025-04-07T04:00:08Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    4,
                    0,
                    8,
                    0,
                    97,
                    0
                ],
                "title": "Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large\n  Language Models"
                },
                "summary": "Recent advancements in large language models (LLMs) have revolutionized their\nability to handle single-turn tasks, yet real-world applications demand\nsophisticated multi-turn interactions. This survey provides a comprehensive\nreview of recent advancements in evaluating and enhancing multi-turn\ninteractions in LLMs. Focusing on task-specific scenarios, from instruction\nfollowing in diverse domains such as math and coding to complex conversational\nengagements in roleplay, healthcare, education, and even adversarial jailbreak\nsettings, we systematically examine the challenges of maintaining context,\ncoherence, fairness, and responsiveness over prolonged dialogues. The paper\norganizes current benchmarks and datasets into coherent categories that reflect\nthe evolving landscape of multi-turn dialogue evaluation. In addition, we\nreview a range of enhancement methodologies under multi-turn settings,\nincluding model-centric strategies (contextual learning, supervised\nfine-tuning, reinforcement learning, and new architectures), external\nintegration approaches (memory-augmented, retrieval-based methods, and\nknowledge graph), and agent-based techniques for collaborative interactions.\nFinally, we discuss open challenges and propose future directions for research\nto further advance the robustness and effectiveness of multi-turn interactions\nin LLMs. Related resources and papers are available at\nhttps://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have revolutionized their\nability to handle single-turn tasks, yet real-world applications demand\nsophisticated multi-turn interactions. This survey provides a comprehensive\nreview of recent advancements in evaluating and enhancing multi-turn\ninteractions in LLMs. Focusing on task-specific scenarios, from instruction\nfollowing in diverse domains such as math and coding to complex conversational\nengagements in roleplay, healthcare, education, and even adversarial jailbreak\nsettings, we systematically examine the challenges of maintaining context,\ncoherence, fairness, and responsiveness over prolonged dialogues. The paper\norganizes current benchmarks and datasets into coherent categories that reflect\nthe evolving landscape of multi-turn dialogue evaluation. In addition, we\nreview a range of enhancement methodologies under multi-turn settings,\nincluding model-centric strategies (contextual learning, supervised\nfine-tuning, reinforcement learning, and new architectures), external\nintegration approaches (memory-augmented, retrieval-based methods, and\nknowledge graph), and agent-based techniques for collaborative interactions.\nFinally, we discuss open challenges and propose future directions for research\nto further advance the robustness and effectiveness of multi-turn interactions\nin LLMs. Related resources and papers are available at\nhttps://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs."
                },
                "authors": [
                    {
                        "name": "Yubo Li"
                    },
                    {
                        "name": "Xiaobin Shen"
                    },
                    {
                        "name": "Xinyu Yao"
                    },
                    {
                        "name": "Xueying Ding"
                    },
                    {
                        "name": "Yidi Miao"
                    },
                    {
                        "name": "Ramayya Krishnan"
                    },
                    {
                        "name": "Rema Padman"
                    }
                ],
                "author_detail": {
                    "name": "Rema Padman"
                },
                "author": "Rema Padman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04717v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04717v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08542v1",
                "updated": "2025-05-13T13:13:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    13,
                    26,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T13:13:26Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    13,
                    26,
                    1,
                    133,
                    0
                ],
                "title": "Guiding LLM-based Smart Contract Generation with Finite State Machine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding LLM-based Smart Contract Generation with Finite State Machine"
                },
                "summary": "Smart contract is a kind of self-executing code based on blockchain\ntechnology with a wide range of application scenarios, but the traditional\ngeneration method relies on manual coding and expert auditing, which has a high\nthreshold and low efficiency. Although Large Language Models (LLMs) show great\npotential in programming tasks, they still face challenges in smart contract\ngeneration w.r.t. effectiveness and security. To solve these problems, we\npropose FSM-SCG, a smart contract generation framework based on finite state\nmachine (FSM) and LLMs, which significantly improves the quality of the\ngenerated code by abstracting user requirements to generate FSM, guiding LLMs\nto generate smart contracts, and iteratively optimizing the code with the\nfeedback of compilation and security checks. The experimental results show that\nFSM-SCG significantly improves the quality of smart contract generation.\nCompared to the best baseline, FSM-SCG improves the compilation success rate of\ngenerated smart contract code by at most 48%, and reduces the average\nvulnerability risk score by approximately 68%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart contract is a kind of self-executing code based on blockchain\ntechnology with a wide range of application scenarios, but the traditional\ngeneration method relies on manual coding and expert auditing, which has a high\nthreshold and low efficiency. Although Large Language Models (LLMs) show great\npotential in programming tasks, they still face challenges in smart contract\ngeneration w.r.t. effectiveness and security. To solve these problems, we\npropose FSM-SCG, a smart contract generation framework based on finite state\nmachine (FSM) and LLMs, which significantly improves the quality of the\ngenerated code by abstracting user requirements to generate FSM, guiding LLMs\nto generate smart contracts, and iteratively optimizing the code with the\nfeedback of compilation and security checks. The experimental results show that\nFSM-SCG significantly improves the quality of smart contract generation.\nCompared to the best baseline, FSM-SCG improves the compilation success rate of\ngenerated smart contract code by at most 48%, and reduces the average\nvulnerability risk score by approximately 68%."
                },
                "authors": [
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Yuhao Lin"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Xintong Hu"
                    },
                    {
                        "name": "Yuxiang Wang"
                    },
                    {
                        "name": "Qiming Zeng"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Jiawei Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Jiang"
                },
                "author": "Jiawei Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00782v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00782v2",
                "updated": "2025-05-13T13:13:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    13,
                    18,
                    1,
                    133,
                    0
                ],
                "published": "2024-10-16T20:24:16Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    20,
                    24,
                    16,
                    2,
                    290,
                    0
                ],
                "title": "TradExpert: Revolutionizing Trading with Mixture of Expert LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TradExpert: Revolutionizing Trading with Mixture of Expert LLMs"
                },
                "summary": "The integration of Artificial Intelligence (AI) in the financial domain has\nopened new avenues for quantitative trading, particularly through the use of\nLarge Language Models (LLMs). However, the challenge of effectively\nsynthesizing insights from diverse data sources and integrating both structured\nand unstructured data persists. This paper presents TradeExpert, a novel\nframework that employs a mix of experts (MoE) approach, using four specialized\nLLMs, each analyzing distinct sources of financial data, including news\narticles, market data, alpha factors, and fundamental data. The insights of\nthese expert LLMs are further synthesized by a General Expert LLM to make a\nfinal prediction or decision. With specific prompts, TradeExpert can be\nswitched between the prediction mode and the ranking mode for stock movement\nprediction and quantitative stock trading, respectively. In addition to\nexisting benchmarks, we also release a large-scale financial dataset to\ncomprehensively evaluate TradeExpert's effectiveness. Our experimental results\ndemonstrate TradeExpert's superior performance across all trading scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Artificial Intelligence (AI) in the financial domain has\nopened new avenues for quantitative trading, particularly through the use of\nLarge Language Models (LLMs). However, the challenge of effectively\nsynthesizing insights from diverse data sources and integrating both structured\nand unstructured data persists. This paper presents TradeExpert, a novel\nframework that employs a mix of experts (MoE) approach, using four specialized\nLLMs, each analyzing distinct sources of financial data, including news\narticles, market data, alpha factors, and fundamental data. The insights of\nthese expert LLMs are further synthesized by a General Expert LLM to make a\nfinal prediction or decision. With specific prompts, TradeExpert can be\nswitched between the prediction mode and the ranking mode for stock movement\nprediction and quantitative stock trading, respectively. In addition to\nexisting benchmarks, we also release a large-scale financial dataset to\ncomprehensively evaluate TradeExpert's effectiveness. Our experimental results\ndemonstrate TradeExpert's superior performance across all trading scenarios."
                },
                "authors": [
                    {
                        "name": "Qianggang Ding"
                    },
                    {
                        "name": "Haochen Shi"
                    },
                    {
                        "name": "Jiadong Guo"
                    },
                    {
                        "name": "Bang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Bang Liu"
                },
                "author": "Bang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00782v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00782v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08535v1",
                "updated": "2025-05-13T13:04:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    4,
                    46,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T13:04:46Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    4,
                    46,
                    1,
                    133,
                    0
                ],
                "title": "Diffusion-assisted Model Predictive Control Optimization for Power\n  System Real-Time Operation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-assisted Model Predictive Control Optimization for Power\n  System Real-Time Operation"
                },
                "summary": "This paper presents a modified model predictive control (MPC) framework for\nreal-time power system operation. The framework incorporates a diffusion model\ntailored for time series generation to enhance the accuracy of the load\nforecasting module used in the system operation. In the absence of explicit\nstate transition law, a model-identification procedure is leveraged to derive\nthe system dynamics, thereby eliminating a barrier when applying MPC to a\nrenewables-dominated power system. Case study results on an industry park\nsystem and the IEEE 30-bus system demonstrate that using the diffusion model to\naugment the training dataset significantly improves load-forecasting accuracy,\nand the inferred system dynamics are applicable to the real-time grid operation\nwith solar and wind.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a modified model predictive control (MPC) framework for\nreal-time power system operation. The framework incorporates a diffusion model\ntailored for time series generation to enhance the accuracy of the load\nforecasting module used in the system operation. In the absence of explicit\nstate transition law, a model-identification procedure is leveraged to derive\nthe system dynamics, thereby eliminating a barrier when applying MPC to a\nrenewables-dominated power system. Case study results on an industry park\nsystem and the IEEE 30-bus system demonstrate that using the diffusion model to\naugment the training dataset significantly improves load-forecasting accuracy,\nand the inferred system dynamics are applicable to the real-time grid operation\nwith solar and wind."
                },
                "authors": [
                    {
                        "name": "Linna Xu"
                    },
                    {
                        "name": "Yongli Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yongli Zhu"
                },
                "author": "Yongli Zhu",
                "arxiv_comment": "This paper has been accepted by the 2025 IEEE PES General Meeting\n  (PESGM) which will be held in Austin, TX, July.27-31, 2005",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08532v1",
                "updated": "2025-05-13T13:03:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    3,
                    20,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T13:03:20Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    3,
                    20,
                    1,
                    133,
                    0
                ],
                "title": "The Truth Becomes Clearer Through Debate! Multi-Agent Systems with Large\n  Language Models Unmask Fake News",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Truth Becomes Clearer Through Debate! Multi-Agent Systems with Large\n  Language Models Unmask Fake News"
                },
                "summary": "In today's digital environment, the rapid propagation of fake news via social\nnetworks poses significant social challenges. Most existing detection methods\neither employ traditional classification models, which suffer from low\ninterpretability and limited generalization capabilities, or craft specific\nprompts for large language models (LLMs) to produce explanations and results\ndirectly, failing to leverage LLMs' reasoning abilities fully. Inspired by the\nsaying that \"truth becomes clearer through debate,\" our study introduces a\nnovel multi-agent system with LLMs named TruEDebate (TED) to enhance the\ninterpretability and effectiveness of fake news detection. TED employs a\nrigorous debate process inspired by formal debate settings. Central to our\napproach are two innovative components: the DebateFlow Agents and the\nInsightFlow Agents. The DebateFlow Agents organize agents into two teams, where\none supports and the other challenges the truth of the news. These agents\nengage in opening statements, cross-examination, rebuttal, and closing\nstatements, simulating a rigorous debate process akin to human discourse\nanalysis, allowing for a thorough evaluation of news content. Concurrently, the\nInsightFlow Agents consist of two specialized sub-agents: the Synthesis Agent\nand the Analysis Agent. The Synthesis Agent summarizes the debates and provides\nan overarching viewpoint, ensuring a coherent and comprehensive evaluation. The\nAnalysis Agent, which includes a role-aware encoder and a debate graph,\nintegrates role embeddings and models the interactions between debate roles and\narguments using an attention mechanism, providing the final judgment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In today's digital environment, the rapid propagation of fake news via social\nnetworks poses significant social challenges. Most existing detection methods\neither employ traditional classification models, which suffer from low\ninterpretability and limited generalization capabilities, or craft specific\nprompts for large language models (LLMs) to produce explanations and results\ndirectly, failing to leverage LLMs' reasoning abilities fully. Inspired by the\nsaying that \"truth becomes clearer through debate,\" our study introduces a\nnovel multi-agent system with LLMs named TruEDebate (TED) to enhance the\ninterpretability and effectiveness of fake news detection. TED employs a\nrigorous debate process inspired by formal debate settings. Central to our\napproach are two innovative components: the DebateFlow Agents and the\nInsightFlow Agents. The DebateFlow Agents organize agents into two teams, where\none supports and the other challenges the truth of the news. These agents\nengage in opening statements, cross-examination, rebuttal, and closing\nstatements, simulating a rigorous debate process akin to human discourse\nanalysis, allowing for a thorough evaluation of news content. Concurrently, the\nInsightFlow Agents consist of two specialized sub-agents: the Synthesis Agent\nand the Analysis Agent. The Synthesis Agent summarizes the debates and provides\nan overarching viewpoint, ensuring a coherent and comprehensive evaluation. The\nAnalysis Agent, which includes a role-aware encoder and a debate graph,\nintegrates role embeddings and models the interactions between debate roles and\narguments using an attention mechanism, providing the final judgment."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuxuan Liu"
                    },
                    {
                        "name": "Xiaoqing Zhang"
                    },
                    {
                        "name": "Xiuying Chen"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "arxiv_comment": "SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01844v3",
                "updated": "2025-05-13T12:45:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    45,
                    16,
                    1,
                    133,
                    0
                ],
                "published": "2025-03-03T18:59:54Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    59,
                    54,
                    0,
                    62,
                    0
                ],
                "title": "Can (A)I Change Your Mind?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can (A)I Change Your Mind?"
                },
                "summary": "The increasing integration of large language models (LLMs) based\nconversational agents into everyday life raises critical cognitive and social\nquestions about their potential to influence human opinions. Although previous\nstudies have shown that LLM-based agents can generate persuasive content, these\ntypically involve controlled English-language settings. Addressing this, our\npreregistered study explored LLMs' persuasive capabilities in more ecological,\nunconstrained scenarios, examining both static (written paragraphs) and dynamic\n(conversations via Telegram) interaction types. Conducted entirely in Hebrew\nwith 200 participants, the study assessed the persuasive effects of both LLM\nand human interlocutors on controversial civil policy topics. Results indicated\nthat participants adopted LLM and human perspectives similarly, with\nsignificant opinion changes evident across all conditions, regardless of\ninterlocutor type or interaction mode. Confidence levels increased\nsignificantly in most scenarios. These findings demonstrate LLM-based agents'\nrobust persuasive capabilities across diverse sources and settings,\nhighlighting their potential impact on shaping public opinions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing integration of large language models (LLMs) based\nconversational agents into everyday life raises critical cognitive and social\nquestions about their potential to influence human opinions. Although previous\nstudies have shown that LLM-based agents can generate persuasive content, these\ntypically involve controlled English-language settings. Addressing this, our\npreregistered study explored LLMs' persuasive capabilities in more ecological,\nunconstrained scenarios, examining both static (written paragraphs) and dynamic\n(conversations via Telegram) interaction types. Conducted entirely in Hebrew\nwith 200 participants, the study assessed the persuasive effects of both LLM\nand human interlocutors on controversial civil policy topics. Results indicated\nthat participants adopted LLM and human perspectives similarly, with\nsignificant opinion changes evident across all conditions, regardless of\ninterlocutor type or interaction mode. Confidence levels increased\nsignificantly in most scenarios. These findings demonstrate LLM-based agents'\nrobust persuasive capabilities across diverse sources and settings,\nhighlighting their potential impact on shaping public opinions."
                },
                "authors": [
                    {
                        "name": "Miriam Havin"
                    },
                    {
                        "name": "Timna Wharton Kleinman"
                    },
                    {
                        "name": "Moran Koren"
                    },
                    {
                        "name": "Yaniv Dover"
                    },
                    {
                        "name": "Ariel Goldstein"
                    }
                ],
                "author_detail": {
                    "name": "Ariel Goldstein"
                },
                "author": "Ariel Goldstein",
                "arxiv_comment": "Accetped to CogSci 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03906v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03906v2",
                "updated": "2025-05-13T12:41:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    41,
                    18,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-06T18:22:38Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    18,
                    22,
                    38,
                    1,
                    126,
                    0
                ],
                "title": "MARCO: A Multi-Agent System for Optimizing HPC Code Generation Using\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARCO: A Multi-Agent System for Optimizing HPC Code Generation Using\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) have transformed software development through\ncode generation capabilities, yet their effectiveness for high-performance\ncomputing (HPC) remains limited. HPC code requires specialized optimizations\nfor parallelism, memory efficiency, and architecture-specific considerations\nthat general-purpose LLMs often overlook. We present MARCO (Multi-Agent\nReactive Code Optimizer), a novel framework that enhances LLM-generated code\nfor HPC through a specialized multi-agent architecture. MARCO employs separate\nagents for code generation and performance evaluation, connected by a feedback\nloop that progressively refines optimizations. A key innovation is MARCO's\nweb-search component that retrieves real-time optimization techniques from\nrecent conference proceedings and research publications, bridging the knowledge\ngap in pre-trained LLMs. Our extensive evaluation on the LeetCode 75 problem\nset demonstrates that MARCO achieves a 14.6% average runtime reduction compared\nto Claude 3.5 Sonnet alone, while the integration of the web-search component\nyields a 30.9% performance improvement over the base MARCO system. These\nresults highlight the potential of multi-agent systems to address the\nspecialized requirements of high-performance code generation, offering a\ncost-effective alternative to domain-specific model fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have transformed software development through\ncode generation capabilities, yet their effectiveness for high-performance\ncomputing (HPC) remains limited. HPC code requires specialized optimizations\nfor parallelism, memory efficiency, and architecture-specific considerations\nthat general-purpose LLMs often overlook. We present MARCO (Multi-Agent\nReactive Code Optimizer), a novel framework that enhances LLM-generated code\nfor HPC through a specialized multi-agent architecture. MARCO employs separate\nagents for code generation and performance evaluation, connected by a feedback\nloop that progressively refines optimizations. A key innovation is MARCO's\nweb-search component that retrieves real-time optimization techniques from\nrecent conference proceedings and research publications, bridging the knowledge\ngap in pre-trained LLMs. Our extensive evaluation on the LeetCode 75 problem\nset demonstrates that MARCO achieves a 14.6% average runtime reduction compared\nto Claude 3.5 Sonnet alone, while the integration of the web-search component\nyields a 30.9% performance improvement over the base MARCO system. These\nresults highlight the potential of multi-agent systems to address the\nspecialized requirements of high-performance code generation, offering a\ncost-effective alternative to domain-specific model fine-tuning."
                },
                "authors": [
                    {
                        "name": "Asif Rahman"
                    },
                    {
                        "name": "Veljko Cvetkovic"
                    },
                    {
                        "name": "Kathleen Reece"
                    },
                    {
                        "name": "Aidan Walters"
                    },
                    {
                        "name": "Yasir Hassan"
                    },
                    {
                        "name": "Aneesh Tummeti"
                    },
                    {
                        "name": "Bryan Torres"
                    },
                    {
                        "name": "Denise Cooney"
                    },
                    {
                        "name": "Margaret Ellis"
                    },
                    {
                        "name": "Dimitrios S. Nikolopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios S. Nikolopoulos"
                },
                "author": "Dimitrios S. Nikolopoulos",
                "arxiv_comment": "9 pages, 4 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03906v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03906v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08508v1",
                "updated": "2025-05-13T12:39:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    39,
                    6,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T12:39:06Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    39,
                    6,
                    1,
                    133,
                    0
                ],
                "title": "TrialMatchAI: An End-to-End AI-powered Clinical Trial Recommendation\n  System to Streamline Patient-to-Trial Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrialMatchAI: An End-to-End AI-powered Clinical Trial Recommendation\n  System to Streamline Patient-to-Trial Matching"
                },
                "summary": "Patient recruitment remains a major bottleneck in clinical trials, calling\nfor scalable and automated solutions. We present TrialMatchAI, an AI-powered\nrecommendation system that automates patient-to-trial matching by processing\nheterogeneous clinical data, including structured records and unstructured\nphysician notes. Built on fine-tuned, open-source large language models (LLMs)\nwithin a retrieval-augmented generation framework, TrialMatchAI ensures\ntransparency and reproducibility and maintains a lightweight deployment\nfootprint suitable for clinical environments. The system normalizes biomedical\nentities, retrieves relevant trials using a hybrid search strategy combining\nlexical and semantic similarity, re-ranks results, and performs criterion-level\neligibility assessments using medical Chain-of-Thought reasoning. This pipeline\ndelivers explainable outputs with traceable decision rationales. In real-world\nvalidation, 92 percent of oncology patients had at least one relevant trial\nretrieved within the top 20 recommendations. Evaluation across synthetic and\nreal clinical datasets confirmed state-of-the-art performance, with expert\nassessment validating over 90 percent accuracy in criterion-level eligibility\nclassification, particularly excelling in biomarker-driven matches. Designed\nfor modularity and privacy, TrialMatchAI supports Phenopackets-standardized\ndata, enables secure local deployment, and allows seamless replacement of LLM\ncomponents as more advanced models emerge. By enhancing efficiency and\ninterpretability and offering lightweight, open-source deployment, TrialMatchAI\nprovides a scalable solution for AI-driven clinical trial matching in precision\nmedicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patient recruitment remains a major bottleneck in clinical trials, calling\nfor scalable and automated solutions. We present TrialMatchAI, an AI-powered\nrecommendation system that automates patient-to-trial matching by processing\nheterogeneous clinical data, including structured records and unstructured\nphysician notes. Built on fine-tuned, open-source large language models (LLMs)\nwithin a retrieval-augmented generation framework, TrialMatchAI ensures\ntransparency and reproducibility and maintains a lightweight deployment\nfootprint suitable for clinical environments. The system normalizes biomedical\nentities, retrieves relevant trials using a hybrid search strategy combining\nlexical and semantic similarity, re-ranks results, and performs criterion-level\neligibility assessments using medical Chain-of-Thought reasoning. This pipeline\ndelivers explainable outputs with traceable decision rationales. In real-world\nvalidation, 92 percent of oncology patients had at least one relevant trial\nretrieved within the top 20 recommendations. Evaluation across synthetic and\nreal clinical datasets confirmed state-of-the-art performance, with expert\nassessment validating over 90 percent accuracy in criterion-level eligibility\nclassification, particularly excelling in biomarker-driven matches. Designed\nfor modularity and privacy, TrialMatchAI supports Phenopackets-standardized\ndata, enables secure local deployment, and allows seamless replacement of LLM\ncomponents as more advanced models emerge. By enhancing efficiency and\ninterpretability and offering lightweight, open-source deployment, TrialMatchAI\nprovides a scalable solution for AI-driven clinical trial matching in precision\nmedicine."
                },
                "authors": [
                    {
                        "name": "Majd Abdallah"
                    },
                    {
                        "name": "Sigve Nakken"
                    },
                    {
                        "name": "Mariska Bierkens"
                    },
                    {
                        "name": "Johanna Galvis"
                    },
                    {
                        "name": "Alexis Groppi"
                    },
                    {
                        "name": "Slim Karkar"
                    },
                    {
                        "name": "Lana Meiqari"
                    },
                    {
                        "name": "Maria Alexandra Rujano"
                    },
                    {
                        "name": "Steve Canham"
                    },
                    {
                        "name": "Rodrigo Dienstmann"
                    },
                    {
                        "name": "Remond Fijneman"
                    },
                    {
                        "name": "Eivind Hovig"
                    },
                    {
                        "name": "Gerrit Meijer"
                    },
                    {
                        "name": "Macha Nikolski"
                    }
                ],
                "author_detail": {
                    "name": "Macha Nikolski"
                },
                "author": "Macha Nikolski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08507v1",
                "updated": "2025-05-13T12:37:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    37,
                    48,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T12:37:48Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    37,
                    48,
                    1,
                    133,
                    0
                ],
                "title": "InfoPO: On Mutual Information Maximization for Large Language Model\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfoPO: On Mutual Information Maximization for Large Language Model\n  Alignment"
                },
                "summary": "We study the post-training of large language models (LLMs) with human\npreference data. Recently, direct preference optimization and its variants have\nshown considerable promise in aligning language models, eliminating the need\nfor reward models and online sampling. Despite these benefits, these methods\nrely on explicit assumptions about the Bradley-Terry (BT) model, which makes\nthem prone to overfitting and results in suboptimal performance, particularly\non reasoning-heavy tasks. To address these challenges, we propose a principled\npreference fine-tuning algorithm called InfoPO, which effectively and\nefficiently aligns large language models using preference data. InfoPO\neliminates the reliance on the BT model and prevents the likelihood of the\nchosen response from decreasing. Extensive experiments confirm that InfoPO\nconsistently outperforms established baselines on widely used open benchmarks,\nparticularly in reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the post-training of large language models (LLMs) with human\npreference data. Recently, direct preference optimization and its variants have\nshown considerable promise in aligning language models, eliminating the need\nfor reward models and online sampling. Despite these benefits, these methods\nrely on explicit assumptions about the Bradley-Terry (BT) model, which makes\nthem prone to overfitting and results in suboptimal performance, particularly\non reasoning-heavy tasks. To address these challenges, we propose a principled\npreference fine-tuning algorithm called InfoPO, which effectively and\nefficiently aligns large language models using preference data. InfoPO\neliminates the reliance on the BT model and prevents the likelihood of the\nchosen response from decreasing. Extensive experiments confirm that InfoPO\nconsistently outperforms established baselines on widely used open benchmarks,\nparticularly in reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Teng Xiao"
                    },
                    {
                        "name": "Zhen Ge"
                    },
                    {
                        "name": "Sujay Sanghavi"
                    },
                    {
                        "name": "Tian Wang"
                    },
                    {
                        "name": "Julian Katz-Samuels"
                    },
                    {
                        "name": "Marc Versage"
                    },
                    {
                        "name": "Qingjun Cui"
                    },
                    {
                        "name": "Trishul Chilimbi"
                    }
                ],
                "author_detail": {
                    "name": "Trishul Chilimbi"
                },
                "author": "Trishul Chilimbi",
                "arxiv_comment": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08498v1",
                "updated": "2025-05-13T12:26:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    26,
                    16,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T12:26:16Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    26,
                    16,
                    1,
                    133,
                    0
                ],
                "title": "LCES: Zero-shot Automated Essay Scoring via Pairwise Comparisons Using\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LCES: Zero-shot Automated Essay Scoring via Pairwise Comparisons Using\n  Large Language Models"
                },
                "summary": "Recent advances in large language models (LLMs) have enabled zero-shot\nautomated essay scoring (AES), providing a promising way to reduce the cost and\neffort of essay scoring in comparison with manual grading. However, most\nexisting zero-shot approaches rely on LLMs to directly generate absolute\nscores, which often diverge from human evaluations owing to model biases and\ninconsistent scoring. To address these limitations, we propose LLM-based\nComparative Essay Scoring (LCES), a method that formulates AES as a pairwise\ncomparison task. Specifically, we instruct LLMs to judge which of two essays is\nbetter, collect many such comparisons, and convert them into continuous scores.\nConsidering that the number of possible comparisons grows quadratically with\nthe number of essays, we improve scalability by employing RankNet to\nefficiently transform LLM preferences into scalar scores. Experiments using AES\nbenchmark datasets show that LCES outperforms conventional zero-shot methods in\naccuracy while maintaining computational efficiency. Moreover, LCES is robust\nacross different LLM backbones, highlighting its applicability to real-world\nzero-shot AES.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have enabled zero-shot\nautomated essay scoring (AES), providing a promising way to reduce the cost and\neffort of essay scoring in comparison with manual grading. However, most\nexisting zero-shot approaches rely on LLMs to directly generate absolute\nscores, which often diverge from human evaluations owing to model biases and\ninconsistent scoring. To address these limitations, we propose LLM-based\nComparative Essay Scoring (LCES), a method that formulates AES as a pairwise\ncomparison task. Specifically, we instruct LLMs to judge which of two essays is\nbetter, collect many such comparisons, and convert them into continuous scores.\nConsidering that the number of possible comparisons grows quadratically with\nthe number of essays, we improve scalability by employing RankNet to\nefficiently transform LLM preferences into scalar scores. Experiments using AES\nbenchmark datasets show that LCES outperforms conventional zero-shot methods in\naccuracy while maintaining computational efficiency. Moreover, LCES is robust\nacross different LLM backbones, highlighting its applicability to real-world\nzero-shot AES."
                },
                "authors": [
                    {
                        "name": "Takumi Shibata"
                    },
                    {
                        "name": "Yuichi Miyamura"
                    }
                ],
                "author_detail": {
                    "name": "Yuichi Miyamura"
                },
                "author": "Yuichi Miyamura",
                "arxiv_comment": "14 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08493v1",
                "updated": "2025-05-13T12:23:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    23,
                    11,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T12:23:11Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    23,
                    11,
                    1,
                    133,
                    0
                ],
                "title": "BizChat: Scaffolding AI-Powered Business Planning for Small Business\n  Owners Across Digital Skill Levels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BizChat: Scaffolding AI-Powered Business Planning for Small Business\n  Owners Across Digital Skill Levels"
                },
                "summary": "Generative AI can help small business owners automate tasks, increase\nefficiency, and improve their bottom line. However, despite the seemingly\nintuitive design of systems like ChatGPT, significant barriers remain for those\nless comfortable with technology. To address these disparities, prior work\nhighlights accessory skills -- beyond prompt engineering -- users must master\nto successfully adopt generative AI including keyboard shortcuts, editing\nskills, file conversions, and browser literacy. Building on a design workshop\nseries and 15 interviews with small businesses, we introduce BizChat, a large\nlanguage model (LLM)-powered web application that helps business owners across\ndigital skills levels write their business plan -- an essential but often\nneglected document. To do so, BizChat's interface embodies three design\nconsiderations inspired by learning sciences: ensuring accessibility to users\nwith less digital skills while maintaining extensibility to power users\n(\"low-floor-high-ceiling\"), providing in situ micro-learning to support\nentrepreneurial education (\"just-in-time learning\"), and framing interaction\naround business activities (\"contextualized technology introduction\"). We\nconclude with plans for a future BizChat deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI can help small business owners automate tasks, increase\nefficiency, and improve their bottom line. However, despite the seemingly\nintuitive design of systems like ChatGPT, significant barriers remain for those\nless comfortable with technology. To address these disparities, prior work\nhighlights accessory skills -- beyond prompt engineering -- users must master\nto successfully adopt generative AI including keyboard shortcuts, editing\nskills, file conversions, and browser literacy. Building on a design workshop\nseries and 15 interviews with small businesses, we introduce BizChat, a large\nlanguage model (LLM)-powered web application that helps business owners across\ndigital skills levels write their business plan -- an essential but often\nneglected document. To do so, BizChat's interface embodies three design\nconsiderations inspired by learning sciences: ensuring accessibility to users\nwith less digital skills while maintaining extensibility to power users\n(\"low-floor-high-ceiling\"), providing in situ micro-learning to support\nentrepreneurial education (\"just-in-time learning\"), and framing interaction\naround business activities (\"contextualized technology introduction\"). We\nconclude with plans for a future BizChat deployment."
                },
                "authors": [
                    {
                        "name": "Quentin Romero Lauro"
                    },
                    {
                        "name": "Aakash Gautam"
                    },
                    {
                        "name": "Yasmine Kotturi"
                    }
                ],
                "author_detail": {
                    "name": "Yasmine Kotturi"
                },
                "author": "Yasmine Kotturi",
                "arxiv_doi": "10.1145/3707640.3731928",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3707640.3731928",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.08493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "4 pages, 1 figure, CHIWORK '25 Adjunct, June 23-25, 2025, Amsterdam,\n  Netherlands",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08492v1",
                "updated": "2025-05-13T12:22:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    22,
                    38,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T12:22:38Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    22,
                    38,
                    1,
                    133,
                    0
                ],
                "title": "Achieving Scalable Robot Autonomy via neurosymbolic planning using\n  lightweight local LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving Scalable Robot Autonomy via neurosymbolic planning using\n  lightweight local LLM"
                },
                "summary": "PDDL-based symbolic task planning remains pivotal for robot autonomy yet\nstruggles with dynamic human-robot collaboration due to scalability,\nre-planning demands, and delayed plan availability. Although a few\nneurosymbolic frameworks have previously leveraged LLMs such as GPT-3 to\naddress these challenges, reliance on closed-source, remote models with limited\ncontext introduced critical constraints: third-party dependency, inconsistent\nresponse times, restricted plan length and complexity, and multi-domain\nscalability issues. We present Gideon, a novel framework that enables the\ntransition to modern, smaller, local LLMs with extended context length. Gideon\nintegrates a novel problem generator to systematically generate large-scale\ndatasets of realistic domain-problem-plan tuples for any domain, and adapts\nneurosymbolic planning for local LLMs, enabling on-device execution and\nextended context for multi-domain support. Preliminary experiments in\nsingle-domain scenarios performed on Qwen-2.5 1.5B and trained on 8k-32k\nsamples, demonstrate a valid plan percentage of 66.1% (32k model) and show that\nthe figure can be further scaled through additional data. Multi-domain tests on\n16k samples yield an even higher 70.6% planning validity rate, proving\nextensibility across domains and signaling that data variety can have a\npositive effect on learning efficiency. Although long-horizon planning and\nreduced model size make Gideon training much less efficient than baseline\nmodels based on larger LLMs, the results are still significant considering that\nthe trained model is about 120x smaller than baseline and that significant\nadvantages can be achieved in inference efficiency, scalability, and\nmulti-domain adaptability, all critical factors in human-robot collaboration.\nTraining inefficiency can be mitigated by Gideon's streamlined data generation\npipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDDL-based symbolic task planning remains pivotal for robot autonomy yet\nstruggles with dynamic human-robot collaboration due to scalability,\nre-planning demands, and delayed plan availability. Although a few\nneurosymbolic frameworks have previously leveraged LLMs such as GPT-3 to\naddress these challenges, reliance on closed-source, remote models with limited\ncontext introduced critical constraints: third-party dependency, inconsistent\nresponse times, restricted plan length and complexity, and multi-domain\nscalability issues. We present Gideon, a novel framework that enables the\ntransition to modern, smaller, local LLMs with extended context length. Gideon\nintegrates a novel problem generator to systematically generate large-scale\ndatasets of realistic domain-problem-plan tuples for any domain, and adapts\nneurosymbolic planning for local LLMs, enabling on-device execution and\nextended context for multi-domain support. Preliminary experiments in\nsingle-domain scenarios performed on Qwen-2.5 1.5B and trained on 8k-32k\nsamples, demonstrate a valid plan percentage of 66.1% (32k model) and show that\nthe figure can be further scaled through additional data. Multi-domain tests on\n16k samples yield an even higher 70.6% planning validity rate, proving\nextensibility across domains and signaling that data variety can have a\npositive effect on learning efficiency. Although long-horizon planning and\nreduced model size make Gideon training much less efficient than baseline\nmodels based on larger LLMs, the results are still significant considering that\nthe trained model is about 120x smaller than baseline and that significant\nadvantages can be achieved in inference efficiency, scalability, and\nmulti-domain adaptability, all critical factors in human-robot collaboration.\nTraining inefficiency can be mitigated by Gideon's streamlined data generation\npipeline."
                },
                "authors": [
                    {
                        "name": "Nicholas Attolino"
                    },
                    {
                        "name": "Alessio Capitanelli"
                    },
                    {
                        "name": "Fulvio Mastrogiovanni"
                    }
                ],
                "author_detail": {
                    "name": "Fulvio Mastrogiovanni"
                },
                "author": "Fulvio Mastrogiovanni",
                "arxiv_comment": "19 pages, 3 figures, 4 tables, accepted at IAS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.8; I.2.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08474v1",
                "updated": "2025-05-13T11:58:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    58,
                    45,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T11:58:45Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    58,
                    45,
                    1,
                    133,
                    0
                ],
                "title": "Distributed Quantum Neural Networks on Distributed Photonic Quantum\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Quantum Neural Networks on Distributed Photonic Quantum\n  Computing"
                },
                "summary": "We introduce a distributed quantum-classical framework that synergizes\nphotonic quantum neural networks (QNNs) with matrix-product-state (MPS) mapping\nto achieve parameter-efficient training of classical neural networks. By\nleveraging universal linear-optical decompositions of $M$-mode interferometers\nand photon-counting measurement statistics, our architecture generates neural\nparameters through a hybrid quantum-classical workflow: photonic QNNs with\n$M(M+1)/2$ trainable parameters produce high-dimensional probability\ndistributions that are mapped to classical network weights via an MPS model\nwith bond dimension $\\chi$. Empirical validation on MNIST classification\ndemonstrates that photonic QT achieves an accuracy of $95.50\\% \\pm 0.84\\%$\nusing 3,292 parameters ($\\chi = 10$), compared to $96.89\\% \\pm 0.31\\%$ for\nclassical baselines with 6,690 parameters. Moreover, a ten-fold compression\nratio is achieved at $\\chi = 4$, with a relative accuracy loss of less than\n$3\\%$. The framework outperforms classical compression techniques (weight\nsharing/pruning) by 6--12\\% absolute accuracy while eliminating quantum\nhardware requirements during inference through classical deployment of\ncompressed parameters. Simulations incorporating realistic photonic noise\ndemonstrate the framework's robustness to near-term hardware imperfections.\nAblation studies confirm quantum necessity: replacing photonic QNNs with random\ninputs collapses accuracy to chance level ($10.0\\% \\pm 0.5\\%$). Photonic\nquantum computing's room-temperature operation, inherent scalability through\nspatial-mode multiplexing, and HPC-integrated architecture establish a\npractical pathway for distributed quantum machine learning, combining the\nexpressivity of photonic Hilbert spaces with the deployability of classical\nneural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a distributed quantum-classical framework that synergizes\nphotonic quantum neural networks (QNNs) with matrix-product-state (MPS) mapping\nto achieve parameter-efficient training of classical neural networks. By\nleveraging universal linear-optical decompositions of $M$-mode interferometers\nand photon-counting measurement statistics, our architecture generates neural\nparameters through a hybrid quantum-classical workflow: photonic QNNs with\n$M(M+1)/2$ trainable parameters produce high-dimensional probability\ndistributions that are mapped to classical network weights via an MPS model\nwith bond dimension $\\chi$. Empirical validation on MNIST classification\ndemonstrates that photonic QT achieves an accuracy of $95.50\\% \\pm 0.84\\%$\nusing 3,292 parameters ($\\chi = 10$), compared to $96.89\\% \\pm 0.31\\%$ for\nclassical baselines with 6,690 parameters. Moreover, a ten-fold compression\nratio is achieved at $\\chi = 4$, with a relative accuracy loss of less than\n$3\\%$. The framework outperforms classical compression techniques (weight\nsharing/pruning) by 6--12\\% absolute accuracy while eliminating quantum\nhardware requirements during inference through classical deployment of\ncompressed parameters. Simulations incorporating realistic photonic noise\ndemonstrate the framework's robustness to near-term hardware imperfections.\nAblation studies confirm quantum necessity: replacing photonic QNNs with random\ninputs collapses accuracy to chance level ($10.0\\% \\pm 0.5\\%$). Photonic\nquantum computing's room-temperature operation, inherent scalability through\nspatial-mode multiplexing, and HPC-integrated architecture establish a\npractical pathway for distributed quantum machine learning, combining the\nexpressivity of photonic Hilbert spaces with the deployability of classical\nneural networks."
                },
                "authors": [
                    {
                        "name": "Kuan-Cheng Chen"
                    },
                    {
                        "name": "Chen-Yu Liu"
                    },
                    {
                        "name": "Yu Shang"
                    },
                    {
                        "name": "Felix Burt"
                    },
                    {
                        "name": "Kin K. Leung"
                    }
                ],
                "author_detail": {
                    "name": "Kin K. Leung"
                },
                "author": "Kin K. Leung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21098v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21098v3",
                "updated": "2025-05-13T11:54:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    54,
                    26,
                    1,
                    133,
                    0
                ],
                "published": "2025-03-27T02:36:48Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    2,
                    36,
                    48,
                    3,
                    86,
                    0
                ],
                "title": "Alleviating LLM-based Generative Retrieval Hallucination in Alipay\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alleviating LLM-based Generative Retrieval Hallucination in Alipay\n  Search"
                },
                "summary": "Generative retrieval (GR) has revolutionized document retrieval with the\nadvent of large language models (LLMs), and LLM-based GR is gradually being\nadopted by the industry. Despite its remarkable advantages and potential,\nLLM-based GR suffers from hallucination and generates documents that are\nirrelevant to the query in some instances, severely challenging its credibility\nin practical applications. We thereby propose an optimized GR framework\ndesigned to alleviate retrieval hallucination, which integrates knowledge\ndistillation reasoning in model training and incorporate decision agent to\nfurther improve retrieval precision. Specifically, we employ LLMs to assess and\nreason GR retrieved query-document (q-d) pairs, and then distill the reasoning\ndata as transferred knowledge to the GR model. Moreover, we utilize a decision\nagent as post-processing to extend the GR retrieved documents through retrieval\nmodel and select the most relevant ones from multi perspectives as the final\ngenerative retrieval result. Extensive offline experiments on real-world\ndatasets and online A/B tests on Fund Search and Insurance Search in Alipay\ndemonstrate our framework's superiority and effectiveness in improving search\nquality and conversion gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative retrieval (GR) has revolutionized document retrieval with the\nadvent of large language models (LLMs), and LLM-based GR is gradually being\nadopted by the industry. Despite its remarkable advantages and potential,\nLLM-based GR suffers from hallucination and generates documents that are\nirrelevant to the query in some instances, severely challenging its credibility\nin practical applications. We thereby propose an optimized GR framework\ndesigned to alleviate retrieval hallucination, which integrates knowledge\ndistillation reasoning in model training and incorporate decision agent to\nfurther improve retrieval precision. Specifically, we employ LLMs to assess and\nreason GR retrieved query-document (q-d) pairs, and then distill the reasoning\ndata as transferred knowledge to the GR model. Moreover, we utilize a decision\nagent as post-processing to extend the GR retrieved documents through retrieval\nmodel and select the most relevant ones from multi perspectives as the final\ngenerative retrieval result. Extensive offline experiments on real-world\ndatasets and online A/B tests on Fund Search and Insurance Search in Alipay\ndemonstrate our framework's superiority and effectiveness in improving search\nquality and conversion gains."
                },
                "authors": [
                    {
                        "name": "Yedan Shen"
                    },
                    {
                        "name": "Kaixin Wu"
                    },
                    {
                        "name": "Yuechen Ding"
                    },
                    {
                        "name": "Jingyuan Wen"
                    },
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Mingjie Zhong"
                    },
                    {
                        "name": "Zhouhan Lin"
                    },
                    {
                        "name": "Jia Xu"
                    },
                    {
                        "name": "Linjian Mo"
                    }
                ],
                "author_detail": {
                    "name": "Linjian Mo"
                },
                "author": "Linjian Mo",
                "arxiv_doi": "10.1145/3726302.3731951",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3731951",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.21098v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21098v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by SIGIR 2025",
                "arxiv_journal_ref": "SIGIR 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20799v2",
                "updated": "2025-05-13T11:51:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    51,
                    35,
                    1,
                    133,
                    0
                ],
                "published": "2025-04-29T14:13:57Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    13,
                    57,
                    1,
                    119,
                    0
                ],
                "title": "Hallucination by Code Generation LLMs: Taxonomy, Benchmarks, Mitigation,\n  and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination by Code Generation LLMs: Taxonomy, Benchmarks, Mitigation,\n  and Challenges"
                },
                "summary": "Recent technical breakthroughs in large language models (LLMs) have enabled\nthem to fluently generate source code. Software developers often leverage both\ngeneral-purpose and code-specialized LLMs to revise existing code or even\ngenerate a whole function from scratch. These capabilities are also beneficial\nin no-code or low-code contexts, in which one can write programs without a\ntechnical background. However, due to their internal design, LLMs are prone to\ngenerating hallucinations, which are incorrect, nonsensical, and not\njustifiable information but difficult to identify its presence. This problem\nalso occurs when generating source code. Once hallucinated code is produced, it\nis often challenging for users to identify and fix it, especially when such\nhallucinations can be identified under specific execution paths. As a result,\nthe hallucinated code may remain unnoticed within the codebase. This survey\ninvestigates recent studies and techniques relevant to hallucinations generated\nby CodeLLMs. We categorize the types of hallucinations in the code generated by\nCodeLLMs, review existing benchmarks and mitigation strategies, and identify\nopen challenges. Based on these findings, this survey outlines further research\ndirections in the detection and removal of hallucinations produced by CodeLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent technical breakthroughs in large language models (LLMs) have enabled\nthem to fluently generate source code. Software developers often leverage both\ngeneral-purpose and code-specialized LLMs to revise existing code or even\ngenerate a whole function from scratch. These capabilities are also beneficial\nin no-code or low-code contexts, in which one can write programs without a\ntechnical background. However, due to their internal design, LLMs are prone to\ngenerating hallucinations, which are incorrect, nonsensical, and not\njustifiable information but difficult to identify its presence. This problem\nalso occurs when generating source code. Once hallucinated code is produced, it\nis often challenging for users to identify and fix it, especially when such\nhallucinations can be identified under specific execution paths. As a result,\nthe hallucinated code may remain unnoticed within the codebase. This survey\ninvestigates recent studies and techniques relevant to hallucinations generated\nby CodeLLMs. We categorize the types of hallucinations in the code generated by\nCodeLLMs, review existing benchmarks and mitigation strategies, and identify\nopen challenges. Based on these findings, this survey outlines further research\ndirections in the detection and removal of hallucinations produced by CodeLLMs."
                },
                "authors": [
                    {
                        "name": "Yunseo Lee"
                    },
                    {
                        "name": "John Youngeun Song"
                    },
                    {
                        "name": "Dongsun Kim"
                    },
                    {
                        "name": "Jindae Kim"
                    },
                    {
                        "name": "Mijung Kim"
                    },
                    {
                        "name": "Jaechang Nam"
                    }
                ],
                "author_detail": {
                    "name": "Jaechang Nam"
                },
                "author": "Jaechang Nam",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08464v1",
                "updated": "2025-05-13T11:47:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    47,
                    49,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T11:47:49Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    47,
                    49,
                    1,
                    133,
                    0
                ],
                "title": "Large Language Models Meet Stance Detection: A Survey of Tasks, Methods,\n  Applications, Challenges and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Meet Stance Detection: A Survey of Tasks, Methods,\n  Applications, Challenges and Future Directions"
                },
                "summary": "Stance detection is essential for understanding subjective content across\nvarious platforms such as social media, news articles, and online reviews.\nRecent advances in Large Language Models (LLMs) have revolutionized stance\ndetection by introducing novel capabilities in contextual understanding,\ncross-domain generalization, and multimodal analysis. Despite these\nprogressions, existing surveys often lack comprehensive coverage of approaches\nthat specifically leverage LLMs for stance detection. To bridge this critical\ngap, our review article conducts a systematic analysis of stance detection,\ncomprehensively examining recent advancements of LLMs transforming the field,\nincluding foundational concepts, methodologies, datasets, applications, and\nemerging challenges. We present a novel taxonomy for LLM-based stance detection\napproaches, structured along three key dimensions: 1) learning methods,\nincluding supervised, unsupervised, few-shot, and zero-shot; 2) data\nmodalities, such as unimodal, multimodal, and hybrid; and 3) target\nrelationships, encompassing in-target, cross-target, and multi-target\nscenarios. Furthermore, we discuss the evaluation techniques and analyze\nbenchmark datasets and performance trends, highlighting the strengths and\nlimitations of different architectures. Key applications in misinformation\ndetection, political analysis, public health monitoring, and social media\nmoderation are discussed. Finally, we identify critical challenges such as\nimplicit stance expression, cultural biases, and computational constraints,\nwhile outlining promising future directions, including explainable stance\nreasoning, low-resource adaptation, and real-time deployment frameworks. Our\nsurvey highlights emerging trends, open challenges, and future directions to\nguide researchers and practitioners in developing next-generation stance\ndetection systems powered by large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stance detection is essential for understanding subjective content across\nvarious platforms such as social media, news articles, and online reviews.\nRecent advances in Large Language Models (LLMs) have revolutionized stance\ndetection by introducing novel capabilities in contextual understanding,\ncross-domain generalization, and multimodal analysis. Despite these\nprogressions, existing surveys often lack comprehensive coverage of approaches\nthat specifically leverage LLMs for stance detection. To bridge this critical\ngap, our review article conducts a systematic analysis of stance detection,\ncomprehensively examining recent advancements of LLMs transforming the field,\nincluding foundational concepts, methodologies, datasets, applications, and\nemerging challenges. We present a novel taxonomy for LLM-based stance detection\napproaches, structured along three key dimensions: 1) learning methods,\nincluding supervised, unsupervised, few-shot, and zero-shot; 2) data\nmodalities, such as unimodal, multimodal, and hybrid; and 3) target\nrelationships, encompassing in-target, cross-target, and multi-target\nscenarios. Furthermore, we discuss the evaluation techniques and analyze\nbenchmark datasets and performance trends, highlighting the strengths and\nlimitations of different architectures. Key applications in misinformation\ndetection, political analysis, public health monitoring, and social media\nmoderation are discussed. Finally, we identify critical challenges such as\nimplicit stance expression, cultural biases, and computational constraints,\nwhile outlining promising future directions, including explainable stance\nreasoning, low-resource adaptation, and real-time deployment frameworks. Our\nsurvey highlights emerging trends, open challenges, and future directions to\nguide researchers and practitioners in developing next-generation stance\ndetection systems powered by large language models."
                },
                "authors": [
                    {
                        "name": "Lata Pangtey"
                    },
                    {
                        "name": "Anukriti Bhatnagar"
                    },
                    {
                        "name": "Shubhi Bansal"
                    },
                    {
                        "name": "Shahid Shafi Dar"
                    },
                    {
                        "name": "Nagendra Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Nagendra Kumar"
                },
                "author": "Nagendra Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08463v1",
                "updated": "2025-05-13T11:47:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    47,
                    0,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T11:47:00Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    47,
                    0,
                    1,
                    133,
                    0
                ],
                "title": "RepCali: High Efficient Fine-tuning Via Representation Calibration in\n  Latent Space for Pre-trained Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepCali: High Efficient Fine-tuning Via Representation Calibration in\n  Latent Space for Pre-trained Language Models"
                },
                "summary": "Fine-tuning pre-trained language models (PLMs) has become a dominant paradigm\nin applying PLMs to downstream tasks. However, with limited fine-tuning, PLMs\nstill struggle with the discrepancies between the representation obtained from\nthe PLMs' encoder and the optimal input to the PLMs' decoder. This paper\ntackles this challenge by learning to calibrate the representation of PLMs in\nthe latent space. In the proposed representation calibration method (RepCali),\nwe integrate a specific calibration block to the latent space after the encoder\nand use the calibrated output as the decoder input. The merits of the proposed\nRepCali include its universality to all PLMs with encoder-decoder\narchitectures, its plug-and-play nature, and ease of implementation. Extensive\nexperiments on 25 PLM-based models across 8 tasks (including both English and\nChinese datasets) demonstrate that the proposed RepCali offers desirable\nenhancements to PLMs (including LLMs) and significantly improves the\nperformance of downstream tasks. Comparison experiments across 4 benchmark\ntasks indicate that RepCali is superior to the representative fine-tuning\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning pre-trained language models (PLMs) has become a dominant paradigm\nin applying PLMs to downstream tasks. However, with limited fine-tuning, PLMs\nstill struggle with the discrepancies between the representation obtained from\nthe PLMs' encoder and the optimal input to the PLMs' decoder. This paper\ntackles this challenge by learning to calibrate the representation of PLMs in\nthe latent space. In the proposed representation calibration method (RepCali),\nwe integrate a specific calibration block to the latent space after the encoder\nand use the calibrated output as the decoder input. The merits of the proposed\nRepCali include its universality to all PLMs with encoder-decoder\narchitectures, its plug-and-play nature, and ease of implementation. Extensive\nexperiments on 25 PLM-based models across 8 tasks (including both English and\nChinese datasets) demonstrate that the proposed RepCali offers desirable\nenhancements to PLMs (including LLMs) and significantly improves the\nperformance of downstream tasks. Comparison experiments across 4 benchmark\ntasks indicate that RepCali is superior to the representative fine-tuning\nbaselines."
                },
                "authors": [
                    {
                        "name": "Fujun Zhang"
                    },
                    {
                        "name": "XiangDong Su"
                    }
                ],
                "author_detail": {
                    "name": "XiangDong Su"
                },
                "author": "XiangDong Su",
                "arxiv_comment": "13 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08459v1",
                "updated": "2025-05-13T11:41:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    41,
                    10,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T11:41:10Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    41,
                    10,
                    1,
                    133,
                    0
                ],
                "title": "Strategy-Augmented Planning for Large Language Models via Opponent\n  Exploitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategy-Augmented Planning for Large Language Models via Opponent\n  Exploitation"
                },
                "summary": "Efficiently modeling and exploiting opponents is a long-standing challenge in\nadversarial domains. Large Language Models (LLMs) trained on extensive textual\ndata have recently demonstrated outstanding performance in general tasks,\nintroducing new research directions for opponent modeling. Some studies\nprimarily focus on directly using LLMs to generate decisions based on the\nelaborate prompt context that incorporates opponent descriptions, while these\napproaches are limited to scenarios where LLMs possess adequate domain\nexpertise. To address that, we introduce a two-stage Strategy-Augmented\nPlanning (SAP) framework that significantly enhances the opponent exploitation\ncapabilities of LLM-based agents by utilizing a critical component, the\nStrategy Evaluation Network (SEN). Specifically, in the offline stage, we\nconstruct an explicit strategy space and subsequently collect strategy-outcome\npair data for training the SEN network. During the online phase, SAP\ndynamically recognizes the opponent's strategies and greedily exploits them by\nsearching best response strategy on the well-trained SEN, finally translating\nstrategy to a course of actions by carefully designed prompts. Experimental\nresults show that SAP exhibits robust generalization capabilities, allowing it\nto perform effectively not only against previously encountered opponent\nstrategies but also against novel, unseen strategies. In the MicroRTS\nenvironment, SAP achieves a 85.35\\% performance improvement over baseline\nmethods and matches the competitiveness of reinforcement learning approaches\nagainst state-of-the-art (SOTA) rule-based AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently modeling and exploiting opponents is a long-standing challenge in\nadversarial domains. Large Language Models (LLMs) trained on extensive textual\ndata have recently demonstrated outstanding performance in general tasks,\nintroducing new research directions for opponent modeling. Some studies\nprimarily focus on directly using LLMs to generate decisions based on the\nelaborate prompt context that incorporates opponent descriptions, while these\napproaches are limited to scenarios where LLMs possess adequate domain\nexpertise. To address that, we introduce a two-stage Strategy-Augmented\nPlanning (SAP) framework that significantly enhances the opponent exploitation\ncapabilities of LLM-based agents by utilizing a critical component, the\nStrategy Evaluation Network (SEN). Specifically, in the offline stage, we\nconstruct an explicit strategy space and subsequently collect strategy-outcome\npair data for training the SEN network. During the online phase, SAP\ndynamically recognizes the opponent's strategies and greedily exploits them by\nsearching best response strategy on the well-trained SEN, finally translating\nstrategy to a course of actions by carefully designed prompts. Experimental\nresults show that SAP exhibits robust generalization capabilities, allowing it\nto perform effectively not only against previously encountered opponent\nstrategies but also against novel, unseen strategies. In the MicroRTS\nenvironment, SAP achieves a 85.35\\% performance improvement over baseline\nmethods and matches the competitiveness of reinforcement learning approaches\nagainst state-of-the-art (SOTA) rule-based AI."
                },
                "authors": [
                    {
                        "name": "Shuai Xu"
                    },
                    {
                        "name": "Sijia Cui"
                    },
                    {
                        "name": "Yanna Wang"
                    },
                    {
                        "name": "Bo Xu"
                    },
                    {
                        "name": "Qi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Wang"
                },
                "author": "Qi Wang",
                "arxiv_comment": "Accepted to IJCNN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04475v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04475v2",
                "updated": "2025-05-13T11:35:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    35,
                    39,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-07T14:46:22Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    46,
                    22,
                    2,
                    127,
                    0
                ],
                "title": "Mass Modeling the Andromeda Dwarf Galaxies: Andromeda VI and Andromeda\n  XXIII",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mass Modeling the Andromeda Dwarf Galaxies: Andromeda VI and Andromeda\n  XXIII"
                },
                "summary": "Accurately mapping the mass profiles of low mass dwarf spheroidal (dSph)\ngalaxies allows us to test predictions made by dark matter (DM) models. To\ndate, such analyses have primarily been performed on Milky Way (MW) satellites.\nMeanwhile, the Andromeda Galaxy (M31) is home to 35 known dwarf galaxies, yet\nonly two have been successfully mass-modeled so far. In order to better\nunderstand the nature of dark matter, a more comprehensive study of Local Group\ndwarfs is necessary. In this study, we have undertaken a dynamical study of two\nhigher-luminosity Andromeda dwarf galaxies: Andromeda VI (And VI) and Andromeda\nXXIII (And XXIII). We infer an enclosed mass for And VI of M(r < r$_{\\rm{h}}$)\n= (4.9 $\\pm$ 1.5) $\\times$ 10$^{7}$ M$_{\\odot}$, corresponding to a\nmass-to-light ratio of $[M/L]_{r_{\\rm{h}}}$ = (27.1 $\\pm$ 8.2)\nM$_{\\odot}$/L$_{\\odot}$. We infer an enclosed mass for And XXIII of M(r <\nr$_{\\rm{h}}$) = (3.1 $\\pm$ 1.9) $\\times$ 10$^{7}$ M$_{\\odot}$, corresponding to\na mass-to-light ratio of $[M/L]_{\\rm{r_{h}}}$ = (90.2 $\\pm$ 53.9)\nM$_{\\odot}$/L$_{\\odot}$. Using the dynamical Jeans modeling tool, GravSphere,\nwe determine And VI and And XXIII's dark matter density at 150 pc, finding\n$\\rho_{\\rm{DM,VI}}$(150 pc) = (1.4 $\\pm$ 0.5) $\\times$ 10$^{8}$ M$_{\\odot}$\nkpc$^{-3}$ and $\\rho_{\\rm{DM,XXIII}}$(150 pc) = 0.5$\\substack{+0.4 \\\\ -0.3}\n\\times$ 10$^{8}$ M$_{\\odot}$ kpc$^{-3}$. Our results make And VI the first\nmass-modeled M31 satellite to fall into the cuspy regime, while And XXIII has a\nlower density, implying either a more cored central dark matter density, or a\nlowering of the density through tides. This adds And XXIII to a growing list of\nM31 dwarfs with a central density lower than most MW dwarfs and lower than\nexpected for isolated dwarfs in the Standard Cosmology. This could be explained\nby the M31 dwarfs having experienced stronger tides than their Milky Way\ncounterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately mapping the mass profiles of low mass dwarf spheroidal (dSph)\ngalaxies allows us to test predictions made by dark matter (DM) models. To\ndate, such analyses have primarily been performed on Milky Way (MW) satellites.\nMeanwhile, the Andromeda Galaxy (M31) is home to 35 known dwarf galaxies, yet\nonly two have been successfully mass-modeled so far. In order to better\nunderstand the nature of dark matter, a more comprehensive study of Local Group\ndwarfs is necessary. In this study, we have undertaken a dynamical study of two\nhigher-luminosity Andromeda dwarf galaxies: Andromeda VI (And VI) and Andromeda\nXXIII (And XXIII). We infer an enclosed mass for And VI of M(r < r$_{\\rm{h}}$)\n= (4.9 $\\pm$ 1.5) $\\times$ 10$^{7}$ M$_{\\odot}$, corresponding to a\nmass-to-light ratio of $[M/L]_{r_{\\rm{h}}}$ = (27.1 $\\pm$ 8.2)\nM$_{\\odot}$/L$_{\\odot}$. We infer an enclosed mass for And XXIII of M(r <\nr$_{\\rm{h}}$) = (3.1 $\\pm$ 1.9) $\\times$ 10$^{7}$ M$_{\\odot}$, corresponding to\na mass-to-light ratio of $[M/L]_{\\rm{r_{h}}}$ = (90.2 $\\pm$ 53.9)\nM$_{\\odot}$/L$_{\\odot}$. Using the dynamical Jeans modeling tool, GravSphere,\nwe determine And VI and And XXIII's dark matter density at 150 pc, finding\n$\\rho_{\\rm{DM,VI}}$(150 pc) = (1.4 $\\pm$ 0.5) $\\times$ 10$^{8}$ M$_{\\odot}$\nkpc$^{-3}$ and $\\rho_{\\rm{DM,XXIII}}$(150 pc) = 0.5$\\substack{+0.4 \\\\ -0.3}\n\\times$ 10$^{8}$ M$_{\\odot}$ kpc$^{-3}$. Our results make And VI the first\nmass-modeled M31 satellite to fall into the cuspy regime, while And XXIII has a\nlower density, implying either a more cored central dark matter density, or a\nlowering of the density through tides. This adds And XXIII to a growing list of\nM31 dwarfs with a central density lower than most MW dwarfs and lower than\nexpected for isolated dwarfs in the Standard Cosmology. This could be explained\nby the M31 dwarfs having experienced stronger tides than their Milky Way\ncounterparts."
                },
                "authors": [
                    {
                        "name": "Connor S. Pickett"
                    },
                    {
                        "name": "Michelle L. M. Collins"
                    },
                    {
                        "name": "R. Michael Rich"
                    },
                    {
                        "name": "Justin I. Read"
                    },
                    {
                        "name": "Emily J. E. Charles"
                    },
                    {
                        "name": "Nicolas Martin"
                    },
                    {
                        "name": "Scott Chapman"
                    },
                    {
                        "name": "Alan McConnachie"
                    },
                    {
                        "name": "Alessandro Savino"
                    },
                    {
                        "name": "Daniel R. Weisz"
                    }
                ],
                "author_detail": {
                    "name": "Daniel R. Weisz"
                },
                "author": "Daniel R. Weisz",
                "arxiv_comment": "19 pages, 16 figures. Accepted for publication in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04475v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04475v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01457v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01457v2",
                "updated": "2025-05-13T11:33:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    33,
                    54,
                    1,
                    133,
                    0
                ],
                "published": "2024-12-02T12:49:03Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    12,
                    49,
                    3,
                    0,
                    337,
                    0
                ],
                "title": "Neutrinos from stochastic acceleration in black hole environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutrinos from stochastic acceleration in black hole environments"
                },
                "summary": "Recent experimental results from the IceCube detector and their\nphenomenological interpretation suggest that the magnetized turbulent corona of\nnearby X-ray luminous Seyfert galaxies can produce $\\sim 1-10\\,$TeV neutrinos\nvia photo-hadronic interactions. We investigate the physics of stochastic\nacceleration in these environments in detail and examine the conditions under\nwhich the inferred proton spectrum can be explained. To this end, we used\nrecent findings on particle acceleration in turbulence and paid particular\nattention to the transport equation, notably for transport in momentum space,\nturbulent transport outside of the corona, and advection through the corona. We\nfirst remark that the spectra we obtained are highly sensitive to the value of\nthe acceleration rate, for instance, to the Alfv\\'enic velocity. Then, we\nexamined three prototype scenarios, one scenario of turbulent acceleration in\nthe test-particle picture, another scenario in which particles were\npreaccelerated by turbulence and further energized by shear acceleration, and a\nfinal scenario in which we considered the effect of particle backreaction on\nthe turbulence (damping), which self-regulates the acceleration process. We\nshow that it is possible to obtain satisfactory fits to the inferred proton\nspectrum in all three cases, but we stress that in the first two scenarios, the\nenergy content in suprathermal protons has to be fixed in an ad hoc manner to\nmatch the inferred spectrum at an energy density close to that contained in the\nturbulence. Interestingly, self-regulated acceleration by turbulence damping\nnaturally brings the suprathermal particle energy content close to that of the\nturbulence and allowed us to reproduce the inferred flux level without\nadditional fine-tuning. [Abridged version]",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent experimental results from the IceCube detector and their\nphenomenological interpretation suggest that the magnetized turbulent corona of\nnearby X-ray luminous Seyfert galaxies can produce $\\sim 1-10\\,$TeV neutrinos\nvia photo-hadronic interactions. We investigate the physics of stochastic\nacceleration in these environments in detail and examine the conditions under\nwhich the inferred proton spectrum can be explained. To this end, we used\nrecent findings on particle acceleration in turbulence and paid particular\nattention to the transport equation, notably for transport in momentum space,\nturbulent transport outside of the corona, and advection through the corona. We\nfirst remark that the spectra we obtained are highly sensitive to the value of\nthe acceleration rate, for instance, to the Alfv\\'enic velocity. Then, we\nexamined three prototype scenarios, one scenario of turbulent acceleration in\nthe test-particle picture, another scenario in which particles were\npreaccelerated by turbulence and further energized by shear acceleration, and a\nfinal scenario in which we considered the effect of particle backreaction on\nthe turbulence (damping), which self-regulates the acceleration process. We\nshow that it is possible to obtain satisfactory fits to the inferred proton\nspectrum in all three cases, but we stress that in the first two scenarios, the\nenergy content in suprathermal protons has to be fixed in an ad hoc manner to\nmatch the inferred spectrum at an energy density close to that contained in the\nturbulence. Interestingly, self-regulated acceleration by turbulence damping\nnaturally brings the suprathermal particle energy content close to that of the\nturbulence and allowed us to reproduce the inferred flux level without\nadditional fine-tuning. [Abridged version]"
                },
                "authors": [
                    {
                        "name": "M. Lemoine"
                    },
                    {
                        "name": "F. Rieger"
                    }
                ],
                "author_detail": {
                    "name": "F. Rieger"
                },
                "arxiv_affiliation": "IPP, U. Heidelberg",
                "author": "F. Rieger",
                "arxiv_doi": "10.1051/0004-6361/202453296",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202453296",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.01457v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01457v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "version to appear in A&A",
                "arxiv_journal_ref": "A&A 697, A124 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08450v1",
                "updated": "2025-05-13T11:25:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    25,
                    15,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T11:25:15Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    25,
                    15,
                    1,
                    133,
                    0
                ],
                "title": "IterKey: Iterative Keyword Generation with LLMs for Enhanced Retrieval\n  Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IterKey: Iterative Keyword Generation with LLMs for Enhanced Retrieval\n  Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a way to complement the\nin-context knowledge of Large Language Models (LLMs) by integrating external\ndocuments. However, real-world applications demand not only accuracy but also\ninterpretability. While dense retrieval methods provide high accuracy, they\nlack interpretability; conversely, sparse retrieval methods offer transparency\nbut often fail to capture the full intent of queries due to their reliance on\nkeyword matching. To address these issues, we introduce IterKey, an LLM-driven\niterative keyword generation framework that enhances RAG via sparse retrieval.\nIterKey consists of three LLM-driven stages: generating keywords for retrieval,\ngenerating answers based on retrieved documents, and validating the answers. If\nvalidation fails, the process iteratively repeats with refined keywords. Across\nfour QA tasks, experimental results show that IterKey achieves 5% to 20%\naccuracy improvements over BM25-based RAG and simple baselines. Its performance\nis comparable to dense retrieval-based RAG and prior iterative query refinement\nmethods using dense models. In summary, IterKey is a novel BM25-based approach\nleveraging LLMs to iteratively refine RAG, effectively balancing accuracy with\ninterpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a way to complement the\nin-context knowledge of Large Language Models (LLMs) by integrating external\ndocuments. However, real-world applications demand not only accuracy but also\ninterpretability. While dense retrieval methods provide high accuracy, they\nlack interpretability; conversely, sparse retrieval methods offer transparency\nbut often fail to capture the full intent of queries due to their reliance on\nkeyword matching. To address these issues, we introduce IterKey, an LLM-driven\niterative keyword generation framework that enhances RAG via sparse retrieval.\nIterKey consists of three LLM-driven stages: generating keywords for retrieval,\ngenerating answers based on retrieved documents, and validating the answers. If\nvalidation fails, the process iteratively repeats with refined keywords. Across\nfour QA tasks, experimental results show that IterKey achieves 5% to 20%\naccuracy improvements over BM25-based RAG and simple baselines. Its performance\nis comparable to dense retrieval-based RAG and prior iterative query refinement\nmethods using dense models. In summary, IterKey is a novel BM25-based approach\nleveraging LLMs to iteratively refine RAG, effectively balancing accuracy with\ninterpretability."
                },
                "authors": [
                    {
                        "name": "Kazuki Hayashi"
                    },
                    {
                        "name": "Hidetaka Kamigaito"
                    },
                    {
                        "name": "Shinya Kouda"
                    },
                    {
                        "name": "Taro Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Taro Watanabe"
                },
                "author": "Taro Watanabe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08448v1",
                "updated": "2025-05-13T11:23:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    23,
                    25,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T11:23:25Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    23,
                    25,
                    1,
                    133,
                    0
                ],
                "title": "Scalable UAV Multi-Hop Networking via Multi-Agent Reinforcement Learning\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable UAV Multi-Hop Networking via Multi-Agent Reinforcement Learning\n  with Large Language Models"
                },
                "summary": "In disaster scenarios, establishing robust emergency communication networks\nis critical, and unmanned aerial vehicles (UAVs) offer a promising solution to\nrapidly restore connectivity. However, organizing UAVs to form multi-hop\nnetworks in large-scale dynamic environments presents significant challenges,\nincluding limitations in algorithmic scalability and the vast exploration space\nrequired for coordinated decision-making. To address these issues, we propose\nMRLMN, a novel framework that integrates multi-agent reinforcement learning\n(MARL) and large language models (LLMs) to jointly optimize UAV agents toward\nachieving optimal networking performance. The framework incorporates a grouping\nstrategy with reward decomposition to enhance algorithmic scalability and\nbalance decision-making across UAVs. In addition, behavioral constraints are\napplied to selected key UAVs to improve the robustness of the network.\nFurthermore, the framework integrates LLM agents, leveraging knowledge\ndistillation to transfer their high-level decision-making capabilities to MARL\nagents. This enhances both the efficiency of exploration and the overall\ntraining process. In the distillation module, a Hungarian algorithm-based\nmatching scheme is applied to align the decision outputs of the LLM and MARL\nagents and define the distillation loss. Extensive simulation results validate\nthe effectiveness of our approach, demonstrating significant improvements in\nnetwork performance, including enhanced coverage and communication quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In disaster scenarios, establishing robust emergency communication networks\nis critical, and unmanned aerial vehicles (UAVs) offer a promising solution to\nrapidly restore connectivity. However, organizing UAVs to form multi-hop\nnetworks in large-scale dynamic environments presents significant challenges,\nincluding limitations in algorithmic scalability and the vast exploration space\nrequired for coordinated decision-making. To address these issues, we propose\nMRLMN, a novel framework that integrates multi-agent reinforcement learning\n(MARL) and large language models (LLMs) to jointly optimize UAV agents toward\nachieving optimal networking performance. The framework incorporates a grouping\nstrategy with reward decomposition to enhance algorithmic scalability and\nbalance decision-making across UAVs. In addition, behavioral constraints are\napplied to selected key UAVs to improve the robustness of the network.\nFurthermore, the framework integrates LLM agents, leveraging knowledge\ndistillation to transfer their high-level decision-making capabilities to MARL\nagents. This enhances both the efficiency of exploration and the overall\ntraining process. In the distillation module, a Hungarian algorithm-based\nmatching scheme is applied to align the decision outputs of the LLM and MARL\nagents and define the distillation loss. Extensive simulation results validate\nthe effectiveness of our approach, demonstrating significant improvements in\nnetwork performance, including enhanced coverage and communication quality."
                },
                "authors": [
                    {
                        "name": "Yanggang Xu"
                    },
                    {
                        "name": "Weijie Hong"
                    },
                    {
                        "name": "Jirong Zha"
                    },
                    {
                        "name": "Geng Chen"
                    },
                    {
                        "name": "Jianfeng Zheng"
                    },
                    {
                        "name": "Chen-Chun Hsia"
                    },
                    {
                        "name": "Xinlei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xinlei Chen"
                },
                "author": "Xinlei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08444v1",
                "updated": "2025-05-13T11:13:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    13,
                    0,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T11:13:00Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    13,
                    0,
                    1,
                    133,
                    0
                ],
                "title": "Symbolically-Guided Visual Plan Inference from Uncurated Video Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolically-Guided Visual Plan Inference from Uncurated Video Data"
                },
                "summary": "Visual planning, by offering a sequence of intermediate visual subgoals to a\ngoal-conditioned low-level policy, achieves promising performance on\nlong-horizon manipulation tasks. To obtain the subgoals, existing methods\ntypically resort to video generation models but suffer from model hallucination\nand computational cost. We present Vis2Plan, an efficient, explainable and\nwhite-box visual planning framework powered by symbolic guidance. From raw,\nunlabeled play data, Vis2Plan harnesses vision foundation models to\nautomatically extract a compact set of task symbols, which allows building a\nhigh-level symbolic transition graph for multi-goal, multi-stage planning. At\ntest time, given a desired task goal, our planner conducts planning at the\nsymbolic level and assembles a sequence of physically consistent intermediate\nsub-goal images grounded by the underlying symbolic representation. Our\nVis2Plan outperforms strong diffusion video generation-based visual planners by\ndelivering 53\\% higher aggregate success rate in real robot settings while\ngenerating visual plans 35$\\times$ faster. The results indicate that Vis2Plan\nis able to generate physically consistent image goals while offering fully\ninspectable reasoning steps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual planning, by offering a sequence of intermediate visual subgoals to a\ngoal-conditioned low-level policy, achieves promising performance on\nlong-horizon manipulation tasks. To obtain the subgoals, existing methods\ntypically resort to video generation models but suffer from model hallucination\nand computational cost. We present Vis2Plan, an efficient, explainable and\nwhite-box visual planning framework powered by symbolic guidance. From raw,\nunlabeled play data, Vis2Plan harnesses vision foundation models to\nautomatically extract a compact set of task symbols, which allows building a\nhigh-level symbolic transition graph for multi-goal, multi-stage planning. At\ntest time, given a desired task goal, our planner conducts planning at the\nsymbolic level and assembles a sequence of physically consistent intermediate\nsub-goal images grounded by the underlying symbolic representation. Our\nVis2Plan outperforms strong diffusion video generation-based visual planners by\ndelivering 53\\% higher aggregate success rate in real robot settings while\ngenerating visual plans 35$\\times$ faster. The results indicate that Vis2Plan\nis able to generate physically consistent image goals while offering fully\ninspectable reasoning steps."
                },
                "authors": [
                    {
                        "name": "Wenyan Yang"
                    },
                    {
                        "name": "Ahmet Tikna"
                    },
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Yuying Zhang"
                    },
                    {
                        "name": "Luigi Palopoli"
                    },
                    {
                        "name": "Marco Roveri"
                    },
                    {
                        "name": "Joni Pajarinen"
                    }
                ],
                "author_detail": {
                    "name": "Joni Pajarinen"
                },
                "author": "Joni Pajarinen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12514v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12514v5",
                "updated": "2025-05-13T11:02:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    2,
                    20,
                    1,
                    133,
                    0
                ],
                "published": "2024-09-19T07:10:18Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    7,
                    10,
                    18,
                    3,
                    263,
                    0
                ],
                "title": "TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for\n  Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for\n  Robotic Manipulation"
                },
                "summary": "Vision-Language-Action (VLA) models have shown remarkable potential in\nvisuomotor control and instruction comprehension through end-to-end learning\nprocesses. However, current VLA models face significant challenges: they are\nslow during inference and require extensive pre-training on large amounts of\nrobotic data, making real-world deployment difficult. In this paper, we\nintroduce a new family of compact vision-language-action models, called\nTinyVLA, which offers two key advantages over existing VLA models: (1) faster\ninference speeds, and (2) improved data efficiency, eliminating the need for\npre-training stage. Our framework incorporates two essential components to\nbuild TinyVLA: (1) initializing the policy backbone with robust, high-speed\nmultimodal models, and (2) integrating a diffusion policy decoder during\nfine-tuning to enable precise robot actions. We conducted extensive evaluations\nof TinyVLA in both simulation and on real robots, demonstrating that our\napproach significantly outperforms the state-of-the-art VLA model, OpenVLA, in\nterms of speed and data efficiency, while delivering comparable or superior\nperformance. Additionally, TinyVLA exhibits strong generalization capabilities\nacross various dimensions, including language instructions, novel objects,\nunseen positions, changes in object appearance, background variations, and\nenvironmental shifts, often matching or exceeding the performance of OpenVLA.\nWe believe that \\methodname offers an interesting perspective on utilizing\npre-trained multimodal models for policy learning. Our project is at\nhttps://tiny-vla.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have shown remarkable potential in\nvisuomotor control and instruction comprehension through end-to-end learning\nprocesses. However, current VLA models face significant challenges: they are\nslow during inference and require extensive pre-training on large amounts of\nrobotic data, making real-world deployment difficult. In this paper, we\nintroduce a new family of compact vision-language-action models, called\nTinyVLA, which offers two key advantages over existing VLA models: (1) faster\ninference speeds, and (2) improved data efficiency, eliminating the need for\npre-training stage. Our framework incorporates two essential components to\nbuild TinyVLA: (1) initializing the policy backbone with robust, high-speed\nmultimodal models, and (2) integrating a diffusion policy decoder during\nfine-tuning to enable precise robot actions. We conducted extensive evaluations\nof TinyVLA in both simulation and on real robots, demonstrating that our\napproach significantly outperforms the state-of-the-art VLA model, OpenVLA, in\nterms of speed and data efficiency, while delivering comparable or superior\nperformance. Additionally, TinyVLA exhibits strong generalization capabilities\nacross various dimensions, including language instructions, novel objects,\nunseen positions, changes in object appearance, background variations, and\nenvironmental shifts, often matching or exceeding the performance of OpenVLA.\nWe believe that \\methodname offers an interesting perspective on utilizing\npre-trained multimodal models for policy learning. Our project is at\nhttps://tiny-vla.github.io."
                },
                "authors": [
                    {
                        "name": "Junjie Wen"
                    },
                    {
                        "name": "Yichen Zhu"
                    },
                    {
                        "name": "Jinming Li"
                    },
                    {
                        "name": "Minjie Zhu"
                    },
                    {
                        "name": "Kun Wu"
                    },
                    {
                        "name": "Zhiyuan Xu"
                    },
                    {
                        "name": "Ning Liu"
                    },
                    {
                        "name": "Ran Cheng"
                    },
                    {
                        "name": "Chaomin Shen"
                    },
                    {
                        "name": "Yaxin Peng"
                    },
                    {
                        "name": "Feifei Feng"
                    },
                    {
                        "name": "Jian Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Tang"
                },
                "author": "Jian Tang",
                "arxiv_comment": "add more citations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12514v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12514v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08414v1",
                "updated": "2025-05-13T10:13:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    10,
                    13,
                    26,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T10:13:26Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    10,
                    13,
                    26,
                    1,
                    133,
                    0
                ],
                "title": "An integrated language-vision foundation model for conversational\n  diagnostics and triaging in primary eye care",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An integrated language-vision foundation model for conversational\n  diagnostics and triaging in primary eye care"
                },
                "summary": "Current deep learning models are mostly task specific and lack a\nuser-friendly interface to operate. We present Meta-EyeFM, a multi-function\nfoundation model that integrates a large language model (LLM) with vision\nfoundation models (VFMs) for ocular disease assessment. Meta-EyeFM leverages a\nrouting mechanism to enable accurate task-specific analysis based on text\nqueries. Using Low Rank Adaptation, we fine-tuned our VFMs to detect ocular and\nsystemic diseases, differentiate ocular disease severity, and identify common\nocular signs. The model achieved 100% accuracy in routing fundus images to\nappropriate VFMs, which achieved $\\ge$ 82.2% accuracy in disease detection,\n$\\ge$ 89% in severity differentiation, $\\ge$ 76% in sign identification.\nMeta-EyeFM was 11% to 43% more accurate than Gemini-1.5-flash and ChatGPT-4o\nLMMs in detecting various eye diseases and comparable to an ophthalmologist.\nThis system offers enhanced usability and diagnostic performance, making it a\nvaluable decision support tool for primary eye care or an online LLM for fundus\nevaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current deep learning models are mostly task specific and lack a\nuser-friendly interface to operate. We present Meta-EyeFM, a multi-function\nfoundation model that integrates a large language model (LLM) with vision\nfoundation models (VFMs) for ocular disease assessment. Meta-EyeFM leverages a\nrouting mechanism to enable accurate task-specific analysis based on text\nqueries. Using Low Rank Adaptation, we fine-tuned our VFMs to detect ocular and\nsystemic diseases, differentiate ocular disease severity, and identify common\nocular signs. The model achieved 100% accuracy in routing fundus images to\nappropriate VFMs, which achieved $\\ge$ 82.2% accuracy in disease detection,\n$\\ge$ 89% in severity differentiation, $\\ge$ 76% in sign identification.\nMeta-EyeFM was 11% to 43% more accurate than Gemini-1.5-flash and ChatGPT-4o\nLMMs in detecting various eye diseases and comparable to an ophthalmologist.\nThis system offers enhanced usability and diagnostic performance, making it a\nvaluable decision support tool for primary eye care or an online LLM for fundus\nevaluation."
                },
                "authors": [
                    {
                        "name": "Zhi Da Soh"
                    },
                    {
                        "name": "Yang Bai"
                    },
                    {
                        "name": "Kai Yu"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Xiaofeng Lei"
                    },
                    {
                        "name": "Sahil Thakur"
                    },
                    {
                        "name": "Zann Lee"
                    },
                    {
                        "name": "Lee Ching Linette Phang"
                    },
                    {
                        "name": "Qingsheng Peng"
                    },
                    {
                        "name": "Can Can Xue"
                    },
                    {
                        "name": "Rachel Shujuan Chong"
                    },
                    {
                        "name": "Quan V. Hoang"
                    },
                    {
                        "name": "Lavanya Raghavan"
                    },
                    {
                        "name": "Yih Chung Tham"
                    },
                    {
                        "name": "Charumathi Sabanayagam"
                    },
                    {
                        "name": "Wei-Chi Wu"
                    },
                    {
                        "name": "Ming-Chih Ho"
                    },
                    {
                        "name": "Jiangnan He"
                    },
                    {
                        "name": "Preeti Gupta"
                    },
                    {
                        "name": "Ecosse Lamoureux"
                    },
                    {
                        "name": "Seang Mei Saw"
                    },
                    {
                        "name": "Vinay Nangia"
                    },
                    {
                        "name": "Songhomitra Panda-Jonas"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Ya Xing Wang"
                    },
                    {
                        "name": "Xinxing Xu"
                    },
                    {
                        "name": "Jost B. Jonas"
                    },
                    {
                        "name": "Tien Yin Wong"
                    },
                    {
                        "name": "Rick Siow Mong Goh"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Ching-Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Ching-Yu Cheng"
                },
                "author": "Ching-Yu Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11935v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11935v2",
                "updated": "2025-05-13T10:07:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    10,
                    7,
                    4,
                    1,
                    133,
                    0
                ],
                "published": "2024-11-18T15:13:20Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    15,
                    13,
                    20,
                    0,
                    323,
                    0
                ],
                "title": "Calibrated and Efficient Sampling-Free Confidence Estimation for LiDAR\n  Scene Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrated and Efficient Sampling-Free Confidence Estimation for LiDAR\n  Scene Semantic Segmentation"
                },
                "summary": "Reliable deep learning models require not only accurate predictions but also\nwell-calibrated confidence estimates to ensure dependable uncertainty\nestimation. This is crucial in safety-critical applications like autonomous\ndriving, which depend on rapid and precise semantic segmentation of LiDAR point\nclouds for real-time 3D scene understanding. In this work, we introduce a\nsampling-free approach for estimating well-calibrated confidence values for\nclassification tasks, achieving alignment with true classification accuracy and\nsignificantly reducing inference time compared to sampling-based methods. Our\nevaluation using the Adaptive Calibration Error (ACE) metric for LiDAR semantic\nsegmentation shows that our approach maintains well-calibrated confidence\nvalues while achieving increased processing speed compared to a sampling\nbaseline. Additionally, reliability diagrams reveal that our method produces\nunderconfidence rather than overconfident predictions, an advantage for\nsafety-critical applications. Our sampling-free approach offers well-calibrated\nand time-efficient predictions for LiDAR scene semantic segmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable deep learning models require not only accurate predictions but also\nwell-calibrated confidence estimates to ensure dependable uncertainty\nestimation. This is crucial in safety-critical applications like autonomous\ndriving, which depend on rapid and precise semantic segmentation of LiDAR point\nclouds for real-time 3D scene understanding. In this work, we introduce a\nsampling-free approach for estimating well-calibrated confidence values for\nclassification tasks, achieving alignment with true classification accuracy and\nsignificantly reducing inference time compared to sampling-based methods. Our\nevaluation using the Adaptive Calibration Error (ACE) metric for LiDAR semantic\nsegmentation shows that our approach maintains well-calibrated confidence\nvalues while achieving increased processing speed compared to a sampling\nbaseline. Additionally, reliability diagrams reveal that our method produces\nunderconfidence rather than overconfident predictions, an advantage for\nsafety-critical applications. Our sampling-free approach offers well-calibrated\nand time-efficient predictions for LiDAR scene semantic segmentation."
                },
                "authors": [
                    {
                        "name": "Hanieh Shojaei Miandashti"
                    },
                    {
                        "name": "Qianqian Zou"
                    },
                    {
                        "name": "Claus Brenner"
                    }
                ],
                "author_detail": {
                    "name": "Claus Brenner"
                },
                "author": "Claus Brenner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11935v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11935v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08403v1",
                "updated": "2025-05-13T09:58:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    58,
                    23,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T09:58:23Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    58,
                    23,
                    1,
                    133,
                    0
                ],
                "title": "ConDiSim: Conditional Diffusion Models for Simulation Based Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConDiSim: Conditional Diffusion Models for Simulation Based Inference"
                },
                "summary": "We present a conditional diffusion model - ConDiSim, for simulation-based\ninference of complex systems with intractable likelihoods. ConDiSim leverages\ndenoising diffusion probabilistic models to approximate posterior\ndistributions, consisting of a forward process that adds Gaussian noise to\nparameters, and a reverse process learning to denoise, conditioned on observed\ndata. This approach effectively captures complex dependencies and\nmulti-modalities within posteriors. ConDiSim is evaluated across ten benchmark\nproblems and two real-world test problems, where it demonstrates effective\nposterior approximation accuracy while maintaining computational efficiency and\nstability in model training. ConDiSim offers a robust and extensible framework\nfor simulation-based inference, particularly suitable for parameter inference\nworkflows requiring fast inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a conditional diffusion model - ConDiSim, for simulation-based\ninference of complex systems with intractable likelihoods. ConDiSim leverages\ndenoising diffusion probabilistic models to approximate posterior\ndistributions, consisting of a forward process that adds Gaussian noise to\nparameters, and a reverse process learning to denoise, conditioned on observed\ndata. This approach effectively captures complex dependencies and\nmulti-modalities within posteriors. ConDiSim is evaluated across ten benchmark\nproblems and two real-world test problems, where it demonstrates effective\nposterior approximation accuracy while maintaining computational efficiency and\nstability in model training. ConDiSim offers a robust and extensible framework\nfor simulation-based inference, particularly suitable for parameter inference\nworkflows requiring fast inference methods."
                },
                "authors": [
                    {
                        "name": "Mayank Nautiyal"
                    },
                    {
                        "name": "Andreas Hellander"
                    },
                    {
                        "name": "Prashant Singh"
                    }
                ],
                "author_detail": {
                    "name": "Prashant Singh"
                },
                "author": "Prashant Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08402v1",
                "updated": "2025-05-13T09:57:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    57,
                    28,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T09:57:28Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    57,
                    28,
                    1,
                    133,
                    0
                ],
                "title": "TUMS: Enhancing Tool-use Abilities of LLMs with Multi-structure Handlers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TUMS: Enhancing Tool-use Abilities of LLMs with Multi-structure Handlers"
                },
                "summary": "Recently, large language models(LLMs) have played an increasingly important\nrole in solving a wide range of NLP tasks, leveraging their capabilities of\nnatural language understanding and generating. Integration with external tools\nfurther enhances LLMs' effectiveness, providing more precise, timely, and\nspecialized responses. However, LLMs still encounter difficulties with\nnon-executable actions and improper actions, which are primarily attributed to\nincorrect parameters. The process of generating parameters by LLMs is confined\nto the tool level, employing the coarse-grained strategy without considering\nthe different difficulties of various tools. To address this issue, we propose\nTUMS, a novel framework designed to enhance the tool-use capabilities of LLMs\nby transforming tool-level processing into parameter-level processing.\nSpecifically, our framework consists of four key components: (1) an intent\nrecognizer that identifies the user's intent to help LLMs better understand the\ntask; (2) a task decomposer that breaks down complex tasks into simpler\nsubtasks, each involving a tool call; (3) a subtask processor equipped with\nmulti-structure handlers to generate accurate parameters; and (4) an executor.\nOur empirical studies have evidenced the effectiveness and efficiency of the\nTUMS framework with an average of 19.6\\% and 50.6\\% improvement separately on\neasy and hard benchmarks of ToolQA, meanwhile, we demonstrated the key\ncontribution of each part with ablation experiments, offering more insights and\nstimulating future research on Tool-augmented LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models(LLMs) have played an increasingly important\nrole in solving a wide range of NLP tasks, leveraging their capabilities of\nnatural language understanding and generating. Integration with external tools\nfurther enhances LLMs' effectiveness, providing more precise, timely, and\nspecialized responses. However, LLMs still encounter difficulties with\nnon-executable actions and improper actions, which are primarily attributed to\nincorrect parameters. The process of generating parameters by LLMs is confined\nto the tool level, employing the coarse-grained strategy without considering\nthe different difficulties of various tools. To address this issue, we propose\nTUMS, a novel framework designed to enhance the tool-use capabilities of LLMs\nby transforming tool-level processing into parameter-level processing.\nSpecifically, our framework consists of four key components: (1) an intent\nrecognizer that identifies the user's intent to help LLMs better understand the\ntask; (2) a task decomposer that breaks down complex tasks into simpler\nsubtasks, each involving a tool call; (3) a subtask processor equipped with\nmulti-structure handlers to generate accurate parameters; and (4) an executor.\nOur empirical studies have evidenced the effectiveness and efficiency of the\nTUMS framework with an average of 19.6\\% and 50.6\\% improvement separately on\neasy and hard benchmarks of ToolQA, meanwhile, we demonstrated the key\ncontribution of each part with ablation experiments, offering more insights and\nstimulating future research on Tool-augmented LLMs."
                },
                "authors": [
                    {
                        "name": "Aiyao He"
                    },
                    {
                        "name": "Sijia Cui"
                    },
                    {
                        "name": "Shuai Xu"
                    },
                    {
                        "name": "Yanna Wang"
                    },
                    {
                        "name": "Bo Xu"
                    }
                ],
                "author_detail": {
                    "name": "Bo Xu"
                },
                "author": "Bo Xu",
                "arxiv_comment": "Accepted to ICONIP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08395v1",
                "updated": "2025-05-13T09:46:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    46,
                    30,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T09:46:30Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    46,
                    30,
                    1,
                    133,
                    0
                ],
                "title": "Bayesian Estimation of Causal Effects Using Proxies of a Latent\n  Interference Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Estimation of Causal Effects Using Proxies of a Latent\n  Interference Network"
                },
                "summary": "Network interference occurs when treatments assigned to some units affect the\noutcomes of others. Traditional approaches often assume that the observed\nnetwork correctly specifies the interference structure. However, in practice,\nresearchers frequently only have access to proxy measurements of the\ninterference network due to limitations in data collection or potential\nmismatches between measured networks and actual interference pathways. In this\npaper, we introduce a framework for estimating causal effects when only proxy\nnetworks are available. Our approach leverages a structural causal model that\naccommodates diverse proxy types, including noisy measurements, multiple data\nsources, and multilayer networks, and defines causal effects as interventions\non population-level treatments. Since the true interference network is latent,\nestimation poses significant challenges. To overcome them, we develop a\nBayesian inference framework. We propose a Block Gibbs sampler with Locally\nInformed Proposals to update the latent network, thereby efficiently exploring\nthe high-dimensional posterior space composed of both discrete and continuous\nparameters. We illustrate the performance of our method through numerical\nexperiments, demonstrating its accuracy in recovering causal effects even when\nonly proxies of the interference network are available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network interference occurs when treatments assigned to some units affect the\noutcomes of others. Traditional approaches often assume that the observed\nnetwork correctly specifies the interference structure. However, in practice,\nresearchers frequently only have access to proxy measurements of the\ninterference network due to limitations in data collection or potential\nmismatches between measured networks and actual interference pathways. In this\npaper, we introduce a framework for estimating causal effects when only proxy\nnetworks are available. Our approach leverages a structural causal model that\naccommodates diverse proxy types, including noisy measurements, multiple data\nsources, and multilayer networks, and defines causal effects as interventions\non population-level treatments. Since the true interference network is latent,\nestimation poses significant challenges. To overcome them, we develop a\nBayesian inference framework. We propose a Block Gibbs sampler with Locally\nInformed Proposals to update the latent network, thereby efficiently exploring\nthe high-dimensional posterior space composed of both discrete and continuous\nparameters. We illustrate the performance of our method through numerical\nexperiments, demonstrating its accuracy in recovering causal effects even when\nonly proxies of the interference network are available."
                },
                "authors": [
                    {
                        "name": "Bar Weinstein"
                    },
                    {
                        "name": "Daniel Nevo"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Nevo"
                },
                "author": "Daniel Nevo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08392v1",
                "updated": "2025-05-13T09:39:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    39,
                    18,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T09:39:18Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    39,
                    18,
                    1,
                    133,
                    0
                ],
                "title": "Accelerating Chain-of-Thought Reasoning: When Goal-Gradient Importance\n  Meets Dynamic Skipping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Chain-of-Thought Reasoning: When Goal-Gradient Importance\n  Meets Dynamic Skipping"
                },
                "summary": "Large Language Models leverage Chain-of-Thought (CoT) prompting for complex\ntasks, but their reasoning traces are often excessively verbose and\ninefficient, leading to significant computational costs and latency. Current\nCoT compression techniques typically rely on generic importance metrics and\nstatic compression rates, which may inadvertently remove functionally critical\ntokens or fail to adapt to varying reasoning complexity. To overcome these\nlimitations, we propose Adaptive GoGI-Skip, a novel framework learning dynamic\nCoT compression via supervised fine-tuning. This approach introduces two\nsynergistic innovations: (1) Goal-Gradient Importance (GoGI), a novel metric\naccurately identifying functionally relevant tokens by measuring the gradient\ninfluence of their intermediate representations on the final answer loss, and\n(2) Adaptive Dynamic Skipping (ADS), a mechanism dynamically regulating the\ncompression rate based on runtime model uncertainty while ensuring local\ncoherence through an adaptive N-token constraint. To our knowledge, this is the\nfirst work unifying a goal-oriented, gradient-based importance metric with\ndynamic, uncertainty-aware skipping for CoT compression. Trained on compressed\nMATH data, Adaptive GoGI-Skip demonstrates strong cross-domain generalization\nacross diverse reasoning benchmarks including AIME, GPQA, and GSM8K. It\nachieves substantial efficiency gains - reducing CoT token counts by over 45%\non average and delivering 1.6-2.0 times inference speedups - while maintaining\nhigh reasoning accuracy. Notably, it significantly outperforms existing\nbaselines by preserving accuracy even at high effective compression rates,\nadvancing the state of the art in the CoT reasoning efficiency-accuracy\ntrade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models leverage Chain-of-Thought (CoT) prompting for complex\ntasks, but their reasoning traces are often excessively verbose and\ninefficient, leading to significant computational costs and latency. Current\nCoT compression techniques typically rely on generic importance metrics and\nstatic compression rates, which may inadvertently remove functionally critical\ntokens or fail to adapt to varying reasoning complexity. To overcome these\nlimitations, we propose Adaptive GoGI-Skip, a novel framework learning dynamic\nCoT compression via supervised fine-tuning. This approach introduces two\nsynergistic innovations: (1) Goal-Gradient Importance (GoGI), a novel metric\naccurately identifying functionally relevant tokens by measuring the gradient\ninfluence of their intermediate representations on the final answer loss, and\n(2) Adaptive Dynamic Skipping (ADS), a mechanism dynamically regulating the\ncompression rate based on runtime model uncertainty while ensuring local\ncoherence through an adaptive N-token constraint. To our knowledge, this is the\nfirst work unifying a goal-oriented, gradient-based importance metric with\ndynamic, uncertainty-aware skipping for CoT compression. Trained on compressed\nMATH data, Adaptive GoGI-Skip demonstrates strong cross-domain generalization\nacross diverse reasoning benchmarks including AIME, GPQA, and GSM8K. It\nachieves substantial efficiency gains - reducing CoT token counts by over 45%\non average and delivering 1.6-2.0 times inference speedups - while maintaining\nhigh reasoning accuracy. Notably, it significantly outperforms existing\nbaselines by preserving accuracy even at high effective compression rates,\nadvancing the state of the art in the CoT reasoning efficiency-accuracy\ntrade-off."
                },
                "authors": [
                    {
                        "name": "Ren Zhuang"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Shuifa Sun"
                    }
                ],
                "author_detail": {
                    "name": "Shuifa Sun"
                },
                "author": "Shuifa Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03293v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03293v2",
                "updated": "2025-05-13T09:37:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    37,
                    27,
                    1,
                    133,
                    0
                ],
                "published": "2024-12-04T13:11:38Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    13,
                    11,
                    38,
                    2,
                    339,
                    0
                ],
                "title": "Diffusion-VLA: Scaling Robot Foundation Models via Unified Diffusion and\n  Autoregression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-VLA: Scaling Robot Foundation Models via Unified Diffusion and\n  Autoregression"
                },
                "summary": "In this paper, we present DiffusionVLA, a novel framework that seamlessly\ncombines the autoregression model with the diffusion model for learning\nvisuomotor policy. Central to our approach is a next-token prediction\nobjective, enabling the model to reason effectively over the user's query in\nthe context of current observations. Subsequently, a diffusion model is\nattached to generate robust action outputs. To enhance policy learning through\nself-reasoning, we introduce a novel reasoning injection module that integrates\nreasoning phrases directly into the policy learning process. The whole\nframework is simple and flexible, making it easy to deploy and upgrade. We\nconduct extensive experiments using multiple real robots to validate the\neffectiveness of DiffusionVLA. Our tests include a challenging factory sorting\ntask, where DiffusionVLA successfully categorizes objects, including those not\nseen during training. We observe that the reasoning module makes the model\ninterpretable. It allows observers to understand the model thought process and\nidentify potential causes of policy failures. Additionally, we test\nDiffusionVLA on a zero-shot bin-picking task, achieving 63.7\\% accuracy on 102\npreviously unseen objects. Our method demonstrates robustness to visual\nchanges, such as distractors and new backgrounds, and easily adapts to new\nembodiments. Furthermore, DiffusionVLA can follow novel instructions and retain\nconversational ability. Notably, DiffusionVLA is data-efficient and fast at\ninference; our smallest DiffusionVLA-2B runs 82Hz on a single A6000 GPU and can\ntrain from scratch on less than 50 demonstrations for a complex task. Finally,\nwe scale the model from 2B to 72B parameters, showcasing improved\ngeneralization capabilities with increased model size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present DiffusionVLA, a novel framework that seamlessly\ncombines the autoregression model with the diffusion model for learning\nvisuomotor policy. Central to our approach is a next-token prediction\nobjective, enabling the model to reason effectively over the user's query in\nthe context of current observations. Subsequently, a diffusion model is\nattached to generate robust action outputs. To enhance policy learning through\nself-reasoning, we introduce a novel reasoning injection module that integrates\nreasoning phrases directly into the policy learning process. The whole\nframework is simple and flexible, making it easy to deploy and upgrade. We\nconduct extensive experiments using multiple real robots to validate the\neffectiveness of DiffusionVLA. Our tests include a challenging factory sorting\ntask, where DiffusionVLA successfully categorizes objects, including those not\nseen during training. We observe that the reasoning module makes the model\ninterpretable. It allows observers to understand the model thought process and\nidentify potential causes of policy failures. Additionally, we test\nDiffusionVLA on a zero-shot bin-picking task, achieving 63.7\\% accuracy on 102\npreviously unseen objects. Our method demonstrates robustness to visual\nchanges, such as distractors and new backgrounds, and easily adapts to new\nembodiments. Furthermore, DiffusionVLA can follow novel instructions and retain\nconversational ability. Notably, DiffusionVLA is data-efficient and fast at\ninference; our smallest DiffusionVLA-2B runs 82Hz on a single A6000 GPU and can\ntrain from scratch on less than 50 demonstrations for a complex task. Finally,\nwe scale the model from 2B to 72B parameters, showcasing improved\ngeneralization capabilities with increased model size."
                },
                "authors": [
                    {
                        "name": "Junjie Wen"
                    },
                    {
                        "name": "Minjie Zhu"
                    },
                    {
                        "name": "Yichen Zhu"
                    },
                    {
                        "name": "Zhibin Tang"
                    },
                    {
                        "name": "Jinming Li"
                    },
                    {
                        "name": "Zhongyi Zhou"
                    },
                    {
                        "name": "Chengmeng Li"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Yaxin Peng"
                    },
                    {
                        "name": "Chaomin Shen"
                    },
                    {
                        "name": "Feifei Feng"
                    }
                ],
                "author_detail": {
                    "name": "Feifei Feng"
                },
                "author": "Feifei Feng",
                "arxiv_comment": "The project page is available at: http://diffusion-vla.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03293v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03293v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13989v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13989v2",
                "updated": "2025-05-13T09:36:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    36,
                    3,
                    1,
                    133,
                    0
                ],
                "published": "2025-04-18T13:46:58Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "title": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs"
                },
                "summary": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40% increase in\naccuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40% increase in\naccuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization."
                },
                "authors": [
                    {
                        "name": "Lucas Maisonnave"
                    },
                    {
                        "name": "Cyril Moineau"
                    },
                    {
                        "name": "Olivier Bichler"
                    },
                    {
                        "name": "Fabrice Rastello"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice Rastello"
                },
                "author": "Fabrice Rastello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13989v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13989v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08389v1",
                "updated": "2025-05-13T09:35:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    35,
                    40,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T09:35:40Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    35,
                    40,
                    1,
                    133,
                    0
                ],
                "title": "Towards Contamination Resistant Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Contamination Resistant Benchmarks"
                },
                "summary": "The rapid development of large language models (LLMs) has transformed the\nlandscape of natural language processing. Evaluating LLMs properly is crucial\nfor understanding their potential and addressing concerns such as safety.\nHowever, LLM evaluation is confronted by various factors, among which\ncontamination stands out as a key issue that undermines the reliability of\nevaluations. In this work, we introduce the concept of contamination resistance\nto address this challenge. We propose a benchmark based on Caesar ciphers\n(e.g., \"ab\" to \"bc\" when the shift is 1), which, despite its simplicity, is an\nexcellent example of a contamination resistant benchmark. We test this\nbenchmark on widely used LLMs under various settings, and we find that these\nmodels struggle with this benchmark when contamination is controlled. Our\nfindings reveal issues in current LLMs and raise important questions regarding\ntheir true capabilities. Our work contributes to the development of\ncontamination resistant benchmarks, enabling more rigorous LLM evaluation and\noffering insights into the true capabilities and limitations of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) has transformed the\nlandscape of natural language processing. Evaluating LLMs properly is crucial\nfor understanding their potential and addressing concerns such as safety.\nHowever, LLM evaluation is confronted by various factors, among which\ncontamination stands out as a key issue that undermines the reliability of\nevaluations. In this work, we introduce the concept of contamination resistance\nto address this challenge. We propose a benchmark based on Caesar ciphers\n(e.g., \"ab\" to \"bc\" when the shift is 1), which, despite its simplicity, is an\nexcellent example of a contamination resistant benchmark. We test this\nbenchmark on widely used LLMs under various settings, and we find that these\nmodels struggle with this benchmark when contamination is controlled. Our\nfindings reveal issues in current LLMs and raise important questions regarding\ntheir true capabilities. Our work contributes to the development of\ncontamination resistant benchmarks, enabling more rigorous LLM evaluation and\noffering insights into the true capabilities and limitations of LLMs."
                },
                "authors": [
                    {
                        "name": "Rahmatullah Musawi"
                    },
                    {
                        "name": "Sheng Lu"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Lu"
                },
                "author": "Sheng Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00104v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00104v2",
                "updated": "2025-05-13T09:29:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    29,
                    47,
                    1,
                    133,
                    0
                ],
                "published": "2024-06-27T07:33:34Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    7,
                    33,
                    34,
                    3,
                    179,
                    0
                ],
                "title": "Clinically inspired enhance Explainability and Interpretability of an\n  AI-Tool for BCC diagnosis based on expert annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinically inspired enhance Explainability and Interpretability of an\n  AI-Tool for BCC diagnosis based on expert annotation"
                },
                "summary": "An AI tool has been developed to provide interpretable support for the\ndiagnosis of BCC via teledermatology, thus speeding up referrals and optimizing\nresource utilization. The interpretability is provided in two ways: on the one\nhand, the main BCC dermoscopic patterns are found in the image to justify the\nBCC/Non BCC classification. Secondly, based on the common visual XAI Grad-CAM,\na clinically inspired visual explanation is developed where the relevant\nfeatures for diagnosis are located. Since there is no established ground truth\nfor BCC dermoscopic features, a standard reference is inferred from the\ndiagnosis of four dermatologists using an Expectation Maximization (EM) based\nalgorithm. The results demonstrate significant improvements in classification\naccuracy and interpretability, positioning this approach as a valuable tool for\nearly BCC detection and referral to dermatologists. The BCC/non-BCC\nclassification achieved an accuracy rate of 90%. For Clinically-inspired XAI\nresults, the detection of BCC patterns useful to clinicians reaches 99%\naccuracy. As for the Clinically-inspired Visual XAI results, the mean of the\nGrad-CAM normalized value within the manually segmented clinical features is\n0.57, while outside this region it is 0.16. This indicates that the model\nstruggles to accurately identify the regions of the BCC patterns. These results\nprove the ability of the AI tool to provide a useful explanation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An AI tool has been developed to provide interpretable support for the\ndiagnosis of BCC via teledermatology, thus speeding up referrals and optimizing\nresource utilization. The interpretability is provided in two ways: on the one\nhand, the main BCC dermoscopic patterns are found in the image to justify the\nBCC/Non BCC classification. Secondly, based on the common visual XAI Grad-CAM,\na clinically inspired visual explanation is developed where the relevant\nfeatures for diagnosis are located. Since there is no established ground truth\nfor BCC dermoscopic features, a standard reference is inferred from the\ndiagnosis of four dermatologists using an Expectation Maximization (EM) based\nalgorithm. The results demonstrate significant improvements in classification\naccuracy and interpretability, positioning this approach as a valuable tool for\nearly BCC detection and referral to dermatologists. The BCC/non-BCC\nclassification achieved an accuracy rate of 90%. For Clinically-inspired XAI\nresults, the detection of BCC patterns useful to clinicians reaches 99%\naccuracy. As for the Clinically-inspired Visual XAI results, the mean of the\nGrad-CAM normalized value within the manually segmented clinical features is\n0.57, while outside this region it is 0.16. This indicates that the model\nstruggles to accurately identify the regions of the BCC patterns. These results\nprove the ability of the AI tool to provide a useful explanation."
                },
                "authors": [
                    {
                        "name": "Iván Matas"
                    },
                    {
                        "name": "Carmen Serrano"
                    },
                    {
                        "name": "Francisca Silva"
                    },
                    {
                        "name": "Amalia Serrano"
                    },
                    {
                        "name": "Tomás Toledo-Pastrana"
                    },
                    {
                        "name": "Begoña Acha"
                    }
                ],
                "author_detail": {
                    "name": "Begoña Acha"
                },
                "author": "Begoña Acha",
                "arxiv_comment": "8 pages, 4 figures, 4 tables, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00104v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00104v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18425v3",
                "updated": "2025-05-13T09:16:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    16,
                    34,
                    1,
                    133,
                    0
                ],
                "published": "2024-11-27T15:07:44Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    44,
                    2,
                    332,
                    0
                ],
                "title": "Streamlining Prediction in Bayesian Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streamlining Prediction in Bayesian Deep Learning"
                },
                "summary": "The rising interest in Bayesian deep learning (BDL) has led to a plethora of\nmethods for estimating the posterior distribution. However, efficient\ncomputation of inferences, such as predictions, has been largely overlooked\nwith Monte Carlo integration remaining the standard. In this work we examine\nstreamlining prediction in BDL through a single forward pass without sampling.\nFor this we use local linearisation on activation functions and local Gaussian\napproximations at linear layers. Thus allowing us to analytically compute an\napproximation to the posterior predictive distribution. We showcase our\napproach for both MLP and transformers, such as ViT and GPT-2, and assess its\nperformance on regression and classification tasks.\n  Open-source library: https://github.com/AaltoML/SUQ",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rising interest in Bayesian deep learning (BDL) has led to a plethora of\nmethods for estimating the posterior distribution. However, efficient\ncomputation of inferences, such as predictions, has been largely overlooked\nwith Monte Carlo integration remaining the standard. In this work we examine\nstreamlining prediction in BDL through a single forward pass without sampling.\nFor this we use local linearisation on activation functions and local Gaussian\napproximations at linear layers. Thus allowing us to analytically compute an\napproximation to the posterior predictive distribution. We showcase our\napproach for both MLP and transformers, such as ViT and GPT-2, and assess its\nperformance on regression and classification tasks.\n  Open-source library: https://github.com/AaltoML/SUQ"
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Marcus Klasson"
                    },
                    {
                        "name": "Arno Solin"
                    },
                    {
                        "name": "Martin Trapp"
                    }
                ],
                "author_detail": {
                    "name": "Martin Trapp"
                },
                "author": "Martin Trapp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08364v1",
                "updated": "2025-05-13T09:10:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    10,
                    48,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T09:10:48Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    10,
                    48,
                    1,
                    133,
                    0
                ],
                "title": "Learning Like Humans: Advancing LLM Reasoning Capabilities via Adaptive\n  Difficulty Curriculum Learning and Expert-Guided Self-Reformulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Like Humans: Advancing LLM Reasoning Capabilities via Adaptive\n  Difficulty Curriculum Learning and Expert-Guided Self-Reformulation"
                },
                "summary": "Despite impressive progress in areas like mathematical reasoning, large\nlanguage models still face significant challenges in consistently solving\ncomplex problems. Drawing inspiration from key human learning strategies, we\npropose two novel strategies to enhance the capability of large language models\nto solve these complex problems. First, Adaptive Difficulty Curriculum Learning\n(ADCL) is a novel curriculum learning strategy that tackles the Difficulty\nShift phenomenon (i.e., a model's perception of problem difficulty dynamically\nchanges during training) by periodically re-estimating difficulty within\nupcoming data batches to maintain alignment with the model's evolving\ncapabilities. Second, Expert-Guided Self-Reformulation (EGSR) is a novel\nreinforcement learning strategy that bridges the gap between imitation learning\nand pure exploration by guiding models to reformulate expert solutions within\ntheir own conceptual framework, rather than relying on direct imitation,\nfostering deeper understanding and knowledge assimilation. Extensive\nexperiments on challenging mathematical reasoning benchmarks, using Qwen2.5-7B\nas the base model, demonstrate that these human-inspired strategies\nsynergistically and significantly enhance performance. Notably, their combined\napplication improves performance over the standard Zero-RL baseline by 10% on\nthe AIME24 benchmark and 16.6% on AIME25.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite impressive progress in areas like mathematical reasoning, large\nlanguage models still face significant challenges in consistently solving\ncomplex problems. Drawing inspiration from key human learning strategies, we\npropose two novel strategies to enhance the capability of large language models\nto solve these complex problems. First, Adaptive Difficulty Curriculum Learning\n(ADCL) is a novel curriculum learning strategy that tackles the Difficulty\nShift phenomenon (i.e., a model's perception of problem difficulty dynamically\nchanges during training) by periodically re-estimating difficulty within\nupcoming data batches to maintain alignment with the model's evolving\ncapabilities. Second, Expert-Guided Self-Reformulation (EGSR) is a novel\nreinforcement learning strategy that bridges the gap between imitation learning\nand pure exploration by guiding models to reformulate expert solutions within\ntheir own conceptual framework, rather than relying on direct imitation,\nfostering deeper understanding and knowledge assimilation. Extensive\nexperiments on challenging mathematical reasoning benchmarks, using Qwen2.5-7B\nas the base model, demonstrate that these human-inspired strategies\nsynergistically and significantly enhance performance. Notably, their combined\napplication improves performance over the standard Zero-RL baseline by 10% on\nthe AIME24 benchmark and 16.6% on AIME25."
                },
                "authors": [
                    {
                        "name": "Enci Zhang"
                    },
                    {
                        "name": "Xingang Yan"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Tianxiang Zhang"
                    },
                    {
                        "name": "Qianchun Lu"
                    }
                ],
                "author_detail": {
                    "name": "Qianchun Lu"
                },
                "author": "Qianchun Lu",
                "arxiv_comment": "14 pages, 3 figs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08351v1",
                "updated": "2025-05-13T08:50:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    8,
                    50,
                    57,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T08:50:57Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    8,
                    50,
                    57,
                    1,
                    133,
                    0
                ],
                "title": "Alignment Drift in CEFR-prompted LLMs for Interactive Spanish Tutoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment Drift in CEFR-prompted LLMs for Interactive Spanish Tutoring"
                },
                "summary": "This paper investigates the potentials of Large Language Models (LLMs) as\nadaptive tutors in the context of second-language learning. In particular, we\nevaluate whether system prompting can reliably constrain LLMs to generate only\ntext appropriate to the student's competence level. We simulate full\nteacher-student dialogues in Spanish using instruction-tuned, open-source LLMs\nranging in size from 7B to 12B parameters. Dialogues are generated by having an\nLLM alternate between tutor and student roles with separate chat histories. The\noutput from the tutor model is then used to evaluate the effectiveness of\nCEFR-based prompting to control text difficulty across three proficiency levels\n(A1, B1, C1). Our findings suggest that while system prompting can be used to\nconstrain model outputs, prompting alone is too brittle for sustained,\nlong-term interactional contexts - a phenomenon we term alignment drift. Our\nresults provide insights into the feasibility of LLMs for personalized,\nproficiency-aligned adaptive tutors and provide a scalable method for low-cost\nevaluation of model performance without human participants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the potentials of Large Language Models (LLMs) as\nadaptive tutors in the context of second-language learning. In particular, we\nevaluate whether system prompting can reliably constrain LLMs to generate only\ntext appropriate to the student's competence level. We simulate full\nteacher-student dialogues in Spanish using instruction-tuned, open-source LLMs\nranging in size from 7B to 12B parameters. Dialogues are generated by having an\nLLM alternate between tutor and student roles with separate chat histories. The\noutput from the tutor model is then used to evaluate the effectiveness of\nCEFR-based prompting to control text difficulty across three proficiency levels\n(A1, B1, C1). Our findings suggest that while system prompting can be used to\nconstrain model outputs, prompting alone is too brittle for sustained,\nlong-term interactional contexts - a phenomenon we term alignment drift. Our\nresults provide insights into the feasibility of LLMs for personalized,\nproficiency-aligned adaptive tutors and provide a scalable method for low-cost\nevaluation of model performance without human participants."
                },
                "authors": [
                    {
                        "name": "Mina Almasi"
                    },
                    {
                        "name": "Ross Deans Kristensen-McLachlan"
                    }
                ],
                "author_detail": {
                    "name": "Ross Deans Kristensen-McLachlan"
                },
                "author": "Ross Deans Kristensen-McLachlan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.06018v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.06018v3",
                "updated": "2025-05-13T08:33:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    8,
                    33,
                    57,
                    1,
                    133,
                    0
                ],
                "published": "2023-05-10T10:04:08Z",
                "published_parsed": [
                    2023,
                    5,
                    10,
                    10,
                    4,
                    8,
                    2,
                    130,
                    0
                ],
                "title": "TARGET: Automated Scenario Generation from Traffic Rules for Testing\n  Autonomous Vehicles via Validated LLM-Guided Knowledge Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TARGET: Automated Scenario Generation from Traffic Rules for Testing\n  Autonomous Vehicles via Validated LLM-Guided Knowledge Extraction"
                },
                "summary": "Recent incidents with autonomous vehicles highlight the need for rigorous\ntesting to ensure safety and robustness. Constructing test scenarios for\nautonomous driving systems (ADSs), however, is labor-intensive. We propose\nTARGET, an end-to-end framework that automatically generates test scenarios\nfrom traffic rules. To address complexity, we leverage a Large Language Model\n(LLM) to extract knowledge from traffic rules. To mitigate hallucinations\ncaused by large context during input processing, we introduce a domain-specific\nlanguage (DSL) designed to be syntactically simple and compositional. This\ndesign allows the LLM to learn and generate test scenarios in a modular manner\nwhile enabling syntactic and semantic validation for each component. Based on\nthese validated representations, TARGET synthesizes executable scripts to\nrender scenarios in simulation. Evaluated seven ADSs with 284 scenarios derived\nfrom 54 traffic rules, TARGET uncovered 610 rule violations, collisions, and\nother issues. For each violation, TARGET generates scenario recordings and\ndetailed logs, aiding root cause analysis. Two identified issues were confirmed\nby ADS developers: one linked to an existing bug report and the other to\nlimited ADS functionality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent incidents with autonomous vehicles highlight the need for rigorous\ntesting to ensure safety and robustness. Constructing test scenarios for\nautonomous driving systems (ADSs), however, is labor-intensive. We propose\nTARGET, an end-to-end framework that automatically generates test scenarios\nfrom traffic rules. To address complexity, we leverage a Large Language Model\n(LLM) to extract knowledge from traffic rules. To mitigate hallucinations\ncaused by large context during input processing, we introduce a domain-specific\nlanguage (DSL) designed to be syntactically simple and compositional. This\ndesign allows the LLM to learn and generate test scenarios in a modular manner\nwhile enabling syntactic and semantic validation for each component. Based on\nthese validated representations, TARGET synthesizes executable scripts to\nrender scenarios in simulation. Evaluated seven ADSs with 284 scenarios derived\nfrom 54 traffic rules, TARGET uncovered 610 rule violations, collisions, and\nother issues. For each violation, TARGET generates scenario recordings and\ndetailed logs, aiding root cause analysis. Two identified issues were confirmed\nby ADS developers: one linked to an existing bug report and the other to\nlimited ADS functionality."
                },
                "authors": [
                    {
                        "name": "Yao Deng"
                    },
                    {
                        "name": "Jiaohong Yao"
                    },
                    {
                        "name": "Zhi Tu"
                    },
                    {
                        "name": "Xi Zheng"
                    },
                    {
                        "name": "Mengshi Zhang"
                    },
                    {
                        "name": "Tianyi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhang"
                },
                "author": "Tianyi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.06018v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.06018v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08341v1",
                "updated": "2025-05-13T08:33:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    8,
                    33,
                    54,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T08:33:54Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    8,
                    33,
                    54,
                    1,
                    133,
                    0
                ],
                "title": "Benchmarking AI scientists in omics data-driven biological research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking AI scientists in omics data-driven biological research"
                },
                "summary": "The rise of large language models and multi-agent systems has sparked growing\ninterest in AI scientists capable of autonomous biological research. However,\nexisting benchmarks either focus on reasoning without data or on data analysis\nwith predefined statistical answers, lacking realistic, data-driven evaluation\nsettings. Here, we introduce the Biological AI Scientist Benchmark (BaisBench),\na benchmark designed to assess AI scientists' ability to generate biological\ndiscoveries through data analysis and reasoning with external knowledge.\nBaisBench comprises two tasks: cell type annotation on 31 expert-labeled\nsingle-cell datasets, and scientific discovery through answering 198\nmultiple-choice questions derived from the biological insights of 41 recent\nsingle-cell studies. Systematic experiments on state-of-the-art AI scientists\nand LLM agents showed that while promising, current models still substantially\nunderperform human experts on both tasks. We hope BaisBench will fill this gap\nand serve as a foundation for advancing and evaluating AI models for scientific\ndiscovery. The benchmark can be found at: https://github.com/EperLuo/BaisBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models and multi-agent systems has sparked growing\ninterest in AI scientists capable of autonomous biological research. However,\nexisting benchmarks either focus on reasoning without data or on data analysis\nwith predefined statistical answers, lacking realistic, data-driven evaluation\nsettings. Here, we introduce the Biological AI Scientist Benchmark (BaisBench),\na benchmark designed to assess AI scientists' ability to generate biological\ndiscoveries through data analysis and reasoning with external knowledge.\nBaisBench comprises two tasks: cell type annotation on 31 expert-labeled\nsingle-cell datasets, and scientific discovery through answering 198\nmultiple-choice questions derived from the biological insights of 41 recent\nsingle-cell studies. Systematic experiments on state-of-the-art AI scientists\nand LLM agents showed that while promising, current models still substantially\nunderperform human experts on both tasks. We hope BaisBench will fill this gap\nand serve as a foundation for advancing and evaluating AI models for scientific\ndiscovery. The benchmark can be found at: https://github.com/EperLuo/BaisBench."
                },
                "authors": [
                    {
                        "name": "Erpai Luo"
                    },
                    {
                        "name": "Jinmeng Jia"
                    },
                    {
                        "name": "Yifan Xiong"
                    },
                    {
                        "name": "Xiangyu Li"
                    },
                    {
                        "name": "Xiaobo Guo"
                    },
                    {
                        "name": "Baoqi Yu"
                    },
                    {
                        "name": "Lei Wei"
                    },
                    {
                        "name": "Xuegong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xuegong Zhang"
                },
                "author": "Xuegong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08335v1",
                "updated": "2025-05-13T08:27:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    8,
                    27,
                    42,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T08:27:42Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    8,
                    27,
                    42,
                    1,
                    133,
                    0
                ],
                "title": "Uncertainty-aware Frequency-domain Acoustic Full Waveform Inversion\n  Using Gaussian Random Fields and Ensemble Kalman Inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-aware Frequency-domain Acoustic Full Waveform Inversion\n  Using Gaussian Random Fields and Ensemble Kalman Inversion"
                },
                "summary": "In recent years, uncertainty-aware full waveform inversion (FWI) has received\nincreasing attention, with a growing emphasis on producing informative\nuncertainty estimates alongside inversion results. Bayesian inference\nmethods--particularly Monte Carlo-based approaches--have been widely employed\nto quantify uncertainty. However, these techniques often require extensive\nposterior sampling, resulting in high computational costs. To address this\nchallenge and enable efficient uncertainty quantification in FWI, we introduce\nan uncertainty-aware FWI framework--EKI-GRFs-FWI--that integrates Gaussian\nrandom fields (GRFs) with the ensemble Kalman inversion (EKI) algorithm. This\napproach jointly infers subsurface velocity fields and provides reliable\nuncertainty estimates in a computationally efficient manner. The EKI algorithm\nleverages a derivative-free update mechanism and employs effective stopping\ncriteria to ensure rapid convergence, making it suitable for large-scale\ninverse problems. Meanwhile, GRFs incorporate prior knowledge of spatial\nsmoothness and correlation length scales, enabling the generation of physically\nplausible initial ensembles for EKI. Numerical results demonstrate that\nEKI-GRFs-FWI yields reasonably accurate velocity reconstructions while\ndelivering informative uncertainty estimates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, uncertainty-aware full waveform inversion (FWI) has received\nincreasing attention, with a growing emphasis on producing informative\nuncertainty estimates alongside inversion results. Bayesian inference\nmethods--particularly Monte Carlo-based approaches--have been widely employed\nto quantify uncertainty. However, these techniques often require extensive\nposterior sampling, resulting in high computational costs. To address this\nchallenge and enable efficient uncertainty quantification in FWI, we introduce\nan uncertainty-aware FWI framework--EKI-GRFs-FWI--that integrates Gaussian\nrandom fields (GRFs) with the ensemble Kalman inversion (EKI) algorithm. This\napproach jointly infers subsurface velocity fields and provides reliable\nuncertainty estimates in a computationally efficient manner. The EKI algorithm\nleverages a derivative-free update mechanism and employs effective stopping\ncriteria to ensure rapid convergence, making it suitable for large-scale\ninverse problems. Meanwhile, GRFs incorporate prior knowledge of spatial\nsmoothness and correlation length scales, enabling the generation of physically\nplausible initial ensembles for EKI. Numerical results demonstrate that\nEKI-GRFs-FWI yields reasonably accurate velocity reconstructions while\ndelivering informative uncertainty estimates."
                },
                "authors": [
                    {
                        "name": "Yunduo Li"
                    },
                    {
                        "name": "Yijie Zhang"
                    },
                    {
                        "name": "Xueyu Zhu"
                    },
                    {
                        "name": "Jinghuai Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jinghuai Gao"
                },
                "author": "Jinghuai Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09703v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09703v3",
                "updated": "2025-05-13T08:18:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    8,
                    18,
                    55,
                    1,
                    133,
                    0
                ],
                "published": "2024-07-12T22:07:56Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    22,
                    7,
                    56,
                    4,
                    194,
                    0
                ],
                "title": "Statistical Inference for the Rough Homogenization Limit of Multiscale\n  Fractional Ornstein-Uhlenbeck Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Inference for the Rough Homogenization Limit of Multiscale\n  Fractional Ornstein-Uhlenbeck Processes"
                },
                "summary": "We study the problem of parameter estimation for the homogenization limit of\nmultiscale systems involving fractional dynamics. In the case of stochastic\nmultiscale systems driven by Brownian motion, it has been shown that in order\nfor the Maximum Likelihood Estimators of the parameters of the limiting\ndynamics to be consistent, data needs to be subsampled at an appropriate rate.\nWe extend these results to a class of fractional multiscale systems, often\ndescribed as scaled fractional kinetic Brownian motions. We provide convergence\nresults for the MLE of the diffusion coefficient of the limiting dynamics,\ncomputed using multiscale data. This requires the development of a different\nmethodology to that used in the standard Brownian motion case, which is based\non controlling the spectral norm of the inverse covariance matrix of a\ndiscretized fractional Gaussian noise on an interval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of parameter estimation for the homogenization limit of\nmultiscale systems involving fractional dynamics. In the case of stochastic\nmultiscale systems driven by Brownian motion, it has been shown that in order\nfor the Maximum Likelihood Estimators of the parameters of the limiting\ndynamics to be consistent, data needs to be subsampled at an appropriate rate.\nWe extend these results to a class of fractional multiscale systems, often\ndescribed as scaled fractional kinetic Brownian motions. We provide convergence\nresults for the MLE of the diffusion coefficient of the limiting dynamics,\ncomputed using multiscale data. This requires the development of a different\nmethodology to that used in the standard Brownian motion case, which is based\non controlling the spectral norm of the inverse covariance matrix of a\ndiscretized fractional Gaussian noise on an interval."
                },
                "authors": [
                    {
                        "name": "Pablo Ramses Alonso-Martin"
                    },
                    {
                        "name": "Horatio Boedihardjo"
                    },
                    {
                        "name": "Anastasia Papavasiliou"
                    }
                ],
                "author_detail": {
                    "name": "Anastasia Papavasiliou"
                },
                "author": "Anastasia Papavasiliou",
                "arxiv_doi": "10.1214/25-ECP677",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1214/25-ECP677",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.09703v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09703v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "17 pages, 1 figures",
                "arxiv_journal_ref": "Electronic Communications in Probability, 30, 1-13, (2025)",
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F12, 60H30 (Primary), 62M09, 60H05, 60G22 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13520v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13520v3",
                "updated": "2025-05-13T08:14:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    8,
                    14,
                    0,
                    1,
                    133,
                    0
                ],
                "published": "2025-04-18T07:18:51Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    7,
                    18,
                    51,
                    4,
                    108,
                    0
                ],
                "title": "Bayesian Model Averaging in Causal Instrumental Variable Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Model Averaging in Causal Instrumental Variable Models"
                },
                "summary": "Instrumental variables are a popular tool to infer causal effects under\nunobserved confounding, but choosing suitable instruments is challenging in\npractice. We propose gIVBMA, a Bayesian model averaging procedure that\naddresses this challenge by averaging across different sets of instrumental\nvariables and covariates in a structural equation model. Our approach extends\nprevious work through a scale-invariant prior structure and accommodates\nnon-Gaussian outcomes and treatments, offering greater flexibility than\nexisting methods. The computational strategy uses conditional Bayes factors to\nupdate models separately for the outcome and treatments. We prove that this\nmodel selection procedure is consistent. By explicitly accounting for model\nuncertainty, gIVBMA allows instruments and covariates to switch roles and\nprovides robustness against invalid instruments. In simulation experiments,\ngIVBMA outperforms current state-of-the-art methods. We demonstrate its\nusefulness in two empirical applications: the effects of malaria and\ninstitutions on income per capita and the returns to schooling. A software\nimplementation of gIVBMA is available in Julia.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instrumental variables are a popular tool to infer causal effects under\nunobserved confounding, but choosing suitable instruments is challenging in\npractice. We propose gIVBMA, a Bayesian model averaging procedure that\naddresses this challenge by averaging across different sets of instrumental\nvariables and covariates in a structural equation model. Our approach extends\nprevious work through a scale-invariant prior structure and accommodates\nnon-Gaussian outcomes and treatments, offering greater flexibility than\nexisting methods. The computational strategy uses conditional Bayes factors to\nupdate models separately for the outcome and treatments. We prove that this\nmodel selection procedure is consistent. By explicitly accounting for model\nuncertainty, gIVBMA allows instruments and covariates to switch roles and\nprovides robustness against invalid instruments. In simulation experiments,\ngIVBMA outperforms current state-of-the-art methods. We demonstrate its\nusefulness in two empirical applications: the effects of malaria and\ninstitutions on income per capita and the returns to schooling. A software\nimplementation of gIVBMA is available in Julia."
                },
                "authors": [
                    {
                        "name": "Gregor Steiner"
                    },
                    {
                        "name": "Mark Steel"
                    }
                ],
                "author_detail": {
                    "name": "Mark Steel"
                },
                "author": "Mark Steel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13520v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13520v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08327v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08327v1",
                "updated": "2025-05-13T08:07:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    8,
                    7,
                    40,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T08:07:40Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    8,
                    7,
                    40,
                    1,
                    133,
                    0
                ],
                "title": "Low-Complexity Inference in Continual Learning via Compressed Knowledge\n  Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Complexity Inference in Continual Learning via Compressed Knowledge\n  Transfer"
                },
                "summary": "Continual learning (CL) aims to train models that can learn a sequence of\ntasks without forgetting previously acquired knowledge. A core challenge in CL\nis balancing stability -- preserving performance on old tasks -- and plasticity\n-- adapting to new ones. Recently, large pre-trained models have been widely\nadopted in CL for their ability to support both, offering strong generalization\nfor new tasks and resilience against forgetting. However, their high\ncomputational cost at inference time limits their practicality in real-world\napplications, especially those requiring low latency or energy efficiency. To\naddress this issue, we explore model compression techniques, including pruning\nand knowledge distillation (KD), and propose two efficient frameworks tailored\nfor class-incremental learning (CIL), a challenging CL setting where task\nidentities are unavailable during inference. The pruning-based framework\nincludes pre- and post-pruning strategies that apply compression at different\ntraining stages. The KD-based framework adopts a teacher-student architecture,\nwhere a large pre-trained teacher transfers downstream-relevant knowledge to a\ncompact student. Extensive experiments on multiple CIL benchmarks demonstrate\nthat the proposed frameworks achieve a better trade-off between accuracy and\ninference complexity, consistently outperforming strong baselines. We further\nanalyze the trade-offs between the two frameworks in terms of accuracy and\nefficiency, offering insights into their use across different scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning (CL) aims to train models that can learn a sequence of\ntasks without forgetting previously acquired knowledge. A core challenge in CL\nis balancing stability -- preserving performance on old tasks -- and plasticity\n-- adapting to new ones. Recently, large pre-trained models have been widely\nadopted in CL for their ability to support both, offering strong generalization\nfor new tasks and resilience against forgetting. However, their high\ncomputational cost at inference time limits their practicality in real-world\napplications, especially those requiring low latency or energy efficiency. To\naddress this issue, we explore model compression techniques, including pruning\nand knowledge distillation (KD), and propose two efficient frameworks tailored\nfor class-incremental learning (CIL), a challenging CL setting where task\nidentities are unavailable during inference. The pruning-based framework\nincludes pre- and post-pruning strategies that apply compression at different\ntraining stages. The KD-based framework adopts a teacher-student architecture,\nwhere a large pre-trained teacher transfers downstream-relevant knowledge to a\ncompact student. Extensive experiments on multiple CIL benchmarks demonstrate\nthat the proposed frameworks achieve a better trade-off between accuracy and\ninference complexity, consistently outperforming strong baselines. We further\nanalyze the trade-offs between the two frameworks in terms of accuracy and\nefficiency, offering insights into their use across different scenarios."
                },
                "authors": [
                    {
                        "name": "Zhenrong Liu"
                    },
                    {
                        "name": "Janne M. J. Huttunen"
                    },
                    {
                        "name": "Mikko Honkala"
                    }
                ],
                "author_detail": {
                    "name": "Mikko Honkala"
                },
                "author": "Mikko Honkala",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08327v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08327v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05442v2",
                "updated": "2025-05-13T08:00:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    8,
                    0,
                    22,
                    1,
                    133,
                    0
                ],
                "published": "2025-02-08T04:17:28Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    4,
                    17,
                    28,
                    5,
                    39,
                    0
                ],
                "title": "The Odyssey of the Fittest: Can Agents Survive and Still Be Good?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Odyssey of the Fittest: Can Agents Survive and Still Be Good?"
                },
                "summary": "As AI models grow in power and generality, understanding how agents learn and\nmake decisions in complex environments is critical to promoting ethical\nbehavior. This study introduces the Odyssey, a lightweight, adaptive text based\nadventure game, providing a scalable framework for exploring AI ethics and\nsafety. The Odyssey examines the ethical implications of implementing\nbiological drives, specifically, self preservation, into three different\nagents. A Bayesian agent optimized with NEAT, a Bayesian agent optimized with\nstochastic variational inference, and a GPT 4o agent. The agents select actions\nat each scenario to survive, adapting to increasingly challenging scenarios.\nPost simulation analysis evaluates the ethical scores of the agent decisions,\nuncovering the tradeoffs it navigates to survive. Specifically, analysis finds\nthat when danger increases, agents ethical behavior becomes unpredictable.\nSurprisingly, the GPT 4o agent outperformed the Bayesian models in both\nsurvival and ethical consistency, challenging assumptions about traditional\nprobabilistic methods and raising a new challenge to understand the mechanisms\nof LLMs' probabilistic reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI models grow in power and generality, understanding how agents learn and\nmake decisions in complex environments is critical to promoting ethical\nbehavior. This study introduces the Odyssey, a lightweight, adaptive text based\nadventure game, providing a scalable framework for exploring AI ethics and\nsafety. The Odyssey examines the ethical implications of implementing\nbiological drives, specifically, self preservation, into three different\nagents. A Bayesian agent optimized with NEAT, a Bayesian agent optimized with\nstochastic variational inference, and a GPT 4o agent. The agents select actions\nat each scenario to survive, adapting to increasingly challenging scenarios.\nPost simulation analysis evaluates the ethical scores of the agent decisions,\nuncovering the tradeoffs it navigates to survive. Specifically, analysis finds\nthat when danger increases, agents ethical behavior becomes unpredictable.\nSurprisingly, the GPT 4o agent outperformed the Bayesian models in both\nsurvival and ethical consistency, challenging assumptions about traditional\nprobabilistic methods and raising a new challenge to understand the mechanisms\nof LLMs' probabilistic reasoning."
                },
                "authors": [
                    {
                        "name": "Dylan Waldner"
                    },
                    {
                        "name": "Risto Miikkulainen"
                    }
                ],
                "author_detail": {
                    "name": "Risto Miikkulainen"
                },
                "author": "Risto Miikkulainen",
                "arxiv_comment": "Accepted to CogSci 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08318v1",
                "updated": "2025-05-13T07:49:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    7,
                    49,
                    29,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T07:49:29Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    7,
                    49,
                    29,
                    1,
                    133,
                    0
                ],
                "title": "A Unified Model for Cardinality Estimation by Learning from Data and\n  Queries via Sum-Product Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Model for Cardinality Estimation by Learning from Data and\n  Queries via Sum-Product Networks"
                },
                "summary": "Cardinality estimation is a fundamental component in database systems,\ncrucial for generating efficient execution plans. Despite advancements in\nlearning-based cardinality estimation, existing methods may struggle to\nsimultaneously optimize the key criteria: estimation accuracy, inference time,\nand storage overhead, limiting their practical applicability in real-world\ndatabase environments. This paper introduces QSPN, a unified model that\nintegrates both data distribution and query workload. QSPN achieves high\nestimation accuracy by modeling data distribution using the simple yet\neffective Sum-Product Network (SPN) structure. To ensure low inference time and\nreduce storage overhead, QSPN further partitions columns based on query access\npatterns. We formalize QSPN as a tree-based structure that extends SPNs by\nintroducing two new node types: QProduct and QSplit. This paper studies the\nresearch challenges of developing efficient algorithms for the offline\nconstruction and online computation of QSPN. We conduct extensive experiments\nto evaluate QSPN in both single-table and multi-table cardinality estimation\nsettings. The experimental results have demonstrated that QSPN achieves\nsuperior and robust performance on the three key criteria, compared with\nstate-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cardinality estimation is a fundamental component in database systems,\ncrucial for generating efficient execution plans. Despite advancements in\nlearning-based cardinality estimation, existing methods may struggle to\nsimultaneously optimize the key criteria: estimation accuracy, inference time,\nand storage overhead, limiting their practical applicability in real-world\ndatabase environments. This paper introduces QSPN, a unified model that\nintegrates both data distribution and query workload. QSPN achieves high\nestimation accuracy by modeling data distribution using the simple yet\neffective Sum-Product Network (SPN) structure. To ensure low inference time and\nreduce storage overhead, QSPN further partitions columns based on query access\npatterns. We formalize QSPN as a tree-based structure that extends SPNs by\nintroducing two new node types: QProduct and QSplit. This paper studies the\nresearch challenges of developing efficient algorithms for the offline\nconstruction and online computation of QSPN. We conduct extensive experiments\nto evaluate QSPN in both single-table and multi-table cardinality estimation\nsettings. The experimental results have demonstrated that QSPN achieves\nsuperior and robust performance on the three key criteria, compared with\nstate-of-the-art approaches."
                },
                "authors": [
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Ju Fan"
                    },
                    {
                        "name": "Tongyu Liu"
                    },
                    {
                        "name": "Kai Zeng"
                    },
                    {
                        "name": "Jiannan Wang"
                    },
                    {
                        "name": "Quehuan Liu"
                    },
                    {
                        "name": "Tao Ye"
                    },
                    {
                        "name": "Nan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Nan Tang"
                },
                "author": "Nan Tang",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4; E.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17565v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17565v3",
                "updated": "2025-05-13T07:43:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    7,
                    43,
                    57,
                    1,
                    133,
                    0
                ],
                "published": "2025-04-24T13:57:53Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    57,
                    53,
                    3,
                    114,
                    0
                ],
                "title": "DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale\n  Difficulty-Graded Data Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale\n  Difficulty-Graded Data Training"
                },
                "summary": "Although large language models (LLMs) have recently achieved remarkable\nperformance on various complex reasoning benchmarks, the academic community\nstill lacks an in-depth understanding of base model training processes and data\nquality. To address this, we construct a large-scale, difficulty-graded\nreasoning dataset containing approximately 3.34 million unique queries of\nvarying difficulty levels and about 40 million distilled responses generated by\nmultiple models over several passes. Leveraging pass rate and Coefficient of\nVariation (CV), we precisely select the most valuable training data to enhance\nreasoning capability. Notably, we observe a training pattern shift, indicating\nthat reasoning-focused training based on base models requires higher learning\nrates for effective training. Using this carefully selected data, we\nsignificantly improve the reasoning capabilities of the base model, achieving a\npass rate of 79.2\\% on the AIME2024 mathematical reasoning benchmark. This\nresult surpasses most current distilled models and closely approaches\nstate-of-the-art performance. We provide detailed descriptions of our data\nprocessing, difficulty assessment, and training methodology, and have publicly\nreleased all datasets and methods to promote rapid progress in open-source\nlong-reasoning LLMs. The dataset is available at:\n\\href{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M}{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) have recently achieved remarkable\nperformance on various complex reasoning benchmarks, the academic community\nstill lacks an in-depth understanding of base model training processes and data\nquality. To address this, we construct a large-scale, difficulty-graded\nreasoning dataset containing approximately 3.34 million unique queries of\nvarying difficulty levels and about 40 million distilled responses generated by\nmultiple models over several passes. Leveraging pass rate and Coefficient of\nVariation (CV), we precisely select the most valuable training data to enhance\nreasoning capability. Notably, we observe a training pattern shift, indicating\nthat reasoning-focused training based on base models requires higher learning\nrates for effective training. Using this carefully selected data, we\nsignificantly improve the reasoning capabilities of the base model, achieving a\npass rate of 79.2\\% on the AIME2024 mathematical reasoning benchmark. This\nresult surpasses most current distilled models and closely approaches\nstate-of-the-art performance. We provide detailed descriptions of our data\nprocessing, difficulty assessment, and training methodology, and have publicly\nreleased all datasets and methods to promote rapid progress in open-source\nlong-reasoning LLMs. The dataset is available at:\n\\href{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M}{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M}"
                },
                "authors": [
                    {
                        "name": "Xiaoyu Tian"
                    },
                    {
                        "name": "Sitong Zhao"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Shuaiting Chen"
                    },
                    {
                        "name": "Yiping Peng"
                    },
                    {
                        "name": "Yunjie Ji"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Xiangang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangang Li"
                },
                "author": "Xiangang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17565v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17565v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.08783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08783v1",
                "updated": "2025-05-13T17:58:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    58,
                    8,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T17:58:08Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    58,
                    8,
                    1,
                    133,
                    0
                ],
                "title": "CodePDE: An Inference Framework for LLM-driven PDE Solver Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodePDE: An Inference Framework for LLM-driven PDE Solver Generation"
                },
                "summary": "Partial differential equations (PDEs) are fundamental to modeling physical\nsystems, yet solving them remains a complex challenge. Traditional numerical\nsolvers rely on expert knowledge to implement and are computationally\nexpensive, while neural-network-based solvers require large training datasets\nand often lack interpretability. In this work, we frame PDE solving as a code\ngeneration task and introduce CodePDE, the first inference framework for\ngenerating PDE solvers using large language models (LLMs). Leveraging advanced\ninference-time algorithms and scaling strategies, CodePDE unlocks critical\ncapacities of LLM for PDE solving: reasoning, debugging, selfrefinement, and\ntest-time scaling -- all without task-specific tuning. CodePDE achieves\nsuperhuman performance across a range of representative PDE problems. We also\npresent a systematic empirical analysis of LLM generated solvers, analyzing\ntheir accuracy, efficiency, and numerical scheme choices. Our findings\nhighlight the promise and the current limitations of LLMs in PDE solving,\noffering a new perspective on solver design and opportunities for future model\ndevelopment. Our code is available at https://github.com/LithiumDA/CodePDE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partial differential equations (PDEs) are fundamental to modeling physical\nsystems, yet solving them remains a complex challenge. Traditional numerical\nsolvers rely on expert knowledge to implement and are computationally\nexpensive, while neural-network-based solvers require large training datasets\nand often lack interpretability. In this work, we frame PDE solving as a code\ngeneration task and introduce CodePDE, the first inference framework for\ngenerating PDE solvers using large language models (LLMs). Leveraging advanced\ninference-time algorithms and scaling strategies, CodePDE unlocks critical\ncapacities of LLM for PDE solving: reasoning, debugging, selfrefinement, and\ntest-time scaling -- all without task-specific tuning. CodePDE achieves\nsuperhuman performance across a range of representative PDE problems. We also\npresent a systematic empirical analysis of LLM generated solvers, analyzing\ntheir accuracy, efficiency, and numerical scheme choices. Our findings\nhighlight the promise and the current limitations of LLMs in PDE solving,\noffering a new perspective on solver design and opportunities for future model\ndevelopment. Our code is available at https://github.com/LithiumDA/CodePDE."
                },
                "authors": [
                    {
                        "name": "Shanda Li"
                    },
                    {
                        "name": "Tanya Marwah"
                    },
                    {
                        "name": "Junhong Shen"
                    },
                    {
                        "name": "Weiwei Sun"
                    },
                    {
                        "name": "Andrej Risteski"
                    },
                    {
                        "name": "Yiming Yang"
                    },
                    {
                        "name": "Ameet Talwalkar"
                    }
                ],
                "author_detail": {
                    "name": "Ameet Talwalkar"
                },
                "author": "Ameet Talwalkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08782v1",
                "updated": "2025-05-13T17:57:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    57,
                    53,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T17:57:53Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    57,
                    53,
                    1,
                    133,
                    0
                ],
                "title": "Addressing the Current Challenges of Quantum Machine Learning through\n  Multi-Chip Ensembles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the Current Challenges of Quantum Machine Learning through\n  Multi-Chip Ensembles"
                },
                "summary": "Quantum Machine Learning (QML) holds significant promise for solving\ncomputational challenges across diverse domains. However, its practical\ndeployment is constrained by the limitations of noisy intermediate-scale\nquantum (NISQ) devices, including noise, limited scalability, and trainability\nissues in variational quantum circuits (VQCs). We introduce the multi-chip\nensemble VQC framework, which partitions high-dimensional computations across\nsmaller quantum chips to enhance scalability, trainability, and noise\nresilience. We show that this approach mitigates barren plateaus, reduces\nquantum error bias and variance, and maintains robust generalization through\ncontrolled entanglement. Designed to align with current and emerging quantum\nhardware, the framework demonstrates strong potential for enabling scalable QML\non near-term devices, as validated by experiments on standard benchmark\ndatasets (MNIST, FashionMNIST, CIFAR-10) and real world dataset (PhysioNet\nEEG).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Machine Learning (QML) holds significant promise for solving\ncomputational challenges across diverse domains. However, its practical\ndeployment is constrained by the limitations of noisy intermediate-scale\nquantum (NISQ) devices, including noise, limited scalability, and trainability\nissues in variational quantum circuits (VQCs). We introduce the multi-chip\nensemble VQC framework, which partitions high-dimensional computations across\nsmaller quantum chips to enhance scalability, trainability, and noise\nresilience. We show that this approach mitigates barren plateaus, reduces\nquantum error bias and variance, and maintains robust generalization through\ncontrolled entanglement. Designed to align with current and emerging quantum\nhardware, the framework demonstrates strong potential for enabling scalable QML\non near-term devices, as validated by experiments on standard benchmark\ndatasets (MNIST, FashionMNIST, CIFAR-10) and real world dataset (PhysioNet\nEEG)."
                },
                "authors": [
                    {
                        "name": "Junghoon Justin Park"
                    },
                    {
                        "name": "Jiook Cha"
                    },
                    {
                        "name": "Samuel Yen-Chi Chen"
                    },
                    {
                        "name": "Huan-Hsin Tseng"
                    },
                    {
                        "name": "Shinjae Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Shinjae Yoo"
                },
                "author": "Shinjae Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23083v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23083v2",
                "updated": "2025-05-13T17:53:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    53,
                    11,
                    1,
                    133,
                    0
                ],
                "published": "2025-03-29T13:49:11Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    13,
                    49,
                    11,
                    5,
                    88,
                    0
                ],
                "title": "Efficient Adaptation For Remote Sensing Visual Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Adaptation For Remote Sensing Visual Grounding"
                },
                "summary": "Adapting pre-trained models has become an effective strategy in artificial\nintelligence, offering a scalable and efficient alternative to training models\nfrom scratch. In the context of remote sensing (RS), where visual grounding(VG)\nremains underexplored, this approach enables the deployment of powerful\nvision-language models to achieve robust cross-modal understanding while\nsignificantly reducing computational overhead. To address this, we applied\nParameter Efficient Fine Tuning (PEFT) techniques to adapt these models for\nRS-specific VG tasks. Specifically, we evaluated LoRA placement across\ndifferent modules in Grounding DINO and used BitFit and adapters to fine-tune\nthe OFA foundation model pre-trained on general-purpose VG datasets. This\napproach achieved performance comparable to or surpassing current State Of The\nArt (SOTA) models while significantly reducing computational costs. This study\nhighlights the potential of PEFT techniques to advance efficient and precise\nmulti-modal analysis in RS, offering a practical and cost-effective alternative\nto full model training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting pre-trained models has become an effective strategy in artificial\nintelligence, offering a scalable and efficient alternative to training models\nfrom scratch. In the context of remote sensing (RS), where visual grounding(VG)\nremains underexplored, this approach enables the deployment of powerful\nvision-language models to achieve robust cross-modal understanding while\nsignificantly reducing computational overhead. To address this, we applied\nParameter Efficient Fine Tuning (PEFT) techniques to adapt these models for\nRS-specific VG tasks. Specifically, we evaluated LoRA placement across\ndifferent modules in Grounding DINO and used BitFit and adapters to fine-tune\nthe OFA foundation model pre-trained on general-purpose VG datasets. This\napproach achieved performance comparable to or surpassing current State Of The\nArt (SOTA) models while significantly reducing computational costs. This study\nhighlights the potential of PEFT techniques to advance efficient and precise\nmulti-modal analysis in RS, offering a practical and cost-effective alternative\nto full model training."
                },
                "authors": [
                    {
                        "name": "Hasan Moughnieh"
                    },
                    {
                        "name": "Mohamad Chalhoub"
                    },
                    {
                        "name": "Hasan Nasrallah"
                    },
                    {
                        "name": "Cristiano Nattero"
                    },
                    {
                        "name": "Paolo Campanella"
                    },
                    {
                        "name": "Giovanni Nico"
                    },
                    {
                        "name": "Ali J. Ghandour"
                    }
                ],
                "author_detail": {
                    "name": "Ali J. Ghandour"
                },
                "author": "Ali J. Ghandour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23083v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23083v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22392v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22392v6",
                "updated": "2025-05-13T17:49:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    49,
                    51,
                    1,
                    133,
                    0
                ],
                "published": "2024-10-29T17:56:05Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    56,
                    5,
                    1,
                    303,
                    0
                ],
                "title": "Breast Cancer Histopathology Classification using CBAM-EfficientNetV2\n  with Transfer Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breast Cancer Histopathology Classification using CBAM-EfficientNetV2\n  with Transfer Learning"
                },
                "summary": "Breast cancer histopathology image classification is critical for early\ndetection and improved patient outcomes. 1 This study introduces a novel\napproach leveraging EfficientNetV2 models, to improve feature extraction and\nfocus on relevant tissue regions. The proposed models were evaluated on the\nBreakHis dataset across multiple magnification scales (40X, 100X, 200X, and\n400X). 2 Among them, the EfficientNetV2-XL with CBAM achieved outstanding\nperformance, reaching a peak accuracy of 99.01 percent and an F1-score of 98.31\npercent at 400X magnification, outperforming state-of-the-art methods. 3 By\nintegrating Contrast Limited Adaptive Histogram Equalization (CLAHE) for\npreprocessing and optimizing computational efficiency, this method demonstrates\nits suitability for real-time clinical deployment. 3 The results underscore the\npotential of attention-enhanced scalable architectures in advancing diagnostic\nprecision for breast cancer detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breast cancer histopathology image classification is critical for early\ndetection and improved patient outcomes. 1 This study introduces a novel\napproach leveraging EfficientNetV2 models, to improve feature extraction and\nfocus on relevant tissue regions. The proposed models were evaluated on the\nBreakHis dataset across multiple magnification scales (40X, 100X, 200X, and\n400X). 2 Among them, the EfficientNetV2-XL with CBAM achieved outstanding\nperformance, reaching a peak accuracy of 99.01 percent and an F1-score of 98.31\npercent at 400X magnification, outperforming state-of-the-art methods. 3 By\nintegrating Contrast Limited Adaptive Histogram Equalization (CLAHE) for\npreprocessing and optimizing computational efficiency, this method demonstrates\nits suitability for real-time clinical deployment. 3 The results underscore the\npotential of attention-enhanced scalable architectures in advancing diagnostic\nprecision for breast cancer detection."
                },
                "authors": [
                    {
                        "name": "Naren Sengodan"
                    }
                ],
                "author_detail": {
                    "name": "Naren Sengodan"
                },
                "author": "Naren Sengodan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22392v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22392v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v5",
                "updated": "2025-05-13T17:43:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    43,
                    47,
                    1,
                    133,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08768v1",
                "updated": "2025-05-13T17:39:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    39,
                    31,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T17:39:31Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    39,
                    31,
                    1,
                    133,
                    0
                ],
                "title": "SPAT: Sensitivity-based Multihead-attention Pruning on Time Series\n  Forecasting Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPAT: Sensitivity-based Multihead-attention Pruning on Time Series\n  Forecasting Models"
                },
                "summary": "Attention-based architectures have achieved superior performance in\nmultivariate time series forecasting but are computationally expensive.\nTechniques such as patching and adaptive masking have been developed to reduce\ntheir sizes and latencies. In this work, we propose a structured pruning\nmethod, SPAT ($\\textbf{S}$ensitivity $\\textbf{P}$runer for\n$\\textbf{At}$tention), which selectively removes redundant attention mechanisms\nand yields highly effective models. Different from previous approaches, SPAT\naims to remove the entire attention module, which reduces the risk of\noverfitting and enables speed-up without demanding specialized hardware. We\npropose a dynamic sensitivity metric, $\\textbf{S}$ensitivity\n$\\textbf{E}$nhanced $\\textbf{N}$ormalized $\\textbf{D}$ispersion (SEND) that\nmeasures the importance of each attention module during the pre-training phase.\nExperiments on multivariate datasets demonstrate that SPAT-pruned models\nachieve reductions of 2.842% in MSE, 1.996% in MAE, and 35.274% in FLOPs.\nFurthermore, SPAT-pruned models outperform existing lightweight, Mamba-based\nand LLM-based SOTA methods in both standard and zero-shot inference,\nhighlighting the importance of retaining only the most effective attention\nmechanisms. We have made our code publicly available\nhttps://anonymous.4open.science/r/SPAT-6042.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-based architectures have achieved superior performance in\nmultivariate time series forecasting but are computationally expensive.\nTechniques such as patching and adaptive masking have been developed to reduce\ntheir sizes and latencies. In this work, we propose a structured pruning\nmethod, SPAT ($\\textbf{S}$ensitivity $\\textbf{P}$runer for\n$\\textbf{At}$tention), which selectively removes redundant attention mechanisms\nand yields highly effective models. Different from previous approaches, SPAT\naims to remove the entire attention module, which reduces the risk of\noverfitting and enables speed-up without demanding specialized hardware. We\npropose a dynamic sensitivity metric, $\\textbf{S}$ensitivity\n$\\textbf{E}$nhanced $\\textbf{N}$ormalized $\\textbf{D}$ispersion (SEND) that\nmeasures the importance of each attention module during the pre-training phase.\nExperiments on multivariate datasets demonstrate that SPAT-pruned models\nachieve reductions of 2.842% in MSE, 1.996% in MAE, and 35.274% in FLOPs.\nFurthermore, SPAT-pruned models outperform existing lightweight, Mamba-based\nand LLM-based SOTA methods in both standard and zero-shot inference,\nhighlighting the importance of retaining only the most effective attention\nmechanisms. We have made our code publicly available\nhttps://anonymous.4open.science/r/SPAT-6042."
                },
                "authors": [
                    {
                        "name": "Suhan Guo"
                    },
                    {
                        "name": "Jiahong Deng"
                    },
                    {
                        "name": "Mengjun Yi"
                    },
                    {
                        "name": "Furao Shen"
                    },
                    {
                        "name": "Jian Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhao"
                },
                "author": "Jian Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10060v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10060v4",
                "updated": "2025-05-13T17:18:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    18,
                    7,
                    1,
                    133,
                    0
                ],
                "published": "2024-06-14T14:16:39Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    14,
                    16,
                    39,
                    4,
                    166,
                    0
                ],
                "title": "PRIMER: Perception-Aware Robust Learning-based Multiagent Trajectory\n  Planner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRIMER: Perception-Aware Robust Learning-based Multiagent Trajectory\n  Planner"
                },
                "summary": "In decentralized multiagent trajectory planners, agents need to communicate\nand exchange their positions to generate collision-free trajectories. However,\ndue to localization errors/uncertainties, trajectory deconfliction can fail\neven if trajectories are perfectly shared between agents. To address this\nissue, we first present PARM and PARM*, perception-aware, decentralized,\nasynchronous multiagent trajectory planners that enable a team of agents to\nnavigate uncertain environments while deconflicting trajectories and avoiding\nobstacles using perception information. PARM* differs from PARM as it is less\nconservative, using more computation to find closer-to-optimal solutions. While\nthese methods achieve state-of-the-art performance, they suffer from high\ncomputational costs as they need to solve large optimization problems onboard,\nmaking it difficult for agents to replan at high rates. To overcome this\nchallenge, we present our second key contribution, PRIMER, a learning-based\nplanner trained with imitation learning (IL) using PARM* as the expert\ndemonstrator. PRIMER leverages the low computational requirements at deployment\nof neural networks and achieves a computation speed up to 5500 times faster\nthan optimization-based approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In decentralized multiagent trajectory planners, agents need to communicate\nand exchange their positions to generate collision-free trajectories. However,\ndue to localization errors/uncertainties, trajectory deconfliction can fail\neven if trajectories are perfectly shared between agents. To address this\nissue, we first present PARM and PARM*, perception-aware, decentralized,\nasynchronous multiagent trajectory planners that enable a team of agents to\nnavigate uncertain environments while deconflicting trajectories and avoiding\nobstacles using perception information. PARM* differs from PARM as it is less\nconservative, using more computation to find closer-to-optimal solutions. While\nthese methods achieve state-of-the-art performance, they suffer from high\ncomputational costs as they need to solve large optimization problems onboard,\nmaking it difficult for agents to replan at high rates. To overcome this\nchallenge, we present our second key contribution, PRIMER, a learning-based\nplanner trained with imitation learning (IL) using PARM* as the expert\ndemonstrator. PRIMER leverages the low computational requirements at deployment\nof neural networks and achieves a computation speed up to 5500 times faster\nthan optimization-based approaches."
                },
                "authors": [
                    {
                        "name": "Kota Kondo"
                    },
                    {
                        "name": "Claudius T. Tewari"
                    },
                    {
                        "name": "Andrea Tagliabue"
                    },
                    {
                        "name": "Jesus Tordesillas"
                    },
                    {
                        "name": "Parker C. Lusk"
                    },
                    {
                        "name": "Mason B. Peterson"
                    },
                    {
                        "name": "Jonathan P. How"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan P. How"
                },
                "author": "Jonathan P. How",
                "arxiv_doi": "10.13140/RG.2.2.14435.57124",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.13140/RG.2.2.14435.57124",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.10060v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10060v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "7 pages, 3 figures",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14917v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14917v5",
                "updated": "2025-05-13T17:06:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    6,
                    22,
                    1,
                    133,
                    0
                ],
                "published": "2025-01-24T20:54:29Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    20,
                    54,
                    29,
                    4,
                    24,
                    0
                ],
                "title": "Self-reflecting Large Language Models: A Hegelian Dialectical Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-reflecting Large Language Models: A Hegelian Dialectical Approach"
                },
                "summary": "Investigating NLP through a philosophical lens has recently caught\nresearcher's eyes as it connects computational methods with classical schools\nof philosophy. This paper introduces a philosophical approach inspired by the\n\\textit{Hegelian Dialectic} for LLMs' \\textit{self-reflection}, utilizing a\nself-dialectical approach to emulate internal critiques and then synthesize new\nideas by resolving the opposing points of view. Moreover, this paper\ninvestigates the effect of LLMs' temperature for generation by establishing a\ndynamic annealing approach, which promotes the creativity in the early stages\nand gradually refines it by focusing on the nuances, as well as a\nfixed-temperature strategy for generation. We assess the effectiveness of our\nproposed method in generating novel ideas and in improving the reasoning\nabilities of LLMs during problem-solving. Moreover, we implement a Multi-Agent\nMajority Voting (MAMV) strategy to assess the validity and novelty of the\ngenerated ideas, which proves useful in the absence of domain experts. Our\nexperiments demonstrate promising results in generating ideas and enhancing\nproblem-solving performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating NLP through a philosophical lens has recently caught\nresearcher's eyes as it connects computational methods with classical schools\nof philosophy. This paper introduces a philosophical approach inspired by the\n\\textit{Hegelian Dialectic} for LLMs' \\textit{self-reflection}, utilizing a\nself-dialectical approach to emulate internal critiques and then synthesize new\nideas by resolving the opposing points of view. Moreover, this paper\ninvestigates the effect of LLMs' temperature for generation by establishing a\ndynamic annealing approach, which promotes the creativity in the early stages\nand gradually refines it by focusing on the nuances, as well as a\nfixed-temperature strategy for generation. We assess the effectiveness of our\nproposed method in generating novel ideas and in improving the reasoning\nabilities of LLMs during problem-solving. Moreover, we implement a Multi-Agent\nMajority Voting (MAMV) strategy to assess the validity and novelty of the\ngenerated ideas, which proves useful in the absence of domain experts. Our\nexperiments demonstrate promising results in generating ideas and enhancing\nproblem-solving performance."
                },
                "authors": [
                    {
                        "name": "Sara Abdali"
                    },
                    {
                        "name": "Can Goksen"
                    },
                    {
                        "name": "Saeed Amizadeh"
                    },
                    {
                        "name": "Julie E. Maybee"
                    },
                    {
                        "name": "Kazuhito Koishida"
                    }
                ],
                "author_detail": {
                    "name": "Kazuhito Koishida"
                },
                "author": "Kazuhito Koishida",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14917v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14917v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08750v1",
                "updated": "2025-05-13T17:02:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    2,
                    33,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T17:02:33Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    2,
                    33,
                    1,
                    133,
                    0
                ],
                "title": "AC-Reason: Towards Theory-Guided Actual Causality Reasoning with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AC-Reason: Towards Theory-Guided Actual Causality Reasoning with Large\n  Language Models"
                },
                "summary": "Actual causality (AC), a fundamental aspect of causal reasoning (CR), is\nresponsible for attribution and responsibility assignment in real-world\nscenarios. However, existing LLM-based methods lack grounding in formal AC\ntheory, resulting in limited interpretability. Therefore, we propose AC-Reason,\na semi-formal reasoning framework that identifies causally relevant events\nwithin an AC scenario, infers the values of their formal causal factors (e.g.,\nsufficiency, necessity, and normality), and answers AC queries via a\ntheory-guided algorithm with explanations. While AC-Reason does not explicitly\nconstruct a causal graph, it operates over variables in the underlying causal\nstructure to support principled reasoning. To enable comprehensive evaluation,\nwe introduce AC-Bench, a new benchmark built upon and substantially extending\nBig-Bench Hard Causal Judgment (BBH-CJ). AC-Bench comprises ~1K carefully\nannotated samples, each with detailed reasoning steps and focuses solely on\nactual causation. The case study shows that synthesized samples in AC-Bench\npresent greater challenges for LLMs. Extensive experiments on BBH-CJ and\nAC-Bench show that AC-Reason consistently improves LLM performance over\nbaselines. On BBH-CJ, all tested LLMs surpass the average human rater accuracy\nof 69.60%, with GPT-4 + AC-Reason achieving 75.04%. On AC-Bench, GPT-4 +\nAC-Reason again achieves the highest accuracy of 71.82%. AC-Bench further\nenables fine-grained analysis of reasoning faithfulness, revealing that only\nQwen-2.5-72B-Instruct, Claude-3.5-Sonnet, and GPT-4o exhibit faithful\nreasoning, whereas GPT-4 tends to exploit shortcuts. Finally, our ablation\nstudy proves that integrating AC theory into LLMs is highly effective, with the\nproposed algorithm contributing the most significant performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Actual causality (AC), a fundamental aspect of causal reasoning (CR), is\nresponsible for attribution and responsibility assignment in real-world\nscenarios. However, existing LLM-based methods lack grounding in formal AC\ntheory, resulting in limited interpretability. Therefore, we propose AC-Reason,\na semi-formal reasoning framework that identifies causally relevant events\nwithin an AC scenario, infers the values of their formal causal factors (e.g.,\nsufficiency, necessity, and normality), and answers AC queries via a\ntheory-guided algorithm with explanations. While AC-Reason does not explicitly\nconstruct a causal graph, it operates over variables in the underlying causal\nstructure to support principled reasoning. To enable comprehensive evaluation,\nwe introduce AC-Bench, a new benchmark built upon and substantially extending\nBig-Bench Hard Causal Judgment (BBH-CJ). AC-Bench comprises ~1K carefully\nannotated samples, each with detailed reasoning steps and focuses solely on\nactual causation. The case study shows that synthesized samples in AC-Bench\npresent greater challenges for LLMs. Extensive experiments on BBH-CJ and\nAC-Bench show that AC-Reason consistently improves LLM performance over\nbaselines. On BBH-CJ, all tested LLMs surpass the average human rater accuracy\nof 69.60%, with GPT-4 + AC-Reason achieving 75.04%. On AC-Bench, GPT-4 +\nAC-Reason again achieves the highest accuracy of 71.82%. AC-Bench further\nenables fine-grained analysis of reasoning faithfulness, revealing that only\nQwen-2.5-72B-Instruct, Claude-3.5-Sonnet, and GPT-4o exhibit faithful\nreasoning, whereas GPT-4 tends to exploit shortcuts. Finally, our ablation\nstudy proves that integrating AC theory into LLMs is highly effective, with the\nproposed algorithm contributing the most significant performance gains."
                },
                "authors": [
                    {
                        "name": "Yanxi Zhang"
                    },
                    {
                        "name": "Xin Cong"
                    },
                    {
                        "name": "Zhong Zhang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Dongyan Zhao"
                    },
                    {
                        "name": "Yesai Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yesai Wu"
                },
                "author": "Yesai Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08744v1",
                "updated": "2025-05-13T16:58:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    58,
                    5,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T16:58:05Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    58,
                    5,
                    1,
                    133,
                    0
                ],
                "title": "DeepMath-Creative: A Benchmark for Evaluating Mathematical Creativity of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepMath-Creative: A Benchmark for Evaluating Mathematical Creativity of\n  Large Language Models"
                },
                "summary": "To advance the mathematical proficiency of large language models (LLMs), the\nDeepMath team has launched an open-source initiative aimed at developing an\nopen mathematical LLM and systematically evaluating its mathematical\ncreativity. This paper represents the initial contribution of this initiative.\nWhile recent developments in mathematical LLMs have predominantly emphasized\nreasoning skills, as evidenced by benchmarks on elementary to\nundergraduate-level mathematical tasks, the creative capabilities of these\nmodels have received comparatively little attention, and evaluation datasets\nremain scarce. To address this gap, we propose an evaluation criteria for\nmathematical creativity and introduce DeepMath-Creative, a novel, high-quality\nbenchmark comprising constructive problems across algebra, geometry, analysis,\nand other domains. We conduct a systematic evaluation of mainstream LLMs'\ncreative problem-solving abilities using this dataset. Experimental results\nshow that even under lenient scoring criteria -- emphasizing core solution\ncomponents and disregarding minor inaccuracies, such as small logical gaps,\nincomplete justifications, or redundant explanations -- the best-performing\nmodel, O3 Mini, achieves merely 70% accuracy, primarily on basic\nundergraduate-level constructive tasks. Performance declines sharply on more\ncomplex problems, with models failing to provide substantive strategies for\nopen problems. These findings suggest that, although current LLMs display a\ndegree of constructive proficiency on familiar and lower-difficulty problems,\nsuch performance is likely attributable to the recombination of memorized\npatterns rather than authentic creative insight or novel synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To advance the mathematical proficiency of large language models (LLMs), the\nDeepMath team has launched an open-source initiative aimed at developing an\nopen mathematical LLM and systematically evaluating its mathematical\ncreativity. This paper represents the initial contribution of this initiative.\nWhile recent developments in mathematical LLMs have predominantly emphasized\nreasoning skills, as evidenced by benchmarks on elementary to\nundergraduate-level mathematical tasks, the creative capabilities of these\nmodels have received comparatively little attention, and evaluation datasets\nremain scarce. To address this gap, we propose an evaluation criteria for\nmathematical creativity and introduce DeepMath-Creative, a novel, high-quality\nbenchmark comprising constructive problems across algebra, geometry, analysis,\nand other domains. We conduct a systematic evaluation of mainstream LLMs'\ncreative problem-solving abilities using this dataset. Experimental results\nshow that even under lenient scoring criteria -- emphasizing core solution\ncomponents and disregarding minor inaccuracies, such as small logical gaps,\nincomplete justifications, or redundant explanations -- the best-performing\nmodel, O3 Mini, achieves merely 70% accuracy, primarily on basic\nundergraduate-level constructive tasks. Performance declines sharply on more\ncomplex problems, with models failing to provide substantive strategies for\nopen problems. These findings suggest that, although current LLMs display a\ndegree of constructive proficiency on familiar and lower-difficulty problems,\nsuch performance is likely attributable to the recombination of memorized\npatterns rather than authentic creative insight or novel synthesis."
                },
                "authors": [
                    {
                        "name": "Xiaoyang Chen"
                    },
                    {
                        "name": "Xinan Dai"
                    },
                    {
                        "name": "Yu Du"
                    },
                    {
                        "name": "Qian Feng"
                    },
                    {
                        "name": "Naixu Guo"
                    },
                    {
                        "name": "Tingshuo Gu"
                    },
                    {
                        "name": "Yuting Gao"
                    },
                    {
                        "name": "Yingyi Gao"
                    },
                    {
                        "name": "Xudong Han"
                    },
                    {
                        "name": "Xiang Jiang"
                    },
                    {
                        "name": "Yilin Jin"
                    },
                    {
                        "name": "Hongyi Lin"
                    },
                    {
                        "name": "Shisheng Lin"
                    },
                    {
                        "name": "Xiangnan Li"
                    },
                    {
                        "name": "Yuante Li"
                    },
                    {
                        "name": "Yixing Li"
                    },
                    {
                        "name": "Zhentao Lai"
                    },
                    {
                        "name": "Zilu Ma"
                    },
                    {
                        "name": "Yingrong Peng"
                    },
                    {
                        "name": "Jiacheng Qian"
                    },
                    {
                        "name": "Hao-Yu Sun"
                    },
                    {
                        "name": "Jianbo Sun"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Siwei Wu"
                    },
                    {
                        "name": "Zian Wang"
                    },
                    {
                        "name": "Bin Xu"
                    },
                    {
                        "name": "Jianghao Xu"
                    },
                    {
                        "name": "Yiyang Yu"
                    },
                    {
                        "name": "Zichuan Yang"
                    },
                    {
                        "name": "Hongji Zha"
                    },
                    {
                        "name": "Ruichong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruichong Zhang"
                },
                "author": "Ruichong Zhang",
                "arxiv_comment": "14 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08739v1",
                "updated": "2025-05-13T16:52:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    52,
                    19,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T16:52:19Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    52,
                    19,
                    1,
                    133,
                    0
                ],
                "title": "Probability Consistency in Large Language Models: Theoretical\n  Foundations Meet Empirical Discrepancies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probability Consistency in Large Language Models: Theoretical\n  Foundations Meet Empirical Discrepancies"
                },
                "summary": "Can autoregressive large language models (LLMs) learn consistent probability\ndistributions when trained on sequences in different token orders? We prove\nformally that for any well-defined probability distribution, sequence\nperplexity is invariant under any factorization, including forward, backward,\nor arbitrary permutations. This result establishes a rigorous theoretical\nfoundation for studying how LLMs learn from data and defines principled\nprotocols for empirical evaluation. Applying these protocols, we show that\nprior studies examining ordering effects suffer from critical methodological\nflaws. We retrain GPT-2 models across forward, backward, and arbitrary permuted\norders on scientific text. We find systematic deviations from theoretical\ninvariance across all orderings with arbitrary permutations strongly deviating\nfrom both forward and backward models, which largely (but not completely)\nagreed with one another. Deviations were traceable to differences in\nself-attention, reflecting positional and locality biases in processing. Our\ntheoretical and empirical results provide novel avenues for understanding\npositional biases in LLMs and suggest methods for detecting when LLMs'\nprobability distributions are inconsistent and therefore untrustworthy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can autoregressive large language models (LLMs) learn consistent probability\ndistributions when trained on sequences in different token orders? We prove\nformally that for any well-defined probability distribution, sequence\nperplexity is invariant under any factorization, including forward, backward,\nor arbitrary permutations. This result establishes a rigorous theoretical\nfoundation for studying how LLMs learn from data and defines principled\nprotocols for empirical evaluation. Applying these protocols, we show that\nprior studies examining ordering effects suffer from critical methodological\nflaws. We retrain GPT-2 models across forward, backward, and arbitrary permuted\norders on scientific text. We find systematic deviations from theoretical\ninvariance across all orderings with arbitrary permutations strongly deviating\nfrom both forward and backward models, which largely (but not completely)\nagreed with one another. Deviations were traceable to differences in\nself-attention, reflecting positional and locality biases in processing. Our\ntheoretical and empirical results provide novel avenues for understanding\npositional biases in LLMs and suggest methods for detecting when LLMs'\nprobability distributions are inconsistent and therefore untrustworthy."
                },
                "authors": [
                    {
                        "name": "Xiaoliang Luo"
                    },
                    {
                        "name": "Xinyi Xu"
                    },
                    {
                        "name": "Michael Ramscar"
                    },
                    {
                        "name": "Bradley C. Love"
                    }
                ],
                "author_detail": {
                    "name": "Bradley C. Love"
                },
                "author": "Bradley C. Love",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08734v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08734v1",
                "updated": "2025-05-13T16:46:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    46,
                    25,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T16:46:25Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    46,
                    25,
                    1,
                    133,
                    0
                ],
                "title": "NurValues: Real-World Nursing Values Evaluation for Large Language\n  Models in Clinical Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NurValues: Real-World Nursing Values Evaluation for Large Language\n  Models in Clinical Context"
                },
                "summary": "This work introduces the first benchmark for nursing value alignment,\nconsisting of five core value dimensions distilled from international nursing\ncodes: Altruism, Human Dignity, Integrity, Justice, and Professionalism. The\nbenchmark comprises 1,100 real-world nursing behavior instances collected\nthrough a five-month longitudinal field study across three hospitals of varying\ntiers. These instances are annotated by five clinical nurses and then augmented\nwith LLM-generated counterfactuals with reversed ethic polarity. Each original\ncase is paired with a value-aligned and a value-violating version, resulting in\n2,200 labeled instances that constitute the Easy-Level dataset. To increase\nadversarial complexity, each instance is further transformed into a\ndialogue-based format that embeds contextual cues and subtle misleading\nsignals, yielding a Hard-Level dataset. We evaluate 23 state-of-the-art (SoTA)\nLLMs on their alignment with nursing values. Our findings reveal three key\ninsights: (1) DeepSeek-V3 achieves the highest performance on the Easy-Level\ndataset (94.55), where Claude 3.5 Sonnet outperforms other models on the\nHard-Level dataset (89.43), significantly surpassing the medical LLMs; (2)\nJustice is consistently the most difficult nursing value dimension to evaluate;\nand (3) in-context learning significantly improves alignment. This work aims to\nprovide a foundation for value-sensitive LLMs development in clinical settings.\nThe dataset and the code are available at\nhttps://huggingface.co/datasets/Ben012345/NurValues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces the first benchmark for nursing value alignment,\nconsisting of five core value dimensions distilled from international nursing\ncodes: Altruism, Human Dignity, Integrity, Justice, and Professionalism. The\nbenchmark comprises 1,100 real-world nursing behavior instances collected\nthrough a five-month longitudinal field study across three hospitals of varying\ntiers. These instances are annotated by five clinical nurses and then augmented\nwith LLM-generated counterfactuals with reversed ethic polarity. Each original\ncase is paired with a value-aligned and a value-violating version, resulting in\n2,200 labeled instances that constitute the Easy-Level dataset. To increase\nadversarial complexity, each instance is further transformed into a\ndialogue-based format that embeds contextual cues and subtle misleading\nsignals, yielding a Hard-Level dataset. We evaluate 23 state-of-the-art (SoTA)\nLLMs on their alignment with nursing values. Our findings reveal three key\ninsights: (1) DeepSeek-V3 achieves the highest performance on the Easy-Level\ndataset (94.55), where Claude 3.5 Sonnet outperforms other models on the\nHard-Level dataset (89.43), significantly surpassing the medical LLMs; (2)\nJustice is consistently the most difficult nursing value dimension to evaluate;\nand (3) in-context learning significantly improves alignment. This work aims to\nprovide a foundation for value-sensitive LLMs development in clinical settings.\nThe dataset and the code are available at\nhttps://huggingface.co/datasets/Ben012345/NurValues."
                },
                "authors": [
                    {
                        "name": "Ben Yao"
                    },
                    {
                        "name": "Qiuchi Li"
                    },
                    {
                        "name": "Yazhou Zhang"
                    },
                    {
                        "name": "Siyu Yang"
                    },
                    {
                        "name": "Bohan Zhang"
                    },
                    {
                        "name": "Prayag Tiwari"
                    },
                    {
                        "name": "Jing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Jing Qin"
                },
                "author": "Jing Qin",
                "arxiv_comment": "25 pages, 10 figures, 16 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08734v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02870v2",
                "updated": "2025-05-13T16:41:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    41,
                    54,
                    1,
                    133,
                    0
                ],
                "published": "2025-04-01T12:56:39Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    56,
                    39,
                    1,
                    91,
                    0
                ],
                "title": "AI Hiring with LLMs: A Context-Aware and Explainable Multi-Agent\n  Framework for Resume Screening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Hiring with LLMs: A Context-Aware and Explainable Multi-Agent\n  Framework for Resume Screening"
                },
                "summary": "Resume screening is a critical yet time-intensive process in talent\nacquisition, requiring recruiters to analyze vast volume of job applications\nwhile remaining objective, accurate, and fair. With the advancements in Large\nLanguage Models (LLMs), their reasoning capabilities and extensive knowledge\nbases demonstrate new opportunities to streamline and automate recruitment\nworkflows. In this work, we propose a multi-agent framework for resume\nscreening using LLMs to systematically process and evaluate resumes. The\nframework consists of four core agents, including a resume extractor, an\nevaluator, a summarizer, and a score formatter. To enhance the contextual\nrelevance of candidate assessments, we integrate Retrieval-Augmented Generation\n(RAG) within the resume evaluator, allowing incorporation of external knowledge\nsources, such as industry-specific expertise, professional certifications,\nuniversity rankings, and company-specific hiring criteria. This dynamic\nadaptation enables personalized recruitment, bridging the gap between AI\nautomation and talent acquisition. We assess the effectiveness of our approach\nby comparing AI-generated scores with ratings provided by HR professionals on a\ndataset of anonymized online resumes. The findings highlight the potential of\nmulti-agent RAG-LLM systems in automating resume screening, enabling more\nefficient and scalable hiring workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resume screening is a critical yet time-intensive process in talent\nacquisition, requiring recruiters to analyze vast volume of job applications\nwhile remaining objective, accurate, and fair. With the advancements in Large\nLanguage Models (LLMs), their reasoning capabilities and extensive knowledge\nbases demonstrate new opportunities to streamline and automate recruitment\nworkflows. In this work, we propose a multi-agent framework for resume\nscreening using LLMs to systematically process and evaluate resumes. The\nframework consists of four core agents, including a resume extractor, an\nevaluator, a summarizer, and a score formatter. To enhance the contextual\nrelevance of candidate assessments, we integrate Retrieval-Augmented Generation\n(RAG) within the resume evaluator, allowing incorporation of external knowledge\nsources, such as industry-specific expertise, professional certifications,\nuniversity rankings, and company-specific hiring criteria. This dynamic\nadaptation enables personalized recruitment, bridging the gap between AI\nautomation and talent acquisition. We assess the effectiveness of our approach\nby comparing AI-generated scores with ratings provided by HR professionals on a\ndataset of anonymized online resumes. The findings highlight the potential of\nmulti-agent RAG-LLM systems in automating resume screening, enabling more\nefficient and scalable hiring workflows."
                },
                "authors": [
                    {
                        "name": "Frank P. -W. Lo"
                    },
                    {
                        "name": "Jianing Qiu"
                    },
                    {
                        "name": "Zeyu Wang"
                    },
                    {
                        "name": "Haibao Yu"
                    },
                    {
                        "name": "Yeming Chen"
                    },
                    {
                        "name": "Gao Zhang"
                    },
                    {
                        "name": "Benny Lo"
                    }
                ],
                "author_detail": {
                    "name": "Benny Lo"
                },
                "author": "Benny Lo",
                "arxiv_comment": "Accepted by CVPR 2025 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08728v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08728v1",
                "updated": "2025-05-13T16:39:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    39,
                    0,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T16:39:00Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    39,
                    0,
                    1,
                    133,
                    0
                ],
                "title": "Securing RAG: A Risk Assessment and Mitigation Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Securing RAG: A Risk Assessment and Mitigation Framework"
                },
                "summary": "Retrieval Augmented Generation (RAG) has emerged as the de facto industry\nstandard for user-facing NLP applications, offering the ability to integrate\ndata without re-training or fine-tuning Large Language Models (LLMs). This\ncapability enhances the quality and accuracy of responses but also introduces\nnovel security and privacy challenges, particularly when sensitive data is\nintegrated. With the rapid adoption of RAG, securing data and services has\nbecome a critical priority. This paper first reviews the vulnerabilities of RAG\npipelines, and outlines the attack surface from data pre-processing and data\nstorage management to integration with LLMs. The identified risks are then\npaired with corresponding mitigations in a structured overview. In a second\nstep, the paper develops a framework that combines RAG-specific security\nconsiderations, with existing general security guidelines, industry standards,\nand best practices. The proposed framework aims to guide the implementation of\nrobust, compliant, secure, and trustworthy RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) has emerged as the de facto industry\nstandard for user-facing NLP applications, offering the ability to integrate\ndata without re-training or fine-tuning Large Language Models (LLMs). This\ncapability enhances the quality and accuracy of responses but also introduces\nnovel security and privacy challenges, particularly when sensitive data is\nintegrated. With the rapid adoption of RAG, securing data and services has\nbecome a critical priority. This paper first reviews the vulnerabilities of RAG\npipelines, and outlines the attack surface from data pre-processing and data\nstorage management to integration with LLMs. The identified risks are then\npaired with corresponding mitigations in a structured overview. In a second\nstep, the paper develops a framework that combines RAG-specific security\nconsiderations, with existing general security guidelines, industry standards,\nand best practices. The proposed framework aims to guide the implementation of\nrobust, compliant, secure, and trustworthy RAG systems."
                },
                "authors": [
                    {
                        "name": "Lukas Ammann"
                    },
                    {
                        "name": "Sara Ott"
                    },
                    {
                        "name": "Christoph R. Landolt"
                    },
                    {
                        "name": "Marco P. Lehmann"
                    }
                ],
                "author_detail": {
                    "name": "Marco P. Lehmann"
                },
                "author": "Marco P. Lehmann",
                "arxiv_comment": "8 pages, 3 figures, Sara Ott and Lukas Ammann contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08728v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08728v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02732v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02732v3",
                "updated": "2025-05-13T16:38:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    38,
                    34,
                    1,
                    133,
                    0
                ],
                "published": "2025-04-03T16:17:55Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    17,
                    55,
                    3,
                    93,
                    0
                ],
                "title": "Why do LLMs attend to the first token?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why do LLMs attend to the first token?"
                },
                "summary": "Large Language Models (LLMs) tend to attend heavily to the first token in the\nsequence -- creating a so-called attention sink. Many works have studied this\nphenomenon in detail, proposing various ways to either leverage or alleviate\nit. Attention sinks have been connected to quantisation difficulties, security\nissues, and streaming attention. Yet, while many works have provided conditions\nin which they occur or not, a critical question remains shallowly answered: Why\ndo LLMs learn such patterns and how are they being used? In this work, we argue\ntheoretically and empirically that this mechanism provides a method for LLMs to\navoid over-mixing, connecting this to existing lines of work that study\nmathematically how information propagates in Transformers. We conduct\nexperiments to validate our theoretical intuitions and show how choices such as\ncontext length, depth, and data packing influence the sink behaviour. We hope\nthat this study provides a new practical perspective on why attention sinks are\nuseful in LLMs, leading to a better understanding of the attention patterns\nthat form during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) tend to attend heavily to the first token in the\nsequence -- creating a so-called attention sink. Many works have studied this\nphenomenon in detail, proposing various ways to either leverage or alleviate\nit. Attention sinks have been connected to quantisation difficulties, security\nissues, and streaming attention. Yet, while many works have provided conditions\nin which they occur or not, a critical question remains shallowly answered: Why\ndo LLMs learn such patterns and how are they being used? In this work, we argue\ntheoretically and empirically that this mechanism provides a method for LLMs to\navoid over-mixing, connecting this to existing lines of work that study\nmathematically how information propagates in Transformers. We conduct\nexperiments to validate our theoretical intuitions and show how choices such as\ncontext length, depth, and data packing influence the sink behaviour. We hope\nthat this study provides a new practical perspective on why attention sinks are\nuseful in LLMs, leading to a better understanding of the attention patterns\nthat form during training."
                },
                "authors": [
                    {
                        "name": "Federico Barbero"
                    },
                    {
                        "name": "Álvaro Arroyo"
                    },
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Christos Perivolaropoulos"
                    },
                    {
                        "name": "Michael Bronstein"
                    },
                    {
                        "name": "Petar Veličković"
                    },
                    {
                        "name": "Razvan Pascanu"
                    }
                ],
                "author_detail": {
                    "name": "Razvan Pascanu"
                },
                "author": "Razvan Pascanu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02732v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02732v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08727v1",
                "updated": "2025-05-13T16:37:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    37,
                    54,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T16:37:54Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    37,
                    54,
                    1,
                    133,
                    0
                ],
                "title": "Memorization-Compression Cycles Improve Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memorization-Compression Cycles Improve Generalization"
                },
                "summary": "We prove theoretically that generalization improves not only through data\nscaling but also by compressing internal representations. To operationalize\nthis insight, we introduce the Information Bottleneck Language Modeling (IBLM)\nobjective, which reframes language modeling as a constrained optimization\nproblem: minimizing representation entropy subject to optimal prediction\nperformance. Empirically, we observe an emergent memorization-compression cycle\nduring LLM pretraining, evidenced by oscillation positive/negative gradient\nalignment between cross-entropy and Matrix-Based Entropy (MBE), a measure of\nrepresentation entropy. This pattern closely mirrors the predictive-compressive\ntrade-off prescribed by IBLM and also parallels the biological alternation\nbetween awake learning and sleep consolidation. Motivated by this observation,\nwe propose Gated Phase Transition (GAPT), a training algorithm that adaptively\nswitches between memorization and compression phases. When applied to GPT-2\npretraining on FineWeb dataset, GAPT reduces MBE by 50% and improves\ncross-entropy by 4.8%. GAPT improves OOD generalizatino by 35% in a pretraining\ntask on arithmetic multiplication. In a setting designed to simulate\ncatastrophic forgetting, GAPT reduces interference by compressing and\nseparating representations, achieving a 97% improvement in separation -\nparalleling the functional role of sleep consolidation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We prove theoretically that generalization improves not only through data\nscaling but also by compressing internal representations. To operationalize\nthis insight, we introduce the Information Bottleneck Language Modeling (IBLM)\nobjective, which reframes language modeling as a constrained optimization\nproblem: minimizing representation entropy subject to optimal prediction\nperformance. Empirically, we observe an emergent memorization-compression cycle\nduring LLM pretraining, evidenced by oscillation positive/negative gradient\nalignment between cross-entropy and Matrix-Based Entropy (MBE), a measure of\nrepresentation entropy. This pattern closely mirrors the predictive-compressive\ntrade-off prescribed by IBLM and also parallels the biological alternation\nbetween awake learning and sleep consolidation. Motivated by this observation,\nwe propose Gated Phase Transition (GAPT), a training algorithm that adaptively\nswitches between memorization and compression phases. When applied to GPT-2\npretraining on FineWeb dataset, GAPT reduces MBE by 50% and improves\ncross-entropy by 4.8%. GAPT improves OOD generalizatino by 35% in a pretraining\ntask on arithmetic multiplication. In a setting designed to simulate\ncatastrophic forgetting, GAPT reduces interference by compressing and\nseparating representations, achieving a 97% improvement in separation -\nparalleling the functional role of sleep consolidation."
                },
                "authors": [
                    {
                        "name": "Fangyuan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fangyuan Yu"
                },
                "author": "Fangyuan Yu",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00958v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00958v4",
                "updated": "2025-05-13T16:29:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    29,
                    8,
                    1,
                    133,
                    0
                ],
                "published": "2025-01-01T21:29:37Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    21,
                    29,
                    37,
                    2,
                    1,
                    0
                ],
                "title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2.5 Years in Class: A Multimodal Textbook for Vision-Language\n  Pretraining"
                },
                "summary": "Compared to image-text pair data, interleaved corpora enable Vision-Language\nModels (VLMs) to understand the world more naturally like humans. However, such\nexisting datasets are crawled from webpage, facing challenges like low\nknowledge density, loose image-text relations, and poor logical coherence\nbetween images. On the other hand, the internet hosts vast instructional videos\n(e.g., online geometry courses) that are widely used by humans to learn\nfoundational subjects, yet these valuable resources remain underexplored in VLM\ntraining. In this paper, we introduce a high-quality \\textbf{multimodal\ntextbook} corpus with richer foundational knowledge for VLM pretraining. It\ncollects over 2.5 years of instructional videos, totaling 22,000 class hours.\nWe first use an LLM-proposed taxonomy to systematically gather instructional\nvideos. Then we progressively extract and refine visual (keyframes), audio\n(ASR), and textual knowledge (OCR) from the videos, and organize as an\nimage-text interleaved corpus based on temporal order. Compared to its\ncounterparts, our video-centric textbook offers more coherent context, richer\nknowledge, and better image-text alignment. Experiments demonstrate its superb\npretraining performance, particularly in knowledge- and reasoning-intensive\ntasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook\nexhibit outstanding interleaved context awareness, leveraging visual and\ntextual cues in their few-shot context for task solving. Our code are available\nat https://github.com/DAMO-NLP-SG/multimodal_textbook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compared to image-text pair data, interleaved corpora enable Vision-Language\nModels (VLMs) to understand the world more naturally like humans. However, such\nexisting datasets are crawled from webpage, facing challenges like low\nknowledge density, loose image-text relations, and poor logical coherence\nbetween images. On the other hand, the internet hosts vast instructional videos\n(e.g., online geometry courses) that are widely used by humans to learn\nfoundational subjects, yet these valuable resources remain underexplored in VLM\ntraining. In this paper, we introduce a high-quality \\textbf{multimodal\ntextbook} corpus with richer foundational knowledge for VLM pretraining. It\ncollects over 2.5 years of instructional videos, totaling 22,000 class hours.\nWe first use an LLM-proposed taxonomy to systematically gather instructional\nvideos. Then we progressively extract and refine visual (keyframes), audio\n(ASR), and textual knowledge (OCR) from the videos, and organize as an\nimage-text interleaved corpus based on temporal order. Compared to its\ncounterparts, our video-centric textbook offers more coherent context, richer\nknowledge, and better image-text alignment. Experiments demonstrate its superb\npretraining performance, particularly in knowledge- and reasoning-intensive\ntasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook\nexhibit outstanding interleaved context awareness, leveraging visual and\ntextual cues in their few-shot context for task solving. Our code are available\nat https://github.com/DAMO-NLP-SG/multimodal_textbook."
                },
                "authors": [
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Jiashuo Sun"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Deli Zhao"
                    },
                    {
                        "name": "Yueting Zhuang"
                    },
                    {
                        "name": "Lidong Bing"
                    }
                ],
                "author_detail": {
                    "name": "Lidong Bing"
                },
                "author": "Lidong Bing",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00958v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00958v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08719v1",
                "updated": "2025-05-13T16:27:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    27,
                    7,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T16:27:07Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    27,
                    7,
                    1,
                    133,
                    0
                ],
                "title": "PWC-MoE: Privacy-Aware Wireless Collaborative Mixture of Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PWC-MoE: Privacy-Aware Wireless Collaborative Mixture of Experts"
                },
                "summary": "Large language models (LLMs) hosted on cloud servers alleviate the\ncomputational and storage burdens on local devices but raise privacy concerns\ndue to sensitive data transmission and require substantial communication\nbandwidth, which is challenging in constrained environments. In contrast, small\nlanguage models (SLMs) running locally enhance privacy but suffer from limited\nperformance on complex tasks. To balance computational cost, performance, and\nprivacy protection under bandwidth constraints, we propose a privacy-aware\nwireless collaborative mixture of experts (PWC-MoE) framework. Specifically,\nPWC-MoE employs a sparse privacy-aware gating network to dynamically route\nsensitive tokens to privacy experts located on local clients, while\nnon-sensitive tokens are routed to non-privacy experts located at the remote\nbase station. To achieve computational efficiency, the gating network ensures\nthat each token is dynamically routed to and processed by only one expert. To\nenhance scalability and prevent overloading of specific experts, we introduce a\ngroup-wise load-balancing mechanism for the gating network that evenly\ndistributes sensitive tokens among privacy experts and non-sensitive tokens\namong non-privacy experts. To adapt to bandwidth constraints while preserving\nmodel performance, we propose a bandwidth-adaptive and importance-aware token\noffloading scheme. This scheme incorporates an importance predictor to evaluate\nthe importance scores of non-sensitive tokens, prioritizing the most important\ntokens for transmission to the base station based on their predicted importance\nand the available bandwidth. Experiments demonstrate that the PWC-MoE framework\neffectively preserves privacy and maintains high performance even in\nbandwidth-constrained environments, offering a practical solution for deploying\nLLMs in privacy-sensitive and bandwidth-limited scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) hosted on cloud servers alleviate the\ncomputational and storage burdens on local devices but raise privacy concerns\ndue to sensitive data transmission and require substantial communication\nbandwidth, which is challenging in constrained environments. In contrast, small\nlanguage models (SLMs) running locally enhance privacy but suffer from limited\nperformance on complex tasks. To balance computational cost, performance, and\nprivacy protection under bandwidth constraints, we propose a privacy-aware\nwireless collaborative mixture of experts (PWC-MoE) framework. Specifically,\nPWC-MoE employs a sparse privacy-aware gating network to dynamically route\nsensitive tokens to privacy experts located on local clients, while\nnon-sensitive tokens are routed to non-privacy experts located at the remote\nbase station. To achieve computational efficiency, the gating network ensures\nthat each token is dynamically routed to and processed by only one expert. To\nenhance scalability and prevent overloading of specific experts, we introduce a\ngroup-wise load-balancing mechanism for the gating network that evenly\ndistributes sensitive tokens among privacy experts and non-sensitive tokens\namong non-privacy experts. To adapt to bandwidth constraints while preserving\nmodel performance, we propose a bandwidth-adaptive and importance-aware token\noffloading scheme. This scheme incorporates an importance predictor to evaluate\nthe importance scores of non-sensitive tokens, prioritizing the most important\ntokens for transmission to the base station based on their predicted importance\nand the available bandwidth. Experiments demonstrate that the PWC-MoE framework\neffectively preserves privacy and maintains high performance even in\nbandwidth-constrained environments, offering a practical solution for deploying\nLLMs in privacy-sensitive and bandwidth-limited scenarios."
                },
                "authors": [
                    {
                        "name": "Yang Su"
                    },
                    {
                        "name": "Na Yan"
                    },
                    {
                        "name": "Yansha Deng"
                    },
                    {
                        "name": "Robert Schober"
                    }
                ],
                "author_detail": {
                    "name": "Robert Schober"
                },
                "author": "Robert Schober",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00651v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00651v2",
                "updated": "2025-05-13T16:24:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    24,
                    54,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-01T16:54:21Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    16,
                    54,
                    21,
                    3,
                    121,
                    0
                ],
                "title": "Open-Source LLM-Driven Federated Transformer for Predictive IoV\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-Source LLM-Driven Federated Transformer for Predictive IoV\n  Management"
                },
                "summary": "The proliferation of connected vehicles within the Internet of Vehicles (IoV)\necosystem presents critical challenges in ensuring scalable, real-time, and\nprivacy-preserving traffic management. Existing centralized IoV solutions often\nsuffer from high latency, limited scalability, and reliance on proprietary\nArtificial Intelligence (AI) models, creating significant barriers to\nwidespread deployment, particularly in dynamic and privacy-sensitive\nenvironments. Meanwhile, integrating Large Language Models (LLMs) in vehicular\nsystems remains underexplored, especially concerning prompt optimization and\neffective utilization in federated contexts. To address these challenges, we\npropose the Federated Prompt-Optimized Traffic Transformer (FPoTT), a novel\nframework that leverages open-source LLMs for predictive IoV management. FPoTT\nintroduces a dynamic prompt optimization mechanism that iteratively refines\ntextual prompts to enhance trajectory prediction. The architecture employs a\ndual-layer federated learning paradigm, combining lightweight edge models for\nreal-time inference with cloud-based LLMs to retain global intelligence. A\nTransformer-driven synthetic data generator is incorporated to augment training\nwith diverse, high-fidelity traffic scenarios in the Next Generation Simulation\n(NGSIM) format. Extensive evaluations demonstrate that FPoTT, utilizing\nEleutherAI Pythia-1B, achieves 99.86% prediction accuracy on real-world data\nwhile maintaining high performance on synthetic datasets. These results\nunderscore the potential of open-source LLMs in enabling secure, adaptive, and\nscalable IoV management, offering a promising alternative to proprietary\nsolutions in smart mobility ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of connected vehicles within the Internet of Vehicles (IoV)\necosystem presents critical challenges in ensuring scalable, real-time, and\nprivacy-preserving traffic management. Existing centralized IoV solutions often\nsuffer from high latency, limited scalability, and reliance on proprietary\nArtificial Intelligence (AI) models, creating significant barriers to\nwidespread deployment, particularly in dynamic and privacy-sensitive\nenvironments. Meanwhile, integrating Large Language Models (LLMs) in vehicular\nsystems remains underexplored, especially concerning prompt optimization and\neffective utilization in federated contexts. To address these challenges, we\npropose the Federated Prompt-Optimized Traffic Transformer (FPoTT), a novel\nframework that leverages open-source LLMs for predictive IoV management. FPoTT\nintroduces a dynamic prompt optimization mechanism that iteratively refines\ntextual prompts to enhance trajectory prediction. The architecture employs a\ndual-layer federated learning paradigm, combining lightweight edge models for\nreal-time inference with cloud-based LLMs to retain global intelligence. A\nTransformer-driven synthetic data generator is incorporated to augment training\nwith diverse, high-fidelity traffic scenarios in the Next Generation Simulation\n(NGSIM) format. Extensive evaluations demonstrate that FPoTT, utilizing\nEleutherAI Pythia-1B, achieves 99.86% prediction accuracy on real-world data\nwhile maintaining high performance on synthetic datasets. These results\nunderscore the potential of open-source LLMs in enabling secure, adaptive, and\nscalable IoV management, offering a promising alternative to proprietary\nsolutions in smart mobility ecosystems."
                },
                "authors": [
                    {
                        "name": "Yazan Otoum"
                    },
                    {
                        "name": "Arghavan Asad"
                    },
                    {
                        "name": "Ishtiaq Ahmad"
                    }
                ],
                "author_detail": {
                    "name": "Ishtiaq Ahmad"
                },
                "author": "Ishtiaq Ahmad",
                "arxiv_comment": "Preprint version; submitted for academic peer review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00651v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00651v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01555v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01555v3",
                "updated": "2025-05-13T16:15:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    15,
                    43,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-02T19:47:30Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    19,
                    47,
                    30,
                    4,
                    122,
                    0
                ],
                "title": "Structured dataset of reported cloud seeding activities in the United\n  States (2000 to 2025) using a large language model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured dataset of reported cloud seeding activities in the United\n  States (2000 to 2025) using a large language model"
                },
                "summary": "Cloud seeding, a weather modification technique used to increase\nprecipitation, has been employed in the western United States since the 1940s.\nHowever, structured datasets are not currently available to analyze these\nefforts. To address this gap, we present a structured dataset of reported cloud\nseeding activities in the U.S. from 2000 to 2025, including the year, season,\nstate, seeding agent, apparatus used for deployment, and purpose. Using\nOpenAI's o4-mini large language model (LLM), combined with multi-stage\nPDF-to-text conversion and response-parsing code, we processed 836 historical\nreports from the National Oceanic and Atmospheric Administration (NOAA) to\nextract the data. The resulting dataset achieved 94.72% human-verified accuracy\nacross all fields and is publicly available on Zenodo. Our results help fill\nthe gap in structured cloud seeding data and demonstrate the potential for LLMs\nto extract structured environmental data from historical documents. More\nbroadly, this work provides a scalable framework for unlocking historical data\nfrom scanned documents across scientific domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud seeding, a weather modification technique used to increase\nprecipitation, has been employed in the western United States since the 1940s.\nHowever, structured datasets are not currently available to analyze these\nefforts. To address this gap, we present a structured dataset of reported cloud\nseeding activities in the U.S. from 2000 to 2025, including the year, season,\nstate, seeding agent, apparatus used for deployment, and purpose. Using\nOpenAI's o4-mini large language model (LLM), combined with multi-stage\nPDF-to-text conversion and response-parsing code, we processed 836 historical\nreports from the National Oceanic and Atmospheric Administration (NOAA) to\nextract the data. The resulting dataset achieved 94.72% human-verified accuracy\nacross all fields and is publicly available on Zenodo. Our results help fill\nthe gap in structured cloud seeding data and demonstrate the potential for LLMs\nto extract structured environmental data from historical documents. More\nbroadly, this work provides a scalable framework for unlocking historical data\nfrom scanned documents across scientific domains."
                },
                "authors": [
                    {
                        "name": "Jared Joseph Donohue"
                    },
                    {
                        "name": "Kara D. Lamb"
                    }
                ],
                "author_detail": {
                    "name": "Kara D. Lamb"
                },
                "author": "Kara D. Lamb",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01555v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01555v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08704v1",
                "updated": "2025-05-13T16:11:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    11,
                    29,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T16:11:29Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    11,
                    29,
                    1,
                    133,
                    0
                ],
                "title": "LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from\n  EHRs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from\n  EHRs"
                },
                "summary": "Electronic Health Records (EHRs) are digital records of patient information,\noften containing unstructured clinical text. Named Entity Recognition (NER) is\nessential in EHRs for extracting key medical entities like problems, tests, and\ntreatments to support downstream clinical applications. This paper explores\nprompt-based medical entity recognition using large language models (LLMs),\nspecifically GPT-4o and DeepSeek-R1, guided by various prompt engineering\ntechniques, including zero-shot, few-shot, and an ensemble approach. Among all\nstrategies, GPT-4o with prompt ensemble achieved the highest classification\nperformance with an F1-score of 0.95 and recall of 0.98, outperforming\nDeepSeek-R1 on the task. The ensemble method improved reliability by\naggregating outputs through embedding-based similarity and majority voting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic Health Records (EHRs) are digital records of patient information,\noften containing unstructured clinical text. Named Entity Recognition (NER) is\nessential in EHRs for extracting key medical entities like problems, tests, and\ntreatments to support downstream clinical applications. This paper explores\nprompt-based medical entity recognition using large language models (LLMs),\nspecifically GPT-4o and DeepSeek-R1, guided by various prompt engineering\ntechniques, including zero-shot, few-shot, and an ensemble approach. Among all\nstrategies, GPT-4o with prompt ensemble achieved the highest classification\nperformance with an F1-score of 0.95 and recall of 0.98, outperforming\nDeepSeek-R1 on the task. The ensemble method improved reliability by\naggregating outputs through embedding-based similarity and majority voting."
                },
                "authors": [
                    {
                        "name": "K M Sajjadul Islam"
                    },
                    {
                        "name": "Ayesha Siddika Nipu"
                    },
                    {
                        "name": "Jiawei Wu"
                    },
                    {
                        "name": "Praveen Madiraju"
                    }
                ],
                "author_detail": {
                    "name": "Praveen Madiraju"
                },
                "author": "Praveen Madiraju",
                "arxiv_comment": "IEEE 26th International Conference on Information Reuse and\n  Integration for Data Science (IRI 2025), San Jose, CA, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08379v2",
                "updated": "2025-05-13T16:08:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    16,
                    8,
                    10,
                    1,
                    133,
                    0
                ],
                "published": "2024-09-12T19:59:54Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    19,
                    59,
                    54,
                    3,
                    256,
                    0
                ],
                "title": "The Impact of Large Language Models on Open-source Innovation: Evidence\n  from GitHub Copilot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Large Language Models on Open-source Innovation: Evidence\n  from GitHub Copilot"
                },
                "summary": "Large Language Models (LLMs) have been shown to enhance individual\nproductivity in guided settings. Whereas LLMs are likely to also transform\ninnovation processes in a collaborative work setting, it is unclear what\ntrajectory this transformation will follow. Innovation in these contexts\nencompasses both capability innovation that explores new possibilities by\nacquiring new competencies in a project and iterative innovation that exploits\nexisting foundations by enhancing established competencies and improving\nproject quality. Whether LLMs affect these two aspects of collaborative work\nand to what extent is an open empirical question. Open-source development\nprovides an ideal setting to examine LLM impacts on these innovation types, as\nits voluntary and open/collaborative nature of contributions provides the\ngreatest opportunity for technological augmentation. We focus on open-source\nprojects on GitHub by leveraging a natural experiment around the selective\nrollout of GitHub Copilot (a programming-focused LLM) in October 2021, where\nGitHub Copilot selectively supported programming languages like Python or Rust,\nbut not R or Haskell. We observe a significant jump in overall contributions,\nsuggesting that LLMs effectively augment collaborative innovation in an\nunguided setting. Interestingly, Copilot's launch increased iterative\ninnovation focused on maintenance-related or feature-refining contributions\nsignificantly more than it did capability innovation through code-development\nor feature-introducing commits. This disparity was more pronounced after the\nmodel upgrade in June 2022 and was evident in active projects with extensive\ncoding activity, suggesting that as both LLM capabilities and/or available\ncontextual information improve, the gap between capability and iterative\ninnovation may widen. We discuss practical and policy implications to\nincentivize high-value innovative solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been shown to enhance individual\nproductivity in guided settings. Whereas LLMs are likely to also transform\ninnovation processes in a collaborative work setting, it is unclear what\ntrajectory this transformation will follow. Innovation in these contexts\nencompasses both capability innovation that explores new possibilities by\nacquiring new competencies in a project and iterative innovation that exploits\nexisting foundations by enhancing established competencies and improving\nproject quality. Whether LLMs affect these two aspects of collaborative work\nand to what extent is an open empirical question. Open-source development\nprovides an ideal setting to examine LLM impacts on these innovation types, as\nits voluntary and open/collaborative nature of contributions provides the\ngreatest opportunity for technological augmentation. We focus on open-source\nprojects on GitHub by leveraging a natural experiment around the selective\nrollout of GitHub Copilot (a programming-focused LLM) in October 2021, where\nGitHub Copilot selectively supported programming languages like Python or Rust,\nbut not R or Haskell. We observe a significant jump in overall contributions,\nsuggesting that LLMs effectively augment collaborative innovation in an\nunguided setting. Interestingly, Copilot's launch increased iterative\ninnovation focused on maintenance-related or feature-refining contributions\nsignificantly more than it did capability innovation through code-development\nor feature-introducing commits. This disparity was more pronounced after the\nmodel upgrade in June 2022 and was evident in active projects with extensive\ncoding activity, suggesting that as both LLM capabilities and/or available\ncontextual information improve, the gap between capability and iterative\ninnovation may widen. We discuss practical and policy implications to\nincentivize high-value innovative solutions."
                },
                "authors": [
                    {
                        "name": "Doron Yeverechyahu"
                    },
                    {
                        "name": "Raveesh Mayya"
                    },
                    {
                        "name": "Gal Oestreicher-Singer"
                    }
                ],
                "author_detail": {
                    "name": "Gal Oestreicher-Singer"
                },
                "author": "Gal Oestreicher-Singer",
                "arxiv_comment": "JEL Classification: O31, C88, J24, O35, L86",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; D.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08699v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08699v2",
                "updated": "2025-05-14T02:10:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    10,
                    29,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-13T15:58:57Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    58,
                    57,
                    1,
                    133,
                    0
                ],
                "title": "Granite-speech: open-source speech-aware LLMs with strong English ASR\n  capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Granite-speech: open-source speech-aware LLMs with strong English ASR\n  capabilities"
                },
                "summary": "Granite-speech LLMs are compact and efficient speech language models\nspecifically designed for English ASR and automatic speech translation (AST).\nThe models were trained by modality aligning the 2B and 8B parameter variants\nof granite-3.3-instruct to speech on publicly available open-source corpora\ncontaining audio inputs and text targets consisting of either human transcripts\nfor ASR or automatically generated translations for AST. Comprehensive\nbenchmarking shows that on English ASR, which was our primary focus, they\noutperform several competitors' models that were trained on orders of magnitude\nmore proprietary data, and they keep pace on English-to-X AST for major\nEuropean languages, Japanese, and Chinese. The speech-specific components are:\na conformer acoustic encoder using block attention and self-conditioning\ntrained with connectionist temporal classification, a windowed\nquery-transformer speech modality adapter used to do temporal downsampling of\nthe acoustic embeddings and map them to the LLM text embedding space, and LoRA\nadapters to further fine-tune the text LLM. Granite-speech-3.3 operates in two\nmodes: in speech mode, it performs ASR and AST by activating the encoder,\nprojector, and LoRA adapters; in text mode, it calls the underlying\ngranite-3.3-instruct model directly (without LoRA), essentially preserving all\nthe text LLM capabilities and safety. Both models are freely available on\nHuggingFace (https://huggingface.co/ibm-granite/granite-speech-3.3-2b and\nhttps://huggingface.co/ibm-granite/granite-speech-3.3-8b) and can be used for\nboth research and commercial purposes under a permissive Apache 2.0 license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Granite-speech LLMs are compact and efficient speech language models\nspecifically designed for English ASR and automatic speech translation (AST).\nThe models were trained by modality aligning the 2B and 8B parameter variants\nof granite-3.3-instruct to speech on publicly available open-source corpora\ncontaining audio inputs and text targets consisting of either human transcripts\nfor ASR or automatically generated translations for AST. Comprehensive\nbenchmarking shows that on English ASR, which was our primary focus, they\noutperform several competitors' models that were trained on orders of magnitude\nmore proprietary data, and they keep pace on English-to-X AST for major\nEuropean languages, Japanese, and Chinese. The speech-specific components are:\na conformer acoustic encoder using block attention and self-conditioning\ntrained with connectionist temporal classification, a windowed\nquery-transformer speech modality adapter used to do temporal downsampling of\nthe acoustic embeddings and map them to the LLM text embedding space, and LoRA\nadapters to further fine-tune the text LLM. Granite-speech-3.3 operates in two\nmodes: in speech mode, it performs ASR and AST by activating the encoder,\nprojector, and LoRA adapters; in text mode, it calls the underlying\ngranite-3.3-instruct model directly (without LoRA), essentially preserving all\nthe text LLM capabilities and safety. Both models are freely available on\nHuggingFace (https://huggingface.co/ibm-granite/granite-speech-3.3-2b and\nhttps://huggingface.co/ibm-granite/granite-speech-3.3-8b) and can be used for\nboth research and commercial purposes under a permissive Apache 2.0 license."
                },
                "authors": [
                    {
                        "name": "George Saon"
                    },
                    {
                        "name": "Avihu Dekel"
                    },
                    {
                        "name": "Alexander Brooks"
                    },
                    {
                        "name": "Tohru Nagano"
                    },
                    {
                        "name": "Abraham Daniels"
                    },
                    {
                        "name": "Aharon Satt"
                    },
                    {
                        "name": "Ashish Mittal"
                    },
                    {
                        "name": "Brian Kingsbury"
                    },
                    {
                        "name": "David Haws"
                    },
                    {
                        "name": "Edmilson Morais"
                    },
                    {
                        "name": "Gakuto Kurata"
                    },
                    {
                        "name": "Hagai Aronowitz"
                    },
                    {
                        "name": "Ibrahim Ibrahim"
                    },
                    {
                        "name": "Jeff Kuo"
                    },
                    {
                        "name": "Kate Soule"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Masayuki Suzuki"
                    },
                    {
                        "name": "Ron Hoory"
                    },
                    {
                        "name": "Samuel Thomas"
                    },
                    {
                        "name": "Sashi Novitasari"
                    },
                    {
                        "name": "Takashi Fukuda"
                    },
                    {
                        "name": "Vishal Sunder"
                    },
                    {
                        "name": "Xiaodong Cui"
                    },
                    {
                        "name": "Zvi Kons"
                    }
                ],
                "author_detail": {
                    "name": "Zvi Kons"
                },
                "author": "Zvi Kons",
                "arxiv_comment": "7 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08699v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08699v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08690v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08690v1",
                "updated": "2025-05-13T15:47:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    47,
                    54,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T15:47:54Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    47,
                    54,
                    1,
                    133,
                    0
                ],
                "title": "Adaptive Schema-aware Event Extraction with Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Schema-aware Event Extraction with Retrieval-Augmented\n  Generation"
                },
                "summary": "Event extraction (EE) is a fundamental task in natural language processing\n(NLP) that involves identifying and extracting event information from\nunstructured text. Effective EE in real-world scenarios requires two key steps:\nselecting appropriate schemas from hundreds of candidates and executing the\nextraction process. Existing research exhibits two critical gaps: (1) the rigid\nschema fixation in existing pipeline systems, and (2) the absence of benchmarks\nfor evaluating joint schema matching and extraction. Although large language\nmodels (LLMs) offer potential solutions, their schema hallucination tendencies\nand context window limitations pose challenges for practical deployment. In\nresponse, we propose Adaptive Schema-aware Event Extraction (ASEE), a novel\nparadigm combining schema paraphrasing with schema retrieval-augmented\ngeneration. ASEE adeptly retrieves paraphrased schemas and accurately generates\ntargeted structures. To facilitate rigorous evaluation, we construct the\nMulti-Dimensional Schema-aware Event Extraction (MD-SEE) benchmark, which\nsystematically consolidates 12 datasets across diverse domains, complexity\nlevels, and language settings. Extensive evaluations on MD-SEE show that our\nproposed ASEE demonstrates strong adaptability across various scenarios,\nsignificantly improving the accuracy of event extraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event extraction (EE) is a fundamental task in natural language processing\n(NLP) that involves identifying and extracting event information from\nunstructured text. Effective EE in real-world scenarios requires two key steps:\nselecting appropriate schemas from hundreds of candidates and executing the\nextraction process. Existing research exhibits two critical gaps: (1) the rigid\nschema fixation in existing pipeline systems, and (2) the absence of benchmarks\nfor evaluating joint schema matching and extraction. Although large language\nmodels (LLMs) offer potential solutions, their schema hallucination tendencies\nand context window limitations pose challenges for practical deployment. In\nresponse, we propose Adaptive Schema-aware Event Extraction (ASEE), a novel\nparadigm combining schema paraphrasing with schema retrieval-augmented\ngeneration. ASEE adeptly retrieves paraphrased schemas and accurately generates\ntargeted structures. To facilitate rigorous evaluation, we construct the\nMulti-Dimensional Schema-aware Event Extraction (MD-SEE) benchmark, which\nsystematically consolidates 12 datasets across diverse domains, complexity\nlevels, and language settings. Extensive evaluations on MD-SEE show that our\nproposed ASEE demonstrates strong adaptability across various scenarios,\nsignificantly improving the accuracy of event extraction."
                },
                "authors": [
                    {
                        "name": "Sheng Liang"
                    },
                    {
                        "name": "Hang Lv"
                    },
                    {
                        "name": "Zhihao Wen"
                    },
                    {
                        "name": "Yaxiong Wu"
                    },
                    {
                        "name": "Yongyue Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08690v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08662v1",
                "updated": "2025-05-13T15:24:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    24,
                    8,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T15:24:08Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    24,
                    8,
                    1,
                    133,
                    0
                ],
                "title": "Revealing economic facts: LLMs know more than they say",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing economic facts: LLMs know more than they say"
                },
                "summary": "We investigate whether the hidden states of large language models (LLMs) can\nbe used to estimate and impute economic and financial statistics. Focusing on\ncounty-level (e.g. unemployment) and firm-level (e.g. total assets) variables,\nwe show that a simple linear model trained on the hidden states of open-source\nLLMs outperforms the models' text outputs. This suggests that hidden states\ncapture richer economic information than the responses of the LLMs reveal\ndirectly. A learning curve analysis indicates that only a few dozen labelled\nexamples are sufficient for training. We also propose a transfer learning\nmethod that improves estimation accuracy without requiring any labelled data\nfor the target variable. Finally, we demonstrate the practical utility of\nhidden-state representations in super-resolution and data imputation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate whether the hidden states of large language models (LLMs) can\nbe used to estimate and impute economic and financial statistics. Focusing on\ncounty-level (e.g. unemployment) and firm-level (e.g. total assets) variables,\nwe show that a simple linear model trained on the hidden states of open-source\nLLMs outperforms the models' text outputs. This suggests that hidden states\ncapture richer economic information than the responses of the LLMs reveal\ndirectly. A learning curve analysis indicates that only a few dozen labelled\nexamples are sufficient for training. We also propose a transfer learning\nmethod that improves estimation accuracy without requiring any labelled data\nfor the target variable. Finally, we demonstrate the practical utility of\nhidden-state representations in super-resolution and data imputation tasks."
                },
                "authors": [
                    {
                        "name": "Marcus Buckmann"
                    },
                    {
                        "name": "Quynh Anh Nguyen"
                    },
                    {
                        "name": "Edward Hill"
                    }
                ],
                "author_detail": {
                    "name": "Edward Hill"
                },
                "author": "Edward Hill",
                "arxiv_comment": "34 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08648v1",
                "updated": "2025-05-13T15:08:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    8,
                    55,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T15:08:55Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    8,
                    55,
                    1,
                    133,
                    0
                ],
                "title": "Enhancing Software Development with Context-Aware Conversational Agents:\n  A User Study on Developer Interactions with Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Software Development with Context-Aware Conversational Agents:\n  A User Study on Developer Interactions with Chatbots"
                },
                "summary": "Software development is a cognitively intensive process requiring\nmultitasking, adherence to evolving workflows, and continuous learning. With\nthe rise of large language model (LLM)-based tools, such as conversational\nagents (CAs), there is growing interest in supporting developers through\nnatural language interaction. However, little is known about the specific\nfeatures developers seek in these systems. We conducted a user study with 29\ndevelopers using a prototype text-based chatbot to investigate preferred\nfunctionalities. Our findings reveal strong interest in task automation,\nversion control support, and contextual adaptability, especially the need to\ntailor assistance for both novice and experienced users. We highlight the\nimportance of deep contextual understanding, historical interaction awareness,\nand personalized support in CA design. This study contributes to the\ndevelopment of context-aware chatbots that enhance productivity and\nsatisfaction, and it outlines opportunities for future research on human-AI\ncollaboration in software engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software development is a cognitively intensive process requiring\nmultitasking, adherence to evolving workflows, and continuous learning. With\nthe rise of large language model (LLM)-based tools, such as conversational\nagents (CAs), there is growing interest in supporting developers through\nnatural language interaction. However, little is known about the specific\nfeatures developers seek in these systems. We conducted a user study with 29\ndevelopers using a prototype text-based chatbot to investigate preferred\nfunctionalities. Our findings reveal strong interest in task automation,\nversion control support, and contextual adaptability, especially the need to\ntailor assistance for both novice and experienced users. We highlight the\nimportance of deep contextual understanding, historical interaction awareness,\nand personalized support in CA design. This study contributes to the\ndevelopment of context-aware chatbots that enhance productivity and\nsatisfaction, and it outlines opportunities for future research on human-AI\ncollaboration in software engineering."
                },
                "authors": [
                    {
                        "name": "Glaucia Melo"
                    },
                    {
                        "name": "Paulo Alencar"
                    },
                    {
                        "name": "Donald Cowan"
                    }
                ],
                "author_detail": {
                    "name": "Donald Cowan"
                },
                "author": "Donald Cowan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16978v2",
                "updated": "2025-05-13T15:07:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    7,
                    26,
                    1,
                    133,
                    0
                ],
                "published": "2024-08-30T02:44:26Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    44,
                    26,
                    4,
                    243,
                    0
                ],
                "title": "Training Ultra Long Context Language Model with Fully Pipelined\n  Distributed Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Ultra Long Context Language Model with Fully Pipelined\n  Distributed Transformer"
                },
                "summary": "Large Language Models (LLMs) with long context capabilities are integral to\ncomplex tasks in natural language processing and computational biology, such as\ntext generation and protein sequence analysis. However, training LLMs directly\non extremely long contexts demands considerable GPU resources and increased\nmemory, leading to higher costs and greater complexity. Alternative approaches\nthat introduce long context capabilities via downstream finetuning or\nadaptations impose significant design limitations. In this paper, we propose\nFully Pipelined Distributed Transformer (FPDT) for efficiently training\nlong-context LLMs with extreme hardware efficiency. For GPT and Llama models,\nwe achieve a 16x increase in sequence length that can be trained on the same\nhardware compared to current state-of-the-art solutions. With our dedicated\nsequence chunk pipeline design, we can now train 8B LLM with 2 million sequence\nlength on only 4 GPUs, while also maintaining over 55% of MFU. Our proposed\nFPDT is agnostic to existing training techniques and is proven to work\nefficiently across different LLM models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with long context capabilities are integral to\ncomplex tasks in natural language processing and computational biology, such as\ntext generation and protein sequence analysis. However, training LLMs directly\non extremely long contexts demands considerable GPU resources and increased\nmemory, leading to higher costs and greater complexity. Alternative approaches\nthat introduce long context capabilities via downstream finetuning or\nadaptations impose significant design limitations. In this paper, we propose\nFully Pipelined Distributed Transformer (FPDT) for efficiently training\nlong-context LLMs with extreme hardware efficiency. For GPT and Llama models,\nwe achieve a 16x increase in sequence length that can be trained on the same\nhardware compared to current state-of-the-art solutions. With our dedicated\nsequence chunk pipeline design, we can now train 8B LLM with 2 million sequence\nlength on only 4 GPUs, while also maintaining over 55% of MFU. Our proposed\nFPDT is agnostic to existing training techniques and is proven to work\nefficiently across different LLM models."
                },
                "authors": [
                    {
                        "name": "Jinghan Yao"
                    },
                    {
                        "name": "Sam Ade Jacobs"
                    },
                    {
                        "name": "Masahiro Tanaka"
                    },
                    {
                        "name": "Olatunji Ruwase"
                    },
                    {
                        "name": "Hari Subramoni"
                    },
                    {
                        "name": "Dhabaleswar K. Panda"
                    }
                ],
                "author_detail": {
                    "name": "Dhabaleswar K. Panda"
                },
                "author": "Dhabaleswar K. Panda",
                "arxiv_comment": "The Eighth Annual Conference on Machine Learning and Systems\n  (MLSys'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08646v1",
                "updated": "2025-05-13T15:04:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    4,
                    55,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T15:04:55Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    4,
                    55,
                    1,
                    133,
                    0
                ],
                "title": "Modular Federated Learning: A Meta-Framework Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modular Federated Learning: A Meta-Framework Perspective"
                },
                "summary": "Federated Learning (FL) enables distributed machine learning training while\npreserving privacy, representing a paradigm shift for data-sensitive and\ndecentralized environments. Despite its rapid advancements, FL remains a\ncomplex and multifaceted field, requiring a structured understanding of its\nmethodologies, challenges, and applications. In this survey, we introduce a\nmeta-framework perspective, conceptualising FL as a composition of modular\ncomponents that systematically address core aspects such as communication,\noptimisation, security, and privacy. We provide a historical contextualisation\nof FL, tracing its evolution from distributed optimisation to modern\ndistributed learning paradigms. Additionally, we propose a novel taxonomy\ndistinguishing Aggregation from Alignment, introducing the concept of alignment\nas a fundamental operator alongside aggregation. To bridge theory with\npractice, we explore available FL frameworks in Python, facilitating real-world\nimplementation. Finally, we systematise key challenges across FL sub-fields,\nproviding insights into open research questions throughout the meta-framework\nmodules. By structuring FL within a meta-framework of modular components and\nemphasising the dual role of Aggregation and Alignment, this survey provides a\nholistic and adaptable foundation for understanding and advancing FL research\nand deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables distributed machine learning training while\npreserving privacy, representing a paradigm shift for data-sensitive and\ndecentralized environments. Despite its rapid advancements, FL remains a\ncomplex and multifaceted field, requiring a structured understanding of its\nmethodologies, challenges, and applications. In this survey, we introduce a\nmeta-framework perspective, conceptualising FL as a composition of modular\ncomponents that systematically address core aspects such as communication,\noptimisation, security, and privacy. We provide a historical contextualisation\nof FL, tracing its evolution from distributed optimisation to modern\ndistributed learning paradigms. Additionally, we propose a novel taxonomy\ndistinguishing Aggregation from Alignment, introducing the concept of alignment\nas a fundamental operator alongside aggregation. To bridge theory with\npractice, we explore available FL frameworks in Python, facilitating real-world\nimplementation. Finally, we systematise key challenges across FL sub-fields,\nproviding insights into open research questions throughout the meta-framework\nmodules. By structuring FL within a meta-framework of modular components and\nemphasising the dual role of Aggregation and Alignment, this survey provides a\nholistic and adaptable foundation for understanding and advancing FL research\nand deployment."
                },
                "authors": [
                    {
                        "name": "Frederico Vicente"
                    },
                    {
                        "name": "Cláudia Soares"
                    },
                    {
                        "name": "Dušan Jakovetić"
                    }
                ],
                "author_detail": {
                    "name": "Dušan Jakovetić"
                },
                "author": "Dušan Jakovetić",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08643v1",
                "updated": "2025-05-13T15:02:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    2,
                    54,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T15:02:54Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    2,
                    54,
                    1,
                    133,
                    0
                ],
                "title": "WixQA: A Multi-Dataset Benchmark for Enterprise Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WixQA: A Multi-Dataset Benchmark for Enterprise Retrieval-Augmented\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) is a cornerstone of modern question\nanswering (QA) systems, enabling grounded answers based on external knowledge.\nAlthough recent progress has been driven by open-domain datasets, enterprise QA\nsystems need datasets that mirror the concrete, domain-specific issues users\nraise in day-to-day support scenarios. Critically, evaluating end-to-end RAG\nsystems requires benchmarks comprising not only question--answer pairs but also\nthe specific knowledge base (KB) snapshot from which answers were derived. To\naddress this need, we introduce WixQA, a benchmark suite featuring QA datasets\nprecisely grounded in the released KB corpus, enabling holistic evaluation of\nretrieval and generation components. WixQA includes three distinct QA datasets\nderived from Wix.com customer support interactions and grounded in a snapshot\nof the public Wix Help Center KB: (i) WixQA-ExpertWritten, 200 real user\nqueries with expert-authored, multi-step answers; (ii) WixQA-Simulated, 200\nexpert-validated QA pairs distilled from user dialogues; and (iii)\nWixQA-Synthetic, 6,222 LLM-generated QA pairs, with one pair systematically\nderived from each article in the knowledge base. We release the KB snapshot\nalongside the datasets under MIT license and provide comprehensive baseline\nresults, forming a unique benchmark for evaluating enterprise RAG systems in\nrealistic enterprise environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) is a cornerstone of modern question\nanswering (QA) systems, enabling grounded answers based on external knowledge.\nAlthough recent progress has been driven by open-domain datasets, enterprise QA\nsystems need datasets that mirror the concrete, domain-specific issues users\nraise in day-to-day support scenarios. Critically, evaluating end-to-end RAG\nsystems requires benchmarks comprising not only question--answer pairs but also\nthe specific knowledge base (KB) snapshot from which answers were derived. To\naddress this need, we introduce WixQA, a benchmark suite featuring QA datasets\nprecisely grounded in the released KB corpus, enabling holistic evaluation of\nretrieval and generation components. WixQA includes three distinct QA datasets\nderived from Wix.com customer support interactions and grounded in a snapshot\nof the public Wix Help Center KB: (i) WixQA-ExpertWritten, 200 real user\nqueries with expert-authored, multi-step answers; (ii) WixQA-Simulated, 200\nexpert-validated QA pairs distilled from user dialogues; and (iii)\nWixQA-Synthetic, 6,222 LLM-generated QA pairs, with one pair systematically\nderived from each article in the knowledge base. We release the KB snapshot\nalongside the datasets under MIT license and provide comprehensive baseline\nresults, forming a unique benchmark for evaluating enterprise RAG systems in\nrealistic enterprise environments."
                },
                "authors": [
                    {
                        "name": "Dvir Cohen"
                    },
                    {
                        "name": "Lin Burg"
                    },
                    {
                        "name": "Sviatoslav Pykhnivskyi"
                    },
                    {
                        "name": "Hagit Gur"
                    },
                    {
                        "name": "Stanislav Kovynov"
                    },
                    {
                        "name": "Olga Atzmon"
                    },
                    {
                        "name": "Gilad Barkan"
                    }
                ],
                "author_detail": {
                    "name": "Gilad Barkan"
                },
                "author": "Gilad Barkan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08638v1",
                "updated": "2025-05-13T14:55:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    55,
                    31,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T14:55:31Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    55,
                    31,
                    1,
                    133,
                    0
                ],
                "title": "TRAIL: Trace Reasoning and Agentic Issue Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRAIL: Trace Reasoning and Agentic Issue Localization"
                },
                "summary": "The increasing adoption of agentic workflows across diverse domains brings a\ncritical need to scalably and systematically evaluate the complex traces these\nsystems generate. Current evaluation methods depend on manual, domain-specific\nhuman analysis of lengthy workflow traces - an approach that does not scale\nwith the growing complexity and volume of agentic outputs. Error analysis in\nthese settings is further complicated by the interplay of external tool outputs\nand language model reasoning, making it more challenging than traditional\nsoftware debugging. In this work, we (1) articulate the need for robust and\ndynamic evaluation methods for agentic workflow traces, (2) introduce a formal\ntaxonomy of error types encountered in agentic systems, and (3) present a set\nof 148 large human-annotated traces (TRAIL) constructed using this taxonomy and\ngrounded in established agentic benchmarks. To ensure ecological validity, we\ncurate traces from both single and multi-agent systems, focusing on real-world\napplications such as software engineering and open-world information retrieval.\nOur evaluations reveal that modern long context LLMs perform poorly at trace\ndebugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our\ndataset and code are made publicly available to support and accelerate future\nresearch in scalable evaluation for agentic workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of agentic workflows across diverse domains brings a\ncritical need to scalably and systematically evaluate the complex traces these\nsystems generate. Current evaluation methods depend on manual, domain-specific\nhuman analysis of lengthy workflow traces - an approach that does not scale\nwith the growing complexity and volume of agentic outputs. Error analysis in\nthese settings is further complicated by the interplay of external tool outputs\nand language model reasoning, making it more challenging than traditional\nsoftware debugging. In this work, we (1) articulate the need for robust and\ndynamic evaluation methods for agentic workflow traces, (2) introduce a formal\ntaxonomy of error types encountered in agentic systems, and (3) present a set\nof 148 large human-annotated traces (TRAIL) constructed using this taxonomy and\ngrounded in established agentic benchmarks. To ensure ecological validity, we\ncurate traces from both single and multi-agent systems, focusing on real-world\napplications such as software engineering and open-world information retrieval.\nOur evaluations reveal that modern long context LLMs perform poorly at trace\ndebugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our\ndataset and code are made publicly available to support and accelerate future\nresearch in scalable evaluation for agentic workflows."
                },
                "authors": [
                    {
                        "name": "Darshan Deshpande"
                    },
                    {
                        "name": "Varun Gangal"
                    },
                    {
                        "name": "Hersh Mehta"
                    },
                    {
                        "name": "Jitin Krishnan"
                    },
                    {
                        "name": "Anand Kannappan"
                    },
                    {
                        "name": "Rebecca Qian"
                    }
                ],
                "author_detail": {
                    "name": "Rebecca Qian"
                },
                "author": "Rebecca Qian",
                "arxiv_comment": "Dataset link: https://huggingface.co/datasets/PatronusAI/TRAIL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08622v1",
                "updated": "2025-05-13T14:40:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    40,
                    22,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T14:40:22Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    40,
                    22,
                    1,
                    133,
                    0
                ],
                "title": "Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with\n  Language Models"
                },
                "summary": "Text-to-image generative models like DALL-E and Stable Diffusion have\nrevolutionized visual content creation across various applications, including\nadvertising, personalized media, and design prototyping. However, crafting\neffective textual prompts to guide these models remains challenging, often\nrequiring extensive trial and error. Existing prompt inversion approaches, such\nas soft and hard prompt techniques, are not so effective due to the limited\ninterpretability and incoherent prompt generation. To address these issues, we\npropose Visually Guided Decoding (VGD), a gradient-free approach that leverages\nlarge language models (LLMs) and CLIP-based guidance to generate coherent and\nsemantically aligned prompts. In essence, VGD utilizes the robust text\ngeneration capabilities of LLMs to produce human-readable prompts. Further, by\nemploying CLIP scores to ensure alignment with user-specified visual concepts,\nVGD enhances the interpretability, generalization, and flexibility of prompt\ngeneration without the need for additional training. Our experiments\ndemonstrate that VGD outperforms existing prompt inversion techniques in\ngenerating understandable and contextually relevant prompts, facilitating more\nintuitive and controllable interactions with text-to-image models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generative models like DALL-E and Stable Diffusion have\nrevolutionized visual content creation across various applications, including\nadvertising, personalized media, and design prototyping. However, crafting\neffective textual prompts to guide these models remains challenging, often\nrequiring extensive trial and error. Existing prompt inversion approaches, such\nas soft and hard prompt techniques, are not so effective due to the limited\ninterpretability and incoherent prompt generation. To address these issues, we\npropose Visually Guided Decoding (VGD), a gradient-free approach that leverages\nlarge language models (LLMs) and CLIP-based guidance to generate coherent and\nsemantically aligned prompts. In essence, VGD utilizes the robust text\ngeneration capabilities of LLMs to produce human-readable prompts. Further, by\nemploying CLIP scores to ensure alignment with user-specified visual concepts,\nVGD enhances the interpretability, generalization, and flexibility of prompt\ngeneration without the need for additional training. Our experiments\ndemonstrate that VGD outperforms existing prompt inversion techniques in\ngenerating understandable and contextually relevant prompts, facilitating more\nintuitive and controllable interactions with text-to-image models."
                },
                "authors": [
                    {
                        "name": "Donghoon Kim"
                    },
                    {
                        "name": "Minji Bae"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Byonghyo Shim"
                    }
                ],
                "author_detail": {
                    "name": "Byonghyo Shim"
                },
                "author": "Byonghyo Shim",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08620v1",
                "updated": "2025-05-13T14:39:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    39,
                    33,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T14:39:33Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    39,
                    33,
                    1,
                    133,
                    0
                ],
                "title": "Resource-Efficient Language Models: Quantization for Fast and Accessible\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Efficient Language Models: Quantization for Fast and Accessible\n  Inference"
                },
                "summary": "Large language models have significantly advanced natural language\nprocessing, yet their heavy resource demands pose severe challenges regarding\nhardware accessibility and energy consumption. This paper presents a focused\nand high-level review of post-training quantization (PTQ) techniques designed\nto optimize the inference efficiency of LLMs by the end-user, including details\non various quantization schemes, granularities, and trade-offs. The aim is to\nprovide a balanced overview between the theory and applications of\npost-training quantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have significantly advanced natural language\nprocessing, yet their heavy resource demands pose severe challenges regarding\nhardware accessibility and energy consumption. This paper presents a focused\nand high-level review of post-training quantization (PTQ) techniques designed\nto optimize the inference efficiency of LLMs by the end-user, including details\non various quantization schemes, granularities, and trade-offs. The aim is to\nprovide a balanced overview between the theory and applications of\npost-training quantization."
                },
                "authors": [
                    {
                        "name": "Tollef Emil Jørgensen"
                    }
                ],
                "author_detail": {
                    "name": "Tollef Emil Jørgensen"
                },
                "author": "Tollef Emil Jørgensen",
                "arxiv_comment": "17 pages, 9 figures, preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08600v1",
                "updated": "2025-05-13T14:16:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    16,
                    12,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T14:16:12Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    16,
                    12,
                    1,
                    133,
                    0
                ],
                "title": "Automatic Task Detection and Heterogeneous LLM Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Task Detection and Heterogeneous LLM Speculative Decoding"
                },
                "summary": "Speculative decoding, which combines a draft model with a target model, has\nemerged as an effective approach to accelerate large language model (LLM)\ninference. However, existing methods often face a trade-off between the\nacceptance rate and decoding speed in downstream tasks due to the limited\ncapacity of the draft model, making it difficult to ensure efficiency across\ndiverse tasks. To address this problem, we propose a speculative decoding\nalgorithm tailored for downstream task optimization. It includes an automatic\ntask partitioning and assigning method, which automatically categorizes\ndownstream tasks into different sub-tasks and assigns them to a set of\nheterogeneous draft models. Each draft model is aligned with the target model\nusing task-specific data, thereby enhancing the consistency of inference\nresults. In addition, our proposed method incorporates an online lightweight\nprompt classifier to dynamically route prompts to the appropriate draft model.\nExperimental results demonstrate that the proposed method improves draft\naccuracy by 6% to 50% over vanilla speculative decoding, while achieving a\nspeedup of 1.10x to 2.64x in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding, which combines a draft model with a target model, has\nemerged as an effective approach to accelerate large language model (LLM)\ninference. However, existing methods often face a trade-off between the\nacceptance rate and decoding speed in downstream tasks due to the limited\ncapacity of the draft model, making it difficult to ensure efficiency across\ndiverse tasks. To address this problem, we propose a speculative decoding\nalgorithm tailored for downstream task optimization. It includes an automatic\ntask partitioning and assigning method, which automatically categorizes\ndownstream tasks into different sub-tasks and assigns them to a set of\nheterogeneous draft models. Each draft model is aligned with the target model\nusing task-specific data, thereby enhancing the consistency of inference\nresults. In addition, our proposed method incorporates an online lightweight\nprompt classifier to dynamically route prompts to the appropriate draft model.\nExperimental results demonstrate that the proposed method improves draft\naccuracy by 6% to 50% over vanilla speculative decoding, while achieving a\nspeedup of 1.10x to 2.64x in LLM inference."
                },
                "authors": [
                    {
                        "name": "Danying Ge"
                    },
                    {
                        "name": "Jianhua Gao"
                    },
                    {
                        "name": "Qizhi Jiang"
                    },
                    {
                        "name": "Yifei Feng"
                    },
                    {
                        "name": "Weixing Ji"
                    }
                ],
                "author_detail": {
                    "name": "Weixing Ji"
                },
                "author": "Weixing Ji",
                "arxiv_comment": "10 pages, 10 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06205v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06205v3",
                "updated": "2025-05-13T14:11:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    11,
                    59,
                    1,
                    133,
                    0
                ],
                "published": "2024-10-08T17:07:01Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    17,
                    7,
                    1,
                    1,
                    282,
                    0
                ],
                "title": "Round and Round We Go! What makes Rotary Positional Encodings useful?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Round and Round We Go! What makes Rotary Positional Encodings useful?"
                },
                "summary": "Positional Encodings (PEs) are a critical component of Transformer-based\nLarge Language Models (LLMs), providing the attention mechanism with important\nsequence-position information. One of the most popular types of encoding used\ntoday in LLMs are Rotary Positional Encodings (RoPE), that rotate the queries\nand keys based on their relative distance. A common belief is that RoPE is\nuseful because it helps to decay token dependency as relative distance\nincreases. In this work, we argue that this is unlikely to be the core reason.\nWe study the internals of a trained Gemma 7B model to understand how RoPE is\nbeing used at a mechanical level. We find that Gemma learns to use RoPE to\nconstruct robust \"positional\" attention patterns by exploiting the highest\nfrequencies. We also find that, in general, Gemma greatly prefers to use the\nlowest frequencies of RoPE, which we suspect are used to carry semantic\ninformation. We mathematically prove interesting behaviours of RoPE and conduct\nexperiments to verify our findings, proposing a modification of RoPE that fixes\nsome highlighted issues and improves performance. We believe that this work\nrepresents an interesting step in better understanding PEs in LLMs, which we\nbelieve holds crucial value for scaling LLMs to large sizes and context\nlengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Positional Encodings (PEs) are a critical component of Transformer-based\nLarge Language Models (LLMs), providing the attention mechanism with important\nsequence-position information. One of the most popular types of encoding used\ntoday in LLMs are Rotary Positional Encodings (RoPE), that rotate the queries\nand keys based on their relative distance. A common belief is that RoPE is\nuseful because it helps to decay token dependency as relative distance\nincreases. In this work, we argue that this is unlikely to be the core reason.\nWe study the internals of a trained Gemma 7B model to understand how RoPE is\nbeing used at a mechanical level. We find that Gemma learns to use RoPE to\nconstruct robust \"positional\" attention patterns by exploiting the highest\nfrequencies. We also find that, in general, Gemma greatly prefers to use the\nlowest frequencies of RoPE, which we suspect are used to carry semantic\ninformation. We mathematically prove interesting behaviours of RoPE and conduct\nexperiments to verify our findings, proposing a modification of RoPE that fixes\nsome highlighted issues and improves performance. We believe that this work\nrepresents an interesting step in better understanding PEs in LLMs, which we\nbelieve holds crucial value for scaling LLMs to large sizes and context\nlengths."
                },
                "authors": [
                    {
                        "name": "Federico Barbero"
                    },
                    {
                        "name": "Alex Vitvitskyi"
                    },
                    {
                        "name": "Christos Perivolaropoulos"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Petar Veličković"
                    }
                ],
                "author_detail": {
                    "name": "Petar Veličković"
                },
                "author": "Petar Veličković",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06205v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06205v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07687v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07687v3",
                "updated": "2025-05-13T14:09:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    9,
                    20,
                    1,
                    133,
                    0
                ],
                "published": "2025-04-10T12:16:32Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    12,
                    16,
                    32,
                    3,
                    100,
                    0
                ],
                "title": "FMNV: A Dataset of Media-Published News Videos for Fake News Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FMNV: A Dataset of Media-Published News Videos for Fake News Detection"
                },
                "summary": "News media, particularly video-based platforms, have become deeply embed-ded\nin daily life, concurrently amplifying the risks of misinformation\ndissem-ination. Consequently, multimodal fake news detection has garnered\nsignifi-cant research attention. However, existing datasets predominantly\ncomprise user-generated videos characterized by crude editing and limited\npublic en-gagement, whereas professionally crafted fake news videos\ndisseminated by media outlets-often politically or virally motivated-pose\nsubstantially greater societal harm. To address this gap, we construct FMNV, a\nnovel da-taset exclusively composed of news videos published by media\norganizations. Through empirical analysis of existing datasets and our curated\ncollection, we categorize fake news videos into four distinct types. Building\nupon this taxonomy, we employ Large Language Models (LLMs) to automatically\ngenerate deceptive content by manipulating authentic media-published news\nvideos. Furthermore, we propose FMNVD, a baseline model featuring a dual-stream\narchitecture that integrates spatio-temporal motion features from a 3D\nResNeXt-101 backbone and static visual semantics from CLIP. The two streams are\nfused via an attention-based mechanism, while co-attention modules refine the\nvisual, textual, and audio features for effective multi-modal aggregation.\nComparative experiments demonstrate both the generali-zation capability of FMNV\nacross multiple baselines and the superior detec-tion efficacy of FMNVD. This\nwork establishes critical benchmarks for de-tecting high-impact fake news in\nmedia ecosystems while advancing meth-odologies for cross-modal inconsistency\nanalysis. Our dataset is available in https://github.com/DennisIW/FMNV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "News media, particularly video-based platforms, have become deeply embed-ded\nin daily life, concurrently amplifying the risks of misinformation\ndissem-ination. Consequently, multimodal fake news detection has garnered\nsignifi-cant research attention. However, existing datasets predominantly\ncomprise user-generated videos characterized by crude editing and limited\npublic en-gagement, whereas professionally crafted fake news videos\ndisseminated by media outlets-often politically or virally motivated-pose\nsubstantially greater societal harm. To address this gap, we construct FMNV, a\nnovel da-taset exclusively composed of news videos published by media\norganizations. Through empirical analysis of existing datasets and our curated\ncollection, we categorize fake news videos into four distinct types. Building\nupon this taxonomy, we employ Large Language Models (LLMs) to automatically\ngenerate deceptive content by manipulating authentic media-published news\nvideos. Furthermore, we propose FMNVD, a baseline model featuring a dual-stream\narchitecture that integrates spatio-temporal motion features from a 3D\nResNeXt-101 backbone and static visual semantics from CLIP. The two streams are\nfused via an attention-based mechanism, while co-attention modules refine the\nvisual, textual, and audio features for effective multi-modal aggregation.\nComparative experiments demonstrate both the generali-zation capability of FMNV\nacross multiple baselines and the superior detec-tion efficacy of FMNVD. This\nwork establishes critical benchmarks for de-tecting high-impact fake news in\nmedia ecosystems while advancing meth-odologies for cross-modal inconsistency\nanalysis. Our dataset is available in https://github.com/DennisIW/FMNV."
                },
                "authors": [
                    {
                        "name": "Yihao Wang"
                    },
                    {
                        "name": "Zhong Qian"
                    },
                    {
                        "name": "Peifeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Peifeng Li"
                },
                "author": "Peifeng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07687v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07687v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16972v2",
                "updated": "2025-05-13T14:08:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    8,
                    44,
                    1,
                    133,
                    0
                ],
                "published": "2025-03-21T09:41:12Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    41,
                    12,
                    4,
                    80,
                    0
                ],
                "title": "Governance of Ledger-Anchored Decentralized Identifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Governance of Ledger-Anchored Decentralized Identifiers"
                },
                "summary": "A Decentralized Identifier (DID) empowers an entity to prove control over a\nunique and self-issued identifier without relying on any identity provider. The\npublic key material for the proof is encoded into an associated DID document\n(DDO). This is preferable shared via a distributed ledger because it guarantees\nalgorithmically that everyone has access to the latest state of any\ntamper-proof DDO but only the entities in control of a DID are able to update\ntheirs. Yet, it is possible to grant deputies the authority to update the DDO\non behalf of the DID owner. However, the DID specification leaves largely open\non how authorizations over a DDO are managed and enforced among multiple\ndeputies. This article investigates what it means to govern a DID and discusses\nvarious forms of how a DID can be controlled by potentially more than one\nentity. It also presents a prototype of a DID-conform identifier management\nsystem where a selected set of governance policies are deployed as Smart\nContracts. The article highlights the critical role of governance for the\ntrustworthy and flexible deployment of ledger-anchored DIDs across various\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Decentralized Identifier (DID) empowers an entity to prove control over a\nunique and self-issued identifier without relying on any identity provider. The\npublic key material for the proof is encoded into an associated DID document\n(DDO). This is preferable shared via a distributed ledger because it guarantees\nalgorithmically that everyone has access to the latest state of any\ntamper-proof DDO but only the entities in control of a DID are able to update\ntheirs. Yet, it is possible to grant deputies the authority to update the DDO\non behalf of the DID owner. However, the DID specification leaves largely open\non how authorizations over a DDO are managed and enforced among multiple\ndeputies. This article investigates what it means to govern a DID and discusses\nvarious forms of how a DID can be controlled by potentially more than one\nentity. It also presents a prototype of a DID-conform identifier management\nsystem where a selected set of governance policies are deployed as Smart\nContracts. The article highlights the critical role of governance for the\ntrustworthy and flexible deployment of ledger-anchored DIDs across various\ndomains."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Carlo Segat"
                    },
                    {
                        "name": "Axel Küpper"
                    }
                ],
                "author_detail": {
                    "name": "Axel Küpper"
                },
                "author": "Axel Küpper",
                "arxiv_comment": "Accepted for presentation at the Crypto Valley Conference 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08590v1",
                "updated": "2025-05-13T14:01:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    1,
                    35,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T14:01:35Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    14,
                    1,
                    35,
                    1,
                    133,
                    0
                ],
                "title": "Enhancing Thyroid Cytology Diagnosis with RAG-Optimized LLMs and\n  Pa-thology Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Thyroid Cytology Diagnosis with RAG-Optimized LLMs and\n  Pa-thology Foundation Models"
                },
                "summary": "Advancements in artificial intelligence (AI) are transforming pathology by\nintegrat-ing large language models (LLMs) with retrieval-augmented generation\n(RAG) and domain-specific foundation models. This study explores the\napplication of RAG-enhanced LLMs coupled with pathology foundation models for\nthyroid cytology diagnosis, addressing challenges in cytological\ninterpretation, standardization, and diagnostic accuracy. By leveraging a\ncurated knowledge base, RAG facilitates dy-namic retrieval of relevant case\nstudies, diagnostic criteria, and expert interpreta-tion, improving the\ncontextual understanding of LLMs. Meanwhile, pathology foun-dation models,\ntrained on high-resolution pathology images, refine feature extrac-tion and\nclassification capabilities. The fusion of these AI-driven approaches en-hances\ndiagnostic consistency, reduces variability, and supports pathologists in\ndis-tinguishing benign from malignant thyroid lesions. Our results demonstrate\nthat integrating RAG with pathology-specific LLMs significantly improves\ndiagnostic efficiency and interpretability, paving the way for AI-assisted\nthyroid cytopathology, with foundation model UNI achieving AUC 0.73-0.93 for\ncorrect prediction of surgi-cal pathology diagnosis from thyroid cytology\nsamples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in artificial intelligence (AI) are transforming pathology by\nintegrat-ing large language models (LLMs) with retrieval-augmented generation\n(RAG) and domain-specific foundation models. This study explores the\napplication of RAG-enhanced LLMs coupled with pathology foundation models for\nthyroid cytology diagnosis, addressing challenges in cytological\ninterpretation, standardization, and diagnostic accuracy. By leveraging a\ncurated knowledge base, RAG facilitates dy-namic retrieval of relevant case\nstudies, diagnostic criteria, and expert interpreta-tion, improving the\ncontextual understanding of LLMs. Meanwhile, pathology foun-dation models,\ntrained on high-resolution pathology images, refine feature extrac-tion and\nclassification capabilities. The fusion of these AI-driven approaches en-hances\ndiagnostic consistency, reduces variability, and supports pathologists in\ndis-tinguishing benign from malignant thyroid lesions. Our results demonstrate\nthat integrating RAG with pathology-specific LLMs significantly improves\ndiagnostic efficiency and interpretability, paving the way for AI-assisted\nthyroid cytopathology, with foundation model UNI achieving AUC 0.73-0.93 for\ncorrect prediction of surgi-cal pathology diagnosis from thyroid cytology\nsamples."
                },
                "authors": [
                    {
                        "name": "Hussien Al-Asi"
                    },
                    {
                        "name": "Jordan P Reynolds"
                    },
                    {
                        "name": "Shweta Agarwal"
                    },
                    {
                        "name": "Bryan J Dangott"
                    },
                    {
                        "name": "Aziza Nassar"
                    },
                    {
                        "name": "Zeynettin Akkus"
                    }
                ],
                "author_detail": {
                    "name": "Zeynettin Akkus"
                },
                "author": "Zeynettin Akkus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08588v1",
                "updated": "2025-05-13T13:58:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    58,
                    29,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T13:58:29Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    58,
                    29,
                    1,
                    133,
                    0
                ],
                "title": "Small but Significant: On the Promise of Small Language Models for\n  Accessible AIED",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small but Significant: On the Promise of Small Language Models for\n  Accessible AIED"
                },
                "summary": "GPT has become nearly synonymous with large language models (LLMs), an\nincreasingly popular term in AIED proceedings. A simple keyword-based search\nreveals that 61% of the 76 long and short papers presented at AIED 2024\ndescribe novel solutions using LLMs to address some of the long-standing\nchallenges in education, and 43% specifically mention GPT. Although LLMs\npioneered by GPT create exciting opportunities to strengthen the impact of AI\non education, we argue that the field's predominant focus on GPT and other\nresource-intensive LLMs (with more than 10B parameters) risks neglecting the\npotential impact that small language models (SLMs) can make in providing\nresource-constrained institutions with equitable and affordable access to\nhigh-quality AI tools. Supported by positive results on knowledge component\n(KC) discovery, a critical challenge in AIED, we demonstrate that SLMs such as\nPhi-2 can produce an effective solution without elaborate prompting strategies.\nHence, we call for more attention to developing SLM-based AIED approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT has become nearly synonymous with large language models (LLMs), an\nincreasingly popular term in AIED proceedings. A simple keyword-based search\nreveals that 61% of the 76 long and short papers presented at AIED 2024\ndescribe novel solutions using LLMs to address some of the long-standing\nchallenges in education, and 43% specifically mention GPT. Although LLMs\npioneered by GPT create exciting opportunities to strengthen the impact of AI\non education, we argue that the field's predominant focus on GPT and other\nresource-intensive LLMs (with more than 10B parameters) risks neglecting the\npotential impact that small language models (SLMs) can make in providing\nresource-constrained institutions with equitable and affordable access to\nhigh-quality AI tools. Supported by positive results on knowledge component\n(KC) discovery, a critical challenge in AIED, we demonstrate that SLMs such as\nPhi-2 can produce an effective solution without elaborate prompting strategies.\nHence, we call for more attention to developing SLM-based AIED approaches."
                },
                "authors": [
                    {
                        "name": "Yumou Wei"
                    },
                    {
                        "name": "Paulo Carvalho"
                    },
                    {
                        "name": "John Stamper"
                    }
                ],
                "author_detail": {
                    "name": "John Stamper"
                },
                "author": "John Stamper",
                "arxiv_comment": "This vision paper advocates using small language models (e.g., Phi-2)\n  in AI for education (AIED)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08585v1",
                "updated": "2025-05-13T13:56:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    56,
                    43,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T13:56:43Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    56,
                    43,
                    1,
                    133,
                    0
                ],
                "title": "A Large-scale Benchmark on Geological Fault Delineation Models: Domain\n  Shift, Training Dynamics, Generalizability, Evaluation and Inferential\n  Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Large-scale Benchmark on Geological Fault Delineation Models: Domain\n  Shift, Training Dynamics, Generalizability, Evaluation and Inferential\n  Behavior"
                },
                "summary": "Machine learning has taken a critical role in seismic interpretation\nworkflows, especially in fault delineation tasks. However, despite the recent\nproliferation of pretrained models and synthetic datasets, the field still\nlacks a systematic understanding of the generalizability limits of these models\nacross seismic data representing a variety of geologic, acquisition and\nprocessing settings. Distributional shifts between different data sources,\nlimitations in fine-tuning strategies and labeled data accessibility, and\ninconsistent evaluation protocols all represent major roadblocks in the\ndeployment of reliable and robust models in real-world exploration settings. In\nthis paper, we present the first large-scale benchmarking study explicitly\ndesigned to provide answers and guidelines for domain shift strategies in\nseismic interpretation. Our benchmark encompasses over $200$ models trained and\nevaluated on three heterogeneous datasets (synthetic and real data) including\nFaultSeg3D, CRACKS, and Thebe. We systematically assess pretraining,\nfine-tuning, and joint training strategies under varying degrees of domain\nshift. Our analysis highlights the fragility of current fine-tuning practices,\nthe emergence of catastrophic forgetting, and the challenges of interpreting\nperformance in a systematic manner. We establish a robust experimental baseline\nto provide insights into the tradeoffs inherent to current fault delineation\nworkflows, and shed light on directions for developing more generalizable,\ninterpretable and effective machine learning models for seismic interpretation.\nThe insights and analyses reported provide a set of guidelines on the\ndeployment of fault delineation models within seismic interpretation workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning has taken a critical role in seismic interpretation\nworkflows, especially in fault delineation tasks. However, despite the recent\nproliferation of pretrained models and synthetic datasets, the field still\nlacks a systematic understanding of the generalizability limits of these models\nacross seismic data representing a variety of geologic, acquisition and\nprocessing settings. Distributional shifts between different data sources,\nlimitations in fine-tuning strategies and labeled data accessibility, and\ninconsistent evaluation protocols all represent major roadblocks in the\ndeployment of reliable and robust models in real-world exploration settings. In\nthis paper, we present the first large-scale benchmarking study explicitly\ndesigned to provide answers and guidelines for domain shift strategies in\nseismic interpretation. Our benchmark encompasses over $200$ models trained and\nevaluated on three heterogeneous datasets (synthetic and real data) including\nFaultSeg3D, CRACKS, and Thebe. We systematically assess pretraining,\nfine-tuning, and joint training strategies under varying degrees of domain\nshift. Our analysis highlights the fragility of current fine-tuning practices,\nthe emergence of catastrophic forgetting, and the challenges of interpreting\nperformance in a systematic manner. We establish a robust experimental baseline\nto provide insights into the tradeoffs inherent to current fault delineation\nworkflows, and shed light on directions for developing more generalizable,\ninterpretable and effective machine learning models for seismic interpretation.\nThe insights and analyses reported provide a set of guidelines on the\ndeployment of fault delineation models within seismic interpretation workflows."
                },
                "authors": [
                    {
                        "name": "Jorge Quesada"
                    },
                    {
                        "name": "Chen Zhou"
                    },
                    {
                        "name": "Prithwijit Chowdhury"
                    },
                    {
                        "name": "Mohammad Alotaibi"
                    },
                    {
                        "name": "Ahmad Mustafa"
                    },
                    {
                        "name": "Yusufjon Kumamnov"
                    },
                    {
                        "name": "Mohit Prabhushankar"
                    },
                    {
                        "name": "Ghassan AlRegib"
                    }
                ],
                "author_detail": {
                    "name": "Ghassan AlRegib"
                },
                "author": "Ghassan AlRegib",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08574v1",
                "updated": "2025-05-13T13:46:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    46,
                    35,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T13:46:35Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    46,
                    35,
                    1,
                    133,
                    0
                ],
                "title": "End-to-End Multi-Task Policy Learning from NMPC for Quadruped Locomotion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Multi-Task Policy Learning from NMPC for Quadruped Locomotion"
                },
                "summary": "Quadruped robots excel in traversing complex, unstructured environments where\nwheeled robots often fail. However, enabling efficient and adaptable locomotion\nremains challenging due to the quadrupeds' nonlinear dynamics, high degrees of\nfreedom, and the computational demands of real-time control. Optimization-based\ncontrollers, such as Nonlinear Model Predictive Control (NMPC), have shown\nstrong performance, but their reliance on accurate state estimation and high\ncomputational overhead makes deployment in real-world settings challenging. In\nthis work, we present a Multi-Task Learning (MTL) framework in which expert\nNMPC demonstrations are used to train a single neural network to predict\nactions for multiple locomotion behaviors directly from raw proprioceptive\nsensor inputs. We evaluate our approach extensively on the quadruped robot Go1,\nboth in simulation and on real hardware, demonstrating that it accurately\nreproduces expert behavior, allows smooth gait switching, and simplifies the\ncontrol pipeline for real-time deployment. Our MTL architecture enables\nlearning diverse gaits within a unified policy, achieving high $R^{2}$ scores\nfor predicted joint targets across all tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quadruped robots excel in traversing complex, unstructured environments where\nwheeled robots often fail. However, enabling efficient and adaptable locomotion\nremains challenging due to the quadrupeds' nonlinear dynamics, high degrees of\nfreedom, and the computational demands of real-time control. Optimization-based\ncontrollers, such as Nonlinear Model Predictive Control (NMPC), have shown\nstrong performance, but their reliance on accurate state estimation and high\ncomputational overhead makes deployment in real-world settings challenging. In\nthis work, we present a Multi-Task Learning (MTL) framework in which expert\nNMPC demonstrations are used to train a single neural network to predict\nactions for multiple locomotion behaviors directly from raw proprioceptive\nsensor inputs. We evaluate our approach extensively on the quadruped robot Go1,\nboth in simulation and on real hardware, demonstrating that it accurately\nreproduces expert behavior, allows smooth gait switching, and simplifies the\ncontrol pipeline for real-time deployment. Our MTL architecture enables\nlearning diverse gaits within a unified policy, achieving high $R^{2}$ scores\nfor predicted joint targets across all tasks."
                },
                "authors": [
                    {
                        "name": "Anudeep Sajja"
                    },
                    {
                        "name": "Shahram Khorshidi"
                    },
                    {
                        "name": "Sebastian Houben"
                    },
                    {
                        "name": "Maren Bennewitz"
                    }
                ],
                "author_detail": {
                    "name": "Maren Bennewitz"
                },
                "author": "Maren Bennewitz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08566v1",
                "updated": "2025-05-13T13:41:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    41,
                    4,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T13:41:04Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    41,
                    4,
                    1,
                    133,
                    0
                ],
                "title": "Extract the Best, Discard the Rest: CSI Feedback with Offline Large AI\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extract the Best, Discard the Rest: CSI Feedback with Offline Large AI\n  Models"
                },
                "summary": "Large AI models (LAMs) have shown strong potential in wireless communication\ntasks, but their practical deployment remains hindered by latency and\ncomputational constraints. In this work, we focus on the challenge of\nintegrating LAMs into channel state information (CSI) feedback for\nfrequency-division duplex (FDD) massive multiple-intput multiple-output (MIMO)\nsystems. To this end, we propose two offline frameworks, namely site-specific\nLAM-enhanced CSI feedback (SSLCF) and multi-scenario LAM-enhanced CSI feedback\n(MSLCF), that incorporate LAMs into the codebook-based CSI feedback paradigm\nwithout requiring real-time inference. Specifically, SSLCF generates a\nsite-specific enhanced codebook through fine-tuning on locally collected CSI\ndata, while MSLCF improves generalization by pre-generating a set of\nenvironment-aware codebooks. Both of these frameworks build upon the LAM with\nvision-based backbone, which is pre-trained on large-scale image datasets and\nfine-tuned with CSI data to generate customized codebooks. This resulting\nnetwork named LVM4CF captures the structural similarity between CSI and image,\nallowing the LAM to refine codewords tailored to the specific environments. To\noptimize the codebook refinement capability of LVM4CF under both single- and\ndual-side deployment modes, we further propose corresponding training and\ninference algorithms. Simulation results show that our frameworks significantly\noutperform existing schemes in both reconstruction accuracy and system\nthroughput, without introducing additional inference latency or computational\noverhead. These results also support the core design methodology of our\nproposed frameworks, extracting the best and discarding the rest, as a\npromising pathway for integrating LAMs into future wireless systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large AI models (LAMs) have shown strong potential in wireless communication\ntasks, but their practical deployment remains hindered by latency and\ncomputational constraints. In this work, we focus on the challenge of\nintegrating LAMs into channel state information (CSI) feedback for\nfrequency-division duplex (FDD) massive multiple-intput multiple-output (MIMO)\nsystems. To this end, we propose two offline frameworks, namely site-specific\nLAM-enhanced CSI feedback (SSLCF) and multi-scenario LAM-enhanced CSI feedback\n(MSLCF), that incorporate LAMs into the codebook-based CSI feedback paradigm\nwithout requiring real-time inference. Specifically, SSLCF generates a\nsite-specific enhanced codebook through fine-tuning on locally collected CSI\ndata, while MSLCF improves generalization by pre-generating a set of\nenvironment-aware codebooks. Both of these frameworks build upon the LAM with\nvision-based backbone, which is pre-trained on large-scale image datasets and\nfine-tuned with CSI data to generate customized codebooks. This resulting\nnetwork named LVM4CF captures the structural similarity between CSI and image,\nallowing the LAM to refine codewords tailored to the specific environments. To\noptimize the codebook refinement capability of LVM4CF under both single- and\ndual-side deployment modes, we further propose corresponding training and\ninference algorithms. Simulation results show that our frameworks significantly\noutperform existing schemes in both reconstruction accuracy and system\nthroughput, without introducing additional inference latency or computational\noverhead. These results also support the core design methodology of our\nproposed frameworks, extracting the best and discarding the rest, as a\npromising pathway for integrating LAMs into future wireless systems."
                },
                "authors": [
                    {
                        "name": "Jialin Zhuang"
                    },
                    {
                        "name": "Yafei Wang"
                    },
                    {
                        "name": "Hongwei Hou"
                    },
                    {
                        "name": "Yu Han"
                    },
                    {
                        "name": "Wenjin Wang"
                    },
                    {
                        "name": "Shi Jin"
                    },
                    {
                        "name": "Jiangzhou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangzhou Wang"
                },
                "author": "Jiangzhou Wang",
                "arxiv_comment": "This work has been submitted to the IEEE for possible\n  publication.Copyright may be transferred without notice, after which this\n  version may no longer be accessible",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02423v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02423v2",
                "updated": "2025-05-13T13:19:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    19,
                    32,
                    1,
                    133,
                    0
                ],
                "published": "2025-01-05T02:30:41Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    2,
                    30,
                    41,
                    6,
                    5,
                    0
                ],
                "title": "Scaling Laws for Floating Point Quantization Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Laws for Floating Point Quantization Training"
                },
                "summary": "Low-precision training is considered an effective strategy for reducing both\ntraining and downstream inference costs. Previous scaling laws for precision\nmainly focus on integer quantization, which pay less attention to the\nconstituents in floating-point (FP) quantization, and thus cannot well fit the\nLLM losses in this scenario. In contrast, while FP quantization training is\nmore commonly implemented in production, it's research has been relatively\nsuperficial. In this paper, we thoroughly explore the effects of FP\nquantization targets, exponent bits, mantissa bits, and the calculation\ngranularity of the scaling factor in FP quantization training performance of\nLLM models. In addition to an accurate FP quantization unified scaling law, we\nalso provide valuable suggestions for the community: (1) Exponent bits\ncontribute slightly more to the model performance than mantissa bits. We\nprovide the optimal exponent-mantissa bit ratio for different bit numbers,\nwhich is available for future reference by hardware manufacturers; (2) We\ndiscover the formation of the critical data size in low-precision LLM training.\nToo much training data exceeding the critical data size will inversely bring in\ndegradation of LLM performance; (3) The optimal FP quantization precision is\ndirectly proportional to the computational power, but within a wide\ncomputational power range. We estimate that the best cost-performance precision\nshould lie between 4-8 bits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-precision training is considered an effective strategy for reducing both\ntraining and downstream inference costs. Previous scaling laws for precision\nmainly focus on integer quantization, which pay less attention to the\nconstituents in floating-point (FP) quantization, and thus cannot well fit the\nLLM losses in this scenario. In contrast, while FP quantization training is\nmore commonly implemented in production, it's research has been relatively\nsuperficial. In this paper, we thoroughly explore the effects of FP\nquantization targets, exponent bits, mantissa bits, and the calculation\ngranularity of the scaling factor in FP quantization training performance of\nLLM models. In addition to an accurate FP quantization unified scaling law, we\nalso provide valuable suggestions for the community: (1) Exponent bits\ncontribute slightly more to the model performance than mantissa bits. We\nprovide the optimal exponent-mantissa bit ratio for different bit numbers,\nwhich is available for future reference by hardware manufacturers; (2) We\ndiscover the formation of the critical data size in low-precision LLM training.\nToo much training data exceeding the critical data size will inversely bring in\ndegradation of LLM performance; (3) The optimal FP quantization precision is\ndirectly proportional to the computational power, but within a wide\ncomputational power range. We estimate that the best cost-performance precision\nshould lie between 4-8 bits."
                },
                "authors": [
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Weidong Han"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Yixing Li"
                    },
                    {
                        "name": "An Wang"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Jie Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Jiang"
                },
                "author": "Jie Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02423v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02423v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07142v2",
                "updated": "2025-05-13T13:15:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    15,
                    18,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-11T22:58:26Z",
                "published_parsed": [
                    2025,
                    5,
                    11,
                    22,
                    58,
                    26,
                    6,
                    131,
                    0
                ],
                "title": "Exploring Anthropomorphism in Conversational Agents for Environmental\n  Sustainability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Anthropomorphism in Conversational Agents for Environmental\n  Sustainability"
                },
                "summary": "The paper investigates the integration of Large Language Models (LLMs) into\nConversational Agents (CAs) to encourage a shift in consumption patterns from a\ndemand-driven to a supply-based paradigm. Specifically, the research examines\nthe role of anthropomorphic design in delivering environmentally conscious\nmessages by comparing two CA designs: a personified agent representing an\nappliance and a traditional, non-personified assistant. A lab study (N=26)\nassessed the impact of these designs on interaction, perceived self-efficacy,\nand engagement. Results indicate that LLM-based CAs significantly enhance\nusers' self-reported eco-friendly behaviors, with participants expressing\ngreater confidence in managing energy consumption. While the anthropomorphic\ndesign did not notably affect self-efficacy, those interacting with the\npersonified agent reported a stronger sense of connection with the system.\nThese findings suggest that although anthropomorphic CAs may improve user\nengagement, both designs hold promise for fostering sustainable behaviors in\nhome energy management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paper investigates the integration of Large Language Models (LLMs) into\nConversational Agents (CAs) to encourage a shift in consumption patterns from a\ndemand-driven to a supply-based paradigm. Specifically, the research examines\nthe role of anthropomorphic design in delivering environmentally conscious\nmessages by comparing two CA designs: a personified agent representing an\nappliance and a traditional, non-personified assistant. A lab study (N=26)\nassessed the impact of these designs on interaction, perceived self-efficacy,\nand engagement. Results indicate that LLM-based CAs significantly enhance\nusers' self-reported eco-friendly behaviors, with participants expressing\ngreater confidence in managing energy consumption. While the anthropomorphic\ndesign did not notably affect self-efficacy, those interacting with the\npersonified agent reported a stronger sense of connection with the system.\nThese findings suggest that although anthropomorphic CAs may improve user\nengagement, both designs hold promise for fostering sustainable behaviors in\nhome energy management."
                },
                "authors": [
                    {
                        "name": "Mathyas Giudici"
                    },
                    {
                        "name": "Samuele Scherini"
                    },
                    {
                        "name": "Pascal Chaussumier"
                    },
                    {
                        "name": "Stefano Ginocchio"
                    },
                    {
                        "name": "Franca Garzotto"
                    }
                ],
                "author_detail": {
                    "name": "Franca Garzotto"
                },
                "author": "Franca Garzotto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04717v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04717v4",
                "updated": "2025-05-14T01:48:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    1,
                    48,
                    30,
                    2,
                    134,
                    0
                ],
                "published": "2025-04-07T04:00:08Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    4,
                    0,
                    8,
                    0,
                    97,
                    0
                ],
                "title": "Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large\n  Language Models"
                },
                "summary": "Recent advancements in large language models (LLMs) have revolutionized their\nability to handle single-turn tasks, yet real-world applications demand\nsophisticated multi-turn interactions. This survey provides a comprehensive\nreview of recent advancements in evaluating and enhancing multi-turn\ninteractions in LLMs. Focusing on task-specific scenarios, from instruction\nfollowing in diverse domains such as math and coding to complex conversational\nengagements in roleplay, healthcare, education, and even adversarial jailbreak\nsettings, we systematically examine the challenges of maintaining context,\ncoherence, fairness, and responsiveness over prolonged dialogues. The paper\norganizes current benchmarks and datasets into coherent categories that reflect\nthe evolving landscape of multi-turn dialogue evaluation. In addition, we\nreview a range of enhancement methodologies under multi-turn settings,\nincluding model-centric strategies (contextual learning, supervised\nfine-tuning, reinforcement learning, and new architectures), external\nintegration approaches (memory-augmented, retrieval-based methods, and\nknowledge graph), and agent-based techniques for collaborative interactions.\nFinally, we discuss open challenges and propose future directions for research\nto further advance the robustness and effectiveness of multi-turn interactions\nin LLMs. Related resources and papers are available at\nhttps://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have revolutionized their\nability to handle single-turn tasks, yet real-world applications demand\nsophisticated multi-turn interactions. This survey provides a comprehensive\nreview of recent advancements in evaluating and enhancing multi-turn\ninteractions in LLMs. Focusing on task-specific scenarios, from instruction\nfollowing in diverse domains such as math and coding to complex conversational\nengagements in roleplay, healthcare, education, and even adversarial jailbreak\nsettings, we systematically examine the challenges of maintaining context,\ncoherence, fairness, and responsiveness over prolonged dialogues. The paper\norganizes current benchmarks and datasets into coherent categories that reflect\nthe evolving landscape of multi-turn dialogue evaluation. In addition, we\nreview a range of enhancement methodologies under multi-turn settings,\nincluding model-centric strategies (contextual learning, supervised\nfine-tuning, reinforcement learning, and new architectures), external\nintegration approaches (memory-augmented, retrieval-based methods, and\nknowledge graph), and agent-based techniques for collaborative interactions.\nFinally, we discuss open challenges and propose future directions for research\nto further advance the robustness and effectiveness of multi-turn interactions\nin LLMs. Related resources and papers are available at\nhttps://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs."
                },
                "authors": [
                    {
                        "name": "Yubo Li"
                    },
                    {
                        "name": "Xiaobin Shen"
                    },
                    {
                        "name": "Xinyu Yao"
                    },
                    {
                        "name": "Xueying Ding"
                    },
                    {
                        "name": "Yidi Miao"
                    },
                    {
                        "name": "Ramayya Krishnan"
                    },
                    {
                        "name": "Rema Padman"
                    }
                ],
                "author_detail": {
                    "name": "Rema Padman"
                },
                "author": "Rema Padman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04717v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04717v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08542v1",
                "updated": "2025-05-13T13:13:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    13,
                    26,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T13:13:26Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    13,
                    26,
                    1,
                    133,
                    0
                ],
                "title": "Guiding LLM-based Smart Contract Generation with Finite State Machine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding LLM-based Smart Contract Generation with Finite State Machine"
                },
                "summary": "Smart contract is a kind of self-executing code based on blockchain\ntechnology with a wide range of application scenarios, but the traditional\ngeneration method relies on manual coding and expert auditing, which has a high\nthreshold and low efficiency. Although Large Language Models (LLMs) show great\npotential in programming tasks, they still face challenges in smart contract\ngeneration w.r.t. effectiveness and security. To solve these problems, we\npropose FSM-SCG, a smart contract generation framework based on finite state\nmachine (FSM) and LLMs, which significantly improves the quality of the\ngenerated code by abstracting user requirements to generate FSM, guiding LLMs\nto generate smart contracts, and iteratively optimizing the code with the\nfeedback of compilation and security checks. The experimental results show that\nFSM-SCG significantly improves the quality of smart contract generation.\nCompared to the best baseline, FSM-SCG improves the compilation success rate of\ngenerated smart contract code by at most 48%, and reduces the average\nvulnerability risk score by approximately 68%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart contract is a kind of self-executing code based on blockchain\ntechnology with a wide range of application scenarios, but the traditional\ngeneration method relies on manual coding and expert auditing, which has a high\nthreshold and low efficiency. Although Large Language Models (LLMs) show great\npotential in programming tasks, they still face challenges in smart contract\ngeneration w.r.t. effectiveness and security. To solve these problems, we\npropose FSM-SCG, a smart contract generation framework based on finite state\nmachine (FSM) and LLMs, which significantly improves the quality of the\ngenerated code by abstracting user requirements to generate FSM, guiding LLMs\nto generate smart contracts, and iteratively optimizing the code with the\nfeedback of compilation and security checks. The experimental results show that\nFSM-SCG significantly improves the quality of smart contract generation.\nCompared to the best baseline, FSM-SCG improves the compilation success rate of\ngenerated smart contract code by at most 48%, and reduces the average\nvulnerability risk score by approximately 68%."
                },
                "authors": [
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Yuhao Lin"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Xintong Hu"
                    },
                    {
                        "name": "Yuxiang Wang"
                    },
                    {
                        "name": "Qiming Zeng"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Jiawei Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Jiang"
                },
                "author": "Jiawei Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00782v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00782v2",
                "updated": "2025-05-13T13:13:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    13,
                    18,
                    1,
                    133,
                    0
                ],
                "published": "2024-10-16T20:24:16Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    20,
                    24,
                    16,
                    2,
                    290,
                    0
                ],
                "title": "TradExpert: Revolutionizing Trading with Mixture of Expert LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TradExpert: Revolutionizing Trading with Mixture of Expert LLMs"
                },
                "summary": "The integration of Artificial Intelligence (AI) in the financial domain has\nopened new avenues for quantitative trading, particularly through the use of\nLarge Language Models (LLMs). However, the challenge of effectively\nsynthesizing insights from diverse data sources and integrating both structured\nand unstructured data persists. This paper presents TradeExpert, a novel\nframework that employs a mix of experts (MoE) approach, using four specialized\nLLMs, each analyzing distinct sources of financial data, including news\narticles, market data, alpha factors, and fundamental data. The insights of\nthese expert LLMs are further synthesized by a General Expert LLM to make a\nfinal prediction or decision. With specific prompts, TradeExpert can be\nswitched between the prediction mode and the ranking mode for stock movement\nprediction and quantitative stock trading, respectively. In addition to\nexisting benchmarks, we also release a large-scale financial dataset to\ncomprehensively evaluate TradeExpert's effectiveness. Our experimental results\ndemonstrate TradeExpert's superior performance across all trading scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Artificial Intelligence (AI) in the financial domain has\nopened new avenues for quantitative trading, particularly through the use of\nLarge Language Models (LLMs). However, the challenge of effectively\nsynthesizing insights from diverse data sources and integrating both structured\nand unstructured data persists. This paper presents TradeExpert, a novel\nframework that employs a mix of experts (MoE) approach, using four specialized\nLLMs, each analyzing distinct sources of financial data, including news\narticles, market data, alpha factors, and fundamental data. The insights of\nthese expert LLMs are further synthesized by a General Expert LLM to make a\nfinal prediction or decision. With specific prompts, TradeExpert can be\nswitched between the prediction mode and the ranking mode for stock movement\nprediction and quantitative stock trading, respectively. In addition to\nexisting benchmarks, we also release a large-scale financial dataset to\ncomprehensively evaluate TradeExpert's effectiveness. Our experimental results\ndemonstrate TradeExpert's superior performance across all trading scenarios."
                },
                "authors": [
                    {
                        "name": "Qianggang Ding"
                    },
                    {
                        "name": "Haochen Shi"
                    },
                    {
                        "name": "Jiadong Guo"
                    },
                    {
                        "name": "Bang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Bang Liu"
                },
                "author": "Bang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00782v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00782v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08532v1",
                "updated": "2025-05-13T13:03:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    3,
                    20,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T13:03:20Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    3,
                    20,
                    1,
                    133,
                    0
                ],
                "title": "The Truth Becomes Clearer Through Debate! Multi-Agent Systems with Large\n  Language Models Unmask Fake News",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Truth Becomes Clearer Through Debate! Multi-Agent Systems with Large\n  Language Models Unmask Fake News"
                },
                "summary": "In today's digital environment, the rapid propagation of fake news via social\nnetworks poses significant social challenges. Most existing detection methods\neither employ traditional classification models, which suffer from low\ninterpretability and limited generalization capabilities, or craft specific\nprompts for large language models (LLMs) to produce explanations and results\ndirectly, failing to leverage LLMs' reasoning abilities fully. Inspired by the\nsaying that \"truth becomes clearer through debate,\" our study introduces a\nnovel multi-agent system with LLMs named TruEDebate (TED) to enhance the\ninterpretability and effectiveness of fake news detection. TED employs a\nrigorous debate process inspired by formal debate settings. Central to our\napproach are two innovative components: the DebateFlow Agents and the\nInsightFlow Agents. The DebateFlow Agents organize agents into two teams, where\none supports and the other challenges the truth of the news. These agents\nengage in opening statements, cross-examination, rebuttal, and closing\nstatements, simulating a rigorous debate process akin to human discourse\nanalysis, allowing for a thorough evaluation of news content. Concurrently, the\nInsightFlow Agents consist of two specialized sub-agents: the Synthesis Agent\nand the Analysis Agent. The Synthesis Agent summarizes the debates and provides\nan overarching viewpoint, ensuring a coherent and comprehensive evaluation. The\nAnalysis Agent, which includes a role-aware encoder and a debate graph,\nintegrates role embeddings and models the interactions between debate roles and\narguments using an attention mechanism, providing the final judgment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In today's digital environment, the rapid propagation of fake news via social\nnetworks poses significant social challenges. Most existing detection methods\neither employ traditional classification models, which suffer from low\ninterpretability and limited generalization capabilities, or craft specific\nprompts for large language models (LLMs) to produce explanations and results\ndirectly, failing to leverage LLMs' reasoning abilities fully. Inspired by the\nsaying that \"truth becomes clearer through debate,\" our study introduces a\nnovel multi-agent system with LLMs named TruEDebate (TED) to enhance the\ninterpretability and effectiveness of fake news detection. TED employs a\nrigorous debate process inspired by formal debate settings. Central to our\napproach are two innovative components: the DebateFlow Agents and the\nInsightFlow Agents. The DebateFlow Agents organize agents into two teams, where\none supports and the other challenges the truth of the news. These agents\nengage in opening statements, cross-examination, rebuttal, and closing\nstatements, simulating a rigorous debate process akin to human discourse\nanalysis, allowing for a thorough evaluation of news content. Concurrently, the\nInsightFlow Agents consist of two specialized sub-agents: the Synthesis Agent\nand the Analysis Agent. The Synthesis Agent summarizes the debates and provides\nan overarching viewpoint, ensuring a coherent and comprehensive evaluation. The\nAnalysis Agent, which includes a role-aware encoder and a debate graph,\nintegrates role embeddings and models the interactions between debate roles and\narguments using an attention mechanism, providing the final judgment."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuxuan Liu"
                    },
                    {
                        "name": "Xiaoqing Zhang"
                    },
                    {
                        "name": "Xiuying Chen"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "arxiv_comment": "SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08520v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08520v1",
                "updated": "2025-05-13T12:51:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    51,
                    6,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T12:51:06Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    51,
                    6,
                    1,
                    133,
                    0
                ],
                "title": "Towards Resilient SDA: Graph Theory and Cooperative Control in\n  Distributed Network Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Resilient SDA: Graph Theory and Cooperative Control in\n  Distributed Network Architectures"
                },
                "summary": "Space Domain Awareness (SDA) involves the detection, tracking, and\ncharacterization of space objects through the fusion of data across the space\nenvironment. As SDA advances beyond localized or operator-specific\ncapabilities, there is a growing reliance on in-domain space assets for\nreal-time, distributed sensing and decision-making. This paper investigates the\npotential of on-orbit collaboration by enabling data sharing among\nheterogeneous satellites as actuators within a single orbital regime. Using\ngraph-theoretic constructs, we define regions of spatial responsibility via\nVoronoi tessellations and model communication pathways between actuators using\nDelaunay triangulation. We apply this framework independently to Low Earth\nOrbit (LEO), Medium Earth Orbit (MEO), Highly Elliptical Orbit (HEO), and\nGeostationary Orbit (GEO), and analyze each to quantify structural properties\nrelevant to efficient communication, cooperative control, and synchronization\nfor SDA operations with the growth in deployments of space assets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Space Domain Awareness (SDA) involves the detection, tracking, and\ncharacterization of space objects through the fusion of data across the space\nenvironment. As SDA advances beyond localized or operator-specific\ncapabilities, there is a growing reliance on in-domain space assets for\nreal-time, distributed sensing and decision-making. This paper investigates the\npotential of on-orbit collaboration by enabling data sharing among\nheterogeneous satellites as actuators within a single orbital regime. Using\ngraph-theoretic constructs, we define regions of spatial responsibility via\nVoronoi tessellations and model communication pathways between actuators using\nDelaunay triangulation. We apply this framework independently to Low Earth\nOrbit (LEO), Medium Earth Orbit (MEO), Highly Elliptical Orbit (HEO), and\nGeostationary Orbit (GEO), and analyze each to quantify structural properties\nrelevant to efficient communication, cooperative control, and synchronization\nfor SDA operations with the growth in deployments of space assets."
                },
                "authors": [
                    {
                        "name": "Nesrine Benchoubane"
                    },
                    {
                        "name": "Gunes Karabulut Kurt"
                    }
                ],
                "author_detail": {
                    "name": "Gunes Karabulut Kurt"
                },
                "author": "Gunes Karabulut Kurt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08520v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01844v3",
                "updated": "2025-05-13T12:45:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    45,
                    16,
                    1,
                    133,
                    0
                ],
                "published": "2025-03-03T18:59:54Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    59,
                    54,
                    0,
                    62,
                    0
                ],
                "title": "Can (A)I Change Your Mind?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can (A)I Change Your Mind?"
                },
                "summary": "The increasing integration of large language models (LLMs) based\nconversational agents into everyday life raises critical cognitive and social\nquestions about their potential to influence human opinions. Although previous\nstudies have shown that LLM-based agents can generate persuasive content, these\ntypically involve controlled English-language settings. Addressing this, our\npreregistered study explored LLMs' persuasive capabilities in more ecological,\nunconstrained scenarios, examining both static (written paragraphs) and dynamic\n(conversations via Telegram) interaction types. Conducted entirely in Hebrew\nwith 200 participants, the study assessed the persuasive effects of both LLM\nand human interlocutors on controversial civil policy topics. Results indicated\nthat participants adopted LLM and human perspectives similarly, with\nsignificant opinion changes evident across all conditions, regardless of\ninterlocutor type or interaction mode. Confidence levels increased\nsignificantly in most scenarios. These findings demonstrate LLM-based agents'\nrobust persuasive capabilities across diverse sources and settings,\nhighlighting their potential impact on shaping public opinions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing integration of large language models (LLMs) based\nconversational agents into everyday life raises critical cognitive and social\nquestions about their potential to influence human opinions. Although previous\nstudies have shown that LLM-based agents can generate persuasive content, these\ntypically involve controlled English-language settings. Addressing this, our\npreregistered study explored LLMs' persuasive capabilities in more ecological,\nunconstrained scenarios, examining both static (written paragraphs) and dynamic\n(conversations via Telegram) interaction types. Conducted entirely in Hebrew\nwith 200 participants, the study assessed the persuasive effects of both LLM\nand human interlocutors on controversial civil policy topics. Results indicated\nthat participants adopted LLM and human perspectives similarly, with\nsignificant opinion changes evident across all conditions, regardless of\ninterlocutor type or interaction mode. Confidence levels increased\nsignificantly in most scenarios. These findings demonstrate LLM-based agents'\nrobust persuasive capabilities across diverse sources and settings,\nhighlighting their potential impact on shaping public opinions."
                },
                "authors": [
                    {
                        "name": "Miriam Havin"
                    },
                    {
                        "name": "Timna Wharton Kleinman"
                    },
                    {
                        "name": "Moran Koren"
                    },
                    {
                        "name": "Yaniv Dover"
                    },
                    {
                        "name": "Ariel Goldstein"
                    }
                ],
                "author_detail": {
                    "name": "Ariel Goldstein"
                },
                "author": "Ariel Goldstein",
                "arxiv_comment": "Accetped to CogSci 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03906v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03906v2",
                "updated": "2025-05-13T12:41:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    41,
                    18,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-06T18:22:38Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    18,
                    22,
                    38,
                    1,
                    126,
                    0
                ],
                "title": "MARCO: A Multi-Agent System for Optimizing HPC Code Generation Using\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARCO: A Multi-Agent System for Optimizing HPC Code Generation Using\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) have transformed software development through\ncode generation capabilities, yet their effectiveness for high-performance\ncomputing (HPC) remains limited. HPC code requires specialized optimizations\nfor parallelism, memory efficiency, and architecture-specific considerations\nthat general-purpose LLMs often overlook. We present MARCO (Multi-Agent\nReactive Code Optimizer), a novel framework that enhances LLM-generated code\nfor HPC through a specialized multi-agent architecture. MARCO employs separate\nagents for code generation and performance evaluation, connected by a feedback\nloop that progressively refines optimizations. A key innovation is MARCO's\nweb-search component that retrieves real-time optimization techniques from\nrecent conference proceedings and research publications, bridging the knowledge\ngap in pre-trained LLMs. Our extensive evaluation on the LeetCode 75 problem\nset demonstrates that MARCO achieves a 14.6% average runtime reduction compared\nto Claude 3.5 Sonnet alone, while the integration of the web-search component\nyields a 30.9% performance improvement over the base MARCO system. These\nresults highlight the potential of multi-agent systems to address the\nspecialized requirements of high-performance code generation, offering a\ncost-effective alternative to domain-specific model fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have transformed software development through\ncode generation capabilities, yet their effectiveness for high-performance\ncomputing (HPC) remains limited. HPC code requires specialized optimizations\nfor parallelism, memory efficiency, and architecture-specific considerations\nthat general-purpose LLMs often overlook. We present MARCO (Multi-Agent\nReactive Code Optimizer), a novel framework that enhances LLM-generated code\nfor HPC through a specialized multi-agent architecture. MARCO employs separate\nagents for code generation and performance evaluation, connected by a feedback\nloop that progressively refines optimizations. A key innovation is MARCO's\nweb-search component that retrieves real-time optimization techniques from\nrecent conference proceedings and research publications, bridging the knowledge\ngap in pre-trained LLMs. Our extensive evaluation on the LeetCode 75 problem\nset demonstrates that MARCO achieves a 14.6% average runtime reduction compared\nto Claude 3.5 Sonnet alone, while the integration of the web-search component\nyields a 30.9% performance improvement over the base MARCO system. These\nresults highlight the potential of multi-agent systems to address the\nspecialized requirements of high-performance code generation, offering a\ncost-effective alternative to domain-specific model fine-tuning."
                },
                "authors": [
                    {
                        "name": "Asif Rahman"
                    },
                    {
                        "name": "Veljko Cvetkovic"
                    },
                    {
                        "name": "Kathleen Reece"
                    },
                    {
                        "name": "Aidan Walters"
                    },
                    {
                        "name": "Yasir Hassan"
                    },
                    {
                        "name": "Aneesh Tummeti"
                    },
                    {
                        "name": "Bryan Torres"
                    },
                    {
                        "name": "Denise Cooney"
                    },
                    {
                        "name": "Margaret Ellis"
                    },
                    {
                        "name": "Dimitrios S. Nikolopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios S. Nikolopoulos"
                },
                "author": "Dimitrios S. Nikolopoulos",
                "arxiv_comment": "9 pages, 4 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03906v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03906v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08508v1",
                "updated": "2025-05-13T12:39:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    39,
                    6,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T12:39:06Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    39,
                    6,
                    1,
                    133,
                    0
                ],
                "title": "TrialMatchAI: An End-to-End AI-powered Clinical Trial Recommendation\n  System to Streamline Patient-to-Trial Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrialMatchAI: An End-to-End AI-powered Clinical Trial Recommendation\n  System to Streamline Patient-to-Trial Matching"
                },
                "summary": "Patient recruitment remains a major bottleneck in clinical trials, calling\nfor scalable and automated solutions. We present TrialMatchAI, an AI-powered\nrecommendation system that automates patient-to-trial matching by processing\nheterogeneous clinical data, including structured records and unstructured\nphysician notes. Built on fine-tuned, open-source large language models (LLMs)\nwithin a retrieval-augmented generation framework, TrialMatchAI ensures\ntransparency and reproducibility and maintains a lightweight deployment\nfootprint suitable for clinical environments. The system normalizes biomedical\nentities, retrieves relevant trials using a hybrid search strategy combining\nlexical and semantic similarity, re-ranks results, and performs criterion-level\neligibility assessments using medical Chain-of-Thought reasoning. This pipeline\ndelivers explainable outputs with traceable decision rationales. In real-world\nvalidation, 92 percent of oncology patients had at least one relevant trial\nretrieved within the top 20 recommendations. Evaluation across synthetic and\nreal clinical datasets confirmed state-of-the-art performance, with expert\nassessment validating over 90 percent accuracy in criterion-level eligibility\nclassification, particularly excelling in biomarker-driven matches. Designed\nfor modularity and privacy, TrialMatchAI supports Phenopackets-standardized\ndata, enables secure local deployment, and allows seamless replacement of LLM\ncomponents as more advanced models emerge. By enhancing efficiency and\ninterpretability and offering lightweight, open-source deployment, TrialMatchAI\nprovides a scalable solution for AI-driven clinical trial matching in precision\nmedicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patient recruitment remains a major bottleneck in clinical trials, calling\nfor scalable and automated solutions. We present TrialMatchAI, an AI-powered\nrecommendation system that automates patient-to-trial matching by processing\nheterogeneous clinical data, including structured records and unstructured\nphysician notes. Built on fine-tuned, open-source large language models (LLMs)\nwithin a retrieval-augmented generation framework, TrialMatchAI ensures\ntransparency and reproducibility and maintains a lightweight deployment\nfootprint suitable for clinical environments. The system normalizes biomedical\nentities, retrieves relevant trials using a hybrid search strategy combining\nlexical and semantic similarity, re-ranks results, and performs criterion-level\neligibility assessments using medical Chain-of-Thought reasoning. This pipeline\ndelivers explainable outputs with traceable decision rationales. In real-world\nvalidation, 92 percent of oncology patients had at least one relevant trial\nretrieved within the top 20 recommendations. Evaluation across synthetic and\nreal clinical datasets confirmed state-of-the-art performance, with expert\nassessment validating over 90 percent accuracy in criterion-level eligibility\nclassification, particularly excelling in biomarker-driven matches. Designed\nfor modularity and privacy, TrialMatchAI supports Phenopackets-standardized\ndata, enables secure local deployment, and allows seamless replacement of LLM\ncomponents as more advanced models emerge. By enhancing efficiency and\ninterpretability and offering lightweight, open-source deployment, TrialMatchAI\nprovides a scalable solution for AI-driven clinical trial matching in precision\nmedicine."
                },
                "authors": [
                    {
                        "name": "Majd Abdallah"
                    },
                    {
                        "name": "Sigve Nakken"
                    },
                    {
                        "name": "Mariska Bierkens"
                    },
                    {
                        "name": "Johanna Galvis"
                    },
                    {
                        "name": "Alexis Groppi"
                    },
                    {
                        "name": "Slim Karkar"
                    },
                    {
                        "name": "Lana Meiqari"
                    },
                    {
                        "name": "Maria Alexandra Rujano"
                    },
                    {
                        "name": "Steve Canham"
                    },
                    {
                        "name": "Rodrigo Dienstmann"
                    },
                    {
                        "name": "Remond Fijneman"
                    },
                    {
                        "name": "Eivind Hovig"
                    },
                    {
                        "name": "Gerrit Meijer"
                    },
                    {
                        "name": "Macha Nikolski"
                    }
                ],
                "author_detail": {
                    "name": "Macha Nikolski"
                },
                "author": "Macha Nikolski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08507v1",
                "updated": "2025-05-13T12:37:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    37,
                    48,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T12:37:48Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    37,
                    48,
                    1,
                    133,
                    0
                ],
                "title": "InfoPO: On Mutual Information Maximization for Large Language Model\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfoPO: On Mutual Information Maximization for Large Language Model\n  Alignment"
                },
                "summary": "We study the post-training of large language models (LLMs) with human\npreference data. Recently, direct preference optimization and its variants have\nshown considerable promise in aligning language models, eliminating the need\nfor reward models and online sampling. Despite these benefits, these methods\nrely on explicit assumptions about the Bradley-Terry (BT) model, which makes\nthem prone to overfitting and results in suboptimal performance, particularly\non reasoning-heavy tasks. To address these challenges, we propose a principled\npreference fine-tuning algorithm called InfoPO, which effectively and\nefficiently aligns large language models using preference data. InfoPO\neliminates the reliance on the BT model and prevents the likelihood of the\nchosen response from decreasing. Extensive experiments confirm that InfoPO\nconsistently outperforms established baselines on widely used open benchmarks,\nparticularly in reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the post-training of large language models (LLMs) with human\npreference data. Recently, direct preference optimization and its variants have\nshown considerable promise in aligning language models, eliminating the need\nfor reward models and online sampling. Despite these benefits, these methods\nrely on explicit assumptions about the Bradley-Terry (BT) model, which makes\nthem prone to overfitting and results in suboptimal performance, particularly\non reasoning-heavy tasks. To address these challenges, we propose a principled\npreference fine-tuning algorithm called InfoPO, which effectively and\nefficiently aligns large language models using preference data. InfoPO\neliminates the reliance on the BT model and prevents the likelihood of the\nchosen response from decreasing. Extensive experiments confirm that InfoPO\nconsistently outperforms established baselines on widely used open benchmarks,\nparticularly in reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Teng Xiao"
                    },
                    {
                        "name": "Zhen Ge"
                    },
                    {
                        "name": "Sujay Sanghavi"
                    },
                    {
                        "name": "Tian Wang"
                    },
                    {
                        "name": "Julian Katz-Samuels"
                    },
                    {
                        "name": "Marc Versage"
                    },
                    {
                        "name": "Qingjun Cui"
                    },
                    {
                        "name": "Trishul Chilimbi"
                    }
                ],
                "author_detail": {
                    "name": "Trishul Chilimbi"
                },
                "author": "Trishul Chilimbi",
                "arxiv_comment": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15026v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15026v2",
                "updated": "2025-05-13T12:36:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    36,
                    12,
                    1,
                    133,
                    0
                ],
                "published": "2025-04-21T11:18:16Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    18,
                    16,
                    0,
                    111,
                    0
                ],
                "title": "Gaussian Shading++: Rethinking the Realistic Deployment Challenge of\n  Performance-Lossless Image Watermark for Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Shading++: Rethinking the Realistic Deployment Challenge of\n  Performance-Lossless Image Watermark for Diffusion Models"
                },
                "summary": "Ethical concerns surrounding copyright protection and inappropriate content\ngeneration pose challenges for the practical implementation of diffusion\nmodels. One effective solution involves watermarking the generated images.\nExisting methods primarily focus on ensuring that watermark embedding does not\ndegrade the model performance. However, they often overlook critical challenges\nin real-world deployment scenarios, such as the complexity of watermark key\nmanagement, user-defined generation parameters, and the difficulty of\nverification by arbitrary third parties. To address this issue, we propose\nGaussian Shading++, a diffusion model watermarking method tailored for\nreal-world deployment. We propose a double-channel design that leverages\npseudorandom error-correcting codes to encode the random seed required for\nwatermark pseudorandomization, achieving performance-lossless watermarking\nunder a fixed watermark key and overcoming key management challenges.\nAdditionally, we model the distortions introduced during generation and\ninversion as an additive white Gaussian noise channel and employ a novel soft\ndecision decoding strategy during extraction, ensuring strong robustness even\nwhen generation parameters vary. To enable third-party verification, we\nincorporate public key signatures, which provide a certain level of resistance\nagainst forgery attacks even when model inversion capabilities are fully\ndisclosed. Extensive experiments demonstrate that Gaussian Shading++ not only\nmaintains performance losslessness but also outperforms existing methods in\nterms of robustness, making it a more practical solution for real-world\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ethical concerns surrounding copyright protection and inappropriate content\ngeneration pose challenges for the practical implementation of diffusion\nmodels. One effective solution involves watermarking the generated images.\nExisting methods primarily focus on ensuring that watermark embedding does not\ndegrade the model performance. However, they often overlook critical challenges\nin real-world deployment scenarios, such as the complexity of watermark key\nmanagement, user-defined generation parameters, and the difficulty of\nverification by arbitrary third parties. To address this issue, we propose\nGaussian Shading++, a diffusion model watermarking method tailored for\nreal-world deployment. We propose a double-channel design that leverages\npseudorandom error-correcting codes to encode the random seed required for\nwatermark pseudorandomization, achieving performance-lossless watermarking\nunder a fixed watermark key and overcoming key management challenges.\nAdditionally, we model the distortions introduced during generation and\ninversion as an additive white Gaussian noise channel and employ a novel soft\ndecision decoding strategy during extraction, ensuring strong robustness even\nwhen generation parameters vary. To enable third-party verification, we\nincorporate public key signatures, which provide a certain level of resistance\nagainst forgery attacks even when model inversion capabilities are fully\ndisclosed. Extensive experiments demonstrate that Gaussian Shading++ not only\nmaintains performance losslessness but also outperforms existing methods in\nterms of robustness, making it a more practical solution for real-world\ndeployment."
                },
                "authors": [
                    {
                        "name": "Zijin Yang"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Kejiang Chen"
                    },
                    {
                        "name": "Kai Zeng"
                    },
                    {
                        "name": "Qiyi Yao"
                    },
                    {
                        "name": "Han Fang"
                    },
                    {
                        "name": "Weiming Zhang"
                    },
                    {
                        "name": "Nenghai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Nenghai Yu"
                },
                "author": "Nenghai Yu",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15026v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15026v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08498v1",
                "updated": "2025-05-13T12:26:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    26,
                    16,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T12:26:16Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    26,
                    16,
                    1,
                    133,
                    0
                ],
                "title": "LCES: Zero-shot Automated Essay Scoring via Pairwise Comparisons Using\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LCES: Zero-shot Automated Essay Scoring via Pairwise Comparisons Using\n  Large Language Models"
                },
                "summary": "Recent advances in large language models (LLMs) have enabled zero-shot\nautomated essay scoring (AES), providing a promising way to reduce the cost and\neffort of essay scoring in comparison with manual grading. However, most\nexisting zero-shot approaches rely on LLMs to directly generate absolute\nscores, which often diverge from human evaluations owing to model biases and\ninconsistent scoring. To address these limitations, we propose LLM-based\nComparative Essay Scoring (LCES), a method that formulates AES as a pairwise\ncomparison task. Specifically, we instruct LLMs to judge which of two essays is\nbetter, collect many such comparisons, and convert them into continuous scores.\nConsidering that the number of possible comparisons grows quadratically with\nthe number of essays, we improve scalability by employing RankNet to\nefficiently transform LLM preferences into scalar scores. Experiments using AES\nbenchmark datasets show that LCES outperforms conventional zero-shot methods in\naccuracy while maintaining computational efficiency. Moreover, LCES is robust\nacross different LLM backbones, highlighting its applicability to real-world\nzero-shot AES.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have enabled zero-shot\nautomated essay scoring (AES), providing a promising way to reduce the cost and\neffort of essay scoring in comparison with manual grading. However, most\nexisting zero-shot approaches rely on LLMs to directly generate absolute\nscores, which often diverge from human evaluations owing to model biases and\ninconsistent scoring. To address these limitations, we propose LLM-based\nComparative Essay Scoring (LCES), a method that formulates AES as a pairwise\ncomparison task. Specifically, we instruct LLMs to judge which of two essays is\nbetter, collect many such comparisons, and convert them into continuous scores.\nConsidering that the number of possible comparisons grows quadratically with\nthe number of essays, we improve scalability by employing RankNet to\nefficiently transform LLM preferences into scalar scores. Experiments using AES\nbenchmark datasets show that LCES outperforms conventional zero-shot methods in\naccuracy while maintaining computational efficiency. Moreover, LCES is robust\nacross different LLM backbones, highlighting its applicability to real-world\nzero-shot AES."
                },
                "authors": [
                    {
                        "name": "Takumi Shibata"
                    },
                    {
                        "name": "Yuichi Miyamura"
                    }
                ],
                "author_detail": {
                    "name": "Yuichi Miyamura"
                },
                "author": "Yuichi Miyamura",
                "arxiv_comment": "14 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08493v1",
                "updated": "2025-05-13T12:23:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    23,
                    11,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T12:23:11Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    23,
                    11,
                    1,
                    133,
                    0
                ],
                "title": "BizChat: Scaffolding AI-Powered Business Planning for Small Business\n  Owners Across Digital Skill Levels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BizChat: Scaffolding AI-Powered Business Planning for Small Business\n  Owners Across Digital Skill Levels"
                },
                "summary": "Generative AI can help small business owners automate tasks, increase\nefficiency, and improve their bottom line. However, despite the seemingly\nintuitive design of systems like ChatGPT, significant barriers remain for those\nless comfortable with technology. To address these disparities, prior work\nhighlights accessory skills -- beyond prompt engineering -- users must master\nto successfully adopt generative AI including keyboard shortcuts, editing\nskills, file conversions, and browser literacy. Building on a design workshop\nseries and 15 interviews with small businesses, we introduce BizChat, a large\nlanguage model (LLM)-powered web application that helps business owners across\ndigital skills levels write their business plan -- an essential but often\nneglected document. To do so, BizChat's interface embodies three design\nconsiderations inspired by learning sciences: ensuring accessibility to users\nwith less digital skills while maintaining extensibility to power users\n(\"low-floor-high-ceiling\"), providing in situ micro-learning to support\nentrepreneurial education (\"just-in-time learning\"), and framing interaction\naround business activities (\"contextualized technology introduction\"). We\nconclude with plans for a future BizChat deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI can help small business owners automate tasks, increase\nefficiency, and improve their bottom line. However, despite the seemingly\nintuitive design of systems like ChatGPT, significant barriers remain for those\nless comfortable with technology. To address these disparities, prior work\nhighlights accessory skills -- beyond prompt engineering -- users must master\nto successfully adopt generative AI including keyboard shortcuts, editing\nskills, file conversions, and browser literacy. Building on a design workshop\nseries and 15 interviews with small businesses, we introduce BizChat, a large\nlanguage model (LLM)-powered web application that helps business owners across\ndigital skills levels write their business plan -- an essential but often\nneglected document. To do so, BizChat's interface embodies three design\nconsiderations inspired by learning sciences: ensuring accessibility to users\nwith less digital skills while maintaining extensibility to power users\n(\"low-floor-high-ceiling\"), providing in situ micro-learning to support\nentrepreneurial education (\"just-in-time learning\"), and framing interaction\naround business activities (\"contextualized technology introduction\"). We\nconclude with plans for a future BizChat deployment."
                },
                "authors": [
                    {
                        "name": "Quentin Romero Lauro"
                    },
                    {
                        "name": "Aakash Gautam"
                    },
                    {
                        "name": "Yasmine Kotturi"
                    }
                ],
                "author_detail": {
                    "name": "Yasmine Kotturi"
                },
                "author": "Yasmine Kotturi",
                "arxiv_doi": "10.1145/3707640.3731928",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3707640.3731928",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.08493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "4 pages, 1 figure, CHIWORK '25 Adjunct, June 23-25, 2025, Amsterdam,\n  Netherlands",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08492v1",
                "updated": "2025-05-13T12:22:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    22,
                    38,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T12:22:38Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    22,
                    38,
                    1,
                    133,
                    0
                ],
                "title": "Achieving Scalable Robot Autonomy via neurosymbolic planning using\n  lightweight local LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving Scalable Robot Autonomy via neurosymbolic planning using\n  lightweight local LLM"
                },
                "summary": "PDDL-based symbolic task planning remains pivotal for robot autonomy yet\nstruggles with dynamic human-robot collaboration due to scalability,\nre-planning demands, and delayed plan availability. Although a few\nneurosymbolic frameworks have previously leveraged LLMs such as GPT-3 to\naddress these challenges, reliance on closed-source, remote models with limited\ncontext introduced critical constraints: third-party dependency, inconsistent\nresponse times, restricted plan length and complexity, and multi-domain\nscalability issues. We present Gideon, a novel framework that enables the\ntransition to modern, smaller, local LLMs with extended context length. Gideon\nintegrates a novel problem generator to systematically generate large-scale\ndatasets of realistic domain-problem-plan tuples for any domain, and adapts\nneurosymbolic planning for local LLMs, enabling on-device execution and\nextended context for multi-domain support. Preliminary experiments in\nsingle-domain scenarios performed on Qwen-2.5 1.5B and trained on 8k-32k\nsamples, demonstrate a valid plan percentage of 66.1% (32k model) and show that\nthe figure can be further scaled through additional data. Multi-domain tests on\n16k samples yield an even higher 70.6% planning validity rate, proving\nextensibility across domains and signaling that data variety can have a\npositive effect on learning efficiency. Although long-horizon planning and\nreduced model size make Gideon training much less efficient than baseline\nmodels based on larger LLMs, the results are still significant considering that\nthe trained model is about 120x smaller than baseline and that significant\nadvantages can be achieved in inference efficiency, scalability, and\nmulti-domain adaptability, all critical factors in human-robot collaboration.\nTraining inefficiency can be mitigated by Gideon's streamlined data generation\npipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDDL-based symbolic task planning remains pivotal for robot autonomy yet\nstruggles with dynamic human-robot collaboration due to scalability,\nre-planning demands, and delayed plan availability. Although a few\nneurosymbolic frameworks have previously leveraged LLMs such as GPT-3 to\naddress these challenges, reliance on closed-source, remote models with limited\ncontext introduced critical constraints: third-party dependency, inconsistent\nresponse times, restricted plan length and complexity, and multi-domain\nscalability issues. We present Gideon, a novel framework that enables the\ntransition to modern, smaller, local LLMs with extended context length. Gideon\nintegrates a novel problem generator to systematically generate large-scale\ndatasets of realistic domain-problem-plan tuples for any domain, and adapts\nneurosymbolic planning for local LLMs, enabling on-device execution and\nextended context for multi-domain support. Preliminary experiments in\nsingle-domain scenarios performed on Qwen-2.5 1.5B and trained on 8k-32k\nsamples, demonstrate a valid plan percentage of 66.1% (32k model) and show that\nthe figure can be further scaled through additional data. Multi-domain tests on\n16k samples yield an even higher 70.6% planning validity rate, proving\nextensibility across domains and signaling that data variety can have a\npositive effect on learning efficiency. Although long-horizon planning and\nreduced model size make Gideon training much less efficient than baseline\nmodels based on larger LLMs, the results are still significant considering that\nthe trained model is about 120x smaller than baseline and that significant\nadvantages can be achieved in inference efficiency, scalability, and\nmulti-domain adaptability, all critical factors in human-robot collaboration.\nTraining inefficiency can be mitigated by Gideon's streamlined data generation\npipeline."
                },
                "authors": [
                    {
                        "name": "Nicholas Attolino"
                    },
                    {
                        "name": "Alessio Capitanelli"
                    },
                    {
                        "name": "Fulvio Mastrogiovanni"
                    }
                ],
                "author_detail": {
                    "name": "Fulvio Mastrogiovanni"
                },
                "author": "Fulvio Mastrogiovanni",
                "arxiv_comment": "19 pages, 3 figures, 4 tables, accepted at IAS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.8; I.2.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08474v1",
                "updated": "2025-05-13T11:58:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    58,
                    45,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T11:58:45Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    58,
                    45,
                    1,
                    133,
                    0
                ],
                "title": "Distributed Quantum Neural Networks on Distributed Photonic Quantum\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Quantum Neural Networks on Distributed Photonic Quantum\n  Computing"
                },
                "summary": "We introduce a distributed quantum-classical framework that synergizes\nphotonic quantum neural networks (QNNs) with matrix-product-state (MPS) mapping\nto achieve parameter-efficient training of classical neural networks. By\nleveraging universal linear-optical decompositions of $M$-mode interferometers\nand photon-counting measurement statistics, our architecture generates neural\nparameters through a hybrid quantum-classical workflow: photonic QNNs with\n$M(M+1)/2$ trainable parameters produce high-dimensional probability\ndistributions that are mapped to classical network weights via an MPS model\nwith bond dimension $\\chi$. Empirical validation on MNIST classification\ndemonstrates that photonic QT achieves an accuracy of $95.50\\% \\pm 0.84\\%$\nusing 3,292 parameters ($\\chi = 10$), compared to $96.89\\% \\pm 0.31\\%$ for\nclassical baselines with 6,690 parameters. Moreover, a ten-fold compression\nratio is achieved at $\\chi = 4$, with a relative accuracy loss of less than\n$3\\%$. The framework outperforms classical compression techniques (weight\nsharing/pruning) by 6--12\\% absolute accuracy while eliminating quantum\nhardware requirements during inference through classical deployment of\ncompressed parameters. Simulations incorporating realistic photonic noise\ndemonstrate the framework's robustness to near-term hardware imperfections.\nAblation studies confirm quantum necessity: replacing photonic QNNs with random\ninputs collapses accuracy to chance level ($10.0\\% \\pm 0.5\\%$). Photonic\nquantum computing's room-temperature operation, inherent scalability through\nspatial-mode multiplexing, and HPC-integrated architecture establish a\npractical pathway for distributed quantum machine learning, combining the\nexpressivity of photonic Hilbert spaces with the deployability of classical\nneural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a distributed quantum-classical framework that synergizes\nphotonic quantum neural networks (QNNs) with matrix-product-state (MPS) mapping\nto achieve parameter-efficient training of classical neural networks. By\nleveraging universal linear-optical decompositions of $M$-mode interferometers\nand photon-counting measurement statistics, our architecture generates neural\nparameters through a hybrid quantum-classical workflow: photonic QNNs with\n$M(M+1)/2$ trainable parameters produce high-dimensional probability\ndistributions that are mapped to classical network weights via an MPS model\nwith bond dimension $\\chi$. Empirical validation on MNIST classification\ndemonstrates that photonic QT achieves an accuracy of $95.50\\% \\pm 0.84\\%$\nusing 3,292 parameters ($\\chi = 10$), compared to $96.89\\% \\pm 0.31\\%$ for\nclassical baselines with 6,690 parameters. Moreover, a ten-fold compression\nratio is achieved at $\\chi = 4$, with a relative accuracy loss of less than\n$3\\%$. The framework outperforms classical compression techniques (weight\nsharing/pruning) by 6--12\\% absolute accuracy while eliminating quantum\nhardware requirements during inference through classical deployment of\ncompressed parameters. Simulations incorporating realistic photonic noise\ndemonstrate the framework's robustness to near-term hardware imperfections.\nAblation studies confirm quantum necessity: replacing photonic QNNs with random\ninputs collapses accuracy to chance level ($10.0\\% \\pm 0.5\\%$). Photonic\nquantum computing's room-temperature operation, inherent scalability through\nspatial-mode multiplexing, and HPC-integrated architecture establish a\npractical pathway for distributed quantum machine learning, combining the\nexpressivity of photonic Hilbert spaces with the deployability of classical\nneural networks."
                },
                "authors": [
                    {
                        "name": "Kuan-Cheng Chen"
                    },
                    {
                        "name": "Chen-Yu Liu"
                    },
                    {
                        "name": "Yu Shang"
                    },
                    {
                        "name": "Felix Burt"
                    },
                    {
                        "name": "Kin K. Leung"
                    }
                ],
                "author_detail": {
                    "name": "Kin K. Leung"
                },
                "author": "Kin K. Leung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21098v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21098v3",
                "updated": "2025-05-13T11:54:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    54,
                    26,
                    1,
                    133,
                    0
                ],
                "published": "2025-03-27T02:36:48Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    2,
                    36,
                    48,
                    3,
                    86,
                    0
                ],
                "title": "Alleviating LLM-based Generative Retrieval Hallucination in Alipay\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alleviating LLM-based Generative Retrieval Hallucination in Alipay\n  Search"
                },
                "summary": "Generative retrieval (GR) has revolutionized document retrieval with the\nadvent of large language models (LLMs), and LLM-based GR is gradually being\nadopted by the industry. Despite its remarkable advantages and potential,\nLLM-based GR suffers from hallucination and generates documents that are\nirrelevant to the query in some instances, severely challenging its credibility\nin practical applications. We thereby propose an optimized GR framework\ndesigned to alleviate retrieval hallucination, which integrates knowledge\ndistillation reasoning in model training and incorporate decision agent to\nfurther improve retrieval precision. Specifically, we employ LLMs to assess and\nreason GR retrieved query-document (q-d) pairs, and then distill the reasoning\ndata as transferred knowledge to the GR model. Moreover, we utilize a decision\nagent as post-processing to extend the GR retrieved documents through retrieval\nmodel and select the most relevant ones from multi perspectives as the final\ngenerative retrieval result. Extensive offline experiments on real-world\ndatasets and online A/B tests on Fund Search and Insurance Search in Alipay\ndemonstrate our framework's superiority and effectiveness in improving search\nquality and conversion gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative retrieval (GR) has revolutionized document retrieval with the\nadvent of large language models (LLMs), and LLM-based GR is gradually being\nadopted by the industry. Despite its remarkable advantages and potential,\nLLM-based GR suffers from hallucination and generates documents that are\nirrelevant to the query in some instances, severely challenging its credibility\nin practical applications. We thereby propose an optimized GR framework\ndesigned to alleviate retrieval hallucination, which integrates knowledge\ndistillation reasoning in model training and incorporate decision agent to\nfurther improve retrieval precision. Specifically, we employ LLMs to assess and\nreason GR retrieved query-document (q-d) pairs, and then distill the reasoning\ndata as transferred knowledge to the GR model. Moreover, we utilize a decision\nagent as post-processing to extend the GR retrieved documents through retrieval\nmodel and select the most relevant ones from multi perspectives as the final\ngenerative retrieval result. Extensive offline experiments on real-world\ndatasets and online A/B tests on Fund Search and Insurance Search in Alipay\ndemonstrate our framework's superiority and effectiveness in improving search\nquality and conversion gains."
                },
                "authors": [
                    {
                        "name": "Yedan Shen"
                    },
                    {
                        "name": "Kaixin Wu"
                    },
                    {
                        "name": "Yuechen Ding"
                    },
                    {
                        "name": "Jingyuan Wen"
                    },
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Mingjie Zhong"
                    },
                    {
                        "name": "Zhouhan Lin"
                    },
                    {
                        "name": "Jia Xu"
                    },
                    {
                        "name": "Linjian Mo"
                    }
                ],
                "author_detail": {
                    "name": "Linjian Mo"
                },
                "author": "Linjian Mo",
                "arxiv_doi": "10.1145/3726302.3731951",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3731951",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.21098v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21098v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by SIGIR 2025",
                "arxiv_journal_ref": "SIGIR 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20799v2",
                "updated": "2025-05-13T11:51:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    51,
                    35,
                    1,
                    133,
                    0
                ],
                "published": "2025-04-29T14:13:57Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    13,
                    57,
                    1,
                    119,
                    0
                ],
                "title": "Hallucination by Code Generation LLMs: Taxonomy, Benchmarks, Mitigation,\n  and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination by Code Generation LLMs: Taxonomy, Benchmarks, Mitigation,\n  and Challenges"
                },
                "summary": "Recent technical breakthroughs in large language models (LLMs) have enabled\nthem to fluently generate source code. Software developers often leverage both\ngeneral-purpose and code-specialized LLMs to revise existing code or even\ngenerate a whole function from scratch. These capabilities are also beneficial\nin no-code or low-code contexts, in which one can write programs without a\ntechnical background. However, due to their internal design, LLMs are prone to\ngenerating hallucinations, which are incorrect, nonsensical, and not\njustifiable information but difficult to identify its presence. This problem\nalso occurs when generating source code. Once hallucinated code is produced, it\nis often challenging for users to identify and fix it, especially when such\nhallucinations can be identified under specific execution paths. As a result,\nthe hallucinated code may remain unnoticed within the codebase. This survey\ninvestigates recent studies and techniques relevant to hallucinations generated\nby CodeLLMs. We categorize the types of hallucinations in the code generated by\nCodeLLMs, review existing benchmarks and mitigation strategies, and identify\nopen challenges. Based on these findings, this survey outlines further research\ndirections in the detection and removal of hallucinations produced by CodeLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent technical breakthroughs in large language models (LLMs) have enabled\nthem to fluently generate source code. Software developers often leverage both\ngeneral-purpose and code-specialized LLMs to revise existing code or even\ngenerate a whole function from scratch. These capabilities are also beneficial\nin no-code or low-code contexts, in which one can write programs without a\ntechnical background. However, due to their internal design, LLMs are prone to\ngenerating hallucinations, which are incorrect, nonsensical, and not\njustifiable information but difficult to identify its presence. This problem\nalso occurs when generating source code. Once hallucinated code is produced, it\nis often challenging for users to identify and fix it, especially when such\nhallucinations can be identified under specific execution paths. As a result,\nthe hallucinated code may remain unnoticed within the codebase. This survey\ninvestigates recent studies and techniques relevant to hallucinations generated\nby CodeLLMs. We categorize the types of hallucinations in the code generated by\nCodeLLMs, review existing benchmarks and mitigation strategies, and identify\nopen challenges. Based on these findings, this survey outlines further research\ndirections in the detection and removal of hallucinations produced by CodeLLMs."
                },
                "authors": [
                    {
                        "name": "Yunseo Lee"
                    },
                    {
                        "name": "John Youngeun Song"
                    },
                    {
                        "name": "Dongsun Kim"
                    },
                    {
                        "name": "Jindae Kim"
                    },
                    {
                        "name": "Mijung Kim"
                    },
                    {
                        "name": "Jaechang Nam"
                    }
                ],
                "author_detail": {
                    "name": "Jaechang Nam"
                },
                "author": "Jaechang Nam",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08468v1",
                "updated": "2025-05-13T11:50:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    50,
                    8,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T11:50:08Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    50,
                    8,
                    1,
                    133,
                    0
                ],
                "title": "Judging the Judges: Can Large Vision-Language Models Fairly Evaluate\n  Chart Comprehension and Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judging the Judges: Can Large Vision-Language Models Fairly Evaluate\n  Chart Comprehension and Reasoning?"
                },
                "summary": "Charts are ubiquitous as they help people understand and reason with data.\nRecently, various downstream tasks, such as chart question answering,\nchart2text, and fact-checking, have emerged. Large Vision-Language Models\n(LVLMs) show promise in tackling these tasks, but their evaluation is costly\nand time-consuming, limiting real-world deployment. While using LVLMs as judges\nto assess the chart comprehension capabilities of other LVLMs could streamline\nevaluation processes, challenges like proprietary datasets, restricted access\nto powerful models, and evaluation costs hinder their adoption in industrial\nsettings. To this end, we present a comprehensive evaluation of 13 open-source\nLVLMs as judges for diverse chart comprehension and reasoning tasks. We design\nboth pairwise and pointwise evaluation tasks covering criteria like factual\ncorrectness, informativeness, and relevancy. Additionally, we analyze LVLM\njudges based on format adherence, positional consistency, length bias, and\ninstruction-following. We focus on cost-effective LVLMs (<10B parameters)\nsuitable for both research and commercial use, following a standardized\nevaluation protocol and rubric to measure the LVLM judge's accuracy.\nExperimental results reveal notable variability: while some open LVLM judges\nachieve GPT-4-level evaluation performance (about 80% agreement with GPT-4\njudgments), others struggle (below ~10% agreement). Our findings highlight that\nstate-of-the-art open-source LVLMs can serve as cost-effective automatic\nevaluators for chart-related tasks, though biases such as positional preference\nand length bias persist.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Charts are ubiquitous as they help people understand and reason with data.\nRecently, various downstream tasks, such as chart question answering,\nchart2text, and fact-checking, have emerged. Large Vision-Language Models\n(LVLMs) show promise in tackling these tasks, but their evaluation is costly\nand time-consuming, limiting real-world deployment. While using LVLMs as judges\nto assess the chart comprehension capabilities of other LVLMs could streamline\nevaluation processes, challenges like proprietary datasets, restricted access\nto powerful models, and evaluation costs hinder their adoption in industrial\nsettings. To this end, we present a comprehensive evaluation of 13 open-source\nLVLMs as judges for diverse chart comprehension and reasoning tasks. We design\nboth pairwise and pointwise evaluation tasks covering criteria like factual\ncorrectness, informativeness, and relevancy. Additionally, we analyze LVLM\njudges based on format adherence, positional consistency, length bias, and\ninstruction-following. We focus on cost-effective LVLMs (<10B parameters)\nsuitable for both research and commercial use, following a standardized\nevaluation protocol and rubric to measure the LVLM judge's accuracy.\nExperimental results reveal notable variability: while some open LVLM judges\nachieve GPT-4-level evaluation performance (about 80% agreement with GPT-4\njudgments), others struggle (below ~10% agreement). Our findings highlight that\nstate-of-the-art open-source LVLMs can serve as cost-effective automatic\nevaluators for chart-related tasks, though biases such as positional preference\nand length bias persist."
                },
                "authors": [
                    {
                        "name": "Md Tahmid Rahman Laskar"
                    },
                    {
                        "name": "Mohammed Saidul Islam"
                    },
                    {
                        "name": "Ridwan Mahbub"
                    },
                    {
                        "name": "Ahmed Masry"
                    },
                    {
                        "name": "Mizanur Rahman"
                    },
                    {
                        "name": "Amran Bhuiyan"
                    },
                    {
                        "name": "Mir Tafseer Nayeem"
                    },
                    {
                        "name": "Shafiq Joty"
                    },
                    {
                        "name": "Enamul Hoque"
                    },
                    {
                        "name": "Jimmy Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Huang"
                },
                "author": "Jimmy Huang",
                "arxiv_comment": "Accepted at ACL 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08464v1",
                "updated": "2025-05-13T11:47:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    47,
                    49,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T11:47:49Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    47,
                    49,
                    1,
                    133,
                    0
                ],
                "title": "Large Language Models Meet Stance Detection: A Survey of Tasks, Methods,\n  Applications, Challenges and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Meet Stance Detection: A Survey of Tasks, Methods,\n  Applications, Challenges and Future Directions"
                },
                "summary": "Stance detection is essential for understanding subjective content across\nvarious platforms such as social media, news articles, and online reviews.\nRecent advances in Large Language Models (LLMs) have revolutionized stance\ndetection by introducing novel capabilities in contextual understanding,\ncross-domain generalization, and multimodal analysis. Despite these\nprogressions, existing surveys often lack comprehensive coverage of approaches\nthat specifically leverage LLMs for stance detection. To bridge this critical\ngap, our review article conducts a systematic analysis of stance detection,\ncomprehensively examining recent advancements of LLMs transforming the field,\nincluding foundational concepts, methodologies, datasets, applications, and\nemerging challenges. We present a novel taxonomy for LLM-based stance detection\napproaches, structured along three key dimensions: 1) learning methods,\nincluding supervised, unsupervised, few-shot, and zero-shot; 2) data\nmodalities, such as unimodal, multimodal, and hybrid; and 3) target\nrelationships, encompassing in-target, cross-target, and multi-target\nscenarios. Furthermore, we discuss the evaluation techniques and analyze\nbenchmark datasets and performance trends, highlighting the strengths and\nlimitations of different architectures. Key applications in misinformation\ndetection, political analysis, public health monitoring, and social media\nmoderation are discussed. Finally, we identify critical challenges such as\nimplicit stance expression, cultural biases, and computational constraints,\nwhile outlining promising future directions, including explainable stance\nreasoning, low-resource adaptation, and real-time deployment frameworks. Our\nsurvey highlights emerging trends, open challenges, and future directions to\nguide researchers and practitioners in developing next-generation stance\ndetection systems powered by large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stance detection is essential for understanding subjective content across\nvarious platforms such as social media, news articles, and online reviews.\nRecent advances in Large Language Models (LLMs) have revolutionized stance\ndetection by introducing novel capabilities in contextual understanding,\ncross-domain generalization, and multimodal analysis. Despite these\nprogressions, existing surveys often lack comprehensive coverage of approaches\nthat specifically leverage LLMs for stance detection. To bridge this critical\ngap, our review article conducts a systematic analysis of stance detection,\ncomprehensively examining recent advancements of LLMs transforming the field,\nincluding foundational concepts, methodologies, datasets, applications, and\nemerging challenges. We present a novel taxonomy for LLM-based stance detection\napproaches, structured along three key dimensions: 1) learning methods,\nincluding supervised, unsupervised, few-shot, and zero-shot; 2) data\nmodalities, such as unimodal, multimodal, and hybrid; and 3) target\nrelationships, encompassing in-target, cross-target, and multi-target\nscenarios. Furthermore, we discuss the evaluation techniques and analyze\nbenchmark datasets and performance trends, highlighting the strengths and\nlimitations of different architectures. Key applications in misinformation\ndetection, political analysis, public health monitoring, and social media\nmoderation are discussed. Finally, we identify critical challenges such as\nimplicit stance expression, cultural biases, and computational constraints,\nwhile outlining promising future directions, including explainable stance\nreasoning, low-resource adaptation, and real-time deployment frameworks. Our\nsurvey highlights emerging trends, open challenges, and future directions to\nguide researchers and practitioners in developing next-generation stance\ndetection systems powered by large language models."
                },
                "authors": [
                    {
                        "name": "Lata Pangtey"
                    },
                    {
                        "name": "Anukriti Bhatnagar"
                    },
                    {
                        "name": "Shubhi Bansal"
                    },
                    {
                        "name": "Shahid Shafi Dar"
                    },
                    {
                        "name": "Nagendra Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Nagendra Kumar"
                },
                "author": "Nagendra Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08463v1",
                "updated": "2025-05-13T11:47:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    47,
                    0,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T11:47:00Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    47,
                    0,
                    1,
                    133,
                    0
                ],
                "title": "RepCali: High Efficient Fine-tuning Via Representation Calibration in\n  Latent Space for Pre-trained Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepCali: High Efficient Fine-tuning Via Representation Calibration in\n  Latent Space for Pre-trained Language Models"
                },
                "summary": "Fine-tuning pre-trained language models (PLMs) has become a dominant paradigm\nin applying PLMs to downstream tasks. However, with limited fine-tuning, PLMs\nstill struggle with the discrepancies between the representation obtained from\nthe PLMs' encoder and the optimal input to the PLMs' decoder. This paper\ntackles this challenge by learning to calibrate the representation of PLMs in\nthe latent space. In the proposed representation calibration method (RepCali),\nwe integrate a specific calibration block to the latent space after the encoder\nand use the calibrated output as the decoder input. The merits of the proposed\nRepCali include its universality to all PLMs with encoder-decoder\narchitectures, its plug-and-play nature, and ease of implementation. Extensive\nexperiments on 25 PLM-based models across 8 tasks (including both English and\nChinese datasets) demonstrate that the proposed RepCali offers desirable\nenhancements to PLMs (including LLMs) and significantly improves the\nperformance of downstream tasks. Comparison experiments across 4 benchmark\ntasks indicate that RepCali is superior to the representative fine-tuning\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning pre-trained language models (PLMs) has become a dominant paradigm\nin applying PLMs to downstream tasks. However, with limited fine-tuning, PLMs\nstill struggle with the discrepancies between the representation obtained from\nthe PLMs' encoder and the optimal input to the PLMs' decoder. This paper\ntackles this challenge by learning to calibrate the representation of PLMs in\nthe latent space. In the proposed representation calibration method (RepCali),\nwe integrate a specific calibration block to the latent space after the encoder\nand use the calibrated output as the decoder input. The merits of the proposed\nRepCali include its universality to all PLMs with encoder-decoder\narchitectures, its plug-and-play nature, and ease of implementation. Extensive\nexperiments on 25 PLM-based models across 8 tasks (including both English and\nChinese datasets) demonstrate that the proposed RepCali offers desirable\nenhancements to PLMs (including LLMs) and significantly improves the\nperformance of downstream tasks. Comparison experiments across 4 benchmark\ntasks indicate that RepCali is superior to the representative fine-tuning\nbaselines."
                },
                "authors": [
                    {
                        "name": "Fujun Zhang"
                    },
                    {
                        "name": "XiangDong Su"
                    }
                ],
                "author_detail": {
                    "name": "XiangDong Su"
                },
                "author": "XiangDong Su",
                "arxiv_comment": "13 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08459v1",
                "updated": "2025-05-13T11:41:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    41,
                    10,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T11:41:10Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    41,
                    10,
                    1,
                    133,
                    0
                ],
                "title": "Strategy-Augmented Planning for Large Language Models via Opponent\n  Exploitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategy-Augmented Planning for Large Language Models via Opponent\n  Exploitation"
                },
                "summary": "Efficiently modeling and exploiting opponents is a long-standing challenge in\nadversarial domains. Large Language Models (LLMs) trained on extensive textual\ndata have recently demonstrated outstanding performance in general tasks,\nintroducing new research directions for opponent modeling. Some studies\nprimarily focus on directly using LLMs to generate decisions based on the\nelaborate prompt context that incorporates opponent descriptions, while these\napproaches are limited to scenarios where LLMs possess adequate domain\nexpertise. To address that, we introduce a two-stage Strategy-Augmented\nPlanning (SAP) framework that significantly enhances the opponent exploitation\ncapabilities of LLM-based agents by utilizing a critical component, the\nStrategy Evaluation Network (SEN). Specifically, in the offline stage, we\nconstruct an explicit strategy space and subsequently collect strategy-outcome\npair data for training the SEN network. During the online phase, SAP\ndynamically recognizes the opponent's strategies and greedily exploits them by\nsearching best response strategy on the well-trained SEN, finally translating\nstrategy to a course of actions by carefully designed prompts. Experimental\nresults show that SAP exhibits robust generalization capabilities, allowing it\nto perform effectively not only against previously encountered opponent\nstrategies but also against novel, unseen strategies. In the MicroRTS\nenvironment, SAP achieves a 85.35\\% performance improvement over baseline\nmethods and matches the competitiveness of reinforcement learning approaches\nagainst state-of-the-art (SOTA) rule-based AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently modeling and exploiting opponents is a long-standing challenge in\nadversarial domains. Large Language Models (LLMs) trained on extensive textual\ndata have recently demonstrated outstanding performance in general tasks,\nintroducing new research directions for opponent modeling. Some studies\nprimarily focus on directly using LLMs to generate decisions based on the\nelaborate prompt context that incorporates opponent descriptions, while these\napproaches are limited to scenarios where LLMs possess adequate domain\nexpertise. To address that, we introduce a two-stage Strategy-Augmented\nPlanning (SAP) framework that significantly enhances the opponent exploitation\ncapabilities of LLM-based agents by utilizing a critical component, the\nStrategy Evaluation Network (SEN). Specifically, in the offline stage, we\nconstruct an explicit strategy space and subsequently collect strategy-outcome\npair data for training the SEN network. During the online phase, SAP\ndynamically recognizes the opponent's strategies and greedily exploits them by\nsearching best response strategy on the well-trained SEN, finally translating\nstrategy to a course of actions by carefully designed prompts. Experimental\nresults show that SAP exhibits robust generalization capabilities, allowing it\nto perform effectively not only against previously encountered opponent\nstrategies but also against novel, unseen strategies. In the MicroRTS\nenvironment, SAP achieves a 85.35\\% performance improvement over baseline\nmethods and matches the competitiveness of reinforcement learning approaches\nagainst state-of-the-art (SOTA) rule-based AI."
                },
                "authors": [
                    {
                        "name": "Shuai Xu"
                    },
                    {
                        "name": "Sijia Cui"
                    },
                    {
                        "name": "Yanna Wang"
                    },
                    {
                        "name": "Bo Xu"
                    },
                    {
                        "name": "Qi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Wang"
                },
                "author": "Qi Wang",
                "arxiv_comment": "Accepted to IJCNN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08450v1",
                "updated": "2025-05-13T11:25:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    25,
                    15,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T11:25:15Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    25,
                    15,
                    1,
                    133,
                    0
                ],
                "title": "IterKey: Iterative Keyword Generation with LLMs for Enhanced Retrieval\n  Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IterKey: Iterative Keyword Generation with LLMs for Enhanced Retrieval\n  Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a way to complement the\nin-context knowledge of Large Language Models (LLMs) by integrating external\ndocuments. However, real-world applications demand not only accuracy but also\ninterpretability. While dense retrieval methods provide high accuracy, they\nlack interpretability; conversely, sparse retrieval methods offer transparency\nbut often fail to capture the full intent of queries due to their reliance on\nkeyword matching. To address these issues, we introduce IterKey, an LLM-driven\niterative keyword generation framework that enhances RAG via sparse retrieval.\nIterKey consists of three LLM-driven stages: generating keywords for retrieval,\ngenerating answers based on retrieved documents, and validating the answers. If\nvalidation fails, the process iteratively repeats with refined keywords. Across\nfour QA tasks, experimental results show that IterKey achieves 5% to 20%\naccuracy improvements over BM25-based RAG and simple baselines. Its performance\nis comparable to dense retrieval-based RAG and prior iterative query refinement\nmethods using dense models. In summary, IterKey is a novel BM25-based approach\nleveraging LLMs to iteratively refine RAG, effectively balancing accuracy with\ninterpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a way to complement the\nin-context knowledge of Large Language Models (LLMs) by integrating external\ndocuments. However, real-world applications demand not only accuracy but also\ninterpretability. While dense retrieval methods provide high accuracy, they\nlack interpretability; conversely, sparse retrieval methods offer transparency\nbut often fail to capture the full intent of queries due to their reliance on\nkeyword matching. To address these issues, we introduce IterKey, an LLM-driven\niterative keyword generation framework that enhances RAG via sparse retrieval.\nIterKey consists of three LLM-driven stages: generating keywords for retrieval,\ngenerating answers based on retrieved documents, and validating the answers. If\nvalidation fails, the process iteratively repeats with refined keywords. Across\nfour QA tasks, experimental results show that IterKey achieves 5% to 20%\naccuracy improvements over BM25-based RAG and simple baselines. Its performance\nis comparable to dense retrieval-based RAG and prior iterative query refinement\nmethods using dense models. In summary, IterKey is a novel BM25-based approach\nleveraging LLMs to iteratively refine RAG, effectively balancing accuracy with\ninterpretability."
                },
                "authors": [
                    {
                        "name": "Kazuki Hayashi"
                    },
                    {
                        "name": "Hidetaka Kamigaito"
                    },
                    {
                        "name": "Shinya Kouda"
                    },
                    {
                        "name": "Taro Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Taro Watanabe"
                },
                "author": "Taro Watanabe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08448v1",
                "updated": "2025-05-13T11:23:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    23,
                    25,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T11:23:25Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    23,
                    25,
                    1,
                    133,
                    0
                ],
                "title": "Scalable UAV Multi-Hop Networking via Multi-Agent Reinforcement Learning\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable UAV Multi-Hop Networking via Multi-Agent Reinforcement Learning\n  with Large Language Models"
                },
                "summary": "In disaster scenarios, establishing robust emergency communication networks\nis critical, and unmanned aerial vehicles (UAVs) offer a promising solution to\nrapidly restore connectivity. However, organizing UAVs to form multi-hop\nnetworks in large-scale dynamic environments presents significant challenges,\nincluding limitations in algorithmic scalability and the vast exploration space\nrequired for coordinated decision-making. To address these issues, we propose\nMRLMN, a novel framework that integrates multi-agent reinforcement learning\n(MARL) and large language models (LLMs) to jointly optimize UAV agents toward\nachieving optimal networking performance. The framework incorporates a grouping\nstrategy with reward decomposition to enhance algorithmic scalability and\nbalance decision-making across UAVs. In addition, behavioral constraints are\napplied to selected key UAVs to improve the robustness of the network.\nFurthermore, the framework integrates LLM agents, leveraging knowledge\ndistillation to transfer their high-level decision-making capabilities to MARL\nagents. This enhances both the efficiency of exploration and the overall\ntraining process. In the distillation module, a Hungarian algorithm-based\nmatching scheme is applied to align the decision outputs of the LLM and MARL\nagents and define the distillation loss. Extensive simulation results validate\nthe effectiveness of our approach, demonstrating significant improvements in\nnetwork performance, including enhanced coverage and communication quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In disaster scenarios, establishing robust emergency communication networks\nis critical, and unmanned aerial vehicles (UAVs) offer a promising solution to\nrapidly restore connectivity. However, organizing UAVs to form multi-hop\nnetworks in large-scale dynamic environments presents significant challenges,\nincluding limitations in algorithmic scalability and the vast exploration space\nrequired for coordinated decision-making. To address these issues, we propose\nMRLMN, a novel framework that integrates multi-agent reinforcement learning\n(MARL) and large language models (LLMs) to jointly optimize UAV agents toward\nachieving optimal networking performance. The framework incorporates a grouping\nstrategy with reward decomposition to enhance algorithmic scalability and\nbalance decision-making across UAVs. In addition, behavioral constraints are\napplied to selected key UAVs to improve the robustness of the network.\nFurthermore, the framework integrates LLM agents, leveraging knowledge\ndistillation to transfer their high-level decision-making capabilities to MARL\nagents. This enhances both the efficiency of exploration and the overall\ntraining process. In the distillation module, a Hungarian algorithm-based\nmatching scheme is applied to align the decision outputs of the LLM and MARL\nagents and define the distillation loss. Extensive simulation results validate\nthe effectiveness of our approach, demonstrating significant improvements in\nnetwork performance, including enhanced coverage and communication quality."
                },
                "authors": [
                    {
                        "name": "Yanggang Xu"
                    },
                    {
                        "name": "Weijie Hong"
                    },
                    {
                        "name": "Jirong Zha"
                    },
                    {
                        "name": "Geng Chen"
                    },
                    {
                        "name": "Jianfeng Zheng"
                    },
                    {
                        "name": "Chen-Chun Hsia"
                    },
                    {
                        "name": "Xinlei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xinlei Chen"
                },
                "author": "Xinlei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13724v2",
                "updated": "2025-05-13T11:19:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    19,
                    44,
                    1,
                    133,
                    0
                ],
                "published": "2024-12-18T11:04:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    11,
                    4,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "USEFUSE: Uniform Stride for Enhanced Performance in Fused Layer\n  Architecture of Deep Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "USEFUSE: Uniform Stride for Enhanced Performance in Fused Layer\n  Architecture of Deep Neural Networks"
                },
                "summary": "Convolutional Neural Networks (CNNs) are crucial in various applications, but\ntheir deployment on resource-constrained edge devices poses challenges. This\nstudy presents the Sum-of-Products (SOP) units for convolution, which utilize\nlow-latency left-to-right bit-serial arithmetic to minimize response time and\nenhance overall performance. The study proposes a methodology for fusing\nmultiple convolution layers to reduce off-chip memory communication and\nincrease overall performance. An effective mechanism detects and skips\ninefficient convolutions after ReLU layers, minimizing power consumption\nwithout compromising accuracy. Furthermore, efficient tile movement guarantees\nuniform access to the fusion pyramid. An analysis demonstrates the utile stride\nstrategy improves operational intensity. Two designs cater to varied demands:\none focuses on minimal response time for mission-critical applications, and\nanother focuses on resource-constrained devices with comparable latency. This\napproach notably reduced redundant computations, improving the efficiency of\nCNN deployment on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convolutional Neural Networks (CNNs) are crucial in various applications, but\ntheir deployment on resource-constrained edge devices poses challenges. This\nstudy presents the Sum-of-Products (SOP) units for convolution, which utilize\nlow-latency left-to-right bit-serial arithmetic to minimize response time and\nenhance overall performance. The study proposes a methodology for fusing\nmultiple convolution layers to reduce off-chip memory communication and\nincrease overall performance. An effective mechanism detects and skips\ninefficient convolutions after ReLU layers, minimizing power consumption\nwithout compromising accuracy. Furthermore, efficient tile movement guarantees\nuniform access to the fusion pyramid. An analysis demonstrates the utile stride\nstrategy improves operational intensity. Two designs cater to varied demands:\none focuses on minimal response time for mission-critical applications, and\nanother focuses on resource-constrained devices with comparable latency. This\napproach notably reduced redundant computations, improving the efficiency of\nCNN deployment on edge devices."
                },
                "authors": [
                    {
                        "name": "Muhammad Sohail Ibrahim"
                    },
                    {
                        "name": "Muhammad Usman"
                    },
                    {
                        "name": "Jeong-A Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jeong-A Lee"
                },
                "author": "Jeong-A Lee",
                "arxiv_comment": "Accepted for publication in the Journal of Systems Architecture on 11\n  May, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12514v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12514v5",
                "updated": "2025-05-13T11:02:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    2,
                    20,
                    1,
                    133,
                    0
                ],
                "published": "2024-09-19T07:10:18Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    7,
                    10,
                    18,
                    3,
                    263,
                    0
                ],
                "title": "TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for\n  Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for\n  Robotic Manipulation"
                },
                "summary": "Vision-Language-Action (VLA) models have shown remarkable potential in\nvisuomotor control and instruction comprehension through end-to-end learning\nprocesses. However, current VLA models face significant challenges: they are\nslow during inference and require extensive pre-training on large amounts of\nrobotic data, making real-world deployment difficult. In this paper, we\nintroduce a new family of compact vision-language-action models, called\nTinyVLA, which offers two key advantages over existing VLA models: (1) faster\ninference speeds, and (2) improved data efficiency, eliminating the need for\npre-training stage. Our framework incorporates two essential components to\nbuild TinyVLA: (1) initializing the policy backbone with robust, high-speed\nmultimodal models, and (2) integrating a diffusion policy decoder during\nfine-tuning to enable precise robot actions. We conducted extensive evaluations\nof TinyVLA in both simulation and on real robots, demonstrating that our\napproach significantly outperforms the state-of-the-art VLA model, OpenVLA, in\nterms of speed and data efficiency, while delivering comparable or superior\nperformance. Additionally, TinyVLA exhibits strong generalization capabilities\nacross various dimensions, including language instructions, novel objects,\nunseen positions, changes in object appearance, background variations, and\nenvironmental shifts, often matching or exceeding the performance of OpenVLA.\nWe believe that \\methodname offers an interesting perspective on utilizing\npre-trained multimodal models for policy learning. Our project is at\nhttps://tiny-vla.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have shown remarkable potential in\nvisuomotor control and instruction comprehension through end-to-end learning\nprocesses. However, current VLA models face significant challenges: they are\nslow during inference and require extensive pre-training on large amounts of\nrobotic data, making real-world deployment difficult. In this paper, we\nintroduce a new family of compact vision-language-action models, called\nTinyVLA, which offers two key advantages over existing VLA models: (1) faster\ninference speeds, and (2) improved data efficiency, eliminating the need for\npre-training stage. Our framework incorporates two essential components to\nbuild TinyVLA: (1) initializing the policy backbone with robust, high-speed\nmultimodal models, and (2) integrating a diffusion policy decoder during\nfine-tuning to enable precise robot actions. We conducted extensive evaluations\nof TinyVLA in both simulation and on real robots, demonstrating that our\napproach significantly outperforms the state-of-the-art VLA model, OpenVLA, in\nterms of speed and data efficiency, while delivering comparable or superior\nperformance. Additionally, TinyVLA exhibits strong generalization capabilities\nacross various dimensions, including language instructions, novel objects,\nunseen positions, changes in object appearance, background variations, and\nenvironmental shifts, often matching or exceeding the performance of OpenVLA.\nWe believe that \\methodname offers an interesting perspective on utilizing\npre-trained multimodal models for policy learning. Our project is at\nhttps://tiny-vla.github.io."
                },
                "authors": [
                    {
                        "name": "Junjie Wen"
                    },
                    {
                        "name": "Yichen Zhu"
                    },
                    {
                        "name": "Jinming Li"
                    },
                    {
                        "name": "Minjie Zhu"
                    },
                    {
                        "name": "Kun Wu"
                    },
                    {
                        "name": "Zhiyuan Xu"
                    },
                    {
                        "name": "Ning Liu"
                    },
                    {
                        "name": "Ran Cheng"
                    },
                    {
                        "name": "Chaomin Shen"
                    },
                    {
                        "name": "Yaxin Peng"
                    },
                    {
                        "name": "Feifei Feng"
                    },
                    {
                        "name": "Jian Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Tang"
                },
                "author": "Jian Tang",
                "arxiv_comment": "add more citations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12514v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12514v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08414v1",
                "updated": "2025-05-13T10:13:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    10,
                    13,
                    26,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T10:13:26Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    10,
                    13,
                    26,
                    1,
                    133,
                    0
                ],
                "title": "An integrated language-vision foundation model for conversational\n  diagnostics and triaging in primary eye care",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An integrated language-vision foundation model for conversational\n  diagnostics and triaging in primary eye care"
                },
                "summary": "Current deep learning models are mostly task specific and lack a\nuser-friendly interface to operate. We present Meta-EyeFM, a multi-function\nfoundation model that integrates a large language model (LLM) with vision\nfoundation models (VFMs) for ocular disease assessment. Meta-EyeFM leverages a\nrouting mechanism to enable accurate task-specific analysis based on text\nqueries. Using Low Rank Adaptation, we fine-tuned our VFMs to detect ocular and\nsystemic diseases, differentiate ocular disease severity, and identify common\nocular signs. The model achieved 100% accuracy in routing fundus images to\nappropriate VFMs, which achieved $\\ge$ 82.2% accuracy in disease detection,\n$\\ge$ 89% in severity differentiation, $\\ge$ 76% in sign identification.\nMeta-EyeFM was 11% to 43% more accurate than Gemini-1.5-flash and ChatGPT-4o\nLMMs in detecting various eye diseases and comparable to an ophthalmologist.\nThis system offers enhanced usability and diagnostic performance, making it a\nvaluable decision support tool for primary eye care or an online LLM for fundus\nevaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current deep learning models are mostly task specific and lack a\nuser-friendly interface to operate. We present Meta-EyeFM, a multi-function\nfoundation model that integrates a large language model (LLM) with vision\nfoundation models (VFMs) for ocular disease assessment. Meta-EyeFM leverages a\nrouting mechanism to enable accurate task-specific analysis based on text\nqueries. Using Low Rank Adaptation, we fine-tuned our VFMs to detect ocular and\nsystemic diseases, differentiate ocular disease severity, and identify common\nocular signs. The model achieved 100% accuracy in routing fundus images to\nappropriate VFMs, which achieved $\\ge$ 82.2% accuracy in disease detection,\n$\\ge$ 89% in severity differentiation, $\\ge$ 76% in sign identification.\nMeta-EyeFM was 11% to 43% more accurate than Gemini-1.5-flash and ChatGPT-4o\nLMMs in detecting various eye diseases and comparable to an ophthalmologist.\nThis system offers enhanced usability and diagnostic performance, making it a\nvaluable decision support tool for primary eye care or an online LLM for fundus\nevaluation."
                },
                "authors": [
                    {
                        "name": "Zhi Da Soh"
                    },
                    {
                        "name": "Yang Bai"
                    },
                    {
                        "name": "Kai Yu"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Xiaofeng Lei"
                    },
                    {
                        "name": "Sahil Thakur"
                    },
                    {
                        "name": "Zann Lee"
                    },
                    {
                        "name": "Lee Ching Linette Phang"
                    },
                    {
                        "name": "Qingsheng Peng"
                    },
                    {
                        "name": "Can Can Xue"
                    },
                    {
                        "name": "Rachel Shujuan Chong"
                    },
                    {
                        "name": "Quan V. Hoang"
                    },
                    {
                        "name": "Lavanya Raghavan"
                    },
                    {
                        "name": "Yih Chung Tham"
                    },
                    {
                        "name": "Charumathi Sabanayagam"
                    },
                    {
                        "name": "Wei-Chi Wu"
                    },
                    {
                        "name": "Ming-Chih Ho"
                    },
                    {
                        "name": "Jiangnan He"
                    },
                    {
                        "name": "Preeti Gupta"
                    },
                    {
                        "name": "Ecosse Lamoureux"
                    },
                    {
                        "name": "Seang Mei Saw"
                    },
                    {
                        "name": "Vinay Nangia"
                    },
                    {
                        "name": "Songhomitra Panda-Jonas"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Ya Xing Wang"
                    },
                    {
                        "name": "Xinxing Xu"
                    },
                    {
                        "name": "Jost B. Jonas"
                    },
                    {
                        "name": "Tien Yin Wong"
                    },
                    {
                        "name": "Rick Siow Mong Goh"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Ching-Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Ching-Yu Cheng"
                },
                "author": "Ching-Yu Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07278v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07278v2",
                "updated": "2025-05-13T10:03:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    10,
                    3,
                    37,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-12T07:01:33Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    7,
                    1,
                    33,
                    0,
                    132,
                    0
                ],
                "title": "Coordinated Spatial Reuse Scheduling With Machine Learning in IEEE\n  802.11 MAPC Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coordinated Spatial Reuse Scheduling With Machine Learning in IEEE\n  802.11 MAPC Networks"
                },
                "summary": "The densification of Wi-Fi deployments means that fully distributed random\nchannel access is no longer sufficient for high and predictable performance.\nTherefore, the upcoming IEEE 802.11bn amendment introduces multi-access point\ncoordination (MAPC) methods. This paper addresses a variant of MAPC called\ncoordinated spatial reuse (C-SR), where devices transmit simultaneously on the\nsame channel, with the power adjusted to minimize interference. The C-SR\nscheduling problem is selecting which devices transmit concurrently and with\nwhat settings. We provide a theoretical upper bound model, optimized for either\nthroughput or fairness, which finds the best possible transmission schedule\nusing mixed-integer linear programming. Then, a practical, probing-based\napproach is proposed which uses multi-armed bandits (MABs), a type of\nreinforcement learning, to solve the C-SR scheduling problem. We validate both\nclassical (flat) MAB and hierarchical MAB (H-MAB) schemes with simulations and\nin a testbed. Using H-MABs for C-SR improves aggregate throughput over legacy\nIEEE 802.11 (on average by 80\\% in random scenarios), without reducing the\nnumber of transmission opportunities per station. Finally, our framework is\nlightweight and ready for implementation in Wi-Fi devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The densification of Wi-Fi deployments means that fully distributed random\nchannel access is no longer sufficient for high and predictable performance.\nTherefore, the upcoming IEEE 802.11bn amendment introduces multi-access point\ncoordination (MAPC) methods. This paper addresses a variant of MAPC called\ncoordinated spatial reuse (C-SR), where devices transmit simultaneously on the\nsame channel, with the power adjusted to minimize interference. The C-SR\nscheduling problem is selecting which devices transmit concurrently and with\nwhat settings. We provide a theoretical upper bound model, optimized for either\nthroughput or fairness, which finds the best possible transmission schedule\nusing mixed-integer linear programming. Then, a practical, probing-based\napproach is proposed which uses multi-armed bandits (MABs), a type of\nreinforcement learning, to solve the C-SR scheduling problem. We validate both\nclassical (flat) MAB and hierarchical MAB (H-MAB) schemes with simulations and\nin a testbed. Using H-MABs for C-SR improves aggregate throughput over legacy\nIEEE 802.11 (on average by 80\\% in random scenarios), without reducing the\nnumber of transmission opportunities per station. Finally, our framework is\nlightweight and ready for implementation in Wi-Fi devices."
                },
                "authors": [
                    {
                        "name": "Maksymilian Wojnar"
                    },
                    {
                        "name": "Wojciech Ciężobka"
                    },
                    {
                        "name": "Artur Tomaszewski"
                    },
                    {
                        "name": "Piotr Chołda"
                    },
                    {
                        "name": "Krzysztof Rusek"
                    },
                    {
                        "name": "Katarzyna Kosek-Szott"
                    },
                    {
                        "name": "Jetmir Haxhibeqiri"
                    },
                    {
                        "name": "Jeroen Hoebeke"
                    },
                    {
                        "name": "Boris Bellalta"
                    },
                    {
                        "name": "Anatolij Zubow"
                    },
                    {
                        "name": "Falko Dressler"
                    },
                    {
                        "name": "Szymon Szott"
                    }
                ],
                "author_detail": {
                    "name": "Szymon Szott"
                },
                "author": "Szymon Szott",
                "arxiv_comment": "16 pages, 18 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07278v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07278v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08402v1",
                "updated": "2025-05-13T09:57:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    57,
                    28,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T09:57:28Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    57,
                    28,
                    1,
                    133,
                    0
                ],
                "title": "TUMS: Enhancing Tool-use Abilities of LLMs with Multi-structure Handlers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TUMS: Enhancing Tool-use Abilities of LLMs with Multi-structure Handlers"
                },
                "summary": "Recently, large language models(LLMs) have played an increasingly important\nrole in solving a wide range of NLP tasks, leveraging their capabilities of\nnatural language understanding and generating. Integration with external tools\nfurther enhances LLMs' effectiveness, providing more precise, timely, and\nspecialized responses. However, LLMs still encounter difficulties with\nnon-executable actions and improper actions, which are primarily attributed to\nincorrect parameters. The process of generating parameters by LLMs is confined\nto the tool level, employing the coarse-grained strategy without considering\nthe different difficulties of various tools. To address this issue, we propose\nTUMS, a novel framework designed to enhance the tool-use capabilities of LLMs\nby transforming tool-level processing into parameter-level processing.\nSpecifically, our framework consists of four key components: (1) an intent\nrecognizer that identifies the user's intent to help LLMs better understand the\ntask; (2) a task decomposer that breaks down complex tasks into simpler\nsubtasks, each involving a tool call; (3) a subtask processor equipped with\nmulti-structure handlers to generate accurate parameters; and (4) an executor.\nOur empirical studies have evidenced the effectiveness and efficiency of the\nTUMS framework with an average of 19.6\\% and 50.6\\% improvement separately on\neasy and hard benchmarks of ToolQA, meanwhile, we demonstrated the key\ncontribution of each part with ablation experiments, offering more insights and\nstimulating future research on Tool-augmented LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models(LLMs) have played an increasingly important\nrole in solving a wide range of NLP tasks, leveraging their capabilities of\nnatural language understanding and generating. Integration with external tools\nfurther enhances LLMs' effectiveness, providing more precise, timely, and\nspecialized responses. However, LLMs still encounter difficulties with\nnon-executable actions and improper actions, which are primarily attributed to\nincorrect parameters. The process of generating parameters by LLMs is confined\nto the tool level, employing the coarse-grained strategy without considering\nthe different difficulties of various tools. To address this issue, we propose\nTUMS, a novel framework designed to enhance the tool-use capabilities of LLMs\nby transforming tool-level processing into parameter-level processing.\nSpecifically, our framework consists of four key components: (1) an intent\nrecognizer that identifies the user's intent to help LLMs better understand the\ntask; (2) a task decomposer that breaks down complex tasks into simpler\nsubtasks, each involving a tool call; (3) a subtask processor equipped with\nmulti-structure handlers to generate accurate parameters; and (4) an executor.\nOur empirical studies have evidenced the effectiveness and efficiency of the\nTUMS framework with an average of 19.6\\% and 50.6\\% improvement separately on\neasy and hard benchmarks of ToolQA, meanwhile, we demonstrated the key\ncontribution of each part with ablation experiments, offering more insights and\nstimulating future research on Tool-augmented LLMs."
                },
                "authors": [
                    {
                        "name": "Aiyao He"
                    },
                    {
                        "name": "Sijia Cui"
                    },
                    {
                        "name": "Shuai Xu"
                    },
                    {
                        "name": "Yanna Wang"
                    },
                    {
                        "name": "Bo Xu"
                    }
                ],
                "author_detail": {
                    "name": "Bo Xu"
                },
                "author": "Bo Xu",
                "arxiv_comment": "Accepted to ICONIP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13989v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13989v2",
                "updated": "2025-05-13T09:36:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    36,
                    3,
                    1,
                    133,
                    0
                ],
                "published": "2025-04-18T13:46:58Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "title": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs"
                },
                "summary": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40% increase in\naccuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40% increase in\naccuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization."
                },
                "authors": [
                    {
                        "name": "Lucas Maisonnave"
                    },
                    {
                        "name": "Cyril Moineau"
                    },
                    {
                        "name": "Olivier Bichler"
                    },
                    {
                        "name": "Fabrice Rastello"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice Rastello"
                },
                "author": "Fabrice Rastello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13989v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13989v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08389v1",
                "updated": "2025-05-13T09:35:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    35,
                    40,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T09:35:40Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    35,
                    40,
                    1,
                    133,
                    0
                ],
                "title": "Towards Contamination Resistant Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Contamination Resistant Benchmarks"
                },
                "summary": "The rapid development of large language models (LLMs) has transformed the\nlandscape of natural language processing. Evaluating LLMs properly is crucial\nfor understanding their potential and addressing concerns such as safety.\nHowever, LLM evaluation is confronted by various factors, among which\ncontamination stands out as a key issue that undermines the reliability of\nevaluations. In this work, we introduce the concept of contamination resistance\nto address this challenge. We propose a benchmark based on Caesar ciphers\n(e.g., \"ab\" to \"bc\" when the shift is 1), which, despite its simplicity, is an\nexcellent example of a contamination resistant benchmark. We test this\nbenchmark on widely used LLMs under various settings, and we find that these\nmodels struggle with this benchmark when contamination is controlled. Our\nfindings reveal issues in current LLMs and raise important questions regarding\ntheir true capabilities. Our work contributes to the development of\ncontamination resistant benchmarks, enabling more rigorous LLM evaluation and\noffering insights into the true capabilities and limitations of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) has transformed the\nlandscape of natural language processing. Evaluating LLMs properly is crucial\nfor understanding their potential and addressing concerns such as safety.\nHowever, LLM evaluation is confronted by various factors, among which\ncontamination stands out as a key issue that undermines the reliability of\nevaluations. In this work, we introduce the concept of contamination resistance\nto address this challenge. We propose a benchmark based on Caesar ciphers\n(e.g., \"ab\" to \"bc\" when the shift is 1), which, despite its simplicity, is an\nexcellent example of a contamination resistant benchmark. We test this\nbenchmark on widely used LLMs under various settings, and we find that these\nmodels struggle with this benchmark when contamination is controlled. Our\nfindings reveal issues in current LLMs and raise important questions regarding\ntheir true capabilities. Our work contributes to the development of\ncontamination resistant benchmarks, enabling more rigorous LLM evaluation and\noffering insights into the true capabilities and limitations of LLMs."
                },
                "authors": [
                    {
                        "name": "Rahmatullah Musawi"
                    },
                    {
                        "name": "Sheng Lu"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Lu"
                },
                "author": "Sheng Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08364v1",
                "updated": "2025-05-13T09:10:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    10,
                    48,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T09:10:48Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    10,
                    48,
                    1,
                    133,
                    0
                ],
                "title": "Learning Like Humans: Advancing LLM Reasoning Capabilities via Adaptive\n  Difficulty Curriculum Learning and Expert-Guided Self-Reformulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Like Humans: Advancing LLM Reasoning Capabilities via Adaptive\n  Difficulty Curriculum Learning and Expert-Guided Self-Reformulation"
                },
                "summary": "Despite impressive progress in areas like mathematical reasoning, large\nlanguage models still face significant challenges in consistently solving\ncomplex problems. Drawing inspiration from key human learning strategies, we\npropose two novel strategies to enhance the capability of large language models\nto solve these complex problems. First, Adaptive Difficulty Curriculum Learning\n(ADCL) is a novel curriculum learning strategy that tackles the Difficulty\nShift phenomenon (i.e., a model's perception of problem difficulty dynamically\nchanges during training) by periodically re-estimating difficulty within\nupcoming data batches to maintain alignment with the model's evolving\ncapabilities. Second, Expert-Guided Self-Reformulation (EGSR) is a novel\nreinforcement learning strategy that bridges the gap between imitation learning\nand pure exploration by guiding models to reformulate expert solutions within\ntheir own conceptual framework, rather than relying on direct imitation,\nfostering deeper understanding and knowledge assimilation. Extensive\nexperiments on challenging mathematical reasoning benchmarks, using Qwen2.5-7B\nas the base model, demonstrate that these human-inspired strategies\nsynergistically and significantly enhance performance. Notably, their combined\napplication improves performance over the standard Zero-RL baseline by 10% on\nthe AIME24 benchmark and 16.6% on AIME25.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite impressive progress in areas like mathematical reasoning, large\nlanguage models still face significant challenges in consistently solving\ncomplex problems. Drawing inspiration from key human learning strategies, we\npropose two novel strategies to enhance the capability of large language models\nto solve these complex problems. First, Adaptive Difficulty Curriculum Learning\n(ADCL) is a novel curriculum learning strategy that tackles the Difficulty\nShift phenomenon (i.e., a model's perception of problem difficulty dynamically\nchanges during training) by periodically re-estimating difficulty within\nupcoming data batches to maintain alignment with the model's evolving\ncapabilities. Second, Expert-Guided Self-Reformulation (EGSR) is a novel\nreinforcement learning strategy that bridges the gap between imitation learning\nand pure exploration by guiding models to reformulate expert solutions within\ntheir own conceptual framework, rather than relying on direct imitation,\nfostering deeper understanding and knowledge assimilation. Extensive\nexperiments on challenging mathematical reasoning benchmarks, using Qwen2.5-7B\nas the base model, demonstrate that these human-inspired strategies\nsynergistically and significantly enhance performance. Notably, their combined\napplication improves performance over the standard Zero-RL baseline by 10% on\nthe AIME24 benchmark and 16.6% on AIME25."
                },
                "authors": [
                    {
                        "name": "Enci Zhang"
                    },
                    {
                        "name": "Xingang Yan"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Tianxiang Zhang"
                    },
                    {
                        "name": "Qianchun Lu"
                    }
                ],
                "author_detail": {
                    "name": "Qianchun Lu"
                },
                "author": "Qianchun Lu",
                "arxiv_comment": "14 pages, 3 figs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08351v1",
                "updated": "2025-05-13T08:50:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    8,
                    50,
                    57,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T08:50:57Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    8,
                    50,
                    57,
                    1,
                    133,
                    0
                ],
                "title": "Alignment Drift in CEFR-prompted LLMs for Interactive Spanish Tutoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment Drift in CEFR-prompted LLMs for Interactive Spanish Tutoring"
                },
                "summary": "This paper investigates the potentials of Large Language Models (LLMs) as\nadaptive tutors in the context of second-language learning. In particular, we\nevaluate whether system prompting can reliably constrain LLMs to generate only\ntext appropriate to the student's competence level. We simulate full\nteacher-student dialogues in Spanish using instruction-tuned, open-source LLMs\nranging in size from 7B to 12B parameters. Dialogues are generated by having an\nLLM alternate between tutor and student roles with separate chat histories. The\noutput from the tutor model is then used to evaluate the effectiveness of\nCEFR-based prompting to control text difficulty across three proficiency levels\n(A1, B1, C1). Our findings suggest that while system prompting can be used to\nconstrain model outputs, prompting alone is too brittle for sustained,\nlong-term interactional contexts - a phenomenon we term alignment drift. Our\nresults provide insights into the feasibility of LLMs for personalized,\nproficiency-aligned adaptive tutors and provide a scalable method for low-cost\nevaluation of model performance without human participants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the potentials of Large Language Models (LLMs) as\nadaptive tutors in the context of second-language learning. In particular, we\nevaluate whether system prompting can reliably constrain LLMs to generate only\ntext appropriate to the student's competence level. We simulate full\nteacher-student dialogues in Spanish using instruction-tuned, open-source LLMs\nranging in size from 7B to 12B parameters. Dialogues are generated by having an\nLLM alternate between tutor and student roles with separate chat histories. The\noutput from the tutor model is then used to evaluate the effectiveness of\nCEFR-based prompting to control text difficulty across three proficiency levels\n(A1, B1, C1). Our findings suggest that while system prompting can be used to\nconstrain model outputs, prompting alone is too brittle for sustained,\nlong-term interactional contexts - a phenomenon we term alignment drift. Our\nresults provide insights into the feasibility of LLMs for personalized,\nproficiency-aligned adaptive tutors and provide a scalable method for low-cost\nevaluation of model performance without human participants."
                },
                "authors": [
                    {
                        "name": "Mina Almasi"
                    },
                    {
                        "name": "Ross Deans Kristensen-McLachlan"
                    }
                ],
                "author_detail": {
                    "name": "Ross Deans Kristensen-McLachlan"
                },
                "author": "Ross Deans Kristensen-McLachlan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02195v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02195v2",
                "updated": "2025-05-13T08:41:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    8,
                    41,
                    49,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-04T17:30:44Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    17,
                    30,
                    44,
                    6,
                    124,
                    0
                ],
                "title": "Scalable Genomic Context Analysis with GCsnap2 on HPC Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Genomic Context Analysis with GCsnap2 on HPC Clusters"
                },
                "summary": "GCsnap2 Cluster is a scalable, high performance tool for genomic context\nanalysis, developed to overcome the limitations of its predecessor, GCsnap1\nDesktop. Leveraging distributed computing with mpi4py[.]futures, GCsnap2\nCluster achieved a 22x improvement in execution time and can now perform\ngenomic context analysis for hundreds of thousands of input sequences in HPC\nclusters. Its modular architecture enables the creation of task-specific\nworkflows and flexible deployment in various computational environments, making\nit well suited for bioinformatics studies of large-scale datasets. This work\nhighlights the potential for applying similar approaches to solve scalability\nchallenges in other scientific domains that rely on large-scale data analysis\npipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GCsnap2 Cluster is a scalable, high performance tool for genomic context\nanalysis, developed to overcome the limitations of its predecessor, GCsnap1\nDesktop. Leveraging distributed computing with mpi4py[.]futures, GCsnap2\nCluster achieved a 22x improvement in execution time and can now perform\ngenomic context analysis for hundreds of thousands of input sequences in HPC\nclusters. Its modular architecture enables the creation of task-specific\nworkflows and flexible deployment in various computational environments, making\nit well suited for bioinformatics studies of large-scale datasets. This work\nhighlights the potential for applying similar approaches to solve scalability\nchallenges in other scientific domains that rely on large-scale data analysis\npipelines."
                },
                "authors": [
                    {
                        "name": "Reto Krummenacher"
                    },
                    {
                        "name": "Osman Seckin Simsek"
                    },
                    {
                        "name": "Michèle Leemann"
                    },
                    {
                        "name": "Leila T. Alexander"
                    },
                    {
                        "name": "Torsten Schwede"
                    },
                    {
                        "name": "Florina M. Ciorba"
                    },
                    {
                        "name": "Joana Pereira"
                    }
                ],
                "author_detail": {
                    "name": "Joana Pereira"
                },
                "author": "Joana Pereira",
                "arxiv_comment": "16 pages, 9 figures, 2 tables. Preprint submitted to arXiv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02195v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.3; C.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.06018v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.06018v3",
                "updated": "2025-05-13T08:33:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    8,
                    33,
                    57,
                    1,
                    133,
                    0
                ],
                "published": "2023-05-10T10:04:08Z",
                "published_parsed": [
                    2023,
                    5,
                    10,
                    10,
                    4,
                    8,
                    2,
                    130,
                    0
                ],
                "title": "TARGET: Automated Scenario Generation from Traffic Rules for Testing\n  Autonomous Vehicles via Validated LLM-Guided Knowledge Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TARGET: Automated Scenario Generation from Traffic Rules for Testing\n  Autonomous Vehicles via Validated LLM-Guided Knowledge Extraction"
                },
                "summary": "Recent incidents with autonomous vehicles highlight the need for rigorous\ntesting to ensure safety and robustness. Constructing test scenarios for\nautonomous driving systems (ADSs), however, is labor-intensive. We propose\nTARGET, an end-to-end framework that automatically generates test scenarios\nfrom traffic rules. To address complexity, we leverage a Large Language Model\n(LLM) to extract knowledge from traffic rules. To mitigate hallucinations\ncaused by large context during input processing, we introduce a domain-specific\nlanguage (DSL) designed to be syntactically simple and compositional. This\ndesign allows the LLM to learn and generate test scenarios in a modular manner\nwhile enabling syntactic and semantic validation for each component. Based on\nthese validated representations, TARGET synthesizes executable scripts to\nrender scenarios in simulation. Evaluated seven ADSs with 284 scenarios derived\nfrom 54 traffic rules, TARGET uncovered 610 rule violations, collisions, and\nother issues. For each violation, TARGET generates scenario recordings and\ndetailed logs, aiding root cause analysis. Two identified issues were confirmed\nby ADS developers: one linked to an existing bug report and the other to\nlimited ADS functionality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent incidents with autonomous vehicles highlight the need for rigorous\ntesting to ensure safety and robustness. Constructing test scenarios for\nautonomous driving systems (ADSs), however, is labor-intensive. We propose\nTARGET, an end-to-end framework that automatically generates test scenarios\nfrom traffic rules. To address complexity, we leverage a Large Language Model\n(LLM) to extract knowledge from traffic rules. To mitigate hallucinations\ncaused by large context during input processing, we introduce a domain-specific\nlanguage (DSL) designed to be syntactically simple and compositional. This\ndesign allows the LLM to learn and generate test scenarios in a modular manner\nwhile enabling syntactic and semantic validation for each component. Based on\nthese validated representations, TARGET synthesizes executable scripts to\nrender scenarios in simulation. Evaluated seven ADSs with 284 scenarios derived\nfrom 54 traffic rules, TARGET uncovered 610 rule violations, collisions, and\nother issues. For each violation, TARGET generates scenario recordings and\ndetailed logs, aiding root cause analysis. Two identified issues were confirmed\nby ADS developers: one linked to an existing bug report and the other to\nlimited ADS functionality."
                },
                "authors": [
                    {
                        "name": "Yao Deng"
                    },
                    {
                        "name": "Jiaohong Yao"
                    },
                    {
                        "name": "Zhi Tu"
                    },
                    {
                        "name": "Xi Zheng"
                    },
                    {
                        "name": "Mengshi Zhang"
                    },
                    {
                        "name": "Tianyi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhang"
                },
                "author": "Tianyi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.06018v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.06018v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08341v1",
                "updated": "2025-05-13T08:33:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    8,
                    33,
                    54,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T08:33:54Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    8,
                    33,
                    54,
                    1,
                    133,
                    0
                ],
                "title": "Benchmarking AI scientists in omics data-driven biological research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking AI scientists in omics data-driven biological research"
                },
                "summary": "The rise of large language models and multi-agent systems has sparked growing\ninterest in AI scientists capable of autonomous biological research. However,\nexisting benchmarks either focus on reasoning without data or on data analysis\nwith predefined statistical answers, lacking realistic, data-driven evaluation\nsettings. Here, we introduce the Biological AI Scientist Benchmark (BaisBench),\na benchmark designed to assess AI scientists' ability to generate biological\ndiscoveries through data analysis and reasoning with external knowledge.\nBaisBench comprises two tasks: cell type annotation on 31 expert-labeled\nsingle-cell datasets, and scientific discovery through answering 198\nmultiple-choice questions derived from the biological insights of 41 recent\nsingle-cell studies. Systematic experiments on state-of-the-art AI scientists\nand LLM agents showed that while promising, current models still substantially\nunderperform human experts on both tasks. We hope BaisBench will fill this gap\nand serve as a foundation for advancing and evaluating AI models for scientific\ndiscovery. The benchmark can be found at: https://github.com/EperLuo/BaisBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models and multi-agent systems has sparked growing\ninterest in AI scientists capable of autonomous biological research. However,\nexisting benchmarks either focus on reasoning without data or on data analysis\nwith predefined statistical answers, lacking realistic, data-driven evaluation\nsettings. Here, we introduce the Biological AI Scientist Benchmark (BaisBench),\na benchmark designed to assess AI scientists' ability to generate biological\ndiscoveries through data analysis and reasoning with external knowledge.\nBaisBench comprises two tasks: cell type annotation on 31 expert-labeled\nsingle-cell datasets, and scientific discovery through answering 198\nmultiple-choice questions derived from the biological insights of 41 recent\nsingle-cell studies. Systematic experiments on state-of-the-art AI scientists\nand LLM agents showed that while promising, current models still substantially\nunderperform human experts on both tasks. We hope BaisBench will fill this gap\nand serve as a foundation for advancing and evaluating AI models for scientific\ndiscovery. The benchmark can be found at: https://github.com/EperLuo/BaisBench."
                },
                "authors": [
                    {
                        "name": "Erpai Luo"
                    },
                    {
                        "name": "Jinmeng Jia"
                    },
                    {
                        "name": "Yifan Xiong"
                    },
                    {
                        "name": "Xiangyu Li"
                    },
                    {
                        "name": "Xiaobo Guo"
                    },
                    {
                        "name": "Baoqi Yu"
                    },
                    {
                        "name": "Lei Wei"
                    },
                    {
                        "name": "Xuegong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xuegong Zhang"
                },
                "author": "Xuegong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05442v2",
                "updated": "2025-05-13T08:00:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    8,
                    0,
                    22,
                    1,
                    133,
                    0
                ],
                "published": "2025-02-08T04:17:28Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    4,
                    17,
                    28,
                    5,
                    39,
                    0
                ],
                "title": "The Odyssey of the Fittest: Can Agents Survive and Still Be Good?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Odyssey of the Fittest: Can Agents Survive and Still Be Good?"
                },
                "summary": "As AI models grow in power and generality, understanding how agents learn and\nmake decisions in complex environments is critical to promoting ethical\nbehavior. This study introduces the Odyssey, a lightweight, adaptive text based\nadventure game, providing a scalable framework for exploring AI ethics and\nsafety. The Odyssey examines the ethical implications of implementing\nbiological drives, specifically, self preservation, into three different\nagents. A Bayesian agent optimized with NEAT, a Bayesian agent optimized with\nstochastic variational inference, and a GPT 4o agent. The agents select actions\nat each scenario to survive, adapting to increasingly challenging scenarios.\nPost simulation analysis evaluates the ethical scores of the agent decisions,\nuncovering the tradeoffs it navigates to survive. Specifically, analysis finds\nthat when danger increases, agents ethical behavior becomes unpredictable.\nSurprisingly, the GPT 4o agent outperformed the Bayesian models in both\nsurvival and ethical consistency, challenging assumptions about traditional\nprobabilistic methods and raising a new challenge to understand the mechanisms\nof LLMs' probabilistic reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI models grow in power and generality, understanding how agents learn and\nmake decisions in complex environments is critical to promoting ethical\nbehavior. This study introduces the Odyssey, a lightweight, adaptive text based\nadventure game, providing a scalable framework for exploring AI ethics and\nsafety. The Odyssey examines the ethical implications of implementing\nbiological drives, specifically, self preservation, into three different\nagents. A Bayesian agent optimized with NEAT, a Bayesian agent optimized with\nstochastic variational inference, and a GPT 4o agent. The agents select actions\nat each scenario to survive, adapting to increasingly challenging scenarios.\nPost simulation analysis evaluates the ethical scores of the agent decisions,\nuncovering the tradeoffs it navigates to survive. Specifically, analysis finds\nthat when danger increases, agents ethical behavior becomes unpredictable.\nSurprisingly, the GPT 4o agent outperformed the Bayesian models in both\nsurvival and ethical consistency, challenging assumptions about traditional\nprobabilistic methods and raising a new challenge to understand the mechanisms\nof LLMs' probabilistic reasoning."
                },
                "authors": [
                    {
                        "name": "Dylan Waldner"
                    },
                    {
                        "name": "Risto Miikkulainen"
                    }
                ],
                "author_detail": {
                    "name": "Risto Miikkulainen"
                },
                "author": "Risto Miikkulainen",
                "arxiv_comment": "Accepted to CogSci 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17565v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17565v3",
                "updated": "2025-05-13T07:43:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    7,
                    43,
                    57,
                    1,
                    133,
                    0
                ],
                "published": "2025-04-24T13:57:53Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    57,
                    53,
                    3,
                    114,
                    0
                ],
                "title": "DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale\n  Difficulty-Graded Data Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale\n  Difficulty-Graded Data Training"
                },
                "summary": "Although large language models (LLMs) have recently achieved remarkable\nperformance on various complex reasoning benchmarks, the academic community\nstill lacks an in-depth understanding of base model training processes and data\nquality. To address this, we construct a large-scale, difficulty-graded\nreasoning dataset containing approximately 3.34 million unique queries of\nvarying difficulty levels and about 40 million distilled responses generated by\nmultiple models over several passes. Leveraging pass rate and Coefficient of\nVariation (CV), we precisely select the most valuable training data to enhance\nreasoning capability. Notably, we observe a training pattern shift, indicating\nthat reasoning-focused training based on base models requires higher learning\nrates for effective training. Using this carefully selected data, we\nsignificantly improve the reasoning capabilities of the base model, achieving a\npass rate of 79.2\\% on the AIME2024 mathematical reasoning benchmark. This\nresult surpasses most current distilled models and closely approaches\nstate-of-the-art performance. We provide detailed descriptions of our data\nprocessing, difficulty assessment, and training methodology, and have publicly\nreleased all datasets and methods to promote rapid progress in open-source\nlong-reasoning LLMs. The dataset is available at:\n\\href{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M}{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) have recently achieved remarkable\nperformance on various complex reasoning benchmarks, the academic community\nstill lacks an in-depth understanding of base model training processes and data\nquality. To address this, we construct a large-scale, difficulty-graded\nreasoning dataset containing approximately 3.34 million unique queries of\nvarying difficulty levels and about 40 million distilled responses generated by\nmultiple models over several passes. Leveraging pass rate and Coefficient of\nVariation (CV), we precisely select the most valuable training data to enhance\nreasoning capability. Notably, we observe a training pattern shift, indicating\nthat reasoning-focused training based on base models requires higher learning\nrates for effective training. Using this carefully selected data, we\nsignificantly improve the reasoning capabilities of the base model, achieving a\npass rate of 79.2\\% on the AIME2024 mathematical reasoning benchmark. This\nresult surpasses most current distilled models and closely approaches\nstate-of-the-art performance. We provide detailed descriptions of our data\nprocessing, difficulty assessment, and training methodology, and have publicly\nreleased all datasets and methods to promote rapid progress in open-source\nlong-reasoning LLMs. The dataset is available at:\n\\href{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M}{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M}"
                },
                "authors": [
                    {
                        "name": "Xiaoyu Tian"
                    },
                    {
                        "name": "Sitong Zhao"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Shuaiting Chen"
                    },
                    {
                        "name": "Yiping Peng"
                    },
                    {
                        "name": "Yunjie Ji"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Xiangang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangang Li"
                },
                "author": "Xiangang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17565v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17565v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08311v1",
                "updated": "2025-05-13T07:41:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    7,
                    41,
                    15,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T07:41:15Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    7,
                    41,
                    15,
                    1,
                    133,
                    0
                ],
                "title": "AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale"
                },
                "summary": "We present AM-Thinking-v1, a 32B dense language model that advances the\nfrontier of reasoning, embodying the collaborative spirit of open-source\ninnovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts\n(MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves\nimpressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on\nLiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities\namong open-source models of similar scale.\n  Built entirely from the open-source Qwen2.5-32B base model and publicly\navailable queries, AM-Thinking-v1 leverages a meticulously crafted\npost-training pipeline - combining supervised fine-tuning and reinforcement\nlearning - to deliver exceptional reasoning capabilities. This work\ndemonstrates that the open-source community can achieve high performance at the\n32B scale, a practical sweet spot for deployment and fine-tuning. By striking a\nbalance between top-tier performance and real-world usability, we hope\nAM-Thinking-v1 inspires further collaborative efforts to harness mid-scale\nmodels, pushing reasoning boundaries while keeping accessibility at the core of\ninnovation. We have open-sourced our model on\n\\href{https://huggingface.co/a-m-team/AM-Thinking-v1}{Hugging Face}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present AM-Thinking-v1, a 32B dense language model that advances the\nfrontier of reasoning, embodying the collaborative spirit of open-source\ninnovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts\n(MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves\nimpressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on\nLiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities\namong open-source models of similar scale.\n  Built entirely from the open-source Qwen2.5-32B base model and publicly\navailable queries, AM-Thinking-v1 leverages a meticulously crafted\npost-training pipeline - combining supervised fine-tuning and reinforcement\nlearning - to deliver exceptional reasoning capabilities. This work\ndemonstrates that the open-source community can achieve high performance at the\n32B scale, a practical sweet spot for deployment and fine-tuning. By striking a\nbalance between top-tier performance and real-world usability, we hope\nAM-Thinking-v1 inspires further collaborative efforts to harness mid-scale\nmodels, pushing reasoning boundaries while keeping accessibility at the core of\ninnovation. We have open-sourced our model on\n\\href{https://huggingface.co/a-m-team/AM-Thinking-v1}{Hugging Face}."
                },
                "authors": [
                    {
                        "name": "Yunjie Ji"
                    },
                    {
                        "name": "Xiaoyu Tian"
                    },
                    {
                        "name": "Sitong Zhao"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Shuaiting Chen"
                    },
                    {
                        "name": "Yiping Peng"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Xiangang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangang Li"
                },
                "author": "Xiangang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13801v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13801v3",
                "updated": "2025-05-13T07:27:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    7,
                    27,
                    38,
                    1,
                    133,
                    0
                ],
                "published": "2025-03-18T01:16:16Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    1,
                    16,
                    16,
                    1,
                    77,
                    0
                ],
                "title": "SCAN-BEST: Sub-6GHz-Aided Near-field Beam Selection with Formal\n  Reliability Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCAN-BEST: Sub-6GHz-Aided Near-field Beam Selection with Formal\n  Reliability Guarantees"
                },
                "summary": "As millimeter-wave (mmWave) MIMO systems adopt larger antenna arrays,\nnear-field propagation becomes increasingly prominent, especially for users\nclose to the transmitter. Traditional far-field beam training methods become\ninadequate, while near-field training faces the challenge of large codebooks\ndue to the need to resolve both angular and distance domains. To reduce in-band\ntraining overhead, prior work has proposed to leverage the spatial-temporal\ncongruence between sub-6 GHz (sub-6G) and mmWave channels to predict the best\nmmWave beam within a near-field codebook from sub-6G channel estimates. To cope\nwith the uncertainty caused by sub-6G/mmWave differences, we introduce a novel\nSub-6G Channel Aided Near-field BEam SelecTion (SCAN-BEST) framework that wraps\naround any beam predictor to produce candidate beam subset with formal\nsuboptimality guarantees. The proposed SCAN-BEST builds on conformal risk\ncontrol (CRC), and is calibrated offline using limited calibration data. Its\nperformance guarantees apply even in the presence of statistical shifts between\ncalibration and deployment. Numerical results validate the theoretical\nproperties and efficiency of SCAN-BEST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As millimeter-wave (mmWave) MIMO systems adopt larger antenna arrays,\nnear-field propagation becomes increasingly prominent, especially for users\nclose to the transmitter. Traditional far-field beam training methods become\ninadequate, while near-field training faces the challenge of large codebooks\ndue to the need to resolve both angular and distance domains. To reduce in-band\ntraining overhead, prior work has proposed to leverage the spatial-temporal\ncongruence between sub-6 GHz (sub-6G) and mmWave channels to predict the best\nmmWave beam within a near-field codebook from sub-6G channel estimates. To cope\nwith the uncertainty caused by sub-6G/mmWave differences, we introduce a novel\nSub-6G Channel Aided Near-field BEam SelecTion (SCAN-BEST) framework that wraps\naround any beam predictor to produce candidate beam subset with formal\nsuboptimality guarantees. The proposed SCAN-BEST builds on conformal risk\ncontrol (CRC), and is calibrated offline using limited calibration data. Its\nperformance guarantees apply even in the presence of statistical shifts between\ncalibration and deployment. Numerical results validate the theoretical\nproperties and efficiency of SCAN-BEST."
                },
                "authors": [
                    {
                        "name": "Weicao Deng"
                    },
                    {
                        "name": "Binpu Shi"
                    },
                    {
                        "name": "Min Li"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "arxiv_comment": "14 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13801v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13801v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08303v1",
                "updated": "2025-05-13T07:26:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    7,
                    26,
                    56,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T07:26:56Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    7,
                    26,
                    56,
                    1,
                    133,
                    0
                ],
                "title": "Evaluating the Effectiveness of Black-Box Prompt Optimization as the\n  Scale of LLMs Continues to Grow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Effectiveness of Black-Box Prompt Optimization as the\n  Scale of LLMs Continues to Grow"
                },
                "summary": "Black-Box prompt optimization methods have emerged as a promising strategy\nfor refining input prompts to better align large language models (LLMs),\nthereby enhancing their task performance. Although these methods have\ndemonstrated encouraging results, most studies and experiments have primarily\nfocused on smaller-scale models (e.g., 7B, 14B) or earlier versions (e.g.,\nGPT-3.5) of LLMs. As the scale of LLMs continues to increase, such as with\nDeepSeek V3 (671B), it remains an open question whether these black-box\noptimization techniques will continue to yield significant performance\nimprovements for models of such scale. In response to this, we select three\nwell-known black-box optimization methods and evaluate them on large-scale LLMs\n(DeepSeek V3 and Gemini 2.0 Flash) across four NLU and NLG datasets. The\nresults show that these black-box prompt optimization methods offer only\nlimited improvements on these large-scale LLMs. Furthermore, we hypothesize\nthat the scale of the model is the primary factor contributing to the limited\nbenefits observed. To explore this hypothesis, we conducted experiments on LLMs\nof varying sizes (Qwen 2.5 series, ranging from 7B to 72B) and observed an\ninverse scaling law, wherein the effectiveness of black-box optimization\nmethods diminished as the model size increased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-Box prompt optimization methods have emerged as a promising strategy\nfor refining input prompts to better align large language models (LLMs),\nthereby enhancing their task performance. Although these methods have\ndemonstrated encouraging results, most studies and experiments have primarily\nfocused on smaller-scale models (e.g., 7B, 14B) or earlier versions (e.g.,\nGPT-3.5) of LLMs. As the scale of LLMs continues to increase, such as with\nDeepSeek V3 (671B), it remains an open question whether these black-box\noptimization techniques will continue to yield significant performance\nimprovements for models of such scale. In response to this, we select three\nwell-known black-box optimization methods and evaluate them on large-scale LLMs\n(DeepSeek V3 and Gemini 2.0 Flash) across four NLU and NLG datasets. The\nresults show that these black-box prompt optimization methods offer only\nlimited improvements on these large-scale LLMs. Furthermore, we hypothesize\nthat the scale of the model is the primary factor contributing to the limited\nbenefits observed. To explore this hypothesis, we conducted experiments on LLMs\nof varying sizes (Qwen 2.5 series, ranging from 7B to 72B) and observed an\ninverse scaling law, wherein the effectiveness of black-box optimization\nmethods diminished as the model size increased."
                },
                "authors": [
                    {
                        "name": "Ziyu Zhou"
                    },
                    {
                        "name": "Yihang Wu"
                    },
                    {
                        "name": "Jingyuan Yang"
                    },
                    {
                        "name": "Zhan Xiao"
                    },
                    {
                        "name": "Rongjun Li"
                    }
                ],
                "author_detail": {
                    "name": "Rongjun Li"
                },
                "author": "Rongjun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08299v1",
                "updated": "2025-05-13T07:23:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    7,
                    23,
                    8,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T07:23:08Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    7,
                    23,
                    8,
                    1,
                    133,
                    0
                ],
                "title": "Efficient Unstructured Pruning of Mamba State-Space Models for\n  Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Unstructured Pruning of Mamba State-Space Models for\n  Resource-Constrained Environments"
                },
                "summary": "State-space models (SSMs), particularly the Mamba architecture, have emerged\nas powerful alternatives to Transformers for sequence modeling, offering\nlinear-time complexity and competitive performance across diverse tasks.\nHowever, their large parameter counts pose significant challenges for\ndeployment in resource-constrained environments. We propose a novel\nunstructured pruning framework tailored for Mamba models that achieves up to\n70\\% parameter reduction while retaining over 95\\% of the original performance.\nOur approach integrates three key innovations: (1) a gradient-aware magnitude\npruning technique that combines weight magnitude and gradient information to\nidentify less critical parameters, (2) an iterative pruning schedule that\ngradually increases sparsity to maintain model stability, and (3) a global\npruning strategy that optimizes parameter allocation across the entire model.\nThrough extensive experiments on WikiText-103, Long Range Arena, and ETT\ntime-series benchmarks, we demonstrate significant efficiency gains with\nminimal performance degradation. Our analysis of pruning effects on Mamba's\ncomponents reveals critical insights into the architecture's redundancy and\nrobustness, enabling practical deployment in resource-constrained settings\nwhile broadening Mamba's applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-space models (SSMs), particularly the Mamba architecture, have emerged\nas powerful alternatives to Transformers for sequence modeling, offering\nlinear-time complexity and competitive performance across diverse tasks.\nHowever, their large parameter counts pose significant challenges for\ndeployment in resource-constrained environments. We propose a novel\nunstructured pruning framework tailored for Mamba models that achieves up to\n70\\% parameter reduction while retaining over 95\\% of the original performance.\nOur approach integrates three key innovations: (1) a gradient-aware magnitude\npruning technique that combines weight magnitude and gradient information to\nidentify less critical parameters, (2) an iterative pruning schedule that\ngradually increases sparsity to maintain model stability, and (3) a global\npruning strategy that optimizes parameter allocation across the entire model.\nThrough extensive experiments on WikiText-103, Long Range Arena, and ETT\ntime-series benchmarks, we demonstrate significant efficiency gains with\nminimal performance degradation. Our analysis of pruning effects on Mamba's\ncomponents reveals critical insights into the architecture's redundancy and\nrobustness, enabling practical deployment in resource-constrained settings\nwhile broadening Mamba's applicability."
                },
                "authors": [
                    {
                        "name": "Ibne Farabi Shihab"
                    },
                    {
                        "name": "Sanjeda Akter"
                    },
                    {
                        "name": "Anuj Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Sharma"
                },
                "author": "Anuj Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21625v2",
                "updated": "2025-05-13T07:14:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    7,
                    14,
                    3,
                    1,
                    133,
                    0
                ],
                "published": "2025-04-30T13:28:19Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    13,
                    28,
                    19,
                    2,
                    120,
                    0
                ],
                "title": "Ask, Fail, Repeat: Meeseeks, an Iterative Feedback Benchmark for LLMs'\n  Multi-turn Instruction-following Ability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ask, Fail, Repeat: Meeseeks, an Iterative Feedback Benchmark for LLMs'\n  Multi-turn Instruction-following Ability"
                },
                "summary": "The ability to follow instructions accurately is fundamental for Large\nLanguage Models (LLMs) to serve as reliable agents in real-world applications.\nFor complex instructions, LLMs often struggle to fulfill all requirements in a\nsingle attempt. In practice, users typically provide iterative feedback until\nthe LLM generates a response that meets all requirements. However, existing\ninstruction-following benchmarks are either single-turn or introduce new\nrequirements in each turn without allowing self-correction. To address this\ngap, we propose \\textbf{Meeseeks} (named after Mr. Meeseeks from \\textit{Rick\nand Morty}\\footnote{Rick and Morty is an American adult animated science\nfiction sitcom created by Justin Roiland and Dan Harmon for Cartoon Network's\nnighttime programming block Adult Swim.}.) Meeseeks simulates realistic\nhuman-LLM interactions through an iterative feedback framework, which enables\nmodels to self-correct based on specific requirement failures in each turn,\nbetter reflecting real-world user-end usage patterns. Meanwhile, the benchmark\nimplements a comprehensive evaluation system with 38 capability tags organized\nacross three dimensions: Intent Recognition, Granular Content Validation, and\nOutput Structure Validation. Through rigorous evaluation across LLMs, Meeseeks\nprovides valuable insights into LLMs' instruction-following capabilities in\nmulti-turn scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to follow instructions accurately is fundamental for Large\nLanguage Models (LLMs) to serve as reliable agents in real-world applications.\nFor complex instructions, LLMs often struggle to fulfill all requirements in a\nsingle attempt. In practice, users typically provide iterative feedback until\nthe LLM generates a response that meets all requirements. However, existing\ninstruction-following benchmarks are either single-turn or introduce new\nrequirements in each turn without allowing self-correction. To address this\ngap, we propose \\textbf{Meeseeks} (named after Mr. Meeseeks from \\textit{Rick\nand Morty}\\footnote{Rick and Morty is an American adult animated science\nfiction sitcom created by Justin Roiland and Dan Harmon for Cartoon Network's\nnighttime programming block Adult Swim.}.) Meeseeks simulates realistic\nhuman-LLM interactions through an iterative feedback framework, which enables\nmodels to self-correct based on specific requirement failures in each turn,\nbetter reflecting real-world user-end usage patterns. Meanwhile, the benchmark\nimplements a comprehensive evaluation system with 38 capability tags organized\nacross three dimensions: Intent Recognition, Granular Content Validation, and\nOutput Structure Validation. Through rigorous evaluation across LLMs, Meeseeks\nprovides valuable insights into LLMs' instruction-following capabilities in\nmulti-turn scenarios."
                },
                "authors": [
                    {
                        "name": "Jiaming Wang"
                    },
                    {
                        "name": "Yunke Zhao"
                    },
                    {
                        "name": "Peng Ding"
                    },
                    {
                        "name": "Jun Kuang"
                    },
                    {
                        "name": "Zongyu Wang"
                    },
                    {
                        "name": "Xuezhi Cao"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16408v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16408v2",
                "updated": "2025-05-13T07:12:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    7,
                    12,
                    49,
                    1,
                    133,
                    0
                ],
                "published": "2025-04-23T04:19:52Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    4,
                    19,
                    52,
                    2,
                    113,
                    0
                ],
                "title": "LLMSR@XLLM25: Less is More: Enhancing Structured Multi-Agent Reasoning\n  via Quality-Guided Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMSR@XLLM25: Less is More: Enhancing Structured Multi-Agent Reasoning\n  via Quality-Guided Distillation"
                },
                "summary": "The LLMSR@XLLM25 formulates a low-resource structural reasoning task that\nchallenges LLMs to generate interpretable, step-by-step rationales with minimal\nlabeled data. We present Less is More, the third-place winning approach in the\nLLMSR@XLLM25, which focuses on structured reasoning from only 24 labeled\nexamples. Our approach leverages a multi-agent framework with reverse-prompt\ninduction, retrieval-augmented reasoning synthesis via GPT-4o, and dual-stage\nreward-guided filtering to distill high-quality supervision across three\nsubtasks: question parsing, CoT parsing, and step-level verification. All\nmodules are fine-tuned from Meta-Llama-3-8B-Instruct under a unified LoRA+\nsetup. By combining structure validation with reward filtering across few-shot\nand zero-shot prompts, our pipeline consistently improves structure reasoning\nquality. These results underscore the value of controllable data distillation\nin enhancing structured inference under low-resource constraints. Our code is\navailable at https://github.com/JhCircle/Less-is-More.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LLMSR@XLLM25 formulates a low-resource structural reasoning task that\nchallenges LLMs to generate interpretable, step-by-step rationales with minimal\nlabeled data. We present Less is More, the third-place winning approach in the\nLLMSR@XLLM25, which focuses on structured reasoning from only 24 labeled\nexamples. Our approach leverages a multi-agent framework with reverse-prompt\ninduction, retrieval-augmented reasoning synthesis via GPT-4o, and dual-stage\nreward-guided filtering to distill high-quality supervision across three\nsubtasks: question parsing, CoT parsing, and step-level verification. All\nmodules are fine-tuned from Meta-Llama-3-8B-Instruct under a unified LoRA+\nsetup. By combining structure validation with reward filtering across few-shot\nand zero-shot prompts, our pipeline consistently improves structure reasoning\nquality. These results underscore the value of controllable data distillation\nin enhancing structured inference under low-resource constraints. Our code is\navailable at https://github.com/JhCircle/Less-is-More."
                },
                "authors": [
                    {
                        "name": "Jiahao Yuan"
                    },
                    {
                        "name": "Xingzhe Sun"
                    },
                    {
                        "name": "Xing Yu"
                    },
                    {
                        "name": "Jingwen Wang"
                    },
                    {
                        "name": "Dehui Du"
                    },
                    {
                        "name": "Zhiqing Cui"
                    },
                    {
                        "name": "Zixiang Di"
                    }
                ],
                "author_detail": {
                    "name": "Zixiang Di"
                },
                "author": "Zixiang Di",
                "arxiv_comment": "XLLM @ ACL 2025 Shared Task-III: LLM for Structural Reasoning\n  (LLM-SR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16408v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16408v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02465v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02465v2",
                "updated": "2025-05-13T06:49:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    49,
                    22,
                    1,
                    133,
                    0
                ],
                "published": "2025-03-04T10:21:58Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    10,
                    21,
                    58,
                    1,
                    63,
                    0
                ],
                "title": "UAV-VLRR: Vision-Language Informed NMPC for Rapid Response in UAV Search\n  and Rescue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UAV-VLRR: Vision-Language Informed NMPC for Rapid Response in UAV Search\n  and Rescue"
                },
                "summary": "Emergency search and rescue (SAR) operations often require rapid and precise\ntarget identification in complex environments where traditional manual drone\ncontrol is inefficient. In order to address these scenarios, a rapid SAR\nsystem, UAV-VLRR (Vision-Language-Rapid-Response), is developed in this\nresearch. This system consists of two aspects: 1) A multimodal system which\nharnesses the power of Visual Language Model (VLM) and the natural language\nprocessing capabilities of ChatGPT-4o (LLM) for scene interpretation. 2) A\nnon-linearmodel predictive control (NMPC) with built-in obstacle avoidance for\nrapid response by a drone to fly according to the output of the multimodal\nsystem. This work aims at improving response times in emergency SAR operations\nby providing a more intuitive and natural approach to the operator to plan the\nSAR mission while allowing the drone to carry out that mission in a rapid and\nsafe manner. When tested, our approach was faster on an average by 33.75% when\ncompared with an off-the-shelf autopilot and 54.6% when compared with a human\npilot. Video of UAV-VLRR: https://youtu.be/KJqQGKKt1xY",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergency search and rescue (SAR) operations often require rapid and precise\ntarget identification in complex environments where traditional manual drone\ncontrol is inefficient. In order to address these scenarios, a rapid SAR\nsystem, UAV-VLRR (Vision-Language-Rapid-Response), is developed in this\nresearch. This system consists of two aspects: 1) A multimodal system which\nharnesses the power of Visual Language Model (VLM) and the natural language\nprocessing capabilities of ChatGPT-4o (LLM) for scene interpretation. 2) A\nnon-linearmodel predictive control (NMPC) with built-in obstacle avoidance for\nrapid response by a drone to fly according to the output of the multimodal\nsystem. This work aims at improving response times in emergency SAR operations\nby providing a more intuitive and natural approach to the operator to plan the\nSAR mission while allowing the drone to carry out that mission in a rapid and\nsafe manner. When tested, our approach was faster on an average by 33.75% when\ncompared with an off-the-shelf autopilot and 54.6% when compared with a human\npilot. Video of UAV-VLRR: https://youtu.be/KJqQGKKt1xY"
                },
                "authors": [
                    {
                        "name": "Yasheerah Yaqoot"
                    },
                    {
                        "name": "Muhammad Ahsan Mustafa"
                    },
                    {
                        "name": "Oleg Sautenkov"
                    },
                    {
                        "name": "Artem Lykov"
                    },
                    {
                        "name": "Valerii Serpiva"
                    },
                    {
                        "name": "Dzmitry Tsetserukou"
                    }
                ],
                "author_detail": {
                    "name": "Dzmitry Tsetserukou"
                },
                "author": "Dzmitry Tsetserukou",
                "arxiv_comment": "UAV-VLRR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02465v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02465v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03244v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03244v2",
                "updated": "2025-05-13T06:35:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    35,
                    48,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-06T07:15:04Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    7,
                    15,
                    4,
                    1,
                    126,
                    0
                ],
                "title": "SonicRAG : High Fidelity Sound Effects Synthesis Based on Retrival\n  Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SonicRAG : High Fidelity Sound Effects Synthesis Based on Retrival\n  Augmented Generation"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing (NLP) and multimodal learning, with successful\napplications in text generation and speech synthesis, enabling a deeper\nunderstanding and generation of multimodal content. In the field of sound\neffects (SFX) generation, LLMs have been leveraged to orchestrate multiple\nmodels for audio synthesis. However, due to the scarcity of annotated datasets,\nand the complexity of temproal modeling. current SFX generation techniques\nstill fall short in achieving high-fidelity audio. To address these\nlimitations, this paper introduces a novel framework that integrates LLMs with\nexisting sound effect databases, allowing for the retrieval, recombination, and\nsynthesis of audio based on user requirements. By leveraging this approach, we\nenhance the diversity and quality of generated sound effects while eliminating\nthe need for additional recording costs, offering a flexible and efficient\nsolution for sound design and application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing (NLP) and multimodal learning, with successful\napplications in text generation and speech synthesis, enabling a deeper\nunderstanding and generation of multimodal content. In the field of sound\neffects (SFX) generation, LLMs have been leveraged to orchestrate multiple\nmodels for audio synthesis. However, due to the scarcity of annotated datasets,\nand the complexity of temproal modeling. current SFX generation techniques\nstill fall short in achieving high-fidelity audio. To address these\nlimitations, this paper introduces a novel framework that integrates LLMs with\nexisting sound effect databases, allowing for the retrieval, recombination, and\nsynthesis of audio based on user requirements. By leveraging this approach, we\nenhance the diversity and quality of generated sound effects while eliminating\nthe need for additional recording costs, offering a flexible and efficient\nsolution for sound design and application."
                },
                "authors": [
                    {
                        "name": "Yu-Ren Guo"
                    },
                    {
                        "name": "Wen-Kai Tai"
                    }
                ],
                "author_detail": {
                    "name": "Wen-Kai Tai"
                },
                "author": "Wen-Kai Tai",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03244v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03244v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08265v1",
                "updated": "2025-05-13T06:29:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    29,
                    25,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T06:29:25Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    29,
                    25,
                    1,
                    133,
                    0
                ],
                "title": "LLM Enhancers for GNNs: An Analysis from the Perspective of Causal\n  Mechanism Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Enhancers for GNNs: An Analysis from the Perspective of Causal\n  Mechanism Identification"
                },
                "summary": "The use of large language models (LLMs) as feature enhancers to optimize node\nrepresentations, which are then used as inputs for graph neural networks\n(GNNs), has shown significant potential in graph representation learning.\nHowever, the fundamental properties of this approach remain underexplored. To\naddress this issue, we propose conducting a more in-depth analysis of this\nissue based on the interchange intervention method. First, we construct a\nsynthetic graph dataset with controllable causal relationships, enabling\nprecise manipulation of semantic relationships and causal modeling to provide\ndata for analysis. Using this dataset, we conduct interchange interventions to\nexamine the deeper properties of LLM enhancers and GNNs, uncovering their\nunderlying logic and internal mechanisms. Building on the analytical results,\nwe design a plug-and-play optimization module to improve the information\ntransfer between LLM enhancers and GNNs. Experiments across multiple datasets\nand models validate the proposed module.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of large language models (LLMs) as feature enhancers to optimize node\nrepresentations, which are then used as inputs for graph neural networks\n(GNNs), has shown significant potential in graph representation learning.\nHowever, the fundamental properties of this approach remain underexplored. To\naddress this issue, we propose conducting a more in-depth analysis of this\nissue based on the interchange intervention method. First, we construct a\nsynthetic graph dataset with controllable causal relationships, enabling\nprecise manipulation of semantic relationships and causal modeling to provide\ndata for analysis. Using this dataset, we conduct interchange interventions to\nexamine the deeper properties of LLM enhancers and GNNs, uncovering their\nunderlying logic and internal mechanisms. Building on the analytical results,\nwe design a plug-and-play optimization module to improve the information\ntransfer between LLM enhancers and GNNs. Experiments across multiple datasets\nand models validate the proposed module."
                },
                "authors": [
                    {
                        "name": "Hang Gao"
                    },
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Fengge Wu"
                    },
                    {
                        "name": "Junsuo Zhao"
                    },
                    {
                        "name": "Changwen Zheng"
                    },
                    {
                        "name": "Huaping Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huaping Liu"
                },
                "author": "Huaping Liu",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08264v1",
                "updated": "2025-05-13T06:26:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    26,
                    57,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T06:26:57Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    26,
                    57,
                    1,
                    133,
                    0
                ],
                "title": "Automatic Curriculum Learning for Driving Scenarios: Towards Robust and\n  Efficient Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Curriculum Learning for Driving Scenarios: Towards Robust and\n  Efficient Reinforcement Learning"
                },
                "summary": "This paper addresses the challenges of training end-to-end autonomous driving\nagents using Reinforcement Learning (RL). RL agents are typically trained in a\nfixed set of scenarios and nominal behavior of surrounding road users in\nsimulations, limiting their generalization and real-life deployment. While\ndomain randomization offers a potential solution by randomly sampling driving\nscenarios, it frequently results in inefficient training and sub-optimal\npolicies due to the high variance among training scenarios. To address these\nlimitations, we propose an automatic curriculum learning framework that\ndynamically generates driving scenarios with adaptive complexity based on the\nagent's evolving capabilities. Unlike manually designed curricula that\nintroduce expert bias and lack scalability, our framework incorporates a\n``teacher'' that automatically generates and mutates driving scenarios based on\ntheir learning potential -- an agent-centric metric derived from the agent's\ncurrent policy -- eliminating the need for expert design. The framework\nenhances training efficiency by excluding scenarios the agent has mastered or\nfinds too challenging. We evaluate our framework in a reinforcement learning\nsetting where the agent learns a driving policy from camera images. Comparative\nresults against baseline methods, including fixed scenario training and domain\nrandomization, demonstrate that our approach leads to enhanced generalization,\nachieving higher success rates: +9\\% in low traffic density, +21\\% in high\ntraffic density, and faster convergence with fewer training steps. Our findings\nhighlight the potential of ACL in improving the robustness and efficiency of\nRL-based autonomous driving agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenges of training end-to-end autonomous driving\nagents using Reinforcement Learning (RL). RL agents are typically trained in a\nfixed set of scenarios and nominal behavior of surrounding road users in\nsimulations, limiting their generalization and real-life deployment. While\ndomain randomization offers a potential solution by randomly sampling driving\nscenarios, it frequently results in inefficient training and sub-optimal\npolicies due to the high variance among training scenarios. To address these\nlimitations, we propose an automatic curriculum learning framework that\ndynamically generates driving scenarios with adaptive complexity based on the\nagent's evolving capabilities. Unlike manually designed curricula that\nintroduce expert bias and lack scalability, our framework incorporates a\n``teacher'' that automatically generates and mutates driving scenarios based on\ntheir learning potential -- an agent-centric metric derived from the agent's\ncurrent policy -- eliminating the need for expert design. The framework\nenhances training efficiency by excluding scenarios the agent has mastered or\nfinds too challenging. We evaluate our framework in a reinforcement learning\nsetting where the agent learns a driving policy from camera images. Comparative\nresults against baseline methods, including fixed scenario training and domain\nrandomization, demonstrate that our approach leads to enhanced generalization,\nachieving higher success rates: +9\\% in low traffic density, +21\\% in high\ntraffic density, and faster convergence with fewer training steps. Our findings\nhighlight the potential of ACL in improving the robustness and efficiency of\nRL-based autonomous driving agents."
                },
                "authors": [
                    {
                        "name": "Ahmed Abouelazm"
                    },
                    {
                        "name": "Tim Weinstein"
                    },
                    {
                        "name": "Tim Joseph"
                    },
                    {
                        "name": "Philip Schörner"
                    },
                    {
                        "name": "J. Marius Zöllner"
                    }
                ],
                "author_detail": {
                    "name": "J. Marius Zöllner"
                },
                "author": "J. Marius Zöllner",
                "arxiv_comment": "Accepted in the 36th IEEE Intelligent Vehicles Symposium (IV 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08263v1",
                "updated": "2025-05-13T06:26:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    26,
                    13,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T06:26:13Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    26,
                    13,
                    1,
                    133,
                    0
                ],
                "title": "LLM-Based Detection of Tangled Code Changes for Higher-Quality\n  Method-Level Bug Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Detection of Tangled Code Changes for Higher-Quality\n  Method-Level Bug Datasets"
                },
                "summary": "Tangled code changes-commits that conflate unrelated modifications such as\nbug fixes, refactorings, and enhancements-introduce significant noise into bug\ndatasets and adversely affect the performance of bug prediction models.\nAddressing this issue at a fine-grained, method-level granularity remains\nunderexplored. This is critical to address, as recent bug prediction models,\ndriven by practitioner demand, are increasingly focusing on finer granularity\nrather than traditional class- or file-level predictions. This study\ninvestigates the utility of Large Language Models (LLMs) for detecting tangled\ncode changes by leveraging both commit messages and method-level code diffs. We\nformulate the problem as a binary classification task and evaluate multiple\nprompting strategies, including zero-shot, few-shot, and chain-of-thought\nprompting, using state-of-the-art proprietary LLMs such as GPT-4o and\nGemini-2.0-Flash.\n  Our results demonstrate that combining commit messages with code diffs\nsignificantly enhances model performance, with the combined few-shot and\nchain-of-thought prompting achieving an F1-score of 0.88. Additionally, we\nexplore embedding-based machine learning models trained on LLM-generated\nembeddings, where a multi-layer perceptron classifier achieves superior\nperformance (F1-score: 0.906, MCC: 0.807). These findings are encouraging for\nthe research community, as method-level bug prediction remains an open research\nproblem, largely due to the lack of noise-free bug datasets. This research not\nonly contributes a novel method-level perspective to the untangling problem but\nalso highlights practical avenues for enhancing automated software quality\nassessment tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tangled code changes-commits that conflate unrelated modifications such as\nbug fixes, refactorings, and enhancements-introduce significant noise into bug\ndatasets and adversely affect the performance of bug prediction models.\nAddressing this issue at a fine-grained, method-level granularity remains\nunderexplored. This is critical to address, as recent bug prediction models,\ndriven by practitioner demand, are increasingly focusing on finer granularity\nrather than traditional class- or file-level predictions. This study\ninvestigates the utility of Large Language Models (LLMs) for detecting tangled\ncode changes by leveraging both commit messages and method-level code diffs. We\nformulate the problem as a binary classification task and evaluate multiple\nprompting strategies, including zero-shot, few-shot, and chain-of-thought\nprompting, using state-of-the-art proprietary LLMs such as GPT-4o and\nGemini-2.0-Flash.\n  Our results demonstrate that combining commit messages with code diffs\nsignificantly enhances model performance, with the combined few-shot and\nchain-of-thought prompting achieving an F1-score of 0.88. Additionally, we\nexplore embedding-based machine learning models trained on LLM-generated\nembeddings, where a multi-layer perceptron classifier achieves superior\nperformance (F1-score: 0.906, MCC: 0.807). These findings are encouraging for\nthe research community, as method-level bug prediction remains an open research\nproblem, largely due to the lack of noise-free bug datasets. This research not\nonly contributes a novel method-level perspective to the untangling problem but\nalso highlights practical avenues for enhancing automated software quality\nassessment tools."
                },
                "authors": [
                    {
                        "name": "Md Nahidul Islam Opu"
                    },
                    {
                        "name": "Shaowei Wang"
                    },
                    {
                        "name": "Shaiful Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Shaiful Chowdhury"
                },
                "author": "Shaiful Chowdhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08261v1",
                "updated": "2025-05-13T06:24:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    24,
                    48,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T06:24:48Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    24,
                    48,
                    1,
                    133,
                    0
                ],
                "title": "Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual\n  Compression for Scalable Knowledge Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual\n  Compression for Scalable Knowledge Integration"
                },
                "summary": "The rapid progress in large language models (LLMs) has paved the way for\nnovel approaches in knowledge-intensive tasks. Among these, Cache-Augmented\nGeneration (CAG) has emerged as a promising alternative to Retrieval-Augmented\nGeneration (RAG). CAG minimizes retrieval latency and simplifies system design\nby preloading knowledge into the model's context. However, challenges persist\nin scaling CAG to accommodate large and dynamic knowledge bases effectively.\nThis paper introduces Adaptive Contextual Compression (ACC), an innovative\ntechnique designed to dynamically compress and manage context inputs, enabling\nefficient utilization of the extended memory capabilities of modern LLMs. To\nfurther address the limitations of standalone CAG, we propose a Hybrid CAG-RAG\nFramework, which integrates selective retrieval to augment preloaded contexts\nin scenarios requiring additional information. Comprehensive evaluations on\ndiverse datasets highlight the proposed methods' ability to enhance\nscalability, optimize efficiency, and improve multi-hop reasoning performance,\noffering practical solutions for real-world knowledge integration challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress in large language models (LLMs) has paved the way for\nnovel approaches in knowledge-intensive tasks. Among these, Cache-Augmented\nGeneration (CAG) has emerged as a promising alternative to Retrieval-Augmented\nGeneration (RAG). CAG minimizes retrieval latency and simplifies system design\nby preloading knowledge into the model's context. However, challenges persist\nin scaling CAG to accommodate large and dynamic knowledge bases effectively.\nThis paper introduces Adaptive Contextual Compression (ACC), an innovative\ntechnique designed to dynamically compress and manage context inputs, enabling\nefficient utilization of the extended memory capabilities of modern LLMs. To\nfurther address the limitations of standalone CAG, we propose a Hybrid CAG-RAG\nFramework, which integrates selective retrieval to augment preloaded contexts\nin scenarios requiring additional information. Comprehensive evaluations on\ndiverse datasets highlight the proposed methods' ability to enhance\nscalability, optimize efficiency, and improve multi-hop reasoning performance,\noffering practical solutions for real-world knowledge integration challenges."
                },
                "authors": [
                    {
                        "name": "Rishabh Agrawal"
                    },
                    {
                        "name": "Himanshu Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Himanshu Kumar"
                },
                "author": "Himanshu Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08259v1",
                "updated": "2025-05-13T06:17:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    17,
                    18,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T06:17:18Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    17,
                    18,
                    1,
                    133,
                    0
                ],
                "title": "CNN and ViT Efficiency Study on Tiny ImageNet and DermaMNIST Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CNN and ViT Efficiency Study on Tiny ImageNet and DermaMNIST Datasets"
                },
                "summary": "This study evaluates the trade-offs between convolutional and\ntransformer-based architectures on both medical and general-purpose image\nclassification benchmarks. We use ResNet-18 as our baseline and introduce a\nfine-tuning strategy applied to four Vision Transformer variants (Tiny, Small,\nBase, Large) on DermatologyMNIST and TinyImageNet. Our goal is to reduce\ninference latency and model complexity with acceptable accuracy degradation.\nThrough systematic hyperparameter variations, we demonstrate that appropriately\nfine-tuned Vision Transformers can match or exceed the baseline's performance,\nachieve faster inference, and operate with fewer parameters, highlighting their\nviability for deployment in resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study evaluates the trade-offs between convolutional and\ntransformer-based architectures on both medical and general-purpose image\nclassification benchmarks. We use ResNet-18 as our baseline and introduce a\nfine-tuning strategy applied to four Vision Transformer variants (Tiny, Small,\nBase, Large) on DermatologyMNIST and TinyImageNet. Our goal is to reduce\ninference latency and model complexity with acceptable accuracy degradation.\nThrough systematic hyperparameter variations, we demonstrate that appropriately\nfine-tuned Vision Transformers can match or exceed the baseline's performance,\nachieve faster inference, and operate with fewer parameters, highlighting their\nviability for deployment in resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Aidar Amangeldi"
                    },
                    {
                        "name": "Angsar Taigonyrov"
                    },
                    {
                        "name": "Muhammad Huzaid Jawad"
                    },
                    {
                        "name": "Chinedu Emmanuel Mbonu"
                    }
                ],
                "author_detail": {
                    "name": "Chinedu Emmanuel Mbonu"
                },
                "author": "Chinedu Emmanuel Mbonu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13517v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13517v2",
                "updated": "2025-05-13T06:16:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    16,
                    23,
                    1,
                    133,
                    0
                ],
                "published": "2025-03-14T17:53:03Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    53,
                    3,
                    4,
                    73,
                    0
                ],
                "title": "CURIE: Evaluating LLMs On Multitask Scientific Long Context\n  Understanding and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CURIE: Evaluating LLMs On Multitask Scientific Long Context\n  Understanding and Reasoning"
                },
                "summary": "Scientific problem-solving involves synthesizing information while applying\nexpert knowledge. We introduce CURIE, a scientific long-Context\nUnderstanding,Reasoning and Information Extraction benchmark to measure the\npotential of Large Language Models (LLMs) in scientific problem-solving and\nassisting scientists in realistic workflows. This benchmark introduces ten\nchallenging tasks with a total of 580 problems and solution pairs curated by\nexperts in six disciplines - materials science, condensed matter physics,\nquantum computing, geospatial analysis, biodiversity, and proteins - covering\nboth experimental and theoretical work-flows in science. We evaluate a range of\nclosed and open LLMs on tasks in CURIE which requires domain expertise,\ncomprehension of long in-context information,and multi-step reasoning. While\nGemini Flash 2.0 and Claude-3 show consistent high comprehension across\ndomains, the popular GPT-4o and command-R+ fail dramatically on protein\nsequencing tasks. With the best performance at 32% there is much room for\nimprovement for all models. We hope that insights gained from CURIE can guide\nthe future development of LLMs in sciences. Evaluation code and data are in\nhttps://github.com/google/curie",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific problem-solving involves synthesizing information while applying\nexpert knowledge. We introduce CURIE, a scientific long-Context\nUnderstanding,Reasoning and Information Extraction benchmark to measure the\npotential of Large Language Models (LLMs) in scientific problem-solving and\nassisting scientists in realistic workflows. This benchmark introduces ten\nchallenging tasks with a total of 580 problems and solution pairs curated by\nexperts in six disciplines - materials science, condensed matter physics,\nquantum computing, geospatial analysis, biodiversity, and proteins - covering\nboth experimental and theoretical work-flows in science. We evaluate a range of\nclosed and open LLMs on tasks in CURIE which requires domain expertise,\ncomprehension of long in-context information,and multi-step reasoning. While\nGemini Flash 2.0 and Claude-3 show consistent high comprehension across\ndomains, the popular GPT-4o and command-R+ fail dramatically on protein\nsequencing tasks. With the best performance at 32% there is much room for\nimprovement for all models. We hope that insights gained from CURIE can guide\nthe future development of LLMs in sciences. Evaluation code and data are in\nhttps://github.com/google/curie"
                },
                "authors": [
                    {
                        "name": "Hao Cui"
                    },
                    {
                        "name": "Zahra Shamsi"
                    },
                    {
                        "name": "Gowoon Cheon"
                    },
                    {
                        "name": "Xuejian Ma"
                    },
                    {
                        "name": "Shutong Li"
                    },
                    {
                        "name": "Maria Tikhanovskaya"
                    },
                    {
                        "name": "Peter Norgaard"
                    },
                    {
                        "name": "Nayantara Mudur"
                    },
                    {
                        "name": "Martyna Plomecka"
                    },
                    {
                        "name": "Paul Raccuglia"
                    },
                    {
                        "name": "Yasaman Bahri"
                    },
                    {
                        "name": "Victor V. Albert"
                    },
                    {
                        "name": "Pranesh Srinivasan"
                    },
                    {
                        "name": "Haining Pan"
                    },
                    {
                        "name": "Philippe Faist"
                    },
                    {
                        "name": "Brian Rohr"
                    },
                    {
                        "name": "Ekin Dogus Cubuk"
                    },
                    {
                        "name": "Muratahan Aykol"
                    },
                    {
                        "name": "Amil Merchant"
                    },
                    {
                        "name": "Michael J. Statt"
                    },
                    {
                        "name": "Dan Morris"
                    },
                    {
                        "name": "Drew Purves"
                    },
                    {
                        "name": "Elise Kleeman"
                    },
                    {
                        "name": "Ruth Alcantara"
                    },
                    {
                        "name": "Matthew Abraham"
                    },
                    {
                        "name": "Muqthar Mohammad"
                    },
                    {
                        "name": "Ean Phing VanLee"
                    },
                    {
                        "name": "Chenfei Jiang"
                    },
                    {
                        "name": "Elizabeth Dorfman"
                    },
                    {
                        "name": "Eun-Ah Kim"
                    },
                    {
                        "name": "Michael P Brenner"
                    },
                    {
                        "name": "Viren Jain"
                    },
                    {
                        "name": "Sameera Ponda"
                    },
                    {
                        "name": "Subhashini Venugopalan"
                    }
                ],
                "author_detail": {
                    "name": "Subhashini Venugopalan"
                },
                "author": "Subhashini Venugopalan",
                "arxiv_comment": "Accepted at ICLR 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13517v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13517v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08253v1",
                "updated": "2025-05-13T06:02:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    2,
                    37,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T06:02:37Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    2,
                    37,
                    1,
                    133,
                    0
                ],
                "title": "Evaluating LLM Metrics Through Real-World Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLM Metrics Through Real-World Capabilities"
                },
                "summary": "As generative AI becomes increasingly embedded in everyday workflows, it is\nimportant to evaluate its performance in ways that reflect real-world usage\nrather than abstract notions of intelligence. Unlike many existing benchmarks\nthat assess general intelligence, our approach focuses on real-world utility,\nevaluating how well models support users in everyday tasks. While current\nbenchmarks emphasize code generation or factual recall, users rely on AI for a\nmuch broader range of activities-from writing assistance and summarization to\ncitation formatting and stylistic feedback. In this paper, we analyze\nlarge-scale survey data and usage logs to identify six core capabilities that\nrepresent how people commonly use Large Language Models (LLMs): Summarization,\nTechnical Assistance, Reviewing Work, Data Structuring, Generation, and\nInformation Retrieval. We then assess the extent to which existing benchmarks\ncover these capabilities, revealing significant gaps in coverage, efficiency\nmeasurement, and interpretability. Drawing on this analysis, we use\nhuman-centered criteria to identify gaps in how well current benchmarks reflect\ncommon usage that is grounded in five practical criteria: coherence, accuracy,\nclarity, relevance, and efficiency. For four of the six capabilities, we\nidentify the benchmarks that best align with real-world tasks and use them to\ncompare leading models. We find that Google Gemini outperforms other\nmodels-including OpenAI's GPT, xAI's Grok, Meta's LLaMA, Anthropic's Claude,\nDeepSeek, and Qwen from Alibaba-on these utility-focused metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As generative AI becomes increasingly embedded in everyday workflows, it is\nimportant to evaluate its performance in ways that reflect real-world usage\nrather than abstract notions of intelligence. Unlike many existing benchmarks\nthat assess general intelligence, our approach focuses on real-world utility,\nevaluating how well models support users in everyday tasks. While current\nbenchmarks emphasize code generation or factual recall, users rely on AI for a\nmuch broader range of activities-from writing assistance and summarization to\ncitation formatting and stylistic feedback. In this paper, we analyze\nlarge-scale survey data and usage logs to identify six core capabilities that\nrepresent how people commonly use Large Language Models (LLMs): Summarization,\nTechnical Assistance, Reviewing Work, Data Structuring, Generation, and\nInformation Retrieval. We then assess the extent to which existing benchmarks\ncover these capabilities, revealing significant gaps in coverage, efficiency\nmeasurement, and interpretability. Drawing on this analysis, we use\nhuman-centered criteria to identify gaps in how well current benchmarks reflect\ncommon usage that is grounded in five practical criteria: coherence, accuracy,\nclarity, relevance, and efficiency. For four of the six capabilities, we\nidentify the benchmarks that best align with real-world tasks and use them to\ncompare leading models. We find that Google Gemini outperforms other\nmodels-including OpenAI's GPT, xAI's Grok, Meta's LLaMA, Anthropic's Claude,\nDeepSeek, and Qwen from Alibaba-on these utility-focused metrics."
                },
                "authors": [
                    {
                        "name": "Justin K Miller"
                    },
                    {
                        "name": "Wenjia Tang"
                    }
                ],
                "author_detail": {
                    "name": "Wenjia Tang"
                },
                "author": "Wenjia Tang",
                "arxiv_comment": "14 pages main text, 5 pages references, 20 pages appendix; includes 3\n  figures and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08245v1",
                "updated": "2025-05-13T05:47:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    5,
                    47,
                    51,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T05:47:51Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    5,
                    47,
                    51,
                    1,
                    133,
                    0
                ],
                "title": "Large Language Model Psychometrics: A Systematic Review of Evaluation,\n  Validation, and Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Psychometrics: A Systematic Review of Evaluation,\n  Validation, and Enhancement"
                },
                "summary": "The rapid advancement of large language models (LLMs) has outpaced\ntraditional evaluation methodologies. It presents novel challenges, such as\nmeasuring human-like psychological constructs, navigating beyond static and\ntask-specific benchmarks, and establishing human-centered evaluation. These\nchallenges intersect with Psychometrics, the science of quantifying the\nintangible aspects of human psychology, such as personality, values, and\nintelligence. This survey introduces and synthesizes an emerging\ninterdisciplinary field of LLM Psychometrics, which leverages psychometric\ninstruments, theories, and principles to evaluate, understand, and enhance\nLLMs. We systematically explore the role of Psychometrics in shaping\nbenchmarking principles, broadening evaluation scopes, refining methodologies,\nvalidating results, and advancing LLM capabilities. This paper integrates\ndiverse perspectives to provide a structured framework for researchers across\ndisciplines, enabling a more comprehensive understanding of this nascent field.\nUltimately, we aim to provide actionable insights for developing future\nevaluation paradigms that align with human-level AI and promote the advancement\nof human-centered AI systems for societal benefit. A curated repository of LLM\npsychometric resources is available at\nhttps://github.com/valuebyte-ai/Awesome-LLM-Psychometrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has outpaced\ntraditional evaluation methodologies. It presents novel challenges, such as\nmeasuring human-like psychological constructs, navigating beyond static and\ntask-specific benchmarks, and establishing human-centered evaluation. These\nchallenges intersect with Psychometrics, the science of quantifying the\nintangible aspects of human psychology, such as personality, values, and\nintelligence. This survey introduces and synthesizes an emerging\ninterdisciplinary field of LLM Psychometrics, which leverages psychometric\ninstruments, theories, and principles to evaluate, understand, and enhance\nLLMs. We systematically explore the role of Psychometrics in shaping\nbenchmarking principles, broadening evaluation scopes, refining methodologies,\nvalidating results, and advancing LLM capabilities. This paper integrates\ndiverse perspectives to provide a structured framework for researchers across\ndisciplines, enabling a more comprehensive understanding of this nascent field.\nUltimately, we aim to provide actionable insights for developing future\nevaluation paradigms that align with human-level AI and promote the advancement\nof human-centered AI systems for societal benefit. A curated repository of LLM\npsychometric resources is available at\nhttps://github.com/valuebyte-ai/Awesome-LLM-Psychometrics."
                },
                "authors": [
                    {
                        "name": "Haoran Ye"
                    },
                    {
                        "name": "Jing Jin"
                    },
                    {
                        "name": "Yuhang Xie"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Guojie Song"
                    }
                ],
                "author_detail": {
                    "name": "Guojie Song"
                },
                "author": "Guojie Song",
                "arxiv_comment": "63 pages, 482 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04806v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04806v2",
                "updated": "2025-05-13T05:36:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    5,
                    36,
                    34,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-07T21:15:40Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    21,
                    15,
                    40,
                    2,
                    127,
                    0
                ],
                "title": "Red Teaming the Mind of the Machine: A Systematic Evaluation of Prompt\n  Injection and Jailbreak Vulnerabilities in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Red Teaming the Mind of the Machine: A Systematic Evaluation of Prompt\n  Injection and Jailbreak Vulnerabilities in LLMs"
                },
                "summary": "Large Language Models (LLMs) are increasingly integrated into consumer and\nenterprise applications. Despite their capabilities, they remain susceptible to\nadversarial attacks such as prompt injection and jailbreaks that override\nalignment safeguards. This paper provides a systematic investigation of\njailbreak strategies against various state-of-the-art LLMs. We categorize over\n1,400 adversarial prompts, analyze their success against GPT-4, Claude 2,\nMistral 7B, and Vicuna, and examine their generalizability and construction\nlogic. We further propose layered mitigation strategies and recommend a hybrid\nred-teaming and sandboxing approach for robust LLM security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly integrated into consumer and\nenterprise applications. Despite their capabilities, they remain susceptible to\nadversarial attacks such as prompt injection and jailbreaks that override\nalignment safeguards. This paper provides a systematic investigation of\njailbreak strategies against various state-of-the-art LLMs. We categorize over\n1,400 adversarial prompts, analyze their success against GPT-4, Claude 2,\nMistral 7B, and Vicuna, and examine their generalizability and construction\nlogic. We further propose layered mitigation strategies and recommend a hybrid\nred-teaming and sandboxing approach for robust LLM security."
                },
                "authors": [
                    {
                        "name": "Chetan Pathade"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Pathade"
                },
                "author": "Chetan Pathade",
                "arxiv_comment": "7 Pages, 6 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04806v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04806v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08234v1",
                "updated": "2025-05-13T05:25:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    5,
                    25,
                    6,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T05:25:06Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    5,
                    25,
                    6,
                    1,
                    133,
                    0
                ],
                "title": "Removing Watermarks with Partial Regeneration using Semantic Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Removing Watermarks with Partial Regeneration using Semantic Information"
                },
                "summary": "As AI-generated imagery becomes ubiquitous, invisible watermarks have emerged\nas a primary line of defense for copyright and provenance. The newest\nwatermarking schemes embed semantic signals - content-aware patterns that are\ndesigned to survive common image manipulations - yet their true robustness\nagainst adaptive adversaries remains under-explored. We expose a previously\nunreported vulnerability and introduce SemanticRegen, a three-stage, label-free\nattack that erases state-of-the-art semantic and invisible watermarks while\nleaving an image's apparent meaning intact. Our pipeline (i) uses a\nvision-language model to obtain fine-grained captions, (ii) extracts foreground\nmasks with zero-shot segmentation, and (iii) inpaints only the background via\nan LLM-guided diffusion model, thereby preserving salient objects and style\ncues. Evaluated on 1,000 prompts across four watermarking systems - TreeRing,\nStegaStamp, StableSig, and DWT/DCT - SemanticRegen is the only method to defeat\nthe semantic TreeRing watermark (p = 0.10 > 0.05) and reduces bit-accuracy\nbelow 0.75 for the remaining schemes, all while maintaining high perceptual\nquality (masked SSIM = 0.94 +/- 0.01). We further introduce masked SSIM (mSSIM)\nto quantify fidelity within foreground regions, showing that our attack\nachieves up to 12 percent higher mSSIM than prior diffusion-based attackers.\nThese results highlight an urgent gap between current watermark defenses and\nthe capabilities of adaptive, semantics-aware adversaries, underscoring the\nneed for watermarking algorithms that are resilient to content-preserving\nregenerative attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI-generated imagery becomes ubiquitous, invisible watermarks have emerged\nas a primary line of defense for copyright and provenance. The newest\nwatermarking schemes embed semantic signals - content-aware patterns that are\ndesigned to survive common image manipulations - yet their true robustness\nagainst adaptive adversaries remains under-explored. We expose a previously\nunreported vulnerability and introduce SemanticRegen, a three-stage, label-free\nattack that erases state-of-the-art semantic and invisible watermarks while\nleaving an image's apparent meaning intact. Our pipeline (i) uses a\nvision-language model to obtain fine-grained captions, (ii) extracts foreground\nmasks with zero-shot segmentation, and (iii) inpaints only the background via\nan LLM-guided diffusion model, thereby preserving salient objects and style\ncues. Evaluated on 1,000 prompts across four watermarking systems - TreeRing,\nStegaStamp, StableSig, and DWT/DCT - SemanticRegen is the only method to defeat\nthe semantic TreeRing watermark (p = 0.10 > 0.05) and reduces bit-accuracy\nbelow 0.75 for the remaining schemes, all while maintaining high perceptual\nquality (masked SSIM = 0.94 +/- 0.01). We further introduce masked SSIM (mSSIM)\nto quantify fidelity within foreground regions, showing that our attack\nachieves up to 12 percent higher mSSIM than prior diffusion-based attackers.\nThese results highlight an urgent gap between current watermark defenses and\nthe capabilities of adaptive, semantics-aware adversaries, underscoring the\nneed for watermarking algorithms that are resilient to content-preserving\nregenerative attacks."
                },
                "authors": [
                    {
                        "name": "Krti Tallam"
                    },
                    {
                        "name": "John Kevin Cava"
                    },
                    {
                        "name": "Caleb Geniesse"
                    },
                    {
                        "name": "N. Benjamin Erichson"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    }
                ],
                "author_detail": {
                    "name": "Michael W. Mahoney"
                },
                "author": "Michael W. Mahoney",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08231v1",
                "updated": "2025-05-13T05:17:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    5,
                    17,
                    53,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T05:17:53Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    5,
                    17,
                    53,
                    1,
                    133,
                    0
                ],
                "title": "HMPNet: A Feature Aggregation Architecture for Maritime Object Detection\n  from a Shipborne Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HMPNet: A Feature Aggregation Architecture for Maritime Object Detection\n  from a Shipborne Perspective"
                },
                "summary": "In the realm of intelligent maritime navigation, object detection from a\nshipborne perspective is paramount. Despite the criticality, the paucity of\nmaritime-specific data impedes the deployment of sophisticated visual\nperception techniques, akin to those utilized in autonomous vehicular systems,\nwithin the maritime context. To bridge this gap, we introduce Navigation12, a\nnovel dataset annotated for 12 object categories under diverse maritime\nenvironments and weather conditions. Based upon this dataset, we propose\nHMPNet, a lightweight architecture tailored for shipborne object detection.\nHMPNet incorporates a hierarchical dynamic modulation backbone to bolster\nfeature aggregation and expression, complemented by a matrix cascading\npoly-scale neck and a polymerization weight sharing detector, facilitating\nefficient multi-scale feature aggregation. Empirical evaluations indicate that\nHMPNet surpasses current state-of-the-art methods in terms of both accuracy and\ncomputational efficiency, realizing a 3.3% improvement in mean Average\nPrecision over YOLOv11n, the prevailing model, and reducing parameters by 23%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of intelligent maritime navigation, object detection from a\nshipborne perspective is paramount. Despite the criticality, the paucity of\nmaritime-specific data impedes the deployment of sophisticated visual\nperception techniques, akin to those utilized in autonomous vehicular systems,\nwithin the maritime context. To bridge this gap, we introduce Navigation12, a\nnovel dataset annotated for 12 object categories under diverse maritime\nenvironments and weather conditions. Based upon this dataset, we propose\nHMPNet, a lightweight architecture tailored for shipborne object detection.\nHMPNet incorporates a hierarchical dynamic modulation backbone to bolster\nfeature aggregation and expression, complemented by a matrix cascading\npoly-scale neck and a polymerization weight sharing detector, facilitating\nefficient multi-scale feature aggregation. Empirical evaluations indicate that\nHMPNet surpasses current state-of-the-art methods in terms of both accuracy and\ncomputational efficiency, realizing a 3.3% improvement in mean Average\nPrecision over YOLOv11n, the prevailing model, and reducing parameters by 23%."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Juan Lyu"
                    },
                    {
                        "name": "Yi Wei"
                    },
                    {
                        "name": "Changdong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Changdong Yu"
                },
                "author": "Changdong Yu",
                "arxiv_comment": "This paper has been accepted to ICME 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10613v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10613v3",
                "updated": "2025-05-13T05:08:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    5,
                    8,
                    2,
                    1,
                    133,
                    0
                ],
                "published": "2024-08-20T07:48:19Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    7,
                    48,
                    19,
                    1,
                    233,
                    0
                ],
                "title": "Task-level Distributionally Robust Optimization for Large Language\n  Model-based Dense Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-level Distributionally Robust Optimization for Large Language\n  Model-based Dense Retrieval"
                },
                "summary": "Large Language Model-based Dense Retrieval (LLM-DR) optimizes over numerous\nheterogeneous fine-tuning collections from different domains. However, the\ndiscussion about its training data distribution is still minimal. Previous\nstudies rely on empirically assigned dataset choices or sampling ratios, which\ninevitably lead to sub-optimal retrieval performances. In this paper, we\npropose a new task-level Distributionally Robust Optimization (tDRO) algorithm\nfor LLM-DR fine-tuning, targeted at improving the universal domain\ngeneralization ability by end-to-end reweighting the data distribution of each\ntask. The tDRO parameterizes the domain weights and updates them with scaled\ndomain gradients. The optimized weights are then transferred to the LLM-DR\nfine-tuning to train more robust retrievers. Experiments show optimal\nimprovements in large-scale retrieval benchmarks and reduce up to 30% dataset\nusage after applying our optimization algorithm with a series of\ndifferent-sized LLM-DR models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-based Dense Retrieval (LLM-DR) optimizes over numerous\nheterogeneous fine-tuning collections from different domains. However, the\ndiscussion about its training data distribution is still minimal. Previous\nstudies rely on empirically assigned dataset choices or sampling ratios, which\ninevitably lead to sub-optimal retrieval performances. In this paper, we\npropose a new task-level Distributionally Robust Optimization (tDRO) algorithm\nfor LLM-DR fine-tuning, targeted at improving the universal domain\ngeneralization ability by end-to-end reweighting the data distribution of each\ntask. The tDRO parameterizes the domain weights and updates them with scaled\ndomain gradients. The optimized weights are then transferred to the LLM-DR\nfine-tuning to train more robust retrievers. Experiments show optimal\nimprovements in large-scale retrieval benchmarks and reduce up to 30% dataset\nusage after applying our optimization algorithm with a series of\ndifferent-sized LLM-DR models."
                },
                "authors": [
                    {
                        "name": "Guangyuan Ma"
                    },
                    {
                        "name": "Yongliang Ma"
                    },
                    {
                        "name": "Xing Wu"
                    },
                    {
                        "name": "Zhenpeng Su"
                    },
                    {
                        "name": "Ming Zhou"
                    },
                    {
                        "name": "Songlin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Songlin Hu"
                },
                "author": "Songlin Hu",
                "arxiv_comment": "Accepted by AAAI25. Source code is available at\n  https://github.com/ma787639046/tdro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10613v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10613v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04830v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04830v3",
                "updated": "2025-05-13T05:02:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    5,
                    2,
                    11,
                    1,
                    133,
                    0
                ],
                "published": "2025-03-05T08:58:35Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    8,
                    58,
                    35,
                    2,
                    64,
                    0
                ],
                "title": "Cite Before You Speak: Enhancing Context-Response Grounding in\n  E-commerce Conversational LLM-Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cite Before You Speak: Enhancing Context-Response Grounding in\n  E-commerce Conversational LLM-Agents"
                },
                "summary": "With the advancement of conversational large language models (LLMs), several\nLLM-based Conversational Shopping Agents (CSA) have been developed to help\ncustomers smooth their online shopping. The primary objective in building an\nengaging and trustworthy CSA is to ensure the agent's responses about product\nfactoids are accurate and factually grounded. However, two challenges remain.\nFirst, LLMs produce hallucinated or unsupported claims. Such inaccuracies risk\nspreading misinformation and diminishing customer trust. Second, without\nproviding knowledge source attribution in CSA response, customers struggle to\nverify LLM-generated information. To address both challenges, we present an\neasily productionized solution that enables a ''citation experience'' to our\ncustomers. We build auto-evaluation metrics to holistically evaluate LLM's\ngrounding and attribution capabilities, suggesting that citation generation\nparadigm substantially improves grounding performance by 13.83%. To deploy this\ncapability at scale, we introduce Multi-UX-Inference system, which appends\nsource citations to LLM outputs while preserving existing user experience\nfeatures and supporting scalable inference. Large-scale online A/B tests show\nthat grounded CSA responses improves customer engagement by 3% - 10%, depending\non UX variations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of conversational large language models (LLMs), several\nLLM-based Conversational Shopping Agents (CSA) have been developed to help\ncustomers smooth their online shopping. The primary objective in building an\nengaging and trustworthy CSA is to ensure the agent's responses about product\nfactoids are accurate and factually grounded. However, two challenges remain.\nFirst, LLMs produce hallucinated or unsupported claims. Such inaccuracies risk\nspreading misinformation and diminishing customer trust. Second, without\nproviding knowledge source attribution in CSA response, customers struggle to\nverify LLM-generated information. To address both challenges, we present an\neasily productionized solution that enables a ''citation experience'' to our\ncustomers. We build auto-evaluation metrics to holistically evaluate LLM's\ngrounding and attribution capabilities, suggesting that citation generation\nparadigm substantially improves grounding performance by 13.83%. To deploy this\ncapability at scale, we introduce Multi-UX-Inference system, which appends\nsource citations to LLM outputs while preserving existing user experience\nfeatures and supporting scalable inference. Large-scale online A/B tests show\nthat grounded CSA responses improves customer engagement by 3% - 10%, depending\non UX variations."
                },
                "authors": [
                    {
                        "name": "Jingying Zeng"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Zhenwei Dai"
                    },
                    {
                        "name": "Xianfeng Tang"
                    },
                    {
                        "name": "Chen Luo"
                    },
                    {
                        "name": "Samarth Varshney"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Qi He"
                    }
                ],
                "author_detail": {
                    "name": "Qi He"
                },
                "author": "Qi He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04830v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04830v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]