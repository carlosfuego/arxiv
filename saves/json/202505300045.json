[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.22618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22618v1",
                "updated": "2025-05-28T17:39:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:39:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding"
                },
                "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22425v1",
                "updated": "2025-05-28T14:52:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    52,
                    15,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T14:52:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    52,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Scaling Reasoning without Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Reasoning without Attention"
                },
                "summary": "Large language models (LLMs) have made significant advances in complex\nreasoning tasks, yet they remain bottlenecked by two core challenges:\narchitectural inefficiency due to reliance on Transformers, and a lack of\nstructured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an\nattention-free language model that addresses both issues through architectural\nand data-centric innovations. Built on the state space dual (SSD) layers of\nMamba-2, our model eliminates the need for self-attention and key-value\ncaching, enabling fixed-memory, constant-time inference. To train it for\ncomplex reasoning, we propose a two-phase curriculum fine-tuning strategy based\non the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically\nstructured problems via abstract concept selection and rationale-guided\ngeneration. On benchmark evaluations, \\ourmodel-7B outperforms strong\nTransformer and hybrid models of comparable scale, and even surpasses the much\nlarger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on\nLivecodebench. These results highlight the potential of state space models as\nefficient and scalable alternatives to attention-based architectures for\nhigh-capacity reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant advances in complex\nreasoning tasks, yet they remain bottlenecked by two core challenges:\narchitectural inefficiency due to reliance on Transformers, and a lack of\nstructured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an\nattention-free language model that addresses both issues through architectural\nand data-centric innovations. Built on the state space dual (SSD) layers of\nMamba-2, our model eliminates the need for self-attention and key-value\ncaching, enabling fixed-memory, constant-time inference. To train it for\ncomplex reasoning, we propose a two-phase curriculum fine-tuning strategy based\non the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically\nstructured problems via abstract concept selection and rationale-guided\ngeneration. On benchmark evaluations, \\ourmodel-7B outperforms strong\nTransformer and hybrid models of comparable scale, and even surpasses the much\nlarger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on\nLivecodebench. These results highlight the potential of state space models as\nefficient and scalable alternatives to attention-based architectures for\nhigh-capacity reasoning."
                },
                "authors": [
                    {
                        "name": "Xueliang Zhao"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Lingpeng Kong"
                    }
                ],
                "author_detail": {
                    "name": "Lingpeng Kong"
                },
                "author": "Lingpeng Kong",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07864v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07864v4",
                "updated": "2025-05-28T12:07:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    12,
                    7,
                    57,
                    2,
                    148,
                    0
                ],
                "published": "2025-02-11T18:20:18Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "title": "TransMLA: Migrating GQA Models to MLA with Full DeepSeek Compatibility\n  and Speedup",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransMLA: Migrating GQA Models to MLA with Full DeepSeek Compatibility\n  and Speedup"
                },
                "summary": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Zengwei Yao"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/fxmeng/TransMLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07864v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07864v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22156v1",
                "updated": "2025-05-28T09:20:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    9,
                    20,
                    18,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T09:20:18Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    9,
                    20,
                    18,
                    2,
                    148,
                    0
                ],
                "title": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing"
                },
                "summary": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method."
                },
                "authors": [
                    {
                        "name": "Shuaiyi Li"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Chenlong Deng"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21919v1",
                "updated": "2025-05-28T03:05:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    3,
                    5,
                    55,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T03:05:55Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    3,
                    5,
                    55,
                    2,
                    148,
                    0
                ],
                "title": "Towards Efficient Key-Value Cache Management for Prefix Prefilling in\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Key-Value Cache Management for Prefix Prefilling in\n  LLM Inference"
                },
                "summary": "The increasing adoption of large language models (LLMs) with extended context\nwindows necessitates efficient Key-Value Cache (KVC) management to optimize\ninference performance. Inference workloads like Retrieval-Augmented Generation\n(RAG) and agents exhibit high cache reusability, making efficient caching\ncritical to reducing redundancy and improving speed. We analyze real-world KVC\naccess patterns using publicly available traces and evaluate commercial\nkey-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1]\nand Sherman [2]) for KVC metadata management. Our work demonstrates the lack of\ntailored storage solution for KVC prefilling, underscores the need for an\nefficient distributed caching system with optimized metadata management for LLM\nworkloads, and provides insights into designing improved KVC management systems\nfor scalable, low-latency inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of large language models (LLMs) with extended context\nwindows necessitates efficient Key-Value Cache (KVC) management to optimize\ninference performance. Inference workloads like Retrieval-Augmented Generation\n(RAG) and agents exhibit high cache reusability, making efficient caching\ncritical to reducing redundancy and improving speed. We analyze real-world KVC\naccess patterns using publicly available traces and evaluate commercial\nkey-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1]\nand Sherman [2]) for KVC metadata management. Our work demonstrates the lack of\ntailored storage solution for KVC prefilling, underscores the need for an\nefficient distributed caching system with optimized metadata management for LLM\nworkloads, and provides insights into designing improved KVC management systems\nfor scalable, low-latency inference."
                },
                "authors": [
                    {
                        "name": "Yue Zhu"
                    },
                    {
                        "name": "Hao Yu"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Zhuoran Liu"
                    },
                    {
                        "name": "Eun Kyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Eun Kyung Lee"
                },
                "author": "Eun Kyung Lee",
                "arxiv_comment": "This paper has been accepted at IEEE Cloud 2025 as WIP paper. The\n  final version will appear in IEEE Xplore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21889v1",
                "updated": "2025-05-28T02:07:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    2,
                    7,
                    3,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T02:07:03Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    2,
                    7,
                    3,
                    2,
                    148,
                    0
                ],
                "title": "EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV\n  Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV\n  Cache Reuse"
                },
                "summary": "Large language models (LLMs) are often used for infilling tasks, which\ninvolve predicting or generating missing information in a given text. These\ntasks typically require multiple interactions with similar context. To reduce\nthe computation of repeated historical tokens, cross-request key-value (KV)\ncache reuse, a technique that stores and reuses intermediate computations, has\nbecome a crucial method in multi-round interactive services. However, in\ninfilling tasks, the KV cache reuse is often hindered by the structure of the\nprompt format, which typically consists of a prefix and suffix relative to the\ninsertion point. Specifically, the KV cache of the prefix or suffix part is\nfrequently invalidated as the other part (suffix or prefix) is incrementally\ngenerated. To address the issue, we propose EFIM, a transformed prompt format\nof FIM to unleash the performance potential of KV cache reuse. Although the\ntransformed prompt can solve the inefficiency, it exposes subtoken generation\nproblems in current LLMs, where they have difficulty generating partial words\naccurately. Therefore, we introduce a fragment tokenization training method\nwhich splits text into multiple fragments before tokenization during data\nprocessing. Experiments on two representative LLMs show that LLM serving with\nEFIM can lower the latency by 52% and improve the throughput by 98% while\nmaintaining the original infilling capability.EFIM's source code is publicly\navailable at https://github.com/gty111/EFIM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are often used for infilling tasks, which\ninvolve predicting or generating missing information in a given text. These\ntasks typically require multiple interactions with similar context. To reduce\nthe computation of repeated historical tokens, cross-request key-value (KV)\ncache reuse, a technique that stores and reuses intermediate computations, has\nbecome a crucial method in multi-round interactive services. However, in\ninfilling tasks, the KV cache reuse is often hindered by the structure of the\nprompt format, which typically consists of a prefix and suffix relative to the\ninsertion point. Specifically, the KV cache of the prefix or suffix part is\nfrequently invalidated as the other part (suffix or prefix) is incrementally\ngenerated. To address the issue, we propose EFIM, a transformed prompt format\nof FIM to unleash the performance potential of KV cache reuse. Although the\ntransformed prompt can solve the inefficiency, it exposes subtoken generation\nproblems in current LLMs, where they have difficulty generating partial words\naccurately. Therefore, we introduce a fragment tokenization training method\nwhich splits text into multiple fragments before tokenization during data\nprocessing. Experiments on two representative LLMs show that LLM serving with\nEFIM can lower the latency by 52% and improve the throughput by 98% while\nmaintaining the original infilling capability.EFIM's source code is publicly\navailable at https://github.com/gty111/EFIM."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Hande Dong"
                    },
                    {
                        "name": "Yichong Leng"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Cheater Lin"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Xianwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xianwei Zhang"
                },
                "author": "Xianwei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14775v2",
                "updated": "2025-05-28T01:38:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    1,
                    38,
                    7,
                    2,
                    148,
                    0
                ],
                "published": "2025-04-21T00:07:49Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "title": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling"
                },
                "summary": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Xianwei Zhang"
                    },
                    {
                        "name": "Jiangsu Du"
                    },
                    {
                        "name": "Zhiguang Chen"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Yutong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Lu"
                },
                "author": "Yutong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07872v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07872v3",
                "updated": "2025-05-28T00:43:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    0,
                    43,
                    47,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-09T21:05:20Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    21,
                    5,
                    20,
                    4,
                    129,
                    0
                ],
                "title": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions"
                },
                "summary": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion."
                },
                "authors": [
                    {
                        "name": "Yijing Zhang"
                    },
                    {
                        "name": "Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07872v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07872v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21669v1",
                "updated": "2025-05-27T18:47:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    18,
                    47,
                    34,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T18:47:34Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    18,
                    47,
                    34,
                    1,
                    147,
                    0
                ],
                "title": "Improved Prefetching Techniques for Linked Data Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Prefetching Techniques for Linked Data Structures"
                },
                "summary": "With ever-increasing main memory stall times, we need novel techniques to\nreduce effective memory access latencies. Prefetching has been shown to be an\neffective solution, especially with contiguous data structures that follow the\ntraditional principles of spatial and temporal locality. However, on linked\ndata structures$-$made up of many nodes linked together with pointers$-$typical\nprefetchers struggle, failing to predict accesses as elements are arbitrarily\nscattered throughout memory and access patters are arbitrarily complex and\nhence difficult to predict. To remedy these issues, we introduce\n$\\textit{Linkey}$, a novel prefetcher that utilizes hints from the\nprogrammer/compiler to cache layout information and accurately prefetch linked\ndata structures. $\\textit{Linkey}$ obtains substantial performance improvements\nover a striding baseline. We achieve a geomean 13% reduction in miss rate with\na maximum improvement of 58.8%, and a 65.4% geomean increase in accuracy, with\nmany benchmarks improving from 0%. On benchmarks where $\\textit{Linkey}$ is\napplicable, we observe a geomean IPC improvement of 1.40%, up to 12.1%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With ever-increasing main memory stall times, we need novel techniques to\nreduce effective memory access latencies. Prefetching has been shown to be an\neffective solution, especially with contiguous data structures that follow the\ntraditional principles of spatial and temporal locality. However, on linked\ndata structures$-$made up of many nodes linked together with pointers$-$typical\nprefetchers struggle, failing to predict accesses as elements are arbitrarily\nscattered throughout memory and access patters are arbitrarily complex and\nhence difficult to predict. To remedy these issues, we introduce\n$\\textit{Linkey}$, a novel prefetcher that utilizes hints from the\nprogrammer/compiler to cache layout information and accurately prefetch linked\ndata structures. $\\textit{Linkey}$ obtains substantial performance improvements\nover a striding baseline. We achieve a geomean 13% reduction in miss rate with\na maximum improvement of 58.8%, and a 65.4% geomean increase in accuracy, with\nmany benchmarks improving from 0%. On benchmarks where $\\textit{Linkey}$ is\napplicable, we observe a geomean IPC improvement of 1.40%, up to 12.1%."
                },
                "authors": [
                    {
                        "name": "Nikola Vuk Maruszewski"
                    }
                ],
                "author_detail": {
                    "name": "Nikola Vuk Maruszewski"
                },
                "author": "Nikola Vuk Maruszewski",
                "arxiv_comment": "73 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.5.3; E.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21487v1",
                "updated": "2025-05-27T17:54:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    54,
                    7,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:54:07Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    54,
                    7,
                    1,
                    147,
                    0
                ],
                "title": "Hardware-Efficient Attention for Fast Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware-Efficient Attention for Fast Decoding"
                },
                "summary": "LLM decoding is bottlenecked for large batches and long contexts by loading\nthe key-value (KV) cache from high-bandwidth memory, which inflates per-token\nlatency, while the sequential nature of decoding limits parallelism. We analyze\nthe interplay among arithmetic intensity, parallelization, and model quality\nand question whether current architectures fully exploit modern hardware. This\nwork redesigns attention to perform more computation per byte loaded from\nmemory to maximize hardware efficiency without trading off parallel\nscalability. We first propose Grouped-Tied Attention (GTA), a simple variant\nthat combines and reuses key and value states, reducing memory transfers\nwithout compromising model quality. We then introduce Grouped Latent Attention\n(GLA), a parallel-friendly latent attention paired with low-level optimizations\nfor fast decoding while maintaining high model quality. Experiments show that\nGTA matches Grouped-Query Attention (GQA) quality while using roughly half the\nKV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier\nto shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for\nexample, in a speculative decoding setting when the query length exceeds one.\nFurthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end\nlatency and increases throughput in online serving benchmarks by up to\n2$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM decoding is bottlenecked for large batches and long contexts by loading\nthe key-value (KV) cache from high-bandwidth memory, which inflates per-token\nlatency, while the sequential nature of decoding limits parallelism. We analyze\nthe interplay among arithmetic intensity, parallelization, and model quality\nand question whether current architectures fully exploit modern hardware. This\nwork redesigns attention to perform more computation per byte loaded from\nmemory to maximize hardware efficiency without trading off parallel\nscalability. We first propose Grouped-Tied Attention (GTA), a simple variant\nthat combines and reuses key and value states, reducing memory transfers\nwithout compromising model quality. We then introduce Grouped Latent Attention\n(GLA), a parallel-friendly latent attention paired with low-level optimizations\nfor fast decoding while maintaining high model quality. Experiments show that\nGTA matches Grouped-Query Attention (GQA) quality while using roughly half the\nKV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier\nto shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for\nexample, in a speculative decoding setting when the query length exceeds one.\nFurthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end\nlatency and increases throughput in online serving benchmarks by up to\n2$\\times$."
                },
                "authors": [
                    {
                        "name": "Ted Zadouri"
                    },
                    {
                        "name": "Hubert Strauss"
                    },
                    {
                        "name": "Tri Dao"
                    }
                ],
                "author_detail": {
                    "name": "Tri Dao"
                },
                "author": "Tri Dao",
                "arxiv_comment": "37 pages, 15 figures, 45 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21467v1",
                "updated": "2025-05-27T17:39:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    39,
                    39,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:39:39Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    39,
                    39,
                    1,
                    147,
                    0
                ],
                "title": "Accelerating Diffusion Language Model Inference via Efficient KV Caching\n  and Guided Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Language Model Inference via Efficient KV Caching\n  and Guided Diffusion"
                },
                "summary": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver up to a 34x\nend-to-end speedup without compromising accuracy. For the first time, diffusion\nlanguage models achieve a comparable and even faster latency as the widely\nadopted autoregressive models. Our work successfully paved the way for scaling\nup the diffusion language model to a broader scope of applications across\ndifferent domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver up to a 34x\nend-to-end speedup without compromising accuracy. For the first time, diffusion\nlanguage models achieve a comparable and even faster latency as the widely\nadopted autoregressive models. Our work successfully paved the way for scaling\nup the diffusion language model to a broader scope of applications across\ndifferent domains."
                },
                "authors": [
                    {
                        "name": "Zhanqiu Hu"
                    },
                    {
                        "name": "Jian Meng"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Jae-sun Seo"
                    },
                    {
                        "name": "Zhiru Zhang"
                    },
                    {
                        "name": "Udit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Udit Gupta"
                },
                "author": "Udit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21259v1",
                "updated": "2025-05-27T14:39:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    39,
                    28,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:39:28Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    39,
                    28,
                    1,
                    147,
                    0
                ],
                "title": "Stochastic Geometry-Based Performance Evaluation for LEO\n  Satellite-Assisted Space Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Geometry-Based Performance Evaluation for LEO\n  Satellite-Assisted Space Caching"
                },
                "summary": "To achieve the Internet of Things (IoT) vision,Mobile Edge Computing (MEC) is\na promising technology aimed at providing low-latency computing services to\nuser equipment (UE). However, terrestrial MEC network struggles to provide\nservice to UEs in remote and maritime region. Low Earth Orbit (LEO) satellite\nnetworks have the potential to overcome geographical restrictions and provide\nseamless global coverage for UEs. In this paper, we provide the first attempt\nto use stochastic geometry to investigate the performance of implementing space\ncaching with LEO satellites (SATs) in the MEC network. We study a LEO\nsatellite-assisted space caching MEC network, and LEO SATs can be equipped with\nservers to enable space caching, with the advantage of seamless coverage to\nassist terrestrial CSs for serving UEs in remote or maritime reigon. Using\nstochastic geometry and queuing theory, we establish an analytical framework\nfor this MEC network. Meanwhile, we develop association strategies for UEs to\nconnect with LEO SATs or CSs and utilize stochastic geometry to derive uplink\nand downlink coverage probabilities, considering the diversity of task and\nservice types. On this basis, we employ the queuing theory to calculate the\naverage delay to evaluate the system performance. Through Monte Carlo\nsimulations and numerical results, the system performance is evaluated. The\nresults show the potential of SAT spatial caching in improving the performance\nof the MEC network. Additionally, our results reveal useful insights such as\nthe significant impact of the altitude and number of LEO SATs on the average\ndelay of the network, providing helpful system-level recommendations for the\ndesign and configuration of the space-caching MEC network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To achieve the Internet of Things (IoT) vision,Mobile Edge Computing (MEC) is\na promising technology aimed at providing low-latency computing services to\nuser equipment (UE). However, terrestrial MEC network struggles to provide\nservice to UEs in remote and maritime region. Low Earth Orbit (LEO) satellite\nnetworks have the potential to overcome geographical restrictions and provide\nseamless global coverage for UEs. In this paper, we provide the first attempt\nto use stochastic geometry to investigate the performance of implementing space\ncaching with LEO satellites (SATs) in the MEC network. We study a LEO\nsatellite-assisted space caching MEC network, and LEO SATs can be equipped with\nservers to enable space caching, with the advantage of seamless coverage to\nassist terrestrial CSs for serving UEs in remote or maritime reigon. Using\nstochastic geometry and queuing theory, we establish an analytical framework\nfor this MEC network. Meanwhile, we develop association strategies for UEs to\nconnect with LEO SATs or CSs and utilize stochastic geometry to derive uplink\nand downlink coverage probabilities, considering the diversity of task and\nservice types. On this basis, we employ the queuing theory to calculate the\naverage delay to evaluate the system performance. Through Monte Carlo\nsimulations and numerical results, the system performance is evaluated. The\nresults show the potential of SAT spatial caching in improving the performance\nof the MEC network. Additionally, our results reveal useful insights such as\nthe significant impact of the altitude and number of LEO SATs on the average\ndelay of the network, providing helpful system-level recommendations for the\ndesign and configuration of the space-caching MEC network."
                },
                "authors": [
                    {
                        "name": "Chunyi Ma"
                    },
                    {
                        "name": "Jiajie Xu"
                    },
                    {
                        "name": "Jianhua Yang"
                    },
                    {
                        "name": "Mustafa A. Kishk"
                    }
                ],
                "author_detail": {
                    "name": "Mustafa A. Kishk"
                },
                "author": "Mustafa A. Kishk",
                "arxiv_doi": "10.1109/JIOT.2025.3574814",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JIOT.2025.3574814",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.21259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 12 figures, be accepted by IEEE IoTJ",
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14488v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14488v3",
                "updated": "2025-05-27T12:05:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    12,
                    5,
                    4,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-20T12:09:34Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    9,
                    34,
                    3,
                    51,
                    0
                ],
                "title": "U-index: A Universal Indexing Framework for Matching Long Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-index: A Universal Indexing Framework for Matching Long Patterns"
                },
                "summary": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but are slow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but are slow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping."
                },
                "authors": [
                    {
                        "name": "Lorraine A. K. Ayad"
                    },
                    {
                        "name": "Gabriele Fici"
                    },
                    {
                        "name": "Ragnar Groot Koerkamp"
                    },
                    {
                        "name": "Grigorios Loukides"
                    },
                    {
                        "name": "Rob Patro"
                    },
                    {
                        "name": "Giulio Ermanno Pibiri"
                    },
                    {
                        "name": "Solon P. Pissis"
                    }
                ],
                "author_detail": {
                    "name": "Solon P. Pissis"
                },
                "author": "Solon P. Pissis",
                "arxiv_comment": "SEA-2025 version. 18 pages, 6 figures, code available at\n  https://github.com/u-index/u-index-rs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14488v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14488v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21070v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21070v2",
                "updated": "2025-05-29T01:34:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    1,
                    34,
                    8,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-27T11:55:22Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    11,
                    55,
                    22,
                    1,
                    147,
                    0
                ],
                "title": "Minute-Long Videos with Dual Parallelisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minute-Long Videos with Dual Parallelisms"
                },
                "summary": "Diffusion Transformer (DiT)-based video diffusion models generate\nhigh-quality videos at scale but incur prohibitive processing latency and\nmemory costs for long videos. To address this, we propose a novel distributed\ninference strategy, termed DualParal. The core idea is that, instead of\ngenerating an entire video on a single GPU, we parallelize both temporal frames\nand model layers across GPUs. However, a naive implementation of this division\nfaces a key limitation: since diffusion models require synchronized noise\nlevels across frames, this implementation leads to the serialization of\noriginal parallelisms. We leverage a block-wise denoising scheme to handle\nthis. Namely, we process a sequence of frame blocks through the pipeline with\nprogressively decreasing noise levels. Each GPU handles a specific block and\nlayer subset while passing previous results to the next GPU, enabling\nasynchronous computation and communication. To further optimize performance, we\nincorporate two key enhancements. Firstly, a feature cache is implemented on\neach GPU to store and reuse features from the prior block as context,\nminimizing inter-GPU communication and redundant computation. Secondly, we\nemploy a coordinated noise initialization strategy, ensuring globally\nconsistent temporal dynamics by sharing initial noise patterns across GPUs\nwithout extra resource costs. Together, these enable fast, artifact-free, and\ninfinitely long video generation. Applied to the latest diffusion transformer\nvideo generator, our method efficiently produces 1,025-frame videos with up to\n6.54$\\times$ lower latency and 1.48$\\times$ lower memory cost on 8$\\times$RTX\n4090 GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT)-based video diffusion models generate\nhigh-quality videos at scale but incur prohibitive processing latency and\nmemory costs for long videos. To address this, we propose a novel distributed\ninference strategy, termed DualParal. The core idea is that, instead of\ngenerating an entire video on a single GPU, we parallelize both temporal frames\nand model layers across GPUs. However, a naive implementation of this division\nfaces a key limitation: since diffusion models require synchronized noise\nlevels across frames, this implementation leads to the serialization of\noriginal parallelisms. We leverage a block-wise denoising scheme to handle\nthis. Namely, we process a sequence of frame blocks through the pipeline with\nprogressively decreasing noise levels. Each GPU handles a specific block and\nlayer subset while passing previous results to the next GPU, enabling\nasynchronous computation and communication. To further optimize performance, we\nincorporate two key enhancements. Firstly, a feature cache is implemented on\neach GPU to store and reuse features from the prior block as context,\nminimizing inter-GPU communication and redundant computation. Secondly, we\nemploy a coordinated noise initialization strategy, ensuring globally\nconsistent temporal dynamics by sharing initial noise patterns across GPUs\nwithout extra resource costs. Together, these enable fast, artifact-free, and\ninfinitely long video generation. Applied to the latest diffusion transformer\nvideo generator, our method efficiently produces 1,025-frame videos with up to\n6.54$\\times$ lower latency and 1.48$\\times$ lower memory cost on 8$\\times$RTX\n4090 GPUs."
                },
                "authors": [
                    {
                        "name": "Zeqing Wang"
                    },
                    {
                        "name": "Bowen Zheng"
                    },
                    {
                        "name": "Xingyi Yang"
                    },
                    {
                        "name": "Zhenxiong Tan"
                    },
                    {
                        "name": "Yuecong Xu"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "The code is available at\n  https://github.com/DualParal-Project/DualParal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21070v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21070v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v3",
                "updated": "2025-05-27T09:24:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    9,
                    24,
                    50,
                    1,
                    147,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Caching for Serving Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Caching for Serving Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) show great capabilities in a wide range of\napplications, but serving them efficiently becomes increasingly challenging as\nrequests (prompts) become more complex. Context caching improves serving\nperformance by reusing Key-Value (KV) vectors, the intermediate representations\nof tokens that are repeated across requests. However, existing context caching\nrequires exact prefix matches across requests, limiting reuse cases in settings\nsuch as few-shot learning and retrieval-augmented generation, where immutable\ncontent (e.g., documents) remains unchanged across requests but is preceded by\nvarying prefixes. Position-Independent Caching (PIC) addresses this issue by\nenabling modular reuse of the KV vectors regardless of prefixes. We formalize\nPIC and advance prior work by introducing EPIC, a serving system incorporating\nour new LegoLink algorithm, which mitigates the inappropriate \"attention sink\"\neffect at every document beginning, to maintain accuracy with minimal\ncomputation. Experiments show that EPIC achieves up to 8x improvements in\nTime-To-First-Token (TTFT) and 7x throughput gains over existing systems, with\nnegligible or no accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show great capabilities in a wide range of\napplications, but serving them efficiently becomes increasingly challenging as\nrequests (prompts) become more complex. Context caching improves serving\nperformance by reusing Key-Value (KV) vectors, the intermediate representations\nof tokens that are repeated across requests. However, existing context caching\nrequires exact prefix matches across requests, limiting reuse cases in settings\nsuch as few-shot learning and retrieval-augmented generation, where immutable\ncontent (e.g., documents) remains unchanged across requests but is preceded by\nvarying prefixes. Position-Independent Caching (PIC) addresses this issue by\nenabling modular reuse of the KV vectors regardless of prefixes. We formalize\nPIC and advance prior work by introducing EPIC, a serving system incorporating\nour new LegoLink algorithm, which mitigates the inappropriate \"attention sink\"\neffect at every document beginning, to maintain accuracy with minimal\ncomputation. Experiments show that EPIC achieves up to 8x improvements in\nTime-To-First-Token (TTFT) and 7x throughput gains over existing systems, with\nnegligible or no accuracy loss."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20776v1",
                "updated": "2025-05-27T06:30:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T06:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences"
                },
                "summary": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models, reducing latency across\nall stages. To improve draft accuracy and speed, we propose Cross-model\nRetrieval, a novel KV cache update strategy that uses the target model's\nattention scores to dynamically select relevant context for the draft model.\nExtensive evaluations on three long-context understanding datasets show that\nSpecExtend accelerates standard tree-based speculative decoding by up to 2.22x\nfor inputs up to 16K tokens, providing an effective solution for speculative\ndecoding of long sequences. The code is available at\nhttps://github.com/jycha98/SpecExtend .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models, reducing latency across\nall stages. To improve draft accuracy and speed, we propose Cross-model\nRetrieval, a novel KV cache update strategy that uses the target model's\nattention scores to dynamically select relevant context for the draft model.\nExtensive evaluations on three long-context understanding datasets show that\nSpecExtend accelerates standard tree-based speculative decoding by up to 2.22x\nfor inputs up to 16K tokens, providing an effective solution for speculative\ndecoding of long sequences. The code is available at\nhttps://github.com/jycha98/SpecExtend ."
                },
                "authors": [
                    {
                        "name": "Jungyoub Cha"
                    },
                    {
                        "name": "Hyunjong Kim"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "arxiv_comment": "8 pages, 3 figures. Under review at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v3",
                "updated": "2025-05-27T04:15:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    4,
                    15,
                    22,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "vCache: Verified Semantic Prompt Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vCache: Verified Semantic Prompt Caching"
                },
                "summary": "Semantic caches return cached LLM-generated responses for semantically\nsimilar prompts to reduce inference latency and cost. They embed cached prompts\nand store them alongside their response in a vector database. Embedding\nsimilarity metrics assign a numerical score to quantify the similarity between\na request and its nearest neighbor prompt from the cache. Existing systems use\nthe same static similarity threshold across all requests to determine whether\ntwo prompts can share similar responses. However, we observe that static\nthresholds do not give formal correctness guarantees, can result in unexpected\nerror rates, and lead to suboptimal cache hit rates. This paper proposes\nvCache, the first verified semantic cache with user-defined error rate\nguarantees. It employs an online learning algorithm to estimate an optimal\nthreshold for each cached prompt, enabling reliable cache responses without\nadditional training. Our experiments show that vCache consistently meets the\nspecified error bounds while outperforming state-of-the-art static-threshold\nand fine-tuned embedding baselines. We release the vCache implementation and\nbenchmarks to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caches return cached LLM-generated responses for semantically\nsimilar prompts to reduce inference latency and cost. They embed cached prompts\nand store them alongside their response in a vector database. Embedding\nsimilarity metrics assign a numerical score to quantify the similarity between\na request and its nearest neighbor prompt from the cache. Existing systems use\nthe same static similarity threshold across all requests to determine whether\ntwo prompts can share similar responses. However, we observe that static\nthresholds do not give formal correctness guarantees, can result in unexpected\nerror rates, and lead to suboptimal cache hit rates. This paper proposes\nvCache, the first verified semantic cache with user-defined error rate\nguarantees. It employs an online learning algorithm to estimate an optimal\nthreshold for each cached prompt, enabling reliable cache responses without\nadditional training. Our experiments show that vCache consistently meets the\nspecified error bounds while outperforming state-of-the-art static-threshold\nand fine-tuned embedding baselines. We release the vCache implementation and\nbenchmarks to support future research."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Aditya Desai"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Kyle Chu"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18458v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18458v2",
                "updated": "2025-05-27T03:57:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    3,
                    57,
                    47,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-24T01:57:12Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    1,
                    57,
                    12,
                    5,
                    144,
                    0
                ],
                "title": "A Survey of LLM $\\times$ DATA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of LLM $\\times$ DATA"
                },
                "summary": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration."
                },
                "authors": [
                    {
                        "name": "Xuanhe Zhou"
                    },
                    {
                        "name": "Junxuan He"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Haodong Chen"
                    },
                    {
                        "name": "Zirui Tang"
                    },
                    {
                        "name": "Haoyu Zhao"
                    },
                    {
                        "name": "Xin Tong"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Youmin Chen"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Zhaojun Sun"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Fan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fan Wu"
                },
                "author": "Fan Wu",
                "arxiv_comment": "Please refer to the paper list at:\n  https://github.com/weAIDB/awesome-data-llm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18458v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18458v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19586v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19586v2",
                "updated": "2025-05-27T03:16:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    3,
                    16,
                    32,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-26T07:00:04Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    0,
                    4,
                    0,
                    146,
                    0
                ],
                "title": "TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV\n  Cache Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV\n  Cache Optimization"
                },
                "summary": "The Key-Value (KV) cache in generative large language models (LLMs)\nintroduces substantial memory overhead. Existing works mitigate this burden by\noffloading or compressing the KV cache. However, loading the entire cache\nincurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU\ncommunication, while aggressive compression causes notable performance\ndegradation. We identify that certain layers in the LLM need to maintain global\ninformation and are unsuitable for selective loading. In contrast, other layers\nprimarily focus on a few tokens with dominant activations that potentially\nincur substantial quantization error. This observation leads to a key insight\nthat loading dominant tokens and quantizing all tokens can complement each\nother. Building on this insight, we propose a hybrid compression method,\nTailorKV, which seamlessly integrates quantization and offloading. TailorKV\ndevelops an inference framework along with a hardware-friendly implementation\nthat leverages these complementary characteristics. Extensive long-context\nevaluations exhibit that TailorKV achieves nearly lossless performance under\naggressive compression settings, outperforming the state-of-the-art.\nParticularly, the Llama-3.1-8B with 128k context can be served within a single\nRTX 3090 GPU, reaching 82 ms per token during decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache in generative large language models (LLMs)\nintroduces substantial memory overhead. Existing works mitigate this burden by\noffloading or compressing the KV cache. However, loading the entire cache\nincurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU\ncommunication, while aggressive compression causes notable performance\ndegradation. We identify that certain layers in the LLM need to maintain global\ninformation and are unsuitable for selective loading. In contrast, other layers\nprimarily focus on a few tokens with dominant activations that potentially\nincur substantial quantization error. This observation leads to a key insight\nthat loading dominant tokens and quantizing all tokens can complement each\nother. Building on this insight, we propose a hybrid compression method,\nTailorKV, which seamlessly integrates quantization and offloading. TailorKV\ndevelops an inference framework along with a hardware-friendly implementation\nthat leverages these complementary characteristics. Extensive long-context\nevaluations exhibit that TailorKV achieves nearly lossless performance under\naggressive compression settings, outperforming the state-of-the-art.\nParticularly, the Llama-3.1-8B with 128k context can be served within a single\nRTX 3090 GPU, reaching 82 ms per token during decoding."
                },
                "authors": [
                    {
                        "name": "Dingyu Yao"
                    },
                    {
                        "name": "Bowen Shen"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19586v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19586v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v4",
                "updated": "2025-05-27T03:08:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    3,
                    8,
                    57,
                    1,
                    147,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20600v1",
                "updated": "2025-05-27T00:36:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    0,
                    36,
                    56,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T00:36:56Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    0,
                    36,
                    56,
                    1,
                    147,
                    0
                ],
                "title": "InstGenIE: Generative Image Editing Made Efficient with Mask-aware\n  Caching and Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstGenIE: Generative Image Editing Made Efficient with Mask-aware\n  Caching and Scheduling"
                },
                "summary": "Generative image editing using diffusion models has become a prevalent\napplication in today's AI cloud services. In production environments, image\nediting typically involves a mask that specifies the regions of an image\ntemplate to be edited. The use of masks provides direct control over the\nediting process and introduces sparsity in the model inference. In this paper,\nwe present InstGenIE, a system that efficiently serves image editing requests.\nThe key insight behind InstGenIE is that image editing only modifies the masked\nregions of image templates while preserving the original content in the\nunmasked areas. Driven by this insight, InstGenIE judiciously skips redundant\ncomputations associated with the unmasked areas by reusing cached intermediate\nactivations from previous inferences. To mitigate the high cache loading\noverhead, InstGenIE employs a bubble-free pipeline scheme that overlaps\ncomputation with cache loading. Additionally, to reduce queuing latency in\nonline serving while improving the GPU utilization, InstGenIE proposes a novel\ncontinuous batching strategy for diffusion model serving, allowing newly\narrived requests to join the running batch in just one step of denoising\ncomputation, without waiting for the entire batch to complete. As heterogeneous\nmasks induce imbalanced loads, InstGenIE also develops a load balancing\nstrategy that takes into account the loads of both computation and cache\nloading. Collectively, InstGenIE outperforms state-of-the-art diffusion serving\nsystems for image editing, achieving up to 3x higher throughput and reducing\naverage request latency by up to 14.7x while ensuring image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative image editing using diffusion models has become a prevalent\napplication in today's AI cloud services. In production environments, image\nediting typically involves a mask that specifies the regions of an image\ntemplate to be edited. The use of masks provides direct control over the\nediting process and introduces sparsity in the model inference. In this paper,\nwe present InstGenIE, a system that efficiently serves image editing requests.\nThe key insight behind InstGenIE is that image editing only modifies the masked\nregions of image templates while preserving the original content in the\nunmasked areas. Driven by this insight, InstGenIE judiciously skips redundant\ncomputations associated with the unmasked areas by reusing cached intermediate\nactivations from previous inferences. To mitigate the high cache loading\noverhead, InstGenIE employs a bubble-free pipeline scheme that overlaps\ncomputation with cache loading. Additionally, to reduce queuing latency in\nonline serving while improving the GPU utilization, InstGenIE proposes a novel\ncontinuous batching strategy for diffusion model serving, allowing newly\narrived requests to join the running batch in just one step of denoising\ncomputation, without waiting for the entire batch to complete. As heterogeneous\nmasks induce imbalanced loads, InstGenIE also develops a load balancing\nstrategy that takes into account the loads of both computation and cache\nloading. Collectively, InstGenIE outperforms state-of-the-art diffusion serving\nsystems for image editing, achieving up to 3x higher throughput and reducing\naverage request latency by up to 14.7x while ensuring image quality."
                },
                "authors": [
                    {
                        "name": "Xiaoxiao Jiang"
                    },
                    {
                        "name": "Suyi Li"
                    },
                    {
                        "name": "Lingyun Yang"
                    },
                    {
                        "name": "Tianyu Feng"
                    },
                    {
                        "name": "Zhipeng Di"
                    },
                    {
                        "name": "Weiyi Lu"
                    },
                    {
                        "name": "Guoxuan Zhu"
                    },
                    {
                        "name": "Xiu Lin"
                    },
                    {
                        "name": "Kan Liu"
                    },
                    {
                        "name": "Yinghao Yu"
                    },
                    {
                        "name": "Tao Lan"
                    },
                    {
                        "name": "Guodong Yang"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Liping Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20438v1",
                "updated": "2025-05-26T18:34:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    18,
                    34,
                    7,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T18:34:07Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    18,
                    34,
                    7,
                    0,
                    146,
                    0
                ],
                "title": "HAMburger: Accelerating LLM Inference via Token Smashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAMburger: Accelerating LLM Inference via Token Smashing"
                },
                "summary": "The growing demand for efficient Large Language Model (LLM) inference\nrequires a holistic optimization on algorithms, systems, and hardware. However,\nvery few works have fundamentally changed the generation pattern: each token\nneeds one forward pass and one KV cache. This can be sub-optimal because we\nfound that LLMs are extremely capable of self-identifying the exact dose of\ninformation that a single KV cache can store, and many tokens can be generated\nconfidently without global context. Based on this insight, we introduce\nHAMburger, a Hierarchically Auto-regressive Model that redefines resource\nallocation in LLMs by moving beyond uniform computation and storage per token\nduring inference. Stacking a compositional embedder and a micro-step decoder in\nbetween a base LLM, HAMburger smashes multiple tokens into a single KV and\ngenerates several tokens per step. Additionally, HAMburger functions as a\nspeculative decoding framework where it can blindly trust self-drafted tokens.\nAs a result, HAMburger shifts the growth of KV cache and forward FLOPs from\nlinear to sub-linear with respect to output length, and adjusts its inference\nspeed based on query perplexity and output structure. Extensive evaluations\nshow that HAMburger reduces the KV cache computation by up to 2$\\times$ and\nachieves up to 2$\\times$ TPS, while maintaining quality in both short- and\nlong-context tasks. Our method explores an extremely challenging inference\nregime that requires both computation- and memory-efficiency with a\nhardware-agnostic design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for efficient Large Language Model (LLM) inference\nrequires a holistic optimization on algorithms, systems, and hardware. However,\nvery few works have fundamentally changed the generation pattern: each token\nneeds one forward pass and one KV cache. This can be sub-optimal because we\nfound that LLMs are extremely capable of self-identifying the exact dose of\ninformation that a single KV cache can store, and many tokens can be generated\nconfidently without global context. Based on this insight, we introduce\nHAMburger, a Hierarchically Auto-regressive Model that redefines resource\nallocation in LLMs by moving beyond uniform computation and storage per token\nduring inference. Stacking a compositional embedder and a micro-step decoder in\nbetween a base LLM, HAMburger smashes multiple tokens into a single KV and\ngenerates several tokens per step. Additionally, HAMburger functions as a\nspeculative decoding framework where it can blindly trust self-drafted tokens.\nAs a result, HAMburger shifts the growth of KV cache and forward FLOPs from\nlinear to sub-linear with respect to output length, and adjusts its inference\nspeed based on query perplexity and output structure. Extensive evaluations\nshow that HAMburger reduces the KV cache computation by up to 2$\\times$ and\nachieves up to 2$\\times$ TPS, while maintaining quality in both short- and\nlong-context tasks. Our method explores an extremely challenging inference\nregime that requires both computation- and memory-efficiency with a\nhardware-agnostic design."
                },
                "authors": [
                    {
                        "name": "Jingyu Liu"
                    },
                    {
                        "name": "Ce Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ce Zhang"
                },
                "author": "Ce Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17644v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17644v5",
                "updated": "2025-05-26T16:16:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    16,
                    16,
                    43,
                    0,
                    146,
                    0
                ],
                "published": "2024-01-31T07:52:48Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    7,
                    52,
                    48,
                    2,
                    31,
                    0
                ],
                "title": "BurstGPT: A Real-world Workload Dataset to Optimize LLM Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BurstGPT: A Real-world Workload Dataset to Optimize LLM Serving Systems"
                },
                "summary": "Serving systems for Large Language Models (LLMs) are often optimized to\nimprove quality of service (QoS) and throughput. However, due to the lack of\nopen-source LLM serving workloads, these systems are frequently evaluated under\nunrealistic workload assumptions. Consequently, performance may degrade when\nsystems are deployed in real-world scenarios. This work presents BurstGPT, an\nLLM serving workload with 10.31 million traces from regional Azure OpenAI GPT\nservices over 213 days. BurstGPT captures LLM serving characteristics from\nuser, model and system perspectives: (1) User request concurrency: burstiness\nvariations of requests in Azure OpenAI GPT services, revealing diversified\nconcurrency patterns in different services and model types. (2) User\nconversation patterns: counts and intervals within conversations for service\noptimizations. (3) Model response lengths: auto-regressive serving processes of\nGPT models, showing statistical relations between requests and their responses.\n(4) System response failures: failures of conversation and API services,\nshowing intensive resource needs and limited availability of LLM services in\nAzure. The details of the characteristics can serve multiple purposes in LLM\nserving optimizations, such as system evaluation and trace provisioning. In our\ndemo evaluation with BurstGPT, frequent variations in BurstGPT reveal declines\nin efficiency, stability, or reliability in realistic LLM serving. We identify\nthat the generalization of KV cache management, scheduling and disaggregation\noptimizations can be improved under realistic workload evaluations. BurstGPT is\npublicly available now at https://github.com/HPMLL/BurstGPT and is widely used\nto develop prototypes of LLM serving frameworks in the industry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving systems for Large Language Models (LLMs) are often optimized to\nimprove quality of service (QoS) and throughput. However, due to the lack of\nopen-source LLM serving workloads, these systems are frequently evaluated under\nunrealistic workload assumptions. Consequently, performance may degrade when\nsystems are deployed in real-world scenarios. This work presents BurstGPT, an\nLLM serving workload with 10.31 million traces from regional Azure OpenAI GPT\nservices over 213 days. BurstGPT captures LLM serving characteristics from\nuser, model and system perspectives: (1) User request concurrency: burstiness\nvariations of requests in Azure OpenAI GPT services, revealing diversified\nconcurrency patterns in different services and model types. (2) User\nconversation patterns: counts and intervals within conversations for service\noptimizations. (3) Model response lengths: auto-regressive serving processes of\nGPT models, showing statistical relations between requests and their responses.\n(4) System response failures: failures of conversation and API services,\nshowing intensive resource needs and limited availability of LLM services in\nAzure. The details of the characteristics can serve multiple purposes in LLM\nserving optimizations, such as system evaluation and trace provisioning. In our\ndemo evaluation with BurstGPT, frequent variations in BurstGPT reveal declines\nin efficiency, stability, or reliability in realistic LLM serving. We identify\nthat the generalization of KV cache management, scheduling and disaggregation\noptimizations can be improved under realistic workload evaluations. BurstGPT is\npublicly available now at https://github.com/HPMLL/BurstGPT and is widely used\nto develop prototypes of LLM serving frameworks in the industry."
                },
                "authors": [
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Yuhan Chen"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Xueze Kang"
                    },
                    {
                        "name": "Yuchu Fang"
                    },
                    {
                        "name": "Yeju Zhou"
                    },
                    {
                        "name": "Yang Zheng"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Rui Guo"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17644v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17644v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17138v2",
                "updated": "2025-05-26T13:20:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    13,
                    20,
                    45,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-22T06:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    12,
                    42,
                    3,
                    142,
                    0
                ],
                "title": "RAP: Runtime-Adaptive Pruning for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAP: Runtime-Adaptive Pruning for LLM Inference"
                },
                "summary": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly."
                },
                "authors": [
                    {
                        "name": "Huanrong Liu"
                    },
                    {
                        "name": "Chunlin Tian"
                    },
                    {
                        "name": "Xuyang Wei"
                    },
                    {
                        "name": "Jiaheng Dai"
                    },
                    {
                        "name": "Qin Liu"
                    },
                    {
                        "name": "Tianqi Wei"
                    },
                    {
                        "name": "Qingbiao Li"
                    },
                    {
                        "name": "Li Li"
                    }
                ],
                "author_detail": {
                    "name": "Li Li"
                },
                "author": "Li Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19880v1",
                "updated": "2025-05-26T12:06:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    6,
                    12,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T12:06:12Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    6,
                    12,
                    0,
                    146,
                    0
                ],
                "title": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing"
                },
                "summary": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead."
                },
                "authors": [
                    {
                        "name": "Saman Akbari"
                    },
                    {
                        "name": "Manfred Hauswirth"
                    }
                ],
                "author_detail": {
                    "name": "Manfred Hauswirth"
                },
                "author": "Manfred Hauswirth",
                "arxiv_comment": "Accepted for publication in 2025 IEEE 18th International Conference\n  on Cloud Computing (CLOUD)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19849v1",
                "updated": "2025-05-26T11:35:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    11,
                    35,
                    4,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T11:35:04Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    11,
                    35,
                    4,
                    0,
                    146,
                    0
                ],
                "title": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems"
                },
                "summary": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://anonymous.4open.science/r/HIT_model-5C23.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://anonymous.4open.science/r/HIT_model-5C23."
                },
                "authors": [
                    {
                        "name": "Haoqiang Yang"
                    },
                    {
                        "name": "Congde Yuan"
                    },
                    {
                        "name": "Kun Bai"
                    },
                    {
                        "name": "Mengzhuo Guo"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Chao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhou"
                },
                "author": "Chao Zhou",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16582v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16582v2",
                "updated": "2025-05-26T10:07:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    10,
                    7,
                    5,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-22T12:17:13Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    12,
                    17,
                    13,
                    3,
                    142,
                    0
                ],
                "title": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended\n  Question Answering"
                },
                "summary": "Large Language Models (LLMs), despite their advancements, are fundamentally\nlimited by their static parametric knowledge, hindering performance on tasks\nrequiring open-domain up-to-date information. While enabling LLMs to interact\nwith external knowledge environments is a promising solution, current efforts\nprimarily address closed-end problems. Open-ended questions, which\ncharacterized by lacking a standard answer or providing non-unique and diverse\nanswers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a\nnovel search agent leveraging reinforcement learning to effectively tackle both\nopen-ended and closed-ended questions in the open domain. O$^2$-Searcher\nleverages an efficient, locally simulated search environment for dynamic\nknowledge acquisition, effectively decoupling the external world knowledge from\nmodel's sophisticated reasoning processes. It employs a unified training\nmechanism with meticulously designed reward functions, enabling the agent to\nidentify problem types and adapt different answer generation strategies.\nFurthermore, to evaluate performance on complex open-ended tasks, we construct\nO$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain\nopen-ended questions with associated web page caches. Extensive experiments\nshow that O$^2$-Searcher, using only a 3B model, significantly surpasses\nleading LLM agents on O$^2$-QA. It also achieves SOTA results on various\nclosed-ended QA benchmarks against similarly-sized models, while performing on\npar with much larger ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), despite their advancements, are fundamentally\nlimited by their static parametric knowledge, hindering performance on tasks\nrequiring open-domain up-to-date information. While enabling LLMs to interact\nwith external knowledge environments is a promising solution, current efforts\nprimarily address closed-end problems. Open-ended questions, which\ncharacterized by lacking a standard answer or providing non-unique and diverse\nanswers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a\nnovel search agent leveraging reinforcement learning to effectively tackle both\nopen-ended and closed-ended questions in the open domain. O$^2$-Searcher\nleverages an efficient, locally simulated search environment for dynamic\nknowledge acquisition, effectively decoupling the external world knowledge from\nmodel's sophisticated reasoning processes. It employs a unified training\nmechanism with meticulously designed reward functions, enabling the agent to\nidentify problem types and adapt different answer generation strategies.\nFurthermore, to evaluate performance on complex open-ended tasks, we construct\nO$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain\nopen-ended questions with associated web page caches. Extensive experiments\nshow that O$^2$-Searcher, using only a 3B model, significantly surpasses\nleading LLM agents on O$^2$-QA. It also achieves SOTA results on various\nclosed-ended QA benchmarks against similarly-sized models, while performing on\npar with much larger ones."
                },
                "authors": [
                    {
                        "name": "Jianbiao Mei"
                    },
                    {
                        "name": "Tao Hu"
                    },
                    {
                        "name": "Daocheng Fu"
                    },
                    {
                        "name": "Licheng Wen"
                    },
                    {
                        "name": "Xuemeng Yang"
                    },
                    {
                        "name": "Rong Wu"
                    },
                    {
                        "name": "Pinlong Cai"
                    },
                    {
                        "name": "Xinyu Cai"
                    },
                    {
                        "name": "Xing Gao"
                    },
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Chengjun Xie"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Yu Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Yu Qiao"
                },
                "author": "Yu Qiao",
                "arxiv_comment": "25 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16582v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16582v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17062v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17062v3",
                "updated": "2025-05-26T08:39:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    8,
                    39,
                    26,
                    0,
                    146,
                    0
                ],
                "published": "2024-05-27T11:31:58Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    11,
                    31,
                    58,
                    0,
                    148,
                    0
                ],
                "title": "UniICL: An Efficient Unified Framework Unifying Compression, Selection,\n  and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniICL: An Efficient Unified Framework Unifying Compression, Selection,\n  and Generation"
                },
                "summary": "In-context learning (ICL) enhances the reasoning abilities of Large Language\nModels (LLMs) by prepending a few demonstrations. It motivates researchers to\nintroduce more examples to provide additional contextual information for the\ngeneration. However, existing methods show a significant limitation due to the\nproblem of excessive growth in context length, which causes a large hardware\nburden. In addition, shallow-relevant examples selected by off-the-shelf tools\nhinder LLMs from capturing useful contextual information for generation. In\nthis paper, we propose \\textbf{UniICL}, a novel \\textbf{Uni}fied \\textbf{ICL}\nframework that unifies demonstration compression, demonstration selection, and\nfinal response generation. Furthermore, to boost inference efficiency, we\ndesign a tailored compression strategy that allows UniICL to cache compression\nresults into \\textbf{Demonstration Bank} (\\textbf{DB}), which avoids repeated\ncompression of the same demonstration. Extensive out-of-domain evaluations\nprove the advantages of UniICL in both effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) enhances the reasoning abilities of Large Language\nModels (LLMs) by prepending a few demonstrations. It motivates researchers to\nintroduce more examples to provide additional contextual information for the\ngeneration. However, existing methods show a significant limitation due to the\nproblem of excessive growth in context length, which causes a large hardware\nburden. In addition, shallow-relevant examples selected by off-the-shelf tools\nhinder LLMs from capturing useful contextual information for generation. In\nthis paper, we propose \\textbf{UniICL}, a novel \\textbf{Uni}fied \\textbf{ICL}\nframework that unifies demonstration compression, demonstration selection, and\nfinal response generation. Furthermore, to boost inference efficiency, we\ndesign a tailored compression strategy that allows UniICL to cache compression\nresults into \\textbf{Demonstration Bank} (\\textbf{DB}), which avoids repeated\ncompression of the same demonstration. Extensive out-of-domain evaluations\nprove the advantages of UniICL in both effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Jun Gao"
                    },
                    {
                        "name": "Qi Lv"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Tianxiang Wu"
                    },
                    {
                        "name": "Ziqiang Cao"
                    },
                    {
                        "name": "Wenjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Li"
                },
                "author": "Wenjie Li",
                "arxiv_comment": "ACL2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17062v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17062v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08192v2",
                "updated": "2025-05-26T07:30:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    30,
                    17,
                    0,
                    146,
                    0
                ],
                "published": "2025-01-14T15:14:10Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "title": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving"
                },
                "summary": "Large language models (LLMs) are typically served from clusters of GPUs/NPUs\nthat consist of large number of devices. Unfortunately, communication between\nthese devices incurs significant overhead, increasing the inference latency and\ncost while limiting the scalability. Prior work addressed this issue by\noverlapping communication with compute, but has severe limitations due to the\ndata dependencies between these operations. In this paper, we propose PRESERVE,\na novel framework that prefetches model weights and KV-cache from off-chip HBM\nmemory to the on-chip cache of AI accelerators during the communication\noperations, which offers various advantages and performance improvements\ncompared to prior methods.\n  Through extensive experiments conducted on commercial AI accelerators, we\ndemonstrate up to 1.6x end-to-end speedup on state-of-the-art, open-source\nLLMs. Additionally, we perform a design space exploration that identifies the\noptimal hardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are typically served from clusters of GPUs/NPUs\nthat consist of large number of devices. Unfortunately, communication between\nthese devices incurs significant overhead, increasing the inference latency and\ncost while limiting the scalability. Prior work addressed this issue by\noverlapping communication with compute, but has severe limitations due to the\ndata dependencies between these operations. In this paper, we propose PRESERVE,\na novel framework that prefetches model weights and KV-cache from off-chip HBM\nmemory to the on-chip cache of AI accelerators during the communication\noperations, which offers various advantages and performance improvements\ncompared to prior methods.\n  Through extensive experiments conducted on commercial AI accelerators, we\ndemonstrate up to 1.6x end-to-end speedup on state-of-the-art, open-source\nLLMs. Additionally, we perform a design space exploration that identifies the\noptimal hardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems."
                },
                "authors": [
                    {
                        "name": "Ahmet Caner Yüzügüler"
                    },
                    {
                        "name": "Jiawei Zhuang"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19602v1",
                "updated": "2025-05-26T07:11:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    11,
                    42,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T07:11:42Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    11,
                    42,
                    0,
                    146,
                    0
                ],
                "title": "Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV\n  Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV\n  Cache Compression"
                },
                "summary": "Visual Autoregressive (VAR) modeling has garnered significant attention for\nits innovative next-scale prediction approach, which yields substantial\nimprovements in efficiency, scalability, and zero-shot generalization.\nNevertheless, the coarse-to-fine methodology inherent in VAR results in\nexponential growth of the KV cache during inference, causing considerable\nmemory consumption and computational redundancy. To address these bottlenecks,\nwe introduce ScaleKV, a novel KV cache compression framework tailored for VAR\narchitectures. ScaleKV leverages two critical observations: varying cache\ndemands across transformer layers and distinct attention patterns at different\nscales. Based on these insights, ScaleKV categorizes transformer layers into\ntwo functional groups: drafters and refiners. Drafters exhibit dispersed\nattention across multiple scales, thereby requiring greater cache capacity.\nConversely, refiners focus attention on the current token map to process local\ndetails, consequently necessitating substantially reduced cache capacity.\nScaleKV optimizes the multi-scale inference pipeline by identifying\nscale-specific drafters and refiners, facilitating differentiated cache\nmanagement tailored to each scale. Evaluation on the state-of-the-art\ntext-to-image VAR model family, Infinity, demonstrates that our approach\neffectively reduces the required KV cache memory to 10% while preserving\npixel-level fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has garnered significant attention for\nits innovative next-scale prediction approach, which yields substantial\nimprovements in efficiency, scalability, and zero-shot generalization.\nNevertheless, the coarse-to-fine methodology inherent in VAR results in\nexponential growth of the KV cache during inference, causing considerable\nmemory consumption and computational redundancy. To address these bottlenecks,\nwe introduce ScaleKV, a novel KV cache compression framework tailored for VAR\narchitectures. ScaleKV leverages two critical observations: varying cache\ndemands across transformer layers and distinct attention patterns at different\nscales. Based on these insights, ScaleKV categorizes transformer layers into\ntwo functional groups: drafters and refiners. Drafters exhibit dispersed\nattention across multiple scales, thereby requiring greater cache capacity.\nConversely, refiners focus attention on the current token map to process local\ndetails, consequently necessitating substantially reduced cache capacity.\nScaleKV optimizes the multi-scale inference pipeline by identifying\nscale-specific drafters and refiners, facilitating differentiated cache\nmanagement tailored to each scale. Evaluation on the state-of-the-art\ntext-to-image VAR model family, Infinity, demonstrates that our approach\neffectively reduces the required KV cache memory to 10% while preserving\npixel-level fidelity."
                },
                "authors": [
                    {
                        "name": "Kunjun Li"
                    },
                    {
                        "name": "Zigeng Chen"
                    },
                    {
                        "name": "Cheng-Yen Yang"
                    },
                    {
                        "name": "Jenq-Neng Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Jenq-Neng Hwang"
                },
                "author": "Jenq-Neng Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20353v1",
                "updated": "2025-05-26T05:58:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    58,
                    49,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T05:58:49Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    58,
                    49,
                    0,
                    146,
                    0
                ],
                "title": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation"
                },
                "summary": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Ben Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v2",
                "updated": "2025-05-26T05:56:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    56,
                    51,
                    0,
                    146,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12392v2",
                "updated": "2025-05-26T05:28:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    28,
                    49,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-18T12:37:56Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    12,
                    37,
                    56,
                    6,
                    138,
                    0
                ],
                "title": "SLOT: Sample-specific Language Model Optimization at Test-time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLOT: Sample-specific Language Model Optimization at Test-time"
                },
                "summary": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT."
                },
                "authors": [
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Xingyu Zhang"
                    },
                    {
                        "name": "Xueji Fang"
                    },
                    {
                        "name": "Zhiyang Chen"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Huatian Zhang"
                    },
                    {
                        "name": "Guojun Qi"
                    }
                ],
                "author_detail": {
                    "name": "Guojun Qi"
                },
                "author": "Guojun Qi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12731v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12731v2",
                "updated": "2025-05-25T13:03:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    25,
                    13,
                    3,
                    54,
                    6,
                    145,
                    0
                ],
                "published": "2025-05-19T05:39:38Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    5,
                    39,
                    38,
                    0,
                    139,
                    0
                ],
                "title": "Accelerating Adaptive Retrieval Augmented Generation via\n  Instruction-Driven Representation Reduction of Retrieval Overlaps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Adaptive Retrieval Augmented Generation via\n  Instruction-Driven Representation Reduction of Retrieval Overlaps"
                },
                "summary": "Retrieval-augmented generation (RAG) has emerged as a pivotal method for\nexpanding the knowledge of large language models. To handle complex queries\nmore effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the\ngenerated quality through multiple interactions with external knowledge bases.\nDespite its effectiveness, A-RAG exacerbates the pre-existing efficiency\nchallenges inherent in RAG, which are attributable to its reliance on multiple\niterations of generation. Existing A-RAG approaches process all retrieved\ncontents from scratch. However, they ignore the situation where there is a\nsignificant overlap in the content of the retrieval results across rounds. The\noverlapping content is redundantly represented, which leads to a large\nproportion of repeated computations, thus affecting the overall efficiency. To\naddress this issue, this paper introduces a model-agnostic approach that can be\ngenerally applied to A-RAG methods, which is dedicated to reducing the\nredundant representation process caused by the overlapping of retrieval\nresults. Specifically, we use cache access and parallel generation to speed up\nthe prefilling and decoding stages respectively. Additionally, we also propose\nan instruction-driven module to further guide the model to more effectively\nattend to each part of the content in a more suitable way for LLMs. Experiments\nshow that our approach achieves 2.79 and 2.33 times significant acceleration on\naverage for prefilling and decoding respectively while maintaining equal\ngeneration quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has emerged as a pivotal method for\nexpanding the knowledge of large language models. To handle complex queries\nmore effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the\ngenerated quality through multiple interactions with external knowledge bases.\nDespite its effectiveness, A-RAG exacerbates the pre-existing efficiency\nchallenges inherent in RAG, which are attributable to its reliance on multiple\niterations of generation. Existing A-RAG approaches process all retrieved\ncontents from scratch. However, they ignore the situation where there is a\nsignificant overlap in the content of the retrieval results across rounds. The\noverlapping content is redundantly represented, which leads to a large\nproportion of repeated computations, thus affecting the overall efficiency. To\naddress this issue, this paper introduces a model-agnostic approach that can be\ngenerally applied to A-RAG methods, which is dedicated to reducing the\nredundant representation process caused by the overlapping of retrieval\nresults. Specifically, we use cache access and parallel generation to speed up\nthe prefilling and decoding stages respectively. Additionally, we also propose\nan instruction-driven module to further guide the model to more effectively\nattend to each part of the content in a more suitable way for LLMs. Experiments\nshow that our approach achieves 2.79 and 2.33 times significant acceleration on\naverage for prefilling and decoding respectively while maintaining equal\ngeneration quality."
                },
                "authors": [
                    {
                        "name": "Jie Ou"
                    },
                    {
                        "name": "Jinyu Guo"
                    },
                    {
                        "name": "Shuaihong Jiang"
                    },
                    {
                        "name": "Zhaokun Wang"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Shunyu Yao"
                    },
                    {
                        "name": "Wenhong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Wenhong Tian"
                },
                "author": "Wenhong Tian",
                "arxiv_comment": "Accepted at Findings of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12731v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12731v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19089v1",
                "updated": "2025-05-25T10:57:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    25,
                    10,
                    57,
                    35,
                    6,
                    145,
                    0
                ],
                "published": "2025-05-25T10:57:35Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    10,
                    57,
                    35,
                    6,
                    145,
                    0
                ],
                "title": "Plug-and-Play Context Feature Reuse for Efficient Masked Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play Context Feature Reuse for Efficient Masked Generation"
                },
                "summary": "Masked generative models (MGMs) have emerged as a powerful framework for\nimage synthesis, combining parallel decoding with strong bidirectional context\nmodeling. However, generating high-quality samples typically requires many\niterative decoding steps, resulting in high inference costs. A straightforward\nway to speed up generation is by decoding more tokens in each step, thereby\nreducing the total number of steps. However, when many tokens are decoded\nsimultaneously, the model can only estimate the univariate marginal\ndistributions independently, failing to capture the dependency among them. As a\nresult, reducing the number of steps significantly compromises generation\nfidelity. In this work, we introduce ReCAP (Reused Context-Aware Prediction), a\nplug-and-play module that accelerates inference in MGMs by constructing\nlow-cost steps via reusing feature embeddings from previously decoded context\ntokens. ReCAP interleaves standard full evaluations with lightweight steps that\ncache and reuse context features, substantially reducing computation while\npreserving the benefits of fine-grained, iterative generation. We demonstrate\nits effectiveness on top of three representative MGMs (MaskGIT, MAGE, and MAR),\nincluding both discrete and continuous token spaces and covering diverse\narchitectural designs. In particular, on ImageNet256 class-conditional\ngeneration, ReCAP achieves up to 2.4x faster inference than the base model with\nminimal performance drop, and consistently delivers better efficiency-fidelity\ntrade-offs under various generation settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked generative models (MGMs) have emerged as a powerful framework for\nimage synthesis, combining parallel decoding with strong bidirectional context\nmodeling. However, generating high-quality samples typically requires many\niterative decoding steps, resulting in high inference costs. A straightforward\nway to speed up generation is by decoding more tokens in each step, thereby\nreducing the total number of steps. However, when many tokens are decoded\nsimultaneously, the model can only estimate the univariate marginal\ndistributions independently, failing to capture the dependency among them. As a\nresult, reducing the number of steps significantly compromises generation\nfidelity. In this work, we introduce ReCAP (Reused Context-Aware Prediction), a\nplug-and-play module that accelerates inference in MGMs by constructing\nlow-cost steps via reusing feature embeddings from previously decoded context\ntokens. ReCAP interleaves standard full evaluations with lightweight steps that\ncache and reuse context features, substantially reducing computation while\npreserving the benefits of fine-grained, iterative generation. We demonstrate\nits effectiveness on top of three representative MGMs (MaskGIT, MAGE, and MAR),\nincluding both discrete and continuous token spaces and covering diverse\narchitectural designs. In particular, on ImageNet256 class-conditional\ngeneration, ReCAP achieves up to 2.4x faster inference than the base model with\nminimal performance drop, and consistently delivers better efficiency-fidelity\ntrade-offs under various generation settings."
                },
                "authors": [
                    {
                        "name": "Xuejie Liu"
                    },
                    {
                        "name": "Anji Liu"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    },
                    {
                        "name": "Yitao Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yitao Liang"
                },
                "author": "Yitao Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00776v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00776v4",
                "updated": "2025-05-25T05:26:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    25,
                    5,
                    26,
                    2,
                    6,
                    145,
                    0
                ],
                "published": "2024-12-01T11:43:46Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    11,
                    43,
                    46,
                    6,
                    336,
                    0
                ],
                "title": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning"
                },
                "summary": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation."
                },
                "authors": [
                    {
                        "name": "Chongyang Zhao"
                    },
                    {
                        "name": "Dong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Gong"
                },
                "author": "Dong Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00776v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00776v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18809v1",
                "updated": "2025-05-24T17:46:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    46,
                    47,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T17:46:47Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    46,
                    47,
                    5,
                    144,
                    0
                ],
                "title": "VORTA: Efficient Video Diffusion via Routing Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VORTA: Efficient Video Diffusion via Routing Sparse Attention"
                },
                "summary": "Video Diffusion Transformers (VDiTs) have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nattention acceleration methods leverage the sparsity of attention patterns to\nimprove efficiency; however, they often overlook inefficiencies of redundant\nlong-range interactions. To address this problem, we propose \\textbf{VORTA}, an\nacceleration framework with two novel components: 1) a sparse attention\nmechanism that efficiently captures long-range dependencies, and 2) a routing\nstrategy that adaptively replaces full 3D attention with specialized sparse\nattention variants throughout the sampling process. It achieves a $1.76\\times$\nend-to-end speedup without quality loss on VBench. Furthermore, VORTA can\nseamlessly integrate with various other acceleration methods, such as caching\nand step distillation, reaching up to $14.41\\times$ speedup with negligible\nperformance degradation. VORTA demonstrates its efficiency and enhances the\npracticality of VDiTs in real-world settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Diffusion Transformers (VDiTs) have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nattention acceleration methods leverage the sparsity of attention patterns to\nimprove efficiency; however, they often overlook inefficiencies of redundant\nlong-range interactions. To address this problem, we propose \\textbf{VORTA}, an\nacceleration framework with two novel components: 1) a sparse attention\nmechanism that efficiently captures long-range dependencies, and 2) a routing\nstrategy that adaptively replaces full 3D attention with specialized sparse\nattention variants throughout the sampling process. It achieves a $1.76\\times$\nend-to-end speedup without quality loss on VBench. Furthermore, VORTA can\nseamlessly integrate with various other acceleration methods, such as caching\nand step distillation, reaching up to $14.41\\times$ speedup with negligible\nperformance degradation. VORTA demonstrates its efficiency and enhances the\npracticality of VDiTs in real-world settings."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "19 pages, 15 figures. The code is available at\n  https://github.com/wenhao728/VORTA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v3",
                "updated": "2025-05-24T17:39:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    39,
                    32,
                    5,
                    144,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "18 pages, 14 figures. Accepted by ICML 2025. The code is available at\n  https://github.com/wenhao728/AsymRnR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16366v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16366v2",
                "updated": "2025-05-24T17:04:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    4,
                    26,
                    5,
                    144,
                    0
                ],
                "published": "2024-04-25T07:09:05Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    7,
                    9,
                    5,
                    3,
                    116,
                    0
                ],
                "title": "Guarding Graph Neural Networks for Unsupervised Graph Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guarding Graph Neural Networks for Unsupervised Graph Anomaly Detection"
                },
                "summary": "Unsupervised graph anomaly detection aims at identifying rare patterns that\ndeviate from the majority in a graph without the aid of labels, which is\nimportant for a variety of real-world applications. Recent advances have\nutilized Graph Neural Networks (GNNs) to learn effective node representations\nby aggregating information from neighborhoods. This is motivated by the\nhypothesis that nodes in the graph tend to exhibit consistent behaviors with\ntheir neighborhoods. However, such consistency can be disrupted by graph\nanomalies in multiple ways. Most existing methods directly employ GNNs to learn\nrepresentations, disregarding the negative impact of graph anomalies on GNNs,\nresulting in sub-optimal node representations and anomaly detection\nperformance. While a few recent approaches have redesigned GNNs for graph\nanomaly detection under semi-supervised label guidance, how to address the\nadverse effects of graph anomalies on GNNs in unsupervised scenarios and learn\neffective representations for anomaly detection are still under-explored. To\nbridge this gap, in this paper, we propose a simple yet effective framework for\nGuarding Graph Neural Networks for Unsupervised Graph Anomaly Detection (G3AD).\nSpecifically, G3AD first introduces two auxiliary networks along with\ncorrelation constraints to guard the GNNs against inconsistent information\nencoding. Furthermore, G3AD introduces an adaptive caching module to guard the\nGNNs from directly reconstructing the observed graph data that contains\nanomalies. Extensive experiments demonstrate that our G3AD can outperform\ntwenty state-of-the-art methods on both synthetic and real-world graph anomaly\ndatasets, with flexible generalization ability in different GNN backbones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised graph anomaly detection aims at identifying rare patterns that\ndeviate from the majority in a graph without the aid of labels, which is\nimportant for a variety of real-world applications. Recent advances have\nutilized Graph Neural Networks (GNNs) to learn effective node representations\nby aggregating information from neighborhoods. This is motivated by the\nhypothesis that nodes in the graph tend to exhibit consistent behaviors with\ntheir neighborhoods. However, such consistency can be disrupted by graph\nanomalies in multiple ways. Most existing methods directly employ GNNs to learn\nrepresentations, disregarding the negative impact of graph anomalies on GNNs,\nresulting in sub-optimal node representations and anomaly detection\nperformance. While a few recent approaches have redesigned GNNs for graph\nanomaly detection under semi-supervised label guidance, how to address the\nadverse effects of graph anomalies on GNNs in unsupervised scenarios and learn\neffective representations for anomaly detection are still under-explored. To\nbridge this gap, in this paper, we propose a simple yet effective framework for\nGuarding Graph Neural Networks for Unsupervised Graph Anomaly Detection (G3AD).\nSpecifically, G3AD first introduces two auxiliary networks along with\ncorrelation constraints to guard the GNNs against inconsistent information\nencoding. Furthermore, G3AD introduces an adaptive caching module to guard the\nGNNs from directly reconstructing the observed graph data that contains\nanomalies. Extensive experiments demonstrate that our G3AD can outperform\ntwenty state-of-the-art methods on both synthetic and real-world graph anomaly\ndatasets, with flexible generalization ability in different GNN backbones."
                },
                "authors": [
                    {
                        "name": "Yuanchen Bei"
                    },
                    {
                        "name": "Sheng Zhou"
                    },
                    {
                        "name": "Jinke Shi"
                    },
                    {
                        "name": "Yao Ma"
                    },
                    {
                        "name": "Haishuai Wang"
                    },
                    {
                        "name": "Jiajun Bu"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Bu"
                },
                "author": "Jiajun Bu",
                "arxiv_comment": "Accepted by IEEE TNNLS (14 pages, 10 figures)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16366v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16366v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20334v1",
                "updated": "2025-05-24T10:34:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    10,
                    34,
                    38,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T10:34:38Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    10,
                    34,
                    38,
                    5,
                    144,
                    0
                ],
                "title": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via\n  Pseudo Query",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via\n  Pseudo Query"
                },
                "summary": "Large language models (LLMs) rely on key-value cache (KV cache) to accelerate\ndecoding by reducing redundant computations. However, the KV cache memory usage\ngrows substantially with longer text sequences, posing challenges for efficient\ndeployment. Existing KV cache eviction methods prune tokens using\nprefilling-stage attention scores, causing inconsistency with actual inference\nqueries, especially under tight memory budgets. In this paper, we propose\nLookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost\npseudo lookahead queries to better approximate the true decoding-stage queries.\nBy using these lookahead queries as the observation window for importance\nestimation, LAQ achieves more consistent and accurate KV cache eviction aligned\nwith real inference scenarios. Experimental results on LongBench and\nNeedle-in-a-Haystack benchmarks show that LAQ outperforms existing methods\nacross various budget levels, achieving a 1 $\\sim$ 4 point improvement on\nLongBench under limited cache budget. Moreover, LAQ is complementary to\nexisting approaches and can be flexibly combined to yield further improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value cache (KV cache) to accelerate\ndecoding by reducing redundant computations. However, the KV cache memory usage\ngrows substantially with longer text sequences, posing challenges for efficient\ndeployment. Existing KV cache eviction methods prune tokens using\nprefilling-stage attention scores, causing inconsistency with actual inference\nqueries, especially under tight memory budgets. In this paper, we propose\nLookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost\npseudo lookahead queries to better approximate the true decoding-stage queries.\nBy using these lookahead queries as the observation window for importance\nestimation, LAQ achieves more consistent and accurate KV cache eviction aligned\nwith real inference scenarios. Experimental results on LongBench and\nNeedle-in-a-Haystack benchmarks show that LAQ outperforms existing methods\nacross various budget levels, achieving a 1 $\\sim$ 4 point improvement on\nLongBench under limited cache budget. Moreover, LAQ is complementary to\nexisting approaches and can be flexibly combined to yield further improvements."
                },
                "authors": [
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shiyu Ji"
                    },
                    {
                        "name": "Yijun Liu"
                    },
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05130v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05130v2",
                "updated": "2025-05-24T09:33:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    9,
                    33,
                    35,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-08T11:07:35Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    7,
                    35,
                    3,
                    128,
                    0
                ],
                "title": "CacheFL: Privacy-Preserving and Efficient Federated Cache Model\n  Fine-Tuning for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFL: Privacy-Preserving and Efficient Federated Cache Model\n  Fine-Tuning for Vision-Language Models"
                },
                "summary": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation."
                },
                "authors": [
                    {
                        "name": "Mengjun Yi"
                    },
                    {
                        "name": "Hanwen Zhang"
                    },
                    {
                        "name": "Hui Dou"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "Furao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Furao Shen"
                },
                "author": "Furao Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05130v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05130v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18610v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18610v1",
                "updated": "2025-05-24T09:18:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    9,
                    18,
                    11,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T09:18:11Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    9,
                    18,
                    11,
                    5,
                    144,
                    0
                ],
                "title": "PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT\n  LLMs"
                },
                "summary": "Recently, significant progress has been made in developing reasoning-capable\nLarge Language Models (LLMs) through long Chain-of-Thought (CoT) techniques.\nHowever, this long-CoT reasoning process imposes substantial memory overhead\ndue to the large Key-Value (KV) Cache memory overhead. Post-training KV Cache\nquantization has emerged as a promising compression technique and has been\nextensively studied in short-context scenarios. However, directly applying\nexisting methods to long-CoT LLMs causes significant performance degradation\ndue to the following two reasons: (1) Large cumulative error: Existing methods\nfail to adequately leverage available memory, and they directly quantize the KV\nCache during each decoding step, leading to large cumulative quantization\nerror. (2) Short-context calibration: Due to Rotary Positional Embedding\n(RoPE), the use of short-context data during calibration fails to account for\nthe distribution of less frequent channels in the Key Cache, resulting in\nperformance loss. We propose Progressive Mixed-Precision KV Cache Quantization\n(PM-KVQ) for long-CoT LLMs to address the above issues in two folds: (1) To\nreduce cumulative error, we design a progressive quantization strategy to\ngradually lower the bit-width of KV Cache in each block. Then, we propose\nblock-wise memory allocation to assign a higher bit-width to more sensitive\ntransformer blocks. (2) To increase the calibration length without additional\noverhead, we propose a new calibration strategy with positional interpolation\nthat leverages short calibration data with positional interpolation to\napproximate the data distribution of long-context data. Extensive experiments\non 7B-70B long-CoT LLMs show that PM-KVQ improves reasoning benchmark\nperformance by up to 8% over SOTA baselines under the same memory budget. Our\ncode is available at https://github.com/thu-nics/PM-KVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, significant progress has been made in developing reasoning-capable\nLarge Language Models (LLMs) through long Chain-of-Thought (CoT) techniques.\nHowever, this long-CoT reasoning process imposes substantial memory overhead\ndue to the large Key-Value (KV) Cache memory overhead. Post-training KV Cache\nquantization has emerged as a promising compression technique and has been\nextensively studied in short-context scenarios. However, directly applying\nexisting methods to long-CoT LLMs causes significant performance degradation\ndue to the following two reasons: (1) Large cumulative error: Existing methods\nfail to adequately leverage available memory, and they directly quantize the KV\nCache during each decoding step, leading to large cumulative quantization\nerror. (2) Short-context calibration: Due to Rotary Positional Embedding\n(RoPE), the use of short-context data during calibration fails to account for\nthe distribution of less frequent channels in the Key Cache, resulting in\nperformance loss. We propose Progressive Mixed-Precision KV Cache Quantization\n(PM-KVQ) for long-CoT LLMs to address the above issues in two folds: (1) To\nreduce cumulative error, we design a progressive quantization strategy to\ngradually lower the bit-width of KV Cache in each block. Then, we propose\nblock-wise memory allocation to assign a higher bit-width to more sensitive\ntransformer blocks. (2) To increase the calibration length without additional\noverhead, we propose a new calibration strategy with positional interpolation\nthat leverages short calibration data with positional interpolation to\napproximate the data distribution of long-context data. Extensive experiments\non 7B-70B long-CoT LLMs show that PM-KVQ improves reasoning benchmark\nperformance by up to 8% over SOTA baselines under the same memory budget. Our\ncode is available at https://github.com/thu-nics/PM-KVQ."
                },
                "authors": [
                    {
                        "name": "Tengxuan Liu"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Jiayi Yang"
                    },
                    {
                        "name": "Tianchen Zhao"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18610v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18577v1",
                "updated": "2025-05-24T07:57:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    7,
                    57,
                    57,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T07:57:57Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    7,
                    57,
                    57,
                    5,
                    144,
                    0
                ],
                "title": "CXL Topology-Aware and Expander-Driven Prefetching: Unlocking SSD\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL Topology-Aware and Expander-Driven Prefetching: Unlocking SSD\n  Performance"
                },
                "summary": "Integrating compute express link (CXL) with SSDs allows scalable access to\nlarge memory but has slower speeds than DRAMs. We present ExPAND, an\nexpander-driven CXL prefetcher that offloads last-level cache (LLC) prefetching\nfrom host CPU to CXL-SSDs. ExPAND uses a heterogeneous prediction algorithm for\nprefetching and ensures data consistency with CXL.mem's back-invalidation. We\nexamine prefetch timeliness for accurate latency estimation. ExPAND, being\naware of CXL multi-tiered switching, provides end-to-end latency for each\nCXL-SSD and precise prefetch timeliness estimations. Our method reduces CXL-SSD\nreliance and enables direct host cache access for most data. ExPAND enhances\ngraph application performance and SPEC CPU's performance by 9.0$\\times$ and\n14.7$\\times$, respectively, surpassing CXL-SSD pools with diverse prefetching\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating compute express link (CXL) with SSDs allows scalable access to\nlarge memory but has slower speeds than DRAMs. We present ExPAND, an\nexpander-driven CXL prefetcher that offloads last-level cache (LLC) prefetching\nfrom host CPU to CXL-SSDs. ExPAND uses a heterogeneous prediction algorithm for\nprefetching and ensures data consistency with CXL.mem's back-invalidation. We\nexamine prefetch timeliness for accurate latency estimation. ExPAND, being\naware of CXL multi-tiered switching, provides end-to-end latency for each\nCXL-SSD and precise prefetch timeliness estimations. Our method reduces CXL-SSD\nreliance and enables direct host cache access for most data. ExPAND enhances\ngraph application performance and SPEC CPU's performance by 9.0$\\times$ and\n14.7$\\times$, respectively, surpassing CXL-SSD pools with diverse prefetching\nstrategies."
                },
                "authors": [
                    {
                        "name": "Dongsuk Oh"
                    },
                    {
                        "name": "Miryeong Kwon"
                    },
                    {
                        "name": "Jiseon Kim"
                    },
                    {
                        "name": "Eunjee Na"
                    },
                    {
                        "name": "Junseok Moon"
                    },
                    {
                        "name": "Hyunkyu Choi"
                    },
                    {
                        "name": "Seonghyeon Jang"
                    },
                    {
                        "name": "Hanjin Choi"
                    },
                    {
                        "name": "Hongjoo Jung"
                    },
                    {
                        "name": "Sangwon Lee"
                    },
                    {
                        "name": "Myoungsoo Jung"
                    }
                ],
                "author_detail": {
                    "name": "Myoungsoo Jung"
                },
                "author": "Myoungsoo Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18554v1",
                "updated": "2025-05-24T06:45:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    6,
                    45,
                    16,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T06:45:16Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    6,
                    45,
                    16,
                    5,
                    144,
                    0
                ],
                "title": "Garibaldi: A Pairwise Instruction-Data Management for Enhancing Shared\n  Last-Level Cache Performance in Server Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Garibaldi: A Pairwise Instruction-Data Management for Enhancing Shared\n  Last-Level Cache Performance in Server Workloads"
                },
                "summary": "Modern CPUs suffer from the frontend bottleneck because the instruction\nfootprint of server workloads exceeds the private cache capacity. Prior works\nhave examined the CPU components or private cache to improve the instruction\nhit rate. The large footprint leads to significant cache misses not only in the\ncore and faster-level cache but also in the last-level cache (LLC). We observe\nthat even with an advanced branch predictor and instruction prefetching\ntechniques, a considerable amount of instruction accesses descend to the LLC.\nHowever, state-of-the-art LLC designs with elaborate data management overlook\nhandling the instruction misses that precede corresponding data accesses.\nSpecifically, when an instruction requiring numerous data accesses is missed,\nthe frontend of a CPU should wait for the instruction fetch, regardless of how\nmuch data are present in the LLC.\n  To preserve hot instructions in the LLC, we propose Garibaldi, a novel\npairwise instruction-data management scheme. Garibaldi tracks the hotness of\ninstruction accesses by coupling it with that of data accesses and adopts\nmanagement techniques. On the one hand, this scheme includes a selective\nprotection mechanism that prevents the cache evictions of high-cost instruction\ncachelines. On the other hand, in the case of unprotected instruction line\nmisses, Garibaldi conservatively issues prefetch requests of the paired data\nlines while handling those misses. In our experiments, we evaluate Garibaldi\nwith 16 server workloads on a 40-core machine. We also implement Garibaldi on\ntop of a modern LLC design, including Mockingjay. Garibaldi improves 13.2% and\n6.1% of CPU performance on baseline LLC design and Mockingjay, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern CPUs suffer from the frontend bottleneck because the instruction\nfootprint of server workloads exceeds the private cache capacity. Prior works\nhave examined the CPU components or private cache to improve the instruction\nhit rate. The large footprint leads to significant cache misses not only in the\ncore and faster-level cache but also in the last-level cache (LLC). We observe\nthat even with an advanced branch predictor and instruction prefetching\ntechniques, a considerable amount of instruction accesses descend to the LLC.\nHowever, state-of-the-art LLC designs with elaborate data management overlook\nhandling the instruction misses that precede corresponding data accesses.\nSpecifically, when an instruction requiring numerous data accesses is missed,\nthe frontend of a CPU should wait for the instruction fetch, regardless of how\nmuch data are present in the LLC.\n  To preserve hot instructions in the LLC, we propose Garibaldi, a novel\npairwise instruction-data management scheme. Garibaldi tracks the hotness of\ninstruction accesses by coupling it with that of data accesses and adopts\nmanagement techniques. On the one hand, this scheme includes a selective\nprotection mechanism that prevents the cache evictions of high-cost instruction\ncachelines. On the other hand, in the case of unprotected instruction line\nmisses, Garibaldi conservatively issues prefetch requests of the paired data\nlines while handling those misses. In our experiments, we evaluate Garibaldi\nwith 16 server workloads on a 40-core machine. We also implement Garibaldi on\ntop of a modern LLC design, including Mockingjay. Garibaldi improves 13.2% and\n6.1% of CPU performance on baseline LLC design and Mockingjay, respectively."
                },
                "authors": [
                    {
                        "name": "Jaewon Kwon"
                    },
                    {
                        "name": "Yongju Lee"
                    },
                    {
                        "name": "Jiwan Kim"
                    },
                    {
                        "name": "Enhyeok Jang"
                    },
                    {
                        "name": "Hongju Kal"
                    },
                    {
                        "name": "Won Woo Ro"
                    }
                ],
                "author_detail": {
                    "name": "Won Woo Ro"
                },
                "arxiv_affiliation": "Yonsei University, Seoul, Republic of Korea",
                "author": "Won Woo Ro",
                "arxiv_comment": "Accepted to ISCA '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02398v2",
                "updated": "2025-05-24T04:37:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    4,
                    37,
                    34,
                    5,
                    144,
                    0
                ],
                "published": "2025-03-04T08:41:40Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "title": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence"
                },
                "summary": "User profile embedded in the prompt template of personalized recommendation\nagents play a crucial role in shaping their decision-making process.\nHigh-quality user profiles are essential for aligning agent behavior with real\nuser interests. Typically, these profiles are constructed by leveraging LLMs\nfor user profile modeling (LLM-UM). However, this process faces several\nchallenges: (1) LLMs struggle with long user behaviors due to context length\nlimitations and performance degradation. (2) Existing methods often extract\nonly partial segments from full historical behavior sequence, inevitably\ndiscarding diverse user interests embedded in the omitted content, leading to\nincomplete modeling and suboptimal profiling. (3) User profiling is often\ntightly coupled with the inference context, requiring online processing, which\nintroduces significant latency overhead. In this paper, we propose PersonaX, an\nagent-agnostic LLM-UM framework to address these challenges. It augments\ndownstream recommendation agents to achieve better recommendation performance\nand inference efficiency. PersonaX (a) segments complete historical behaviors\ninto clustered groups, (b) selects multiple sub behavior sequences (SBS) with a\nbalance of prototypicality and diversity to form a high quality core set, (c)\nperforms offline multi-persona profiling to capture diverse user interests and\ngenerate fine grained, cached textual personas, and (d) decouples user\nprofiling from online inference, enabling profile retrieval instead of real\ntime generation. Extensive experiments demonstrate its effectiveness: using\nonly 30 to 50% of behavioral data (sequence length 480), PersonaX enhances\nAgentCF by 3 to 11% and Agent4Rec by 10 to 50%. As a scalable and\nmodel-agnostic LLM-UM solution, PersonaX sets a new benchmark in scalable user\nmodeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User profile embedded in the prompt template of personalized recommendation\nagents play a crucial role in shaping their decision-making process.\nHigh-quality user profiles are essential for aligning agent behavior with real\nuser interests. Typically, these profiles are constructed by leveraging LLMs\nfor user profile modeling (LLM-UM). However, this process faces several\nchallenges: (1) LLMs struggle with long user behaviors due to context length\nlimitations and performance degradation. (2) Existing methods often extract\nonly partial segments from full historical behavior sequence, inevitably\ndiscarding diverse user interests embedded in the omitted content, leading to\nincomplete modeling and suboptimal profiling. (3) User profiling is often\ntightly coupled with the inference context, requiring online processing, which\nintroduces significant latency overhead. In this paper, we propose PersonaX, an\nagent-agnostic LLM-UM framework to address these challenges. It augments\ndownstream recommendation agents to achieve better recommendation performance\nand inference efficiency. PersonaX (a) segments complete historical behaviors\ninto clustered groups, (b) selects multiple sub behavior sequences (SBS) with a\nbalance of prototypicality and diversity to form a high quality core set, (c)\nperforms offline multi-persona profiling to capture diverse user interests and\ngenerate fine grained, cached textual personas, and (d) decouples user\nprofiling from online inference, enabling profile retrieval instead of real\ntime generation. Extensive experiments demonstrate its effectiveness: using\nonly 30 to 50% of behavioral data (sequence length 480), PersonaX enhances\nAgentCF by 3 to 11% and Agent4Rec by 10 to 50%. As a scalable and\nmodel-agnostic LLM-UM solution, PersonaX sets a new benchmark in scalable user\nmodeling."
                },
                "authors": [
                    {
                        "name": "Yunxiao Shi"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Zeqi Zhang"
                    },
                    {
                        "name": "Xing Zi"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Min Xu"
                    }
                ],
                "author_detail": {
                    "name": "Min Xu"
                },
                "author": "Min Xu",
                "arxiv_comment": "2025 ACL Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18300v1",
                "updated": "2025-05-23T18:46:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    46,
                    10,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T18:46:10Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    46,
                    10,
                    4,
                    143,
                    0
                ],
                "title": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs"
                },
                "summary": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs."
                },
                "authors": [
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yi-Ting Ma"
                    },
                    {
                        "name": "Do Young Eun"
                    }
                ],
                "author_detail": {
                    "name": "Do Young Eun"
                },
                "author": "Do Young Eun",
                "arxiv_comment": "Accepted at ICML 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20325v1",
                "updated": "2025-05-23T18:19:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    19,
                    9,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T18:19:09Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    19,
                    9,
                    4,
                    143,
                    0
                ],
                "title": "Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic\n  Confidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic\n  Confidence"
                },
                "summary": "Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM)\nreasoning often incur substantial computational costs, primarily due to\nextensive reliance on external Process Reward Models (PRMs) or sampling methods\nlike Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient\nself-guided TTS framework that achieves PRM-level performance without costly\nexternal verifier models. Our method employs a lightweight tree search guided\nsolely by intrinsic LLM signals, token-level confidence and step novelty. One\ncritical innovation is improving the reliability of internal confidence\nestimates via a targeted reinforcement learning fine-tuning phase. Empirical\nevaluations on challenging mathematical reasoning benchmarks demonstrate that\nGG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching\nor surpassing significantly larger models (e.g., 32B-70B parameters), while\nreducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG\nachieves comparable accuracy with 8x faster inference speeds and 4-5x lower\nmemory usage. Additionally, GG reduces KV cache memory usage by approximately\n50% compared to the BoN strategy, facilitating more efficient and practical\ndeployment of TTS techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM)\nreasoning often incur substantial computational costs, primarily due to\nextensive reliance on external Process Reward Models (PRMs) or sampling methods\nlike Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient\nself-guided TTS framework that achieves PRM-level performance without costly\nexternal verifier models. Our method employs a lightweight tree search guided\nsolely by intrinsic LLM signals, token-level confidence and step novelty. One\ncritical innovation is improving the reliability of internal confidence\nestimates via a targeted reinforcement learning fine-tuning phase. Empirical\nevaluations on challenging mathematical reasoning benchmarks demonstrate that\nGG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching\nor surpassing significantly larger models (e.g., 32B-70B parameters), while\nreducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG\nachieves comparable accuracy with 8x faster inference speeds and 4-5x lower\nmemory usage. Additionally, GG reduces KV cache memory usage by approximately\n50% compared to the BoN strategy, facilitating more efficient and practical\ndeployment of TTS techniques."
                },
                "authors": [
                    {
                        "name": "Amirhosein Ghasemabadi"
                    },
                    {
                        "name": "Keith G. Mills"
                    },
                    {
                        "name": "Baochun Li"
                    },
                    {
                        "name": "Di Niu"
                    }
                ],
                "author_detail": {
                    "name": "Di Niu"
                },
                "author": "Di Niu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v3",
                "updated": "2025-05-23T17:02:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    17,
                    2,
                    5,
                    4,
                    143,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence \\emph{after} the aLoRA is invoked. This change crucially\nallows aLoRA to accept the base model's KV cache of the input string, meaning\nthat aLoRA can be instantly activated whenever needed in a chain without\nrecomputing the cache. This enables building what we call \\emph{intrinsics},\ni.e. specialized models invoked to perform well-defined operations on portions\nof an input chain or conversation that otherwise uses the base model by\ndefault. We train a set of aLoRA-based intrinsics models, demonstrating\ncompetitive accuracy with standard LoRA while achieving significant inference\nbenefits. We include a codebase implementing aLoRA in the supplementary\nmaterial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence \\emph{after} the aLoRA is invoked. This change crucially\nallows aLoRA to accept the base model's KV cache of the input string, meaning\nthat aLoRA can be instantly activated whenever needed in a chain without\nrecomputing the cache. This enables building what we call \\emph{intrinsics},\ni.e. specialized models invoked to perform well-defined operations on portions\nof an input chain or conversation that otherwise uses the base model by\ndefault. We train a set of aLoRA-based intrinsics models, demonstrating\ncompetitive accuracy with standard LoRA while achieving significant inference\nbenefits. We include a codebase implementing aLoRA in the supplementary\nmaterial."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06261v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06261v3",
                "updated": "2025-05-23T16:36:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    16,
                    36,
                    12,
                    4,
                    143,
                    0
                ],
                "published": "2025-04-08T17:59:41Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    41,
                    1,
                    98,
                    0
                ],
                "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention"
                },
                "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the LLM instances to come up with their own collaboration\nstrategy for the problem at hand, all the while \"seeing\" each other's memory in\nthe concurrent KV cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\nmemory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE)\nto avoid recomputation while improving parallel hardware utilization. We find\nthat modern reasoning-capable LLMs can perform inference with shared Key-Value\ncache out of the box, without additional fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the LLM instances to come up with their own collaboration\nstrategy for the problem at hand, all the while \"seeing\" each other's memory in\nthe concurrent KV cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\nmemory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE)\nto avoid recomputation while improving parallel hardware utilization. We find\nthat modern reasoning-capable LLMs can perform inference with shared Key-Value\ncache out of the box, without additional fine-tuning."
                },
                "authors": [
                    {
                        "name": "Gleb Rodionov"
                    },
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "George Yakushev"
                    },
                    {
                        "name": "Erik Schultheis"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Anton Sinitsin"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06261v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06261v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18013v1",
                "updated": "2025-05-23T15:15:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    15,
                    15,
                    21,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T15:15:21Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    15,
                    15,
                    21,
                    4,
                    143,
                    0
                ],
                "title": "DiFache: Efficient and Scalable Caching on Disaggregated Memory using\n  Decentralized Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiFache: Efficient and Scalable Caching on Disaggregated Memory using\n  Decentralized Coherence"
                },
                "summary": "The disaggregated memory (DM) architecture offers high resource elasticity at\nthe cost of data access performance. While caching frequently accessed data in\ncompute nodes (CNs) reduces access overhead, it requires costly centralized\nmaintenance of cache coherence across CNs. This paper presents DiFache, an\nefficient, scalable, and coherent CN-side caching framework for DM\napplications. Observing that DM applications already serialize conflicting\nremote data access internally rather than relying on the cache layer, DiFache\nintroduces decentralized coherence that aligns its consistency model with\nmemory nodes instead of CPU caches, thereby eliminating the need for\ncentralized management. DiFache features a decentralized invalidation mechanism\nto independently invalidate caches on remote CNs and a fine-grained adaptive\nscheme to cache objects with varying read-write ratios. Evaluations using 54\nreal-world traces from Twitter show that DiFache outperforms existing\napproaches by up to 10.83$\\times$ (5.53$\\times$ on average). By integrating\nDiFache, the peak throughput of two real-world DM applications increases by\n7.94$\\times$ and 2.19$\\times$, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The disaggregated memory (DM) architecture offers high resource elasticity at\nthe cost of data access performance. While caching frequently accessed data in\ncompute nodes (CNs) reduces access overhead, it requires costly centralized\nmaintenance of cache coherence across CNs. This paper presents DiFache, an\nefficient, scalable, and coherent CN-side caching framework for DM\napplications. Observing that DM applications already serialize conflicting\nremote data access internally rather than relying on the cache layer, DiFache\nintroduces decentralized coherence that aligns its consistency model with\nmemory nodes instead of CPU caches, thereby eliminating the need for\ncentralized management. DiFache features a decentralized invalidation mechanism\nto independently invalidate caches on remote CNs and a fine-grained adaptive\nscheme to cache objects with varying read-write ratios. Evaluations using 54\nreal-world traces from Twitter show that DiFache outperforms existing\napproaches by up to 10.83$\\times$ (5.53$\\times$ on average). By integrating\nDiFache, the peak throughput of two real-world DM applications increases by\n7.94$\\times$ and 2.19$\\times$, respectively."
                },
                "authors": [
                    {
                        "name": "Hanze Zhang"
                    },
                    {
                        "name": "Kaiming Wang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17934v1",
                "updated": "2025-05-23T14:12:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    14,
                    12,
                    5,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T14:12:05Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    14,
                    12,
                    5,
                    4,
                    143,
                    0
                ],
                "title": "Evaluating the impact of the L3 cache size of AMD EPYC CPUs on the\n  performance of CFD applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the impact of the L3 cache size of AMD EPYC CPUs on the\n  performance of CFD applications"
                },
                "summary": "In this work, the authors focus on assessing the impact of the AMD EPYC\nprocessor architecture on the performance of CFD applications. Several\ngenerations of architectures were analyzed, such as Rome, Milan, Milan X,\nGenoa, Genoa X and Bergamo, characterized by a different number of cores\n(64-128), L3 cache size (256 - 1152 MB) and RAM type (8-channel DDR4 or\n12-channel DDR5). The research was conducted based on the OpenFOAM application\nusing two memory-bound models: motorBike and Urban Air Pollution. In order to\ncompare the performance of applications on different architectures, the FVOPS\n(Finite VOlumes solved Per Second) metric was introduced, which allows a direct\ncomparison of the performance on the different architectures. It was noticed\nthat local maximum performance occurs in the grid sizes assigned to the\nprocessing process, which is related to individual processor attributes.\nAdditionally, the behavior of the models was analyzed in detail using the\nsoftware profiling analysis tool AMD uProf to reveal the applications'\ninteraction with the hardware. It enabled fine-tuned monitoring of the CPU's\nbehaviours and identified potential inefficiencies in AMD EPYC CPUs. Particular\nattention was paid to the effective use of L2 and L3 cache memory in the\ncontext of their capacity and the bandwidth of memory channels, which are a key\nfactor in memory-bound applications. Processor features were analyzed from a\ncross-platform perspective, which allowed for the determination of metrics of\nparticular importance in terms of their impact on the performance achieved by\nCFD applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, the authors focus on assessing the impact of the AMD EPYC\nprocessor architecture on the performance of CFD applications. Several\ngenerations of architectures were analyzed, such as Rome, Milan, Milan X,\nGenoa, Genoa X and Bergamo, characterized by a different number of cores\n(64-128), L3 cache size (256 - 1152 MB) and RAM type (8-channel DDR4 or\n12-channel DDR5). The research was conducted based on the OpenFOAM application\nusing two memory-bound models: motorBike and Urban Air Pollution. In order to\ncompare the performance of applications on different architectures, the FVOPS\n(Finite VOlumes solved Per Second) metric was introduced, which allows a direct\ncomparison of the performance on the different architectures. It was noticed\nthat local maximum performance occurs in the grid sizes assigned to the\nprocessing process, which is related to individual processor attributes.\nAdditionally, the behavior of the models was analyzed in detail using the\nsoftware profiling analysis tool AMD uProf to reveal the applications'\ninteraction with the hardware. It enabled fine-tuned monitoring of the CPU's\nbehaviours and identified potential inefficiencies in AMD EPYC CPUs. Particular\nattention was paid to the effective use of L2 and L3 cache memory in the\ncontext of their capacity and the bandwidth of memory channels, which are a key\nfactor in memory-bound applications. Processor features were analyzed from a\ncross-platform perspective, which allowed for the determination of metrics of\nparticular importance in terms of their impact on the performance achieved by\nCFD applications."
                },
                "authors": [
                    {
                        "name": "Marcin Lawenda"
                    },
                    {
                        "name": "Łukasz Szustak"
                    },
                    {
                        "name": "László Környei"
                    },
                    {
                        "name": "Flavio Cesar Cunha Galeazzo"
                    },
                    {
                        "name": "Paweł Bratek"
                    }
                ],
                "author_detail": {
                    "name": "Paweł Bratek"
                },
                "author": "Paweł Bratek",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18231v1",
                "updated": "2025-05-23T12:40:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    12,
                    40,
                    7,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T12:40:07Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    12,
                    40,
                    7,
                    4,
                    143,
                    0
                ],
                "title": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit\n  Vector Quantization of KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit\n  Vector Quantization of KV Cache"
                },
                "summary": "Large Language Model (LLM) inference is typically memory-intensive,\nespecially when processing large batch sizes and long sequences, due to the\nlarge size of key-value (KV) cache. Vector Quantization (VQ) is recently\nadopted to alleviate this issue, but we find that the existing approach is\nsusceptible to distribution shift due to its reliance on calibration datasets.\nTo address this limitation, we introduce NSNQuant, a calibration-free Vector\nQuantization (VQ) technique designed for low-bit compression of the KV cache.\nBy applying a three-step transformation-1) a token-wise normalization\n(Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise\nnormalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns\nthe token distribution with the standard normal distribution. This alignment\nenables robust, calibration-free vector quantization using a single reusable\ncodebook. Extensive experiments show that NSNQuant consistently outperforms\nprior methods in both 1-bit and 2-bit settings, offering strong generalization\nand up to 3$\\times$ throughput gain over full-precision baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference is typically memory-intensive,\nespecially when processing large batch sizes and long sequences, due to the\nlarge size of key-value (KV) cache. Vector Quantization (VQ) is recently\nadopted to alleviate this issue, but we find that the existing approach is\nsusceptible to distribution shift due to its reliance on calibration datasets.\nTo address this limitation, we introduce NSNQuant, a calibration-free Vector\nQuantization (VQ) technique designed for low-bit compression of the KV cache.\nBy applying a three-step transformation-1) a token-wise normalization\n(Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise\nnormalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns\nthe token distribution with the standard normal distribution. This alignment\nenables robust, calibration-free vector quantization using a single reusable\ncodebook. Extensive experiments show that NSNQuant consistently outperforms\nprior methods in both 1-bit and 2-bit settings, offering strong generalization\nand up to 3$\\times$ throughput gain over full-precision baselines."
                },
                "authors": [
                    {
                        "name": "Donghyun Son"
                    },
                    {
                        "name": "Euntae Choi"
                    },
                    {
                        "name": "Sungjoo Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Sungjoo Yoo"
                },
                "author": "Sungjoo Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17787v1",
                "updated": "2025-05-23T12:00:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    12,
                    0,
                    9,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T12:00:09Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    12,
                    0,
                    9,
                    4,
                    143,
                    0
                ],
                "title": "Titanus: Enabling KV Cache Pruning and Quantization On-the-Fly for LLM\n  Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Titanus: Enabling KV Cache Pruning and Quantization On-the-Fly for LLM\n  Acceleration"
                },
                "summary": "Large language models (LLMs) have gained great success in various domains.\nExisting systems cache Key and Value within the attention block to avoid\nredundant computations. However, the size of key-value cache (KV cache) is\nunpredictable and can even be tens of times larger than the weights in the long\ncontext length scenario. In this work, we propose Titanus, a software-hardware\nco-design to efficiently compress the KV cache on-the-fly. We first propose the\ncascade pruning-quantization (CPQ) method to reduce the KV cache movement. The\nhierarchical quantization extension strategy is introduced to tackle the\nnon-independent per-channel quantization issue. To further reduce KV cache\nmovement, we transfer only the non-zero KV cache between the accelerator and\noff-chip memory. Moreover, we customize a two-stage design space exploration\nframework for the CPQ method. A novel pipeline and parallelism dataflow is\ndesigned to reduce the first token generation time. Experiments show that\nTitanus achieves 159.9x (49.6x) and 34.8x (29.2x) energy efficiency\n(throughput) compared to Nvidia A100 GPU and FlightLLM respectively. The code\nfor Titanus is available at\nhttps://github.com/peilin-chen/Titanus-for-LLM-acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained great success in various domains.\nExisting systems cache Key and Value within the attention block to avoid\nredundant computations. However, the size of key-value cache (KV cache) is\nunpredictable and can even be tens of times larger than the weights in the long\ncontext length scenario. In this work, we propose Titanus, a software-hardware\nco-design to efficiently compress the KV cache on-the-fly. We first propose the\ncascade pruning-quantization (CPQ) method to reduce the KV cache movement. The\nhierarchical quantization extension strategy is introduced to tackle the\nnon-independent per-channel quantization issue. To further reduce KV cache\nmovement, we transfer only the non-zero KV cache between the accelerator and\noff-chip memory. Moreover, we customize a two-stage design space exploration\nframework for the CPQ method. A novel pipeline and parallelism dataflow is\ndesigned to reduce the first token generation time. Experiments show that\nTitanus achieves 159.9x (49.6x) and 34.8x (29.2x) energy efficiency\n(throughput) compared to Nvidia A100 GPU and FlightLLM respectively. The code\nfor Titanus is available at\nhttps://github.com/peilin-chen/Titanus-for-LLM-acceleration."
                },
                "authors": [
                    {
                        "name": "Peilin Chen"
                    },
                    {
                        "name": "Xiaoxuan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxuan Yang"
                },
                "author": "Xiaoxuan Yang",
                "arxiv_comment": "Accepted to GLSVLSI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15684v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15684v2",
                "updated": "2025-05-23T11:59:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    11,
                    59,
                    22,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-21T15:58:16Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    16,
                    2,
                    141,
                    0
                ],
                "title": "ThinkLess: A Training-Free Inference-Efficient Method for Reducing\n  Reasoning Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinkLess: A Training-Free Inference-Efficient Method for Reducing\n  Reasoning Redundancy"
                },
                "summary": "While Chain-of-Thought (CoT) prompting improves reasoning in large language\nmodels (LLMs), the excessive length of reasoning tokens increases latency and\nKV cache memory usage, and may even truncate final answers under context\nlimits. We propose ThinkLess, an inference-efficient framework that terminates\nreasoning generation early and maintains output quality without modifying the\nmodel. Atttention analysis reveals that answer tokens focus minimally on\nearlier reasoning steps and primarily attend to the reasoning terminator token,\ndue to information migration under causal masking. Building on this insight,\nThinkLess inserts the terminator token at earlier positions to skip redundant\nreasoning while preserving the underlying knowledge transfer. To prevent format\ndiscruption casued by early termination, ThinkLess employs a lightweight\npost-regulation mechanism, relying on the model's natural instruction-following\nability to produce well-structured answers. Without fine-tuning or auxiliary\ndata, ThinkLess achieves comparable accuracy to full-length CoT decoding while\ngreatly reducing decoding time and memory consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Chain-of-Thought (CoT) prompting improves reasoning in large language\nmodels (LLMs), the excessive length of reasoning tokens increases latency and\nKV cache memory usage, and may even truncate final answers under context\nlimits. We propose ThinkLess, an inference-efficient framework that terminates\nreasoning generation early and maintains output quality without modifying the\nmodel. Atttention analysis reveals that answer tokens focus minimally on\nearlier reasoning steps and primarily attend to the reasoning terminator token,\ndue to information migration under causal masking. Building on this insight,\nThinkLess inserts the terminator token at earlier positions to skip redundant\nreasoning while preserving the underlying knowledge transfer. To prevent format\ndiscruption casued by early termination, ThinkLess employs a lightweight\npost-regulation mechanism, relying on the model's natural instruction-following\nability to produce well-structured answers. Without fine-tuning or auxiliary\ndata, ThinkLess achieves comparable accuracy to full-length CoT decoding while\ngreatly reducing decoding time and memory consumption."
                },
                "authors": [
                    {
                        "name": "Gengyang Li"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Yuming Li"
                    },
                    {
                        "name": "Yunfang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yunfang Wu"
                },
                "author": "Yunfang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15684v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15684v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v4",
                "updated": "2025-05-23T10:45:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    10,
                    45,
                    9,
                    4,
                    143,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "ARCANE: Adaptive Routing with Caching and Aware Network Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive Routing with Caching and Aware Network Exploration"
                },
                "summary": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose ARCANE, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. ARCANE adapts to network conditions by caching\ngood-performing paths. In case of a network failure, ARCANE re-routes traffic\naway from it in less than 100 microseconds. ARCANE is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nARCANE in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose ARCANE, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. ARCANE adapts to network conditions by caching\ngood-performing paths. In case of a network failure, ARCANE re-routes traffic\naway from it in less than 100 microseconds. ARCANE is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nARCANE in large-scale simulations and FPGA-based NICs."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17694v1",
                "updated": "2025-05-23T10:03:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    10,
                    3,
                    28,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-23T10:03:28Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    10,
                    3,
                    28,
                    4,
                    143,
                    0
                ],
                "title": "FlashForge: Ultra-Efficient Prefix-Aware Attention for LLM Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashForge: Ultra-Efficient Prefix-Aware Attention for LLM Decoding"
                },
                "summary": "Prefix-sharing among multiple prompts presents opportunities to combine the\noperations of the shared prefix, while attention computation in the decode\nstage, which becomes a critical bottleneck with increasing context lengths, is\na memory-intensive process requiring heavy memory access on the key-value (KV)\ncache of the prefixes. Therefore, in this paper, we explore the potential of\nprefix-sharing in the attention computation of the decode stage. However, the\ntree structure of the prefix-sharing mechanism presents significant challenges\nfor attention computation in efficiently processing shared KV cache access\npatterns while managing complex dependencies and balancing irregular workloads.\nTo address the above challenges, we propose a dedicated attention kernel to\ncombine the memory access of shared prefixes in the decoding stage, namely\nFlashForge. FlashForge delivers two key innovations: a novel shared-prefix\nattention kernel that optimizes memory hierarchy and exploits both intra-block\nand inter-block parallelism, and a comprehensive workload balancing mechanism\nthat efficiently estimates cost, divides tasks, and schedules execution.\nExperimental results show that FlashForge achieves an average 1.9x speedup and\n120.9x memory access reduction compared to the state-of-the-art FlashDecoding\nkernel regarding attention computation in the decode stage and 3.8x end-to-end\ntime per output token compared to the vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefix-sharing among multiple prompts presents opportunities to combine the\noperations of the shared prefix, while attention computation in the decode\nstage, which becomes a critical bottleneck with increasing context lengths, is\na memory-intensive process requiring heavy memory access on the key-value (KV)\ncache of the prefixes. Therefore, in this paper, we explore the potential of\nprefix-sharing in the attention computation of the decode stage. However, the\ntree structure of the prefix-sharing mechanism presents significant challenges\nfor attention computation in efficiently processing shared KV cache access\npatterns while managing complex dependencies and balancing irregular workloads.\nTo address the above challenges, we propose a dedicated attention kernel to\ncombine the memory access of shared prefixes in the decoding stage, namely\nFlashForge. FlashForge delivers two key innovations: a novel shared-prefix\nattention kernel that optimizes memory hierarchy and exploits both intra-block\nand inter-block parallelism, and a comprehensive workload balancing mechanism\nthat efficiently estimates cost, divides tasks, and schedules execution.\nExperimental results show that FlashForge achieves an average 1.9x speedup and\n120.9x memory access reduction compared to the state-of-the-art FlashDecoding\nkernel regarding attention computation in the decode stage and 3.8x end-to-end\ntime per output token compared to the vLLM."
                },
                "authors": [
                    {
                        "name": "Zhibin Wang"
                    },
                    {
                        "name": "Rui Ning"
                    },
                    {
                        "name": "Chao Fang"
                    },
                    {
                        "name": "Zhonghui Zhang"
                    },
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Shaobo Ma"
                    },
                    {
                        "name": "Mo Zhou"
                    },
                    {
                        "name": "Xue Li"
                    },
                    {
                        "name": "Zhongfeng Wang"
                    },
                    {
                        "name": "Chengying Huan"
                    },
                    {
                        "name": "Rong Gu"
                    },
                    {
                        "name": "Kun Yang"
                    },
                    {
                        "name": "Guihai Chen"
                    },
                    {
                        "name": "Sheng Zhong"
                    },
                    {
                        "name": "Chen Tian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Tian"
                },
                "author": "Chen Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02536v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02536v3",
                "updated": "2025-05-23T10:01:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    10,
                    1,
                    57,
                    4,
                    143,
                    0
                ],
                "published": "2024-06-04T17:55:38Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    55,
                    38,
                    1,
                    156,
                    0
                ],
                "title": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension"
                },
                "summary": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Chin-Yew Lin"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "Accepted at Findings of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02536v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02536v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12486v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12486v3",
                "updated": "2025-05-23T09:33:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    9,
                    33,
                    32,
                    4,
                    143,
                    0
                ],
                "published": "2024-12-17T02:43:54Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    43,
                    54,
                    1,
                    352,
                    0
                ],
                "title": "Boosting Long-Context Management via Query-Guided Activation Refilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Long-Context Management via Query-Guided Activation Refilling"
                },
                "summary": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Defu Lian"
                    }
                ],
                "author_detail": {
                    "name": "Defu Lian"
                },
                "author": "Defu Lian",
                "arxiv_comment": "ACL25 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12486v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12486v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11820v2",
                "updated": "2025-05-23T08:12:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    8,
                    12,
                    10,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-17T04:06:12Z",
                "published_parsed": [
                    2025,
                    5,
                    17,
                    4,
                    6,
                    12,
                    5,
                    137,
                    0
                ],
                "title": "Chain-of-Model Learning for Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Model Learning for Language Model"
                },
                "summary": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model\n(CoM), which incorporates the causal relationship into the hidden states of\neach layer as a chain style, thereby introducing great scaling efficiency in\nmodel training and inference flexibility in deployment. We introduce the\nconcept of Chain-of-Representation (CoR), which formulates the hidden states at\neach layer as a combination of multiple sub-representations (i.e., chains) at\nthe hidden dimension level. In each layer, each chain from the output\nrepresentations can only view all of its preceding chains in the input\nrepresentations. Consequently, the model built upon CoM framework can\nprogressively scale up the model size by increasing the chains based on the\nprevious models (i.e., chains), and offer multiple sub-models at varying sizes\nfor elastic inference by using different chain numbers. Based on this\nprinciple, we devise Chain-of-Language-Model (CoLM), which incorporates the\nidea of CoM into each layer of Transformer architecture. Based on CoLM, we\nfurther introduce CoLM-Air by introducing a KV sharing mechanism, that computes\nall keys and values within the first chain and then shares across all chains.\nThis design demonstrates additional extensibility, such as enabling seamless LM\nswitching, prefilling acceleration and so on. Experimental results demonstrate\nour CoLM family can achieve comparable performance to the standard Transformer,\nwhile simultaneously enabling greater flexiblity, such as progressive scaling\nto improve training efficiency and offer multiple varying model sizes for\nelastic inference, paving a a new way toward building language models. Our code\nwill be released in the future at: https://github.com/microsoft/CoLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model\n(CoM), which incorporates the causal relationship into the hidden states of\neach layer as a chain style, thereby introducing great scaling efficiency in\nmodel training and inference flexibility in deployment. We introduce the\nconcept of Chain-of-Representation (CoR), which formulates the hidden states at\neach layer as a combination of multiple sub-representations (i.e., chains) at\nthe hidden dimension level. In each layer, each chain from the output\nrepresentations can only view all of its preceding chains in the input\nrepresentations. Consequently, the model built upon CoM framework can\nprogressively scale up the model size by increasing the chains based on the\nprevious models (i.e., chains), and offer multiple sub-models at varying sizes\nfor elastic inference by using different chain numbers. Based on this\nprinciple, we devise Chain-of-Language-Model (CoLM), which incorporates the\nidea of CoM into each layer of Transformer architecture. Based on CoLM, we\nfurther introduce CoLM-Air by introducing a KV sharing mechanism, that computes\nall keys and values within the first chain and then shares across all chains.\nThis design demonstrates additional extensibility, such as enabling seamless LM\nswitching, prefilling acceleration and so on. Experimental results demonstrate\nour CoLM family can achieve comparable performance to the standard Transformer,\nwhile simultaneously enabling greater flexiblity, such as progressive scaling\nto improve training efficiency and offer multiple varying model sizes for\nelastic inference, paving a a new way toward building language models. Our code\nwill be released in the future at: https://github.com/microsoft/CoLM."
                },
                "authors": [
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Xiaohua Wang"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Cen LU"
                    },
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "Zifan Song"
                    },
                    {
                        "name": "Caihua Shan"
                    },
                    {
                        "name": "Yansen Wang"
                    },
                    {
                        "name": "Kan Ren"
                    },
                    {
                        "name": "Xiaoqing Zheng"
                    },
                    {
                        "name": "Tao Qin"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16839v2",
                "updated": "2025-05-23T07:07:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    7,
                    7,
                    29,
                    4,
                    143,
                    0
                ],
                "published": "2025-05-22T16:07:12Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    7,
                    12,
                    3,
                    142,
                    0
                ],
                "title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding"
                },
                "summary": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version."
                },
                "authors": [
                    {
                        "name": "Shufan Li"
                    },
                    {
                        "name": "Konstantinos Kallidromitis"
                    },
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "Akash Gokul"
                    },
                    {
                        "name": "Yusuke Kato"
                    },
                    {
                        "name": "Kazuki Kozuka"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Zhe Lin"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "25 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15075v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15075v2",
                "updated": "2025-05-23T04:58:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    23,
                    4,
                    58,
                    47,
                    4,
                    143,
                    0
                ],
                "published": "2025-02-20T22:24:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "Quantize What Counts: Bit Allocation Insights Informed by Spectral Gaps\n  in Keys and Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantize What Counts: Bit Allocation Insights Informed by Spectral Gaps\n  in Keys and Values"
                },
                "summary": "Large Language Models (LLMs) have introduced significant advancements to the\ncapabilities of Natural Language Processing (NLP) in recent years. However, as\nthese models continue to scale in size, memory constraints pose substantial\nchallenge. Key and Value cache (KV cache) quantization has been well-documented\nas a promising solution to this limitation. In this work, we provide two novel\ntheorems aimed at enhancing KV quantization methods. Our first theorem, termed\nKey-Value Norm Disparity, states that the key weight matrices by nature carry\nricher information compared to the value weight matrices, as evidenced by\nhigher spectral and Frobenius norms across most of the layers. Our second\ntheorem, Key-Driven Quantization, posits that prioritizing the quantization\nprecision of keys over values induces significant improvements to the overall\nquantization performance. In particular, assigning greater precision to the\nkeys compared to the values achieves a higher degree of precision reduction\nwith minimal impact on model accuracy. We validate these theorems through\ntheory and extensive experiments on several state-of-the-art LLM architectures\nand benchmarks. These findings offer valuable guidelines for improving KV cache\nquantization strategies, facilitating more efficient memory utilization without\ncompromising model performance across diverse NLP tasks. Source code is\navailable at https://github.com/mohsenhariri/spectral-kv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have introduced significant advancements to the\ncapabilities of Natural Language Processing (NLP) in recent years. However, as\nthese models continue to scale in size, memory constraints pose substantial\nchallenge. Key and Value cache (KV cache) quantization has been well-documented\nas a promising solution to this limitation. In this work, we provide two novel\ntheorems aimed at enhancing KV quantization methods. Our first theorem, termed\nKey-Value Norm Disparity, states that the key weight matrices by nature carry\nricher information compared to the value weight matrices, as evidenced by\nhigher spectral and Frobenius norms across most of the layers. Our second\ntheorem, Key-Driven Quantization, posits that prioritizing the quantization\nprecision of keys over values induces significant improvements to the overall\nquantization performance. In particular, assigning greater precision to the\nkeys compared to the values achieves a higher degree of precision reduction\nwith minimal impact on model accuracy. We validate these theorems through\ntheory and extensive experiments on several state-of-the-art LLM architectures\nand benchmarks. These findings offer valuable guidelines for improving KV cache\nquantization strategies, facilitating more efficient memory utilization without\ncompromising model performance across diverse NLP tasks. Source code is\navailable at https://github.com/mohsenhariri/spectral-kv."
                },
                "authors": [
                    {
                        "name": "Mohsen Hariri"
                    },
                    {
                        "name": "Alan Luo"
                    },
                    {
                        "name": "Mohammadreza Nemati"
                    },
                    {
                        "name": "Lam Nguyen"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Vipin Chaudhary"
                },
                "author": "Vipin Chaudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15075v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15075v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17331v1",
                "updated": "2025-05-22T22:54:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    22,
                    54,
                    21,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T22:54:21Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    22,
                    54,
                    21,
                    3,
                    142,
                    0
                ],
                "title": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training"
                },
                "summary": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Maryam Dialameh"
                    },
                    {
                        "name": "Rezaul Karim"
                    },
                    {
                        "name": "Hossein Rajabzadeh"
                    },
                    {
                        "name": "Omar Mohamed Awad"
                    },
                    {
                        "name": "Hyock Ju Kwon"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Walid Ahmed"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17272v1",
                "updated": "2025-05-22T20:39:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    20,
                    39,
                    57,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T20:39:57Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    20,
                    39,
                    57,
                    3,
                    142,
                    0
                ],
                "title": "Zebra-Llama: Towards Extremely Efficient Hybrid Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zebra-Llama: Towards Extremely Efficient Hybrid Models"
                },
                "summary": "With the growing demand for deploying large language models (LLMs) across\ndiverse applications, improving their inference efficiency is crucial for\nsustainable and democratized access. However, retraining LLMs to meet new\nuser-specific requirements is prohibitively expensive and environmentally\nunsustainable. In this work, we propose a practical and scalable alternative:\ncomposing efficient hybrid language models from existing pre-trained models.\nOur approach, Zebra-Llama, introduces a family of 1B, 3B, and 8B hybrid models\nby combining State Space Models (SSMs) and Multi-head Latent Attention (MLA)\nlayers, using a refined initialization and post-training pipeline to\nefficiently transfer knowledge from pre-trained Transformers. Zebra-Llama\nachieves Transformer-level accuracy with near-SSM efficiency using only 7-11B\ntraining tokens (compared to trillions of tokens required for pre-training) and\nan 8B teacher. Moreover, Zebra-Llama dramatically reduces KV cache size -down\nto 3.9%, 2%, and 2.73% of the original for the 1B, 3B, and 8B variants,\nrespectively-while preserving 100%, 100%, and >97% of average zero-shot\nperformance on LM Harness tasks. Compared to models like MambaInLLaMA,\nX-EcoMLA, Minitron, and Llamba, Zebra-Llama consistently delivers competitive\nor superior accuracy while using significantly fewer tokens, smaller teachers,\nand vastly reduced KV cache memory. Notably, Zebra-Llama-8B surpasses\nMinitron-8B in few-shot accuracy by 7% while using 8x fewer training tokens,\nover 12x smaller KV cache, and a smaller teacher (8B vs. 15B). It also achieves\n2.6x-3.8x higher throughput (tokens/s) than MambaInLlama up to a 32k context\nlength. We will release code and model checkpoints upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing demand for deploying large language models (LLMs) across\ndiverse applications, improving their inference efficiency is crucial for\nsustainable and democratized access. However, retraining LLMs to meet new\nuser-specific requirements is prohibitively expensive and environmentally\nunsustainable. In this work, we propose a practical and scalable alternative:\ncomposing efficient hybrid language models from existing pre-trained models.\nOur approach, Zebra-Llama, introduces a family of 1B, 3B, and 8B hybrid models\nby combining State Space Models (SSMs) and Multi-head Latent Attention (MLA)\nlayers, using a refined initialization and post-training pipeline to\nefficiently transfer knowledge from pre-trained Transformers. Zebra-Llama\nachieves Transformer-level accuracy with near-SSM efficiency using only 7-11B\ntraining tokens (compared to trillions of tokens required for pre-training) and\nan 8B teacher. Moreover, Zebra-Llama dramatically reduces KV cache size -down\nto 3.9%, 2%, and 2.73% of the original for the 1B, 3B, and 8B variants,\nrespectively-while preserving 100%, 100%, and >97% of average zero-shot\nperformance on LM Harness tasks. Compared to models like MambaInLLaMA,\nX-EcoMLA, Minitron, and Llamba, Zebra-Llama consistently delivers competitive\nor superior accuracy while using significantly fewer tokens, smaller teachers,\nand vastly reduced KV cache memory. Notably, Zebra-Llama-8B surpasses\nMinitron-8B in few-shot accuracy by 7% while using 8x fewer training tokens,\nover 12x smaller KV cache, and a smaller teacher (8B vs. 15B). It also achieves\n2.6x-3.8x higher throughput (tokens/s) than MambaInLlama up to a 32k context\nlength. We will release code and model checkpoints upon acceptance."
                },
                "authors": [
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01002v2",
                "updated": "2025-05-22T20:10:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    20,
                    10,
                    16,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-02T04:57:06Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "title": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber"
                },
                "summary": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses."
                },
                "authors": [
                    {
                        "name": "NEXT Collaboration"
                    },
                    {
                        "name": "C. Adams"
                    },
                    {
                        "name": "H. Almazán"
                    },
                    {
                        "name": "V. Álvarez"
                    },
                    {
                        "name": "K. Bailey"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "B. J. P. Jones"
                    },
                    {
                        "name": "S. Johnston"
                    },
                    {
                        "name": "K. Mistry"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "D. R. Nygren"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "L. Rogers"
                    },
                    {
                        "name": "J. Waldschmidt"
                    },
                    {
                        "name": "B. Aparicio"
                    },
                    {
                        "name": "A. I. Aranburu"
                    },
                    {
                        "name": "L. Arazi"
                    },
                    {
                        "name": "I. J. Arnquist"
                    },
                    {
                        "name": "F. Auria-Luna"
                    },
                    {
                        "name": "S. Ayet"
                    },
                    {
                        "name": "C. D. R. Azevedo"
                    },
                    {
                        "name": "F. Ballester"
                    },
                    {
                        "name": "M. del Barrio-Torregrosa"
                    },
                    {
                        "name": "A. Bayo"
                    },
                    {
                        "name": "J. M. Benlloch-Rodríguez"
                    },
                    {
                        "name": "F. I. G. M. Borges"
                    },
                    {
                        "name": "A. Brodolin"
                    },
                    {
                        "name": "S. Cárcel"
                    },
                    {
                        "name": "A. Castillo"
                    },
                    {
                        "name": "L. Cid"
                    },
                    {
                        "name": "C. A. N. Conde"
                    },
                    {
                        "name": "T. Contreras"
                    },
                    {
                        "name": "F. P. Cossío"
                    },
                    {
                        "name": "R. Coupe"
                    },
                    {
                        "name": "E. Dey"
                    },
                    {
                        "name": "G. Díaz"
                    },
                    {
                        "name": "C. Echevarria"
                    },
                    {
                        "name": "M. Elorza"
                    },
                    {
                        "name": "J. Escada"
                    },
                    {
                        "name": "R. Esteve"
                    },
                    {
                        "name": "R. Felkai"
                    },
                    {
                        "name": "L. M. P. Fernandes"
                    },
                    {
                        "name": "P. Ferrario"
                    },
                    {
                        "name": "A. L. Ferreira"
                    },
                    {
                        "name": "F. W. Foss"
                    },
                    {
                        "name": "Z. Freixa"
                    },
                    {
                        "name": "J. García-Barrena"
                    },
                    {
                        "name": "J. J. Gómez-Cadenas"
                    },
                    {
                        "name": "J. W. R. Grocott"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "J. Hauptman"
                    },
                    {
                        "name": "C. A. O. Henriques"
                    },
                    {
                        "name": "J. A. Hernando Morata"
                    },
                    {
                        "name": "P. Herrero-Gómez"
                    },
                    {
                        "name": "V. Herrero"
                    },
                    {
                        "name": "C. Hervés Carrete"
                    },
                    {
                        "name": "Y. Ifergan"
                    },
                    {
                        "name": "F. Kellerer"
                    },
                    {
                        "name": "L. Larizgoitia"
                    },
                    {
                        "name": "A. Larumbe"
                    },
                    {
                        "name": "P. Lebrun"
                    },
                    {
                        "name": "F. Lopez"
                    },
                    {
                        "name": "N. López-March"
                    },
                    {
                        "name": "R. Madigan"
                    },
                    {
                        "name": "R. D. P. Mano"
                    },
                    {
                        "name": "A. P. Marques"
                    },
                    {
                        "name": "J. Martín-Albo"
                    },
                    {
                        "name": "G. Martínez-Lema"
                    },
                    {
                        "name": "M. Martínez-Vara"
                    },
                    {
                        "name": "R. L. Miller"
                    },
                    {
                        "name": "J. Molina-Canteras"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "C. M. B. Monteiro"
                    },
                    {
                        "name": "F. J. Mora"
                    },
                    {
                        "name": "P. Novella"
                    },
                    {
                        "name": "A. Nuñez"
                    },
                    {
                        "name": "E. Oblak"
                    },
                    {
                        "name": "J. Palacio"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "A. Para"
                    },
                    {
                        "name": "A. Pazos"
                    },
                    {
                        "name": "J. Pelegrin"
                    },
                    {
                        "name": "M. Pérez Maneiro"
                    },
                    {
                        "name": "M. Querol"
                    },
                    {
                        "name": "J. Renner"
                    },
                    {
                        "name": "I. Rivilla"
                    },
                    {
                        "name": "C. Rogero"
                    },
                    {
                        "name": "B. Romeo"
                    },
                    {
                        "name": "C. Romo-Luque"
                    },
                    {
                        "name": "V. San Nacienciano"
                    },
                    {
                        "name": "F. P. Santos"
                    },
                    {
                        "name": "J. M. F. dos Santos"
                    },
                    {
                        "name": "M. Seemann"
                    },
                    {
                        "name": "I. Shomroni"
                    },
                    {
                        "name": "P. A. O. C. Silva"
                    },
                    {
                        "name": "A. Simón"
                    },
                    {
                        "name": "S. R. Soleti"
                    },
                    {
                        "name": "M. Sorel"
                    },
                    {
                        "name": "J. Soto-Oton"
                    },
                    {
                        "name": "J. M. R. Teixeira"
                    },
                    {
                        "name": "S. Teruel-Pardo"
                    },
                    {
                        "name": "J. F. Toledo"
                    },
                    {
                        "name": "C. Tonnelé"
                    },
                    {
                        "name": "S. Torelli"
                    },
                    {
                        "name": "J. Torrent"
                    },
                    {
                        "name": "A. Trettin"
                    },
                    {
                        "name": "A. Usón"
                    },
                    {
                        "name": "P. R. G. Valle"
                    },
                    {
                        "name": "J. F. C. A. Veloso"
                    },
                    {
                        "name": "J. Waiton"
                    },
                    {
                        "name": "A. Yubero-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "A. Yubero-Navarro"
                },
                "author": "A. Yubero-Navarro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16986v1",
                "updated": "2025-05-22T17:54:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    54,
                    32,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:54:32Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    54,
                    32,
                    3,
                    142,
                    0
                ],
                "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-source language models. We present results powered by\nT1-Agent, highlighting their ability to plan and reason in complex,\ntool-dependent scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-source language models. We present results powered by\nT1-Agent, highlighting their ability to plan and reason in complex,\ntool-dependent scenarios."
                },
                "authors": [
                    {
                        "name": "Amartya Chakraborty"
                    },
                    {
                        "name": "Paresh Dashore"
                    },
                    {
                        "name": "Nadia Bathaee"
                    },
                    {
                        "name": "Anmol Jain"
                    },
                    {
                        "name": "Anirban Das"
                    },
                    {
                        "name": "Shi-Xiong Zhang"
                    },
                    {
                        "name": "Sambit Sahu"
                    },
                    {
                        "name": "Milind Naphade"
                    },
                    {
                        "name": "Genta Indra Winata"
                    }
                ],
                "author_detail": {
                    "name": "Genta Indra Winata"
                },
                "author": "Genta Indra Winata",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16950v1",
                "updated": "2025-05-22T17:33:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    33,
                    49,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:33:49Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    33,
                    49,
                    3,
                    142,
                    0
                ],
                "title": "Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised\n  Reasoning"
                },
                "summary": "Despite their impressive capabilities, Large Language Models struggle with\ngeneralisation beyond their training distribution, often exhibiting\nsophisticated pattern interpolation rather than true abstract reasoning\n(extrapolation). In this work, we approach this limitation through the lens of\nInformation Bottleneck (IB) theory, which posits that model generalisation\nemerges from an optimal balance between input compression and retention of\npredictive information in latent representations. We prove using IB theory that\ndecoder-only Transformers are inherently constrained in their ability to form\ntask-optimal sequence representations. We then use this result to demonstrate\nthat periodic global transformation of the internal sequence-level\nrepresentations (KV cache) is a necessary computational step for improving\nTransformer generalisation in reasoning tasks. Based on these theoretical\ninsights, we propose a modification to the Transformer architecture, in the\nform of an additional module that globally rewrites the KV cache at periodic\nintervals, shifting its capacity away from memorising input prefixes and toward\nencoding features most useful for predicting future tokens. Our model delivers\nsubstantial gains on mathematical reasoning benchmarks, outperforming both\nvanilla Transformers with up to 3.5x more parameters, as well as\nheuristic-driven pruning mechanisms for cache compression. Our approach can be\nseen as a principled generalisation of existing KV-cache compression methods;\nwhereas such methods focus solely on compressing input representations, they\noften do so at the expense of retaining predictive information, and thus their\ncapabilities are inherently bounded by those of an unconstrained model. This\nestablishes a principled framework to manipulate Transformer memory using\ninformation theory, addressing fundamental reasoning limitations that scaling\nalone cannot overcome.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their impressive capabilities, Large Language Models struggle with\ngeneralisation beyond their training distribution, often exhibiting\nsophisticated pattern interpolation rather than true abstract reasoning\n(extrapolation). In this work, we approach this limitation through the lens of\nInformation Bottleneck (IB) theory, which posits that model generalisation\nemerges from an optimal balance between input compression and retention of\npredictive information in latent representations. We prove using IB theory that\ndecoder-only Transformers are inherently constrained in their ability to form\ntask-optimal sequence representations. We then use this result to demonstrate\nthat periodic global transformation of the internal sequence-level\nrepresentations (KV cache) is a necessary computational step for improving\nTransformer generalisation in reasoning tasks. Based on these theoretical\ninsights, we propose a modification to the Transformer architecture, in the\nform of an additional module that globally rewrites the KV cache at periodic\nintervals, shifting its capacity away from memorising input prefixes and toward\nencoding features most useful for predicting future tokens. Our model delivers\nsubstantial gains on mathematical reasoning benchmarks, outperforming both\nvanilla Transformers with up to 3.5x more parameters, as well as\nheuristic-driven pruning mechanisms for cache compression. Our approach can be\nseen as a principled generalisation of existing KV-cache compression methods;\nwhereas such methods focus solely on compressing input representations, they\noften do so at the expense of retaining predictive information, and thus their\ncapabilities are inherently bounded by those of an unconstrained model. This\nestablishes a principled framework to manipulate Transformer memory using\ninformation theory, addressing fundamental reasoning limitations that scaling\nalone cannot overcome."
                },
                "authors": [
                    {
                        "name": "Adnan Oomerjee"
                    },
                    {
                        "name": "Zafeirios Fountas"
                    },
                    {
                        "name": "Zhongwei Yu"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15431v2",
                "updated": "2025-05-22T06:44:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    44,
                    25,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-21T12:11:53Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    11,
                    53,
                    2,
                    141,
                    0
                ],
                "title": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought"
                },
                "summary": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models."
                },
                "authors": [
                    {
                        "name": "Tencent Hunyuan Team"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Botong Zhou"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Chayse Zhou"
                    },
                    {
                        "name": "ChenChen Zhang"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Chenhao Wang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Guanwei Zhang"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Haipeng Luo"
                    },
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Kejiao Li"
                    },
                    {
                        "name": "Keyao Wang"
                    },
                    {
                        "name": "Lan Jiang"
                    },
                    {
                        "name": "Lixin Liu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Peiqi Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Qianbiao Xiang"
                    },
                    {
                        "name": "Qibin Liu"
                    },
                    {
                        "name": "Qingfeng Sun"
                    },
                    {
                        "name": "Richard Guo"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Tian Zhang"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Weidong Han"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Weijin Zhou"
                    },
                    {
                        "name": "Weikang Wang"
                    },
                    {
                        "name": "Wesleye Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yang Du"
                    },
                    {
                        "name": "Yang Zhen"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Yulong Wang"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Zenan Xu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "ZhenXiang Yan"
                    },
                    {
                        "name": "Zheng Fang"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Zhuoyu Li"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Alex Yan"
                    },
                    {
                        "name": "Ande Liang"
                    },
                    {
                        "name": "Baitong Liu"
                    },
                    {
                        "name": "Beiping Pan"
                    },
                    {
                        "name": "Bin Xing"
                    },
                    {
                        "name": "Binghong Wu"
                    },
                    {
                        "name": "Bingxin Qu"
                    },
                    {
                        "name": "Bolin Ni"
                    },
                    {
                        "name": "Boyu Wu"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Cheng Jiang"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Chengjun Liu"
                    },
                    {
                        "name": "Chengxu Yang"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Chiyu Wang"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Daisy Yi"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Fanyang Lu"
                    },
                    {
                        "name": "Fei Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Feng Zheng"
                    },
                    {
                        "name": "Guanghua Yu"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Guohua Wang"
                    },
                    {
                        "name": "Haisheng Lin"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Hao Lu"
                    },
                    {
                        "name": "Haoqing Jiang"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Haotian Zhu"
                    },
                    {
                        "name": "Huangjin Dai"
                    },
                    {
                        "name": "Huankui Chen"
                    },
                    {
                        "name": "Huawen Feng"
                    },
                    {
                        "name": "Huihui Cai"
                    },
                    {
                        "name": "Huxin Peng"
                    },
                    {
                        "name": "Jackson Lv"
                    },
                    {
                        "name": "Jiacheng Shi"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Jiangtao Guan"
                    },
                    {
                        "name": "Jianing Xu"
                    },
                    {
                        "name": "Jianwei Cai"
                    },
                    {
                        "name": "Jiarong Zhang"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Jieneng Yang"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jin lv"
                    },
                    {
                        "name": "Jing Zhao"
                    },
                    {
                        "name": "Jinjian Li"
                    },
                    {
                        "name": "Jinxing Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Juntao Guo"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Lei Fu"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Li Liu"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Liya Zhan"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Long Xu"
                    },
                    {
                        "name": "Mao Zheng"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Nanli Chen"
                    },
                    {
                        "name": "Peirui Chen"
                    },
                    {
                        "name": "Peng He"
                    },
                    {
                        "name": "Pengju Pan"
                    },
                    {
                        "name": "Pengzhi Wei"
                    },
                    {
                        "name": "Qi Yang"
                    },
                    {
                        "name": "Qi Yi"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Ruixu Zhou"
                    },
                    {
                        "name": "Shaofeng Zhang"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Shihao Xu"
                    },
                    {
                        "name": "Shuaishuai Chang"
                    },
                    {
                        "name": "Shulin Liu"
                    },
                    {
                        "name": "SiQi Wang"
                    },
                    {
                        "name": "Songjia Feng"
                    },
                    {
                        "name": "Songling Yuan"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Tianjiao Lang"
                    },
                    {
                        "name": "Tongkai Li"
                    },
                    {
                        "name": "Wei Deng"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Weigang Zhang"
                    },
                    {
                        "name": "Weixuan Sun"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Wenzhi Sun"
                    },
                    {
                        "name": "Wenzhuo Jia"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Xiangyu He"
                    },
                    {
                        "name": "Xianshun Ren"
                    },
                    {
                        "name": "XiaoYing Zhu"
                    },
                    {
                        "name": "Xiaolong Guo"
                    },
                    {
                        "name": "Xiaoxue Li"
                    },
                    {
                        "name": "Xiaoyu Ma"
                    },
                    {
                        "name": "Xican Lu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Xinyu Guan"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xudong Gao"
                    },
                    {
                        "name": "Xun Luo"
                    },
                    {
                        "name": "Xuxiang Qi"
                    },
                    {
                        "name": "Yangkun Chen"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yanling Xiao"
                    },
                    {
                        "name": "Yantao Mai"
                    },
                    {
                        "name": "Yanze Chen"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Yeting Yang"
                    },
                    {
                        "name": "YiFan Song"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Yijiao Zhu"
                    },
                    {
                        "name": "Yinhe Wu"
                    },
                    {
                        "name": "Yixian Liu"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuanjun Cai"
                    },
                    {
                        "name": "Yuanlin Tu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Yuhao Jiang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Yuhui Hu"
                    },
                    {
                        "name": "Yujin Lin"
                    },
                    {
                        "name": "Yun Yang"
                    },
                    {
                        "name": "Yunhao Wang"
                    },
                    {
                        "name": "Yusong Zhang"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Zelong Zhang"
                    },
                    {
                        "name": "Zhan Yu"
                    },
                    {
                        "name": "Zhaoliang Yang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Zhenyu Huang"
                    },
                    {
                        "name": "Zhiguang Liu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    },
                    {
                        "name": "Zhiqing Kui"
                    },
                    {
                        "name": "Zhiyin Zeng"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Zhuo Han"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Zigang Geng"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Ziyan Tang"
                    },
                    {
                        "name": "Ziyuan Zhu"
                    },
                    {
                        "name": "Zonglei Zhu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijiang Xu"
                },
                "author": "Zhijiang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16271v1",
                "updated": "2025-05-22T06:04:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    4,
                    20,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T06:04:20Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    4,
                    20,
                    3,
                    142,
                    0
                ],
                "title": "Variations of the Near-Surface Electric field measured at Aragats during\n  Geomagnetic Storms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variations of the Near-Surface Electric field measured at Aragats during\n  Geomagnetic Storms"
                },
                "summary": "At least two mechanisms effectively transfer interplanetary magnetic field\n(IMF) disturbances into the atmosphere. First, the inflow of solar wind into\nthe ionosphere at low latitudes significantly enhances the total vertical\nelectron content, increasing atmospheric conductivity. Second, Forbush\ndecreases (FD) reduce the cosmic ray flux by a few percent, lowering ionization\nlevels at middle latitudes and decreasing conductivity. Changes in atmospheric\nconductivity affect the global electric circuit and atmospheric electric field\n(AEF). However, to study the response of AEF to geomagnetic storms (GMS), it is\nnecessary to carefully monitor atmospheric conditions before and during storms,\nas meteorological influences can be much stronger than those of GMS. Charged\nclouds above detectors, lightning flashes, and abrupt weather changes\nsignificantly impact near-surface electric field (NSEF) variations, which serve\nas a proxy for AEF measured at the Earth's surface. The facilities at Aragats\nstation monitor all environmental parameters on a one-minute timescale. We\nanalyze four GMS events described in previous studies, detailing the\ncorresponding weather conditions to isolate the genuine influence of GMS on\nNSEF. The GMS of June 22, 2015, and September 8, 2017, occurred under\nfair-weather conditions, providing clear evidence of GMS influence on NSEF.\nThese events were long-lasting, positive, and modest, ranging from 0.2 to 0.3\nkV/m, and coincided with the depletion phase of FD. The sky was clear, no rain\nwas detected, and lightning flashes from previous thunderstorms were more than\n20 km from the station. The other two events did not meet favorable weather\ncriteria, and their occurrence during GMS seemed incidental. We identify a\nfeature that may indicate the solar (FD) origin of NSEF enhancement: a dip in\nthe enhanced NSEF during the daytime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At least two mechanisms effectively transfer interplanetary magnetic field\n(IMF) disturbances into the atmosphere. First, the inflow of solar wind into\nthe ionosphere at low latitudes significantly enhances the total vertical\nelectron content, increasing atmospheric conductivity. Second, Forbush\ndecreases (FD) reduce the cosmic ray flux by a few percent, lowering ionization\nlevels at middle latitudes and decreasing conductivity. Changes in atmospheric\nconductivity affect the global electric circuit and atmospheric electric field\n(AEF). However, to study the response of AEF to geomagnetic storms (GMS), it is\nnecessary to carefully monitor atmospheric conditions before and during storms,\nas meteorological influences can be much stronger than those of GMS. Charged\nclouds above detectors, lightning flashes, and abrupt weather changes\nsignificantly impact near-surface electric field (NSEF) variations, which serve\nas a proxy for AEF measured at the Earth's surface. The facilities at Aragats\nstation monitor all environmental parameters on a one-minute timescale. We\nanalyze four GMS events described in previous studies, detailing the\ncorresponding weather conditions to isolate the genuine influence of GMS on\nNSEF. The GMS of June 22, 2015, and September 8, 2017, occurred under\nfair-weather conditions, providing clear evidence of GMS influence on NSEF.\nThese events were long-lasting, positive, and modest, ranging from 0.2 to 0.3\nkV/m, and coincided with the depletion phase of FD. The sky was clear, no rain\nwas detected, and lightning flashes from previous thunderstorms were more than\n20 km from the station. The other two events did not meet favorable weather\ncriteria, and their occurrence during GMS seemed incidental. We identify a\nfeature that may indicate the solar (FD) origin of NSEF enhancement: a dip in\nthe enhanced NSEF during the daytime."
                },
                "authors": [
                    {
                        "name": "A. Chilingarian"
                    }
                ],
                "author_detail": {
                    "name": "A. Chilingarian"
                },
                "author": "A. Chilingarian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15793v2",
                "updated": "2025-05-22T04:48:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    4,
                    48,
                    12,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-21T17:47:24Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    47,
                    24,
                    2,
                    141,
                    0
                ],
                "title": "HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for\n  Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for\n  Autonomous Driving"
                },
                "summary": "Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can\nenhance autonomous driving (AD) performance in complex scenarios. However,\ncurrent LLM-Dominated RL methods over-rely on LLM outputs, which are prone to\nhallucinations. Evaluations show that state-of-the-art LLM indicates a\nnon-hallucination rate of only approximately 57.95% when assessed on essential\ndriving-related tasks. Thus, in these methods, hallucinations from the LLM can\ndirectly jeopardize the performance of driving policies. This paper argues that\nmaintaining relative independence between the LLM and the RL is vital for\nsolving the hallucinations problem. Consequently, this paper is devoted to\npropose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic\nhints for state augmentation and policy optimization to assist RL agent in\nmotion planning, while the RL agent counteracts potential erroneous semantic\nindications through policy learning to achieve excellent driving performance.\nBased on this paradigm, we propose the HCRMP (LLM-Hinted Contextual\nReinforcement Learning Motion Planner) architecture, which is designed that\nincludes Augmented Semantic Representation Module to extend state space.\nContextual Stability Anchor Module enhances the reliability of multi-critic\nweight hints by utilizing information from the knowledge base. Semantic Cache\nModule is employed to seamlessly integrate LLM low-frequency guidance with RL\nhigh-frequency control. Extensive experiments in CARLA validate HCRMP's strong\noverall driving performance. HCRMP achieves a task success rate of up to 80.3%\nunder diverse driving conditions with different traffic densities. Under\nsafety-critical driving conditions, HCRMP significantly reduces the collision\nrate by 11.4%, which effectively improves the driving performance in complex\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can\nenhance autonomous driving (AD) performance in complex scenarios. However,\ncurrent LLM-Dominated RL methods over-rely on LLM outputs, which are prone to\nhallucinations. Evaluations show that state-of-the-art LLM indicates a\nnon-hallucination rate of only approximately 57.95% when assessed on essential\ndriving-related tasks. Thus, in these methods, hallucinations from the LLM can\ndirectly jeopardize the performance of driving policies. This paper argues that\nmaintaining relative independence between the LLM and the RL is vital for\nsolving the hallucinations problem. Consequently, this paper is devoted to\npropose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic\nhints for state augmentation and policy optimization to assist RL agent in\nmotion planning, while the RL agent counteracts potential erroneous semantic\nindications through policy learning to achieve excellent driving performance.\nBased on this paradigm, we propose the HCRMP (LLM-Hinted Contextual\nReinforcement Learning Motion Planner) architecture, which is designed that\nincludes Augmented Semantic Representation Module to extend state space.\nContextual Stability Anchor Module enhances the reliability of multi-critic\nweight hints by utilizing information from the knowledge base. Semantic Cache\nModule is employed to seamlessly integrate LLM low-frequency guidance with RL\nhigh-frequency control. Extensive experiments in CARLA validate HCRMP's strong\noverall driving performance. HCRMP achieves a task success rate of up to 80.3%\nunder diverse driving conditions with different traffic densities. Under\nsafety-critical driving conditions, HCRMP significantly reduces the collision\nrate by 11.4%, which effectively improves the driving performance in complex\nscenarios."
                },
                "authors": [
                    {
                        "name": "Zhiwen Chen"
                    },
                    {
                        "name": "Bo Leng"
                    },
                    {
                        "name": "Zhuoren Li"
                    },
                    {
                        "name": "Hanming Deng"
                    },
                    {
                        "name": "Guizhe Jin"
                    },
                    {
                        "name": "Ran Yu"
                    },
                    {
                        "name": "Huanxi Wen"
                    }
                ],
                "author_detail": {
                    "name": "Huanxi Wen"
                },
                "author": "Huanxi Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16210v1",
                "updated": "2025-05-22T04:23:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    4,
                    23,
                    19,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T04:23:19Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    4,
                    23,
                    19,
                    3,
                    142,
                    0
                ],
                "title": "NQKV: A KV Cache Quantization Scheme Based on Normal Distribution\n  Characteristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NQKV: A KV Cache Quantization Scheme Based on Normal Distribution\n  Characteristics"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na wide range of tasks. However, LLMs often require larger batch sizes to\nenhance throughput or longer context lengths to meet task demands, which\nsignificantly increases the memory resource consumption of the Key-Value (KV)\ncache during inference, becoming a major bottleneck in LLM deployment. To\naddress this issue, quantization is a common and straightforward approach.\nCurrently, quantization methods for activations are limited to 8-bit, and\nquantization to even lower bits can lead to substantial accuracy drops. To\nfurther save space by quantizing the KV cache to even lower bits, we analyzed\nthe element distribution of the KV cache and designed the NQKV algorithm. Since\nthe elements within each block of the KV cache follow a normal distribution,\nNQKV employs per-block quantile quantization to achieve\ninformation-theoretically optimal quantization error. Without significantly\ncompromising model output quality, NQKV enables the OPT model to perform\ninference with an 2x larger batch size or a 4x longer context length, and it\nimproves throughput by 9.3x compared to when the KV cache is not used.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na wide range of tasks. However, LLMs often require larger batch sizes to\nenhance throughput or longer context lengths to meet task demands, which\nsignificantly increases the memory resource consumption of the Key-Value (KV)\ncache during inference, becoming a major bottleneck in LLM deployment. To\naddress this issue, quantization is a common and straightforward approach.\nCurrently, quantization methods for activations are limited to 8-bit, and\nquantization to even lower bits can lead to substantial accuracy drops. To\nfurther save space by quantizing the KV cache to even lower bits, we analyzed\nthe element distribution of the KV cache and designed the NQKV algorithm. Since\nthe elements within each block of the KV cache follow a normal distribution,\nNQKV employs per-block quantile quantization to achieve\ninformation-theoretically optimal quantization error. Without significantly\ncompromising model output quality, NQKV enables the OPT model to perform\ninference with an 2x larger batch size or a 4x longer context length, and it\nimproves throughput by 9.3x compared to when the KV cache is not used."
                },
                "authors": [
                    {
                        "name": "Zhihang Cai"
                    },
                    {
                        "name": "Xingjun Zhang"
                    },
                    {
                        "name": "Zhendong Tan"
                    },
                    {
                        "name": "Zheng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Wei"
                },
                "author": "Zheng Wei",
                "arxiv_comment": "11 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16175v1",
                "updated": "2025-05-22T03:26:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    26,
                    50,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T03:26:50Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    26,
                    50,
                    3,
                    142,
                    0
                ],
                "title": "QuickVideo: Real-Time Long Video Understanding with System Algorithm\n  Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuickVideo: Real-Time Long Video Understanding with System Algorithm\n  Co-Design"
                },
                "summary": "Long-video understanding has emerged as a crucial capability in real-world\napplications such as video surveillance, meeting summarization, educational\nlecture analysis, and sports broadcasting. However, it remains computationally\nprohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential\nvideo decoding, the process of converting the raw bit stream to RGB frames can\ntake up to a minute for hour-long video inputs, and 2) costly prefilling of up\nto several million tokens for LLM inference, resulting in high latency and\nmemory use. To address these challenges, we propose QuickVideo, a\nsystem-algorithm co-design that substantially accelerates long-video\nunderstanding to support real-time downstream applications. It comprises three\nkey innovations: QuickDecoder, a parallelized CPU-based video decoder that\nachieves 2-3 times speedup by splitting videos into keyframe-aligned intervals\nprocessed concurrently; QuickPrefill, a memory-efficient prefilling method\nusing KV-cache pruning to support more frames with less GPU memory; and an\noverlapping scheme that overlaps CPU video decoding with GPU inference.\nTogether, these components infernece time reduce by a minute on long video\ninputs, enabling scalable, high-quality video understanding even on limited\nhardware. Experiments show that QuickVideo generalizes across durations and\nsampling rates, making long video processing feasible in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-video understanding has emerged as a crucial capability in real-world\napplications such as video surveillance, meeting summarization, educational\nlecture analysis, and sports broadcasting. However, it remains computationally\nprohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential\nvideo decoding, the process of converting the raw bit stream to RGB frames can\ntake up to a minute for hour-long video inputs, and 2) costly prefilling of up\nto several million tokens for LLM inference, resulting in high latency and\nmemory use. To address these challenges, we propose QuickVideo, a\nsystem-algorithm co-design that substantially accelerates long-video\nunderstanding to support real-time downstream applications. It comprises three\nkey innovations: QuickDecoder, a parallelized CPU-based video decoder that\nachieves 2-3 times speedup by splitting videos into keyframe-aligned intervals\nprocessed concurrently; QuickPrefill, a memory-efficient prefilling method\nusing KV-cache pruning to support more frames with less GPU memory; and an\noverlapping scheme that overlaps CPU video decoding with GPU inference.\nTogether, these components infernece time reduce by a minute on long video\ninputs, enabling scalable, high-quality video understanding even on limited\nhardware. Experiments show that QuickVideo generalizes across durations and\nsampling rates, making long video processing feasible in practice."
                },
                "authors": [
                    {
                        "name": "Benjamin Schneider"
                    },
                    {
                        "name": "Dongfu Jiang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "arxiv_comment": "19 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17132v1",
                "updated": "2025-05-22T03:00:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    0,
                    39,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T03:00:39Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    0,
                    39,
                    3,
                    142,
                    0
                ],
                "title": "Robustifying Vision-Language Models via Dynamic Token Reweighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Vision-Language Models via Dynamic Token Reweighting"
                },
                "summary": "Large vision-language models (VLMs) are highly vulnerable to jailbreak\nattacks that exploit visual-textual interactions to bypass safety guardrails.\nIn this paper, we present DTR, a novel inference-time defense that mitigates\nmultimodal jailbreak attacks through optimizing the model's key-value (KV)\ncaches. Rather than relying on curated safety-specific data or costly\nimage-to-text conversion, we introduce a new formulation of the safety-relevant\ndistributional shift induced by the visual modality. This formulation enables\nDTR to dynamically adjust visual token weights, minimizing the impact of\nadversarial visual inputs while preserving the model's general capabilities and\ninference efficiency. Extensive evaluation across diverse VLMs and attack\nbenchmarks demonstrates that \\sys outperforms existing defenses in both attack\nrobustness and benign task performance, marking the first successful\napplication of KV cache optimization for safety enhancement in multimodal\nfoundation models. The code for replicating DTR is available:\nhttps://anonymous.4open.science/r/DTR-2755 (warning: this paper contains\npotentially harmful content generated by VLMs.)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (VLMs) are highly vulnerable to jailbreak\nattacks that exploit visual-textual interactions to bypass safety guardrails.\nIn this paper, we present DTR, a novel inference-time defense that mitigates\nmultimodal jailbreak attacks through optimizing the model's key-value (KV)\ncaches. Rather than relying on curated safety-specific data or costly\nimage-to-text conversion, we introduce a new formulation of the safety-relevant\ndistributional shift induced by the visual modality. This formulation enables\nDTR to dynamically adjust visual token weights, minimizing the impact of\nadversarial visual inputs while preserving the model's general capabilities and\ninference efficiency. Extensive evaluation across diverse VLMs and attack\nbenchmarks demonstrates that \\sys outperforms existing defenses in both attack\nrobustness and benign task performance, marking the first successful\napplication of KV cache optimization for safety enhancement in multimodal\nfoundation models. The code for replicating DTR is available:\nhttps://anonymous.4open.science/r/DTR-2755 (warning: this paper contains\npotentially harmful content generated by VLMs.)"
                },
                "authors": [
                    {
                        "name": "Tanqiu Jiang"
                    },
                    {
                        "name": "Jiacheng Liang"
                    },
                    {
                        "name": "Rongyi Zhu"
                    },
                    {
                        "name": "Jiawei Zhou"
                    },
                    {
                        "name": "Fenglong Ma"
                    },
                    {
                        "name": "Ting Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ting Wang"
                },
                "author": "Ting Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16076v1",
                "updated": "2025-05-21T23:23:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    23,
                    23,
                    37,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T23:23:37Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    23,
                    23,
                    37,
                    2,
                    141,
                    0
                ],
                "title": "AudioMorphix: Training-free audio editing with diffusion probabilistic\n  models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AudioMorphix: Training-free audio editing with diffusion probabilistic\n  models"
                },
                "summary": "Editing sound with precision is a crucial yet underexplored challenge in\naudio content creation. While existing works can manipulate sounds by text\ninstructions or audio exemplar pairs, they often struggled to modify audio\ncontent precisely while preserving fidelity to the original recording. In this\nwork, we introduce a novel editing approach that enables localized\nmodifications to specific time-frequency regions while keeping the remaining of\nthe audio intact by operating on spectrograms directly. To achieve this, we\npropose AudioMorphix, a training-free audio editor that manipulates a target\nregion on the spectrogram by referring to another recording. Inspired by\nmorphing theory, we conceptualize audio mixing as a process where different\nsounds blend seamlessly through morphing and can be decomposed back into\nindividual components via demorphing. Our AudioMorphix optimizes the noised\nlatent conditioned on raw input and reference audio while rectifying the guided\ndiffusion process through a series of energy functions. Additionally, we\nenhance self-attention layers with a cache mechanism to preserve detailed\ncharacteristics from the original recordings. To advance audio editing\nresearch, we devise a new evaluation benchmark, which includes a curated\ndataset with a variety of editing instructions. Extensive experiments\ndemonstrate that AudioMorphix yields promising performance on various audio\nediting tasks, including addition, removal, time shifting and stretching, and\npitch shifting, achieving high fidelity and precision. Demo and code are\navailable at this url.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Editing sound with precision is a crucial yet underexplored challenge in\naudio content creation. While existing works can manipulate sounds by text\ninstructions or audio exemplar pairs, they often struggled to modify audio\ncontent precisely while preserving fidelity to the original recording. In this\nwork, we introduce a novel editing approach that enables localized\nmodifications to specific time-frequency regions while keeping the remaining of\nthe audio intact by operating on spectrograms directly. To achieve this, we\npropose AudioMorphix, a training-free audio editor that manipulates a target\nregion on the spectrogram by referring to another recording. Inspired by\nmorphing theory, we conceptualize audio mixing as a process where different\nsounds blend seamlessly through morphing and can be decomposed back into\nindividual components via demorphing. Our AudioMorphix optimizes the noised\nlatent conditioned on raw input and reference audio while rectifying the guided\ndiffusion process through a series of energy functions. Additionally, we\nenhance self-attention layers with a cache mechanism to preserve detailed\ncharacteristics from the original recordings. To advance audio editing\nresearch, we devise a new evaluation benchmark, which includes a curated\ndataset with a variety of editing instructions. Extensive experiments\ndemonstrate that AudioMorphix yields promising performance on various audio\nediting tasks, including addition, removal, time shifting and stretching, and\npitch shifting, achieving high fidelity and precision. Demo and code are\navailable at this url."
                },
                "authors": [
                    {
                        "name": "Jinhua Liang"
                    },
                    {
                        "name": "Yuanzhe Chen"
                    },
                    {
                        "name": "Yi Yuan"
                    },
                    {
                        "name": "Dongya Jia"
                    },
                    {
                        "name": "Xiaobin Zhuang"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Yuping Wang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuxuan Wang"
                },
                "author": "Yuxuan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16056v1",
                "updated": "2025-05-21T22:13:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    22,
                    13,
                    9,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T22:13:09Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    22,
                    13,
                    9,
                    2,
                    141,
                    0
                ],
                "title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models"
                },
                "summary": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc ."
                },
                "authors": [
                    {
                        "name": "Jingcong Liang"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Miren Tian"
                    },
                    {
                        "name": "Yitong Li"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10510v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10510v2",
                "updated": "2025-05-21T20:52:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    20,
                    52,
                    59,
                    2,
                    141,
                    0
                ],
                "published": "2024-11-15T16:24:02Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "title": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models."
                },
                "authors": [
                    {
                        "name": "Joseph Liu"
                    },
                    {
                        "name": "Joshua Geddes"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Haomiao Jiang"
                    },
                    {
                        "name": "Mahesh Kumar Nandwana"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Kumar Nandwana"
                },
                "author": "Mahesh Kumar Nandwana",
                "arxiv_comment": "Code can be found at https://github.com/Roblox/SmoothCache. Accepted\n  at CVPR eLVM workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10510v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10510v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16002v2",
                "updated": "2025-05-21T20:42:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    20,
                    42,
                    8,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-21T23:34:29Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"
                },
                "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines."
                },
                "authors": [
                    {
                        "name": "Jingbo Yang"
                    },
                    {
                        "name": "Bairu Hou"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yujia Bao"
                    },
                    {
                        "name": "Shiyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Chang"
                },
                "author": "Shiyu Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15781v1",
                "updated": "2025-05-21T17:32:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    32,
                    10,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T17:32:10Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    32,
                    10,
                    2,
                    141,
                    0
                ],
                "title": "dKV-Cache: The Cache for Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dKV-Cache: The Cache for Diffusion Language Models"
                },
                "summary": "Diffusion Language Models (DLMs) have been seen as a promising competitor for\nautoregressive language models. However, diffusion language models have long\nbeen constrained by slow inference. A core challenge is that their\nnon-autoregressive architecture and bidirectional attention preclude the\nkey-value cache that accelerates decoding. We address this bottleneck by\nproposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising\nprocess of DLMs. Our approach is motivated by the observation that different\ntokens have distinct representation dynamics throughout the diffusion process.\nAccordingly, we propose a delayed and conditioned caching strategy for key and\nvalue states. We design two complementary variants to cache key and value\nstep-by-step: (1) dKV-Cache-Decode, which provides almost lossless\nacceleration, and even improves performance on long sequences, suggesting that\nexisting DLMs may under-utilise contextual information during inference. (2)\ndKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving\nhigher speed-ups with quadratic time complexity at the cost of some performance\ndegradation. dKV-Cache, in final, achieves from 2-10x speedup in inference,\nlargely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on\nseveral benchmarks, delivering acceleration across general language\nunderstanding, mathematical, and code-generation benchmarks. Experiments\ndemonstrate that cache can also be used in DLMs, even in a training-free manner\nfrom current DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Language Models (DLMs) have been seen as a promising competitor for\nautoregressive language models. However, diffusion language models have long\nbeen constrained by slow inference. A core challenge is that their\nnon-autoregressive architecture and bidirectional attention preclude the\nkey-value cache that accelerates decoding. We address this bottleneck by\nproposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising\nprocess of DLMs. Our approach is motivated by the observation that different\ntokens have distinct representation dynamics throughout the diffusion process.\nAccordingly, we propose a delayed and conditioned caching strategy for key and\nvalue states. We design two complementary variants to cache key and value\nstep-by-step: (1) dKV-Cache-Decode, which provides almost lossless\nacceleration, and even improves performance on long sequences, suggesting that\nexisting DLMs may under-utilise contextual information during inference. (2)\ndKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving\nhigher speed-ups with quadratic time complexity at the cost of some performance\ndegradation. dKV-Cache, in final, achieves from 2-10x speedup in inference,\nlargely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on\nseveral benchmarks, delivering acceleration across general language\nunderstanding, mathematical, and code-generation benchmarks. Experiments\ndemonstrate that cache can also be used in DLMs, even in a training-free manner\nfrom current DLMs."
                },
                "authors": [
                    {
                        "name": "Xinyin Ma"
                    },
                    {
                        "name": "Runpeng Yu"
                    },
                    {
                        "name": "Gongfan Fang"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "The code is available at https://github.com/horseee/dKV-Cache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15683v1",
                "updated": "2025-05-21T15:58:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T15:58:08Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "title": "A Federated Splitting Framework for LLMs: Security, Efficiency, and\n  Adaptability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Federated Splitting Framework for LLMs: Security, Efficiency, and\n  Adaptability"
                },
                "summary": "Private data is typically larger and of higher quality than public data,\noffering great potential to improve LLM. However, its scattered distribution\nacross data silos and the high computational demands of LLMs limit their\ndeployment in federated environments. To address this, the transformer-based\nsplit learning model has emerged, offloading most model parameters to the\nserver while retaining only the embedding and output layers on clients to\nensure privacy. However, it still faces significant challenges in security,\nefficiency, and adaptability: 1) embedding gradients are vulnerable to attacks,\nleading to reverse engineering of private data; 2) the autoregressive nature of\nLLMs means that federated split learning can only train and infer sequentially,\ncausing high communication overhead; 3) fixed partition points lack\nadaptability to downstream tasks. In this paper, we introduce FL-LLaMA, a\nsecure, efficient, and adaptive federated split framework based on LLaMA2.\nFirst, we place some input and output blocks on the local client and inject\nGaussian noise into forward-pass hidden states, enabling secure end-to-end\npropagation. Second, we employ client-batch and server-hierarchical strategies\nto achieve parallel training, along with attention-mask compression and KV\ncache mechanisms to accelerate inference, reducing communication costs\neffectively. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements and hardware\nlimitations. Experiments on NLU, summarization and conversational QA tasks show\nthat FL-LLaMA maintains performance comparable to centralized LLaMA2, and\nachieves up to 2x train speedups and 8x inference speedups. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FL-LLaMA in security and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private data is typically larger and of higher quality than public data,\noffering great potential to improve LLM. However, its scattered distribution\nacross data silos and the high computational demands of LLMs limit their\ndeployment in federated environments. To address this, the transformer-based\nsplit learning model has emerged, offloading most model parameters to the\nserver while retaining only the embedding and output layers on clients to\nensure privacy. However, it still faces significant challenges in security,\nefficiency, and adaptability: 1) embedding gradients are vulnerable to attacks,\nleading to reverse engineering of private data; 2) the autoregressive nature of\nLLMs means that federated split learning can only train and infer sequentially,\ncausing high communication overhead; 3) fixed partition points lack\nadaptability to downstream tasks. In this paper, we introduce FL-LLaMA, a\nsecure, efficient, and adaptive federated split framework based on LLaMA2.\nFirst, we place some input and output blocks on the local client and inject\nGaussian noise into forward-pass hidden states, enabling secure end-to-end\npropagation. Second, we employ client-batch and server-hierarchical strategies\nto achieve parallel training, along with attention-mask compression and KV\ncache mechanisms to accelerate inference, reducing communication costs\neffectively. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements and hardware\nlimitations. Experiments on NLU, summarization and conversational QA tasks show\nthat FL-LLaMA maintains performance comparable to centralized LLaMA2, and\nachieves up to 2x train speedups and 8x inference speedups. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FL-LLaMA in security and adaptability."
                },
                "authors": [
                    {
                        "name": "Zishuai Zhang"
                    },
                    {
                        "name": "Hainan Zhang"
                    },
                    {
                        "name": "Jiaying Zheng"
                    },
                    {
                        "name": "Ziwei Wang"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Jin Dong"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15558v1",
                "updated": "2025-05-21T14:17:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    17,
                    6,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T14:17:06Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    17,
                    6,
                    2,
                    141,
                    0
                ],
                "title": "Robo-DM: Data Management For Large Robot Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robo-DM: Data Management For Large Robot Datasets"
                },
                "summary": "Recent results suggest that very large datasets of teleoperated robot\ndemonstrations can be used to train transformer-based models that have the\npotential to generalize to new scenes, robots, and tasks. However, curating,\ndistributing, and loading large datasets of robot trajectories, which typically\nconsist of video, textual, and numerical modalities - including streams from\nmultiple cameras - remains challenging. We propose Robo-DM, an efficient\nopen-source cloud-based data management toolkit for collecting, sharing, and\nlearning with robot data. With Robo-DM, robot datasets are stored in a\nself-contained format with Extensible Binary Meta Language (EBML). Robo-DM can\nsignificantly reduce the size of robot trajectory data, transfer costs, and\ndata load time during training. Compared to the RLDS format used in OXE\ndatasets, Robo-DM's compression saves space by up to 70x (lossy) and 3.5x\n(lossless). Robo-DM also accelerates data retrieval by load-balancing video\ndecoding with memory-mapped decoding caches. Compared to LeRobot, a framework\nthat also uses lossy video compression, Robo-DM is up to 50x faster when\ndecoding sequentially. We physically evaluate a model trained by Robo-DM with\nlossy compression, a pick-and-place task, and In-Context Robot Transformer.\nRobo-DM uses 75x compression of the original dataset and does not suffer\nreduction in downstream task accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent results suggest that very large datasets of teleoperated robot\ndemonstrations can be used to train transformer-based models that have the\npotential to generalize to new scenes, robots, and tasks. However, curating,\ndistributing, and loading large datasets of robot trajectories, which typically\nconsist of video, textual, and numerical modalities - including streams from\nmultiple cameras - remains challenging. We propose Robo-DM, an efficient\nopen-source cloud-based data management toolkit for collecting, sharing, and\nlearning with robot data. With Robo-DM, robot datasets are stored in a\nself-contained format with Extensible Binary Meta Language (EBML). Robo-DM can\nsignificantly reduce the size of robot trajectory data, transfer costs, and\ndata load time during training. Compared to the RLDS format used in OXE\ndatasets, Robo-DM's compression saves space by up to 70x (lossy) and 3.5x\n(lossless). Robo-DM also accelerates data retrieval by load-balancing video\ndecoding with memory-mapped decoding caches. Compared to LeRobot, a framework\nthat also uses lossy video compression, Robo-DM is up to 50x faster when\ndecoding sequentially. We physically evaluate a model trained by Robo-DM with\nlossy compression, a pick-and-place task, and In-Context Robot Transformer.\nRobo-DM uses 75x compression of the original dataset and does not suffer\nreduction in downstream task accuracy."
                },
                "authors": [
                    {
                        "name": "Kaiyuan Chen"
                    },
                    {
                        "name": "Letian Fu"
                    },
                    {
                        "name": "David Huang"
                    },
                    {
                        "name": "Yanxiang Zhang"
                    },
                    {
                        "name": "Lawrence Yunliang Chen"
                    },
                    {
                        "name": "Huang Huang"
                    },
                    {
                        "name": "Kush Hari"
                    },
                    {
                        "name": "Ashwin Balakrishna"
                    },
                    {
                        "name": "Ted Xiao"
                    },
                    {
                        "name": "Pannag R Sanketi"
                    },
                    {
                        "name": "John Kubiatowicz"
                    },
                    {
                        "name": "Ken Goldberg"
                    }
                ],
                "author_detail": {
                    "name": "Ken Goldberg"
                },
                "author": "Ken Goldberg",
                "arxiv_comment": "Best paper finalist of IEEE ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15535v1",
                "updated": "2025-05-21T13:56:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    56,
                    16,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T13:56:16Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    56,
                    16,
                    2,
                    141,
                    0
                ],
                "title": "Matrix-Free Methods for Finite-Strain Elasticity: Automatic Code\n  Generation with No Performance Overhead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix-Free Methods for Finite-Strain Elasticity: Automatic Code\n  Generation with No Performance Overhead"
                },
                "summary": "This study explores matrix-free tangent evaluations in finite-strain\nelasticity with the use of automatically-generated code for the\nquadrature-point level calculations. The code generation is done via automatic\ndifferentiation (AD) with AceGen. We compare hand-written and AD-generated\ncodes under two computing strategies: on-the-fly evaluation and caching\nintermediate results. The comparison reveals that the AD-generated code\nachieves superior performance in matrix-free computations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores matrix-free tangent evaluations in finite-strain\nelasticity with the use of automatically-generated code for the\nquadrature-point level calculations. The code generation is done via automatic\ndifferentiation (AD) with AceGen. We compare hand-written and AD-generated\ncodes under two computing strategies: on-the-fly evaluation and caching\nintermediate results. The comparison reveals that the AD-generated code\nachieves superior performance in matrix-free computations."
                },
                "authors": [
                    {
                        "name": "Michał Wichrowski"
                    },
                    {
                        "name": "Mohsen Rezaee-Hajidehi"
                    },
                    {
                        "name": "Jože Korelc"
                    },
                    {
                        "name": "Martin Kronbichler"
                    },
                    {
                        "name": "Stanisław Stupkiewicz"
                    }
                ],
                "author_detail": {
                    "name": "Stanisław Stupkiewicz"
                },
                "author": "Stanisław Stupkiewicz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65M60, 74B20, 74S05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15531v1",
                "updated": "2025-05-21T13:52:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    52,
                    45,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T13:52:45Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    52,
                    45,
                    2,
                    141,
                    0
                ],
                "title": "Modeling and Optimizing Latency for Delayed Hit Caching with Stochastic\n  Miss Latency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Optimizing Latency for Delayed Hit Caching with Stochastic\n  Miss Latency"
                },
                "summary": "Caching is crucial for system performance, but the delayed hit phenomenon,\nwhere requests queue during lengthy fetches after a cache miss, significantly\ndegrades user-perceived latency in modern high-throughput systems. While prior\nworks address delayed hits by estimating aggregate delay, they universally\nassume deterministic fetch latencies. This paper tackles the more realistic,\nyet unexplored, scenario where fetch latencies are stochastic. We present, to\nour knowledge, the first theoretical analysis of delayed hits under this\ncondition, deriving analytical expressions for both the mean and variance of\nthe aggregate delay assuming exponentially distributed fetch latency.\nLeveraging these insights, we develop a novel variance-aware ranking function\ntailored for this stochastic setting to guide cache eviction decisions more\neffectively. The simulations on synthetic and real-world datasets demonstrate\nthat our proposed algorithm significantly reduces overall latency compared to\nstate-of-the-art delayed-hit strategies, achieving a $3\\%-30\\%$ reduction on\nsynthetic datasets and approximately $1\\%-7\\%$ reduction on real-world traces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is crucial for system performance, but the delayed hit phenomenon,\nwhere requests queue during lengthy fetches after a cache miss, significantly\ndegrades user-perceived latency in modern high-throughput systems. While prior\nworks address delayed hits by estimating aggregate delay, they universally\nassume deterministic fetch latencies. This paper tackles the more realistic,\nyet unexplored, scenario where fetch latencies are stochastic. We present, to\nour knowledge, the first theoretical analysis of delayed hits under this\ncondition, deriving analytical expressions for both the mean and variance of\nthe aggregate delay assuming exponentially distributed fetch latency.\nLeveraging these insights, we develop a novel variance-aware ranking function\ntailored for this stochastic setting to guide cache eviction decisions more\neffectively. The simulations on synthetic and real-world datasets demonstrate\nthat our proposed algorithm significantly reduces overall latency compared to\nstate-of-the-art delayed-hit strategies, achieving a $3\\%-30\\%$ reduction on\nsynthetic datasets and approximately $1\\%-7\\%$ reduction on real-world traces."
                },
                "authors": [
                    {
                        "name": "Bowen Jiang"
                    },
                    {
                        "name": "Chaofan Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chaofan Ma"
                },
                "author": "Chaofan Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v2",
                "updated": "2025-05-21T10:38:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    38,
                    37,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "41 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16375v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16375v2",
                "updated": "2025-05-21T10:38:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    38,
                    1,
                    2,
                    141,
                    0
                ],
                "published": "2024-11-25T13:33:41Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "title": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing"
                },
                "summary": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available:\nhttps://github.com/Dawn-LX/CausalCache-VDM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available:\nhttps://github.com/Dawn-LX/CausalCache-VDM"
                },
                "authors": [
                    {
                        "name": "Kaifeng Gao"
                    },
                    {
                        "name": "Jiaxin Shi"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Chunping Wang"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Long Chen"
                    }
                ],
                "author_detail": {
                    "name": "Long Chen"
                },
                "author": "Long Chen",
                "arxiv_comment": "Accepted by ICML 2025. Code is available:\n  https://github.com/Dawn-LX/CausalCache-VDM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16375v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01941v2",
                "updated": "2025-05-21T10:37:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    37,
                    50,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-04T02:23:06Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    2,
                    23,
                    6,
                    1,
                    35,
                    0
                ],
                "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?"
                },
                "summary": "This paper investigates an underexplored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. Although existing methods achieve impressive compression ratios\non long-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive benchmark KVFundaBench to\nsystematically evaluate the effects of KV cache compression across diverse\nfundamental LLM capabilities, spanning world knowledge, commonsense reasoning,\narithmetic reasoning, code generation, safety, and long-context understanding\nand generation.Our analysis reveals serval key findings: (1)\n\\textit{Task-Dependent Degradation}; (2) \\textit{Model-Type Robustness} (3)\n\\textit{Prompt Length Vulnerability}; (4) \\textit{Chunk-Level Superiority}; (5)\n\\textit{Prompt-Gain Sensitivity}; (6) \\textit{Long-Context Generation\nSensitivity}. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates an underexplored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. Although existing methods achieve impressive compression ratios\non long-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive benchmark KVFundaBench to\nsystematically evaluate the effects of KV cache compression across diverse\nfundamental LLM capabilities, spanning world knowledge, commonsense reasoning,\narithmetic reasoning, code generation, safety, and long-context understanding\nand generation.Our analysis reveals serval key findings: (1)\n\\textit{Task-Dependent Degradation}; (2) \\textit{Model-Type Robustness} (3)\n\\textit{Prompt Length Vulnerability}; (4) \\textit{Chunk-Level Superiority}; (5)\n\\textit{Prompt-Gain Sensitivity}; (6) \\textit{Long-Context Generation\nSensitivity}. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Xiuze Zhou"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15347v1",
                "updated": "2025-05-21T10:20:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    20,
                    46,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T10:20:46Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    20,
                    46,
                    2,
                    141,
                    0
                ],
                "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via\n  Isolated Key-Value Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via\n  Isolated Key-Value Cache Management"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in multi-turn\nconversational applications, where the management of the Key-Value (KV) Cache\npresents a significant bottleneck. The linear growth of the KV Cache with\ndialogue history imposes substantial computational costs, and existing eviction\nstrategies often degrade performance by repeatedly compressing early\nconversational context, leading to information loss and context forgetting.\nThis paper introduces FlowKV, a novel \\textbf{multi-turn isolation mechanism}\nfor KV Cache management, which can be applied to any KV Cache compression\nmethod without training. FlowKV's core innovation is a multi-turn isolation\nmechanism that preserves the accumulated compressed KV cache from past turns.\nCompression is then strategically applied only to the newly generated KV pairs\nof the latest completed turn, effectively preventing the re-compression of\nolder context and thereby mitigating catastrophic forgetting. Our results\ndemonstrate that FlowKV consistently and significantly outperforms baseline\nstrategies in maintaining instruction-following accuracy and user preference\nretention from 10.90\\% to 75.40\\%, particularly in later conversational turns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in multi-turn\nconversational applications, where the management of the Key-Value (KV) Cache\npresents a significant bottleneck. The linear growth of the KV Cache with\ndialogue history imposes substantial computational costs, and existing eviction\nstrategies often degrade performance by repeatedly compressing early\nconversational context, leading to information loss and context forgetting.\nThis paper introduces FlowKV, a novel \\textbf{multi-turn isolation mechanism}\nfor KV Cache management, which can be applied to any KV Cache compression\nmethod without training. FlowKV's core innovation is a multi-turn isolation\nmechanism that preserves the accumulated compressed KV cache from past turns.\nCompression is then strategically applied only to the newly generated KV pairs\nof the latest completed turn, effectively preventing the re-compression of\nolder context and thereby mitigating catastrophic forgetting. Our results\ndemonstrate that FlowKV consistently and significantly outperforms baseline\nstrategies in maintaining instruction-following accuracy and user preference\nretention from 10.90\\% to 75.40\\%, particularly in later conversational turns."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15269v1",
                "updated": "2025-05-21T08:47:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    8,
                    47,
                    15,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T08:47:15Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    8,
                    47,
                    15,
                    2,
                    141,
                    0
                ],
                "title": "LiveVLM: Efficient Online Video Understanding via Streaming-Oriented KV\n  Cache and Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiveVLM: Efficient Online Video Understanding via Streaming-Oriented KV\n  Cache and Retrieval"
                },
                "summary": "Recent developments in Video Large Language Models (Video LLMs) have enabled\nmodels to process long video sequences and demonstrate remarkable performance.\nNonetheless, studies predominantly focus on offline video question answering,\nneglecting memory usage and response speed that are essential in various\nreal-world applications, such as Deepseek services, autonomous driving, and\nrobotics. To mitigate these challenges, we propose $\\textbf{LiveVLM}$, a\ntraining-free framework specifically designed for streaming, online video\nunderstanding and real-time interaction. Unlike existing works that process\nvideos only after one question is posed, LiveVLM constructs an innovative\nstreaming-oriented KV cache to process video streams in real-time, retain\nlong-term video details and eliminate redundant KVs, ensuring prompt responses\nto user queries. For continuous video streams, LiveVLM generates and compresses\nvideo key-value tensors (video KVs) to reserve visual information while\nimproving memory efficiency. Furthermore, when a new question is proposed,\nLiveVLM incorporates an online question-answering process that efficiently\nfetches both short-term and long-term visual information, while minimizing\ninterference from redundant context. Extensive experiments demonstrate that\nLiveVLM enables the foundation LLaVA-OneVision model to process 44$\\times$\nnumber of frames on the same device, and achieves up to 5$\\times$ speedup in\nresponse speed compared with SoTA online methods at an input of 256 frames,\nwhile maintaining the same or better model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in Video Large Language Models (Video LLMs) have enabled\nmodels to process long video sequences and demonstrate remarkable performance.\nNonetheless, studies predominantly focus on offline video question answering,\nneglecting memory usage and response speed that are essential in various\nreal-world applications, such as Deepseek services, autonomous driving, and\nrobotics. To mitigate these challenges, we propose $\\textbf{LiveVLM}$, a\ntraining-free framework specifically designed for streaming, online video\nunderstanding and real-time interaction. Unlike existing works that process\nvideos only after one question is posed, LiveVLM constructs an innovative\nstreaming-oriented KV cache to process video streams in real-time, retain\nlong-term video details and eliminate redundant KVs, ensuring prompt responses\nto user queries. For continuous video streams, LiveVLM generates and compresses\nvideo key-value tensors (video KVs) to reserve visual information while\nimproving memory efficiency. Furthermore, when a new question is proposed,\nLiveVLM incorporates an online question-answering process that efficiently\nfetches both short-term and long-term visual information, while minimizing\ninterference from redundant context. Extensive experiments demonstrate that\nLiveVLM enables the foundation LLaVA-OneVision model to process 44$\\times$\nnumber of frames on the same device, and achieves up to 5$\\times$ speedup in\nresponse speed compared with SoTA online methods at an input of 256 frames,\nwhile maintaining the same or better model performance."
                },
                "authors": [
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Qihao Jin"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jieru Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jieru Zhao"
                },
                "author": "Jieru Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01068v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01068v2",
                "updated": "2025-05-21T06:45:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    6,
                    45,
                    58,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-03T05:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "title": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation"
                },
                "summary": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial key-value (KV) caches to store contextual information,\nwhich can heavily burden computational efficiency and memory usage. Previous\nefforts to compress these KV caches primarily focused on reducing memory\ndemands but were limited in enhancing latency. To address this issue, we\nintroduce FastKV, a KV cache compression method designed to reduce latency for\nlong-context inference. FastKV improves processing speed while preserving\naccuracy by adopting Token-Selective Propagation (TSP). This approach preserves\nfull-context information in early layers of LLMs and selectively propagates\nonly a portion of this information in later layers. This design enables FastKV\nto minimize redundant computation without sacrificing contextual fidelity. Our\nexperimental results show that FastKV achieves up to 1.97$\\times$ and\n4.82$\\times$ improvements in time-to-first-token (TTFT) and throughput,\nrespectively, compared to baseline without KV cache compression. Moreover,\nFastKV successfully maintains accuracy within 1\\% of the baseline on\nlong-context benchmarks. Our code is available at\nhttps://github.com/dongwonjo/FastKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial key-value (KV) caches to store contextual information,\nwhich can heavily burden computational efficiency and memory usage. Previous\nefforts to compress these KV caches primarily focused on reducing memory\ndemands but were limited in enhancing latency. To address this issue, we\nintroduce FastKV, a KV cache compression method designed to reduce latency for\nlong-context inference. FastKV improves processing speed while preserving\naccuracy by adopting Token-Selective Propagation (TSP). This approach preserves\nfull-context information in early layers of LLMs and selectively propagates\nonly a portion of this information in later layers. This design enables FastKV\nto minimize redundant computation without sacrificing contextual fidelity. Our\nexperimental results show that FastKV achieves up to 1.97$\\times$ and\n4.82$\\times$ improvements in time-to-first-token (TTFT) and throughput,\nrespectively, compared to baseline without KV cache compression. Moreover,\nFastKV successfully maintains accuracy within 1\\% of the baseline on\nlong-context benchmarks. Our code is available at\nhttps://github.com/dongwonjo/FastKV."
                },
                "authors": [
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01068v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01068v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15859v1",
                "updated": "2025-05-21T04:32:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    4,
                    32,
                    35,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T04:32:35Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    4,
                    32,
                    35,
                    2,
                    141,
                    0
                ],
                "title": "AutoData: A Multi-Agent System for Open Web Data Collection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoData: A Multi-Agent System for Open Web Data Collection"
                },
                "summary": "The exponential growth of data-driven systems and AI technologies has\nintensified the demand for high-quality web-sourced datasets. While existing\ndatasets have proven valuable, conventional web data collection approaches face\nsignificant limitations in terms of human effort and scalability. Current\ndata-collecting solutions fall into two categories: wrapper-based methods that\nstruggle with adaptability and reproducibility, and large language model\n(LLM)-based approaches that incur substantial computational and financial\ncosts. To address these challenges, we propose AutoData, a novel multi-agent\nsystem for Automated web Data collection, that requires minimal human\nintervention, i.e., only necessitating a natural language instruction\nspecifying the desired dataset. In addition, AutoData is designed with a robust\nmulti-agent architecture, featuring a novel oriented message hypergraph\ncoordinated by a central task manager, to efficiently organize agents across\nresearch and development squads. Besides, we introduce a novel hypergraph cache\nsystem to advance the multi-agent collaboration process that enables efficient\nautomated data collection and mitigates the token cost issues prevalent in\nexisting LLM-based systems. Moreover, we introduce Instruct2DS, a new benchmark\ndataset supporting live data collection from web sources across three domains:\nacademic, finance, and sports. Comprehensive evaluations over Instruct2DS and\nthree existing benchmark datasets demonstrate AutoData's superior performance\ncompared to baseline methods. Case studies on challenging tasks such as picture\nbook collection and paper extraction from surveys further validate its\napplicability. Our source code and dataset are available at\nhttps://github.com/GraphResearcher/AutoData.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data-driven systems and AI technologies has\nintensified the demand for high-quality web-sourced datasets. While existing\ndatasets have proven valuable, conventional web data collection approaches face\nsignificant limitations in terms of human effort and scalability. Current\ndata-collecting solutions fall into two categories: wrapper-based methods that\nstruggle with adaptability and reproducibility, and large language model\n(LLM)-based approaches that incur substantial computational and financial\ncosts. To address these challenges, we propose AutoData, a novel multi-agent\nsystem for Automated web Data collection, that requires minimal human\nintervention, i.e., only necessitating a natural language instruction\nspecifying the desired dataset. In addition, AutoData is designed with a robust\nmulti-agent architecture, featuring a novel oriented message hypergraph\ncoordinated by a central task manager, to efficiently organize agents across\nresearch and development squads. Besides, we introduce a novel hypergraph cache\nsystem to advance the multi-agent collaboration process that enables efficient\nautomated data collection and mitigates the token cost issues prevalent in\nexisting LLM-based systems. Moreover, we introduce Instruct2DS, a new benchmark\ndataset supporting live data collection from web sources across three domains:\nacademic, finance, and sports. Comprehensive evaluations over Instruct2DS and\nthree existing benchmark datasets demonstrate AutoData's superior performance\ncompared to baseline methods. Case studies on challenging tasks such as picture\nbook collection and paper extraction from surveys further validate its\napplicability. Our source code and dataset are available at\nhttps://github.com/GraphResearcher/AutoData."
                },
                "authors": [
                    {
                        "name": "Tianyi Ma"
                    },
                    {
                        "name": "Yiyue Qian"
                    },
                    {
                        "name": "Zheyuan Zhang"
                    },
                    {
                        "name": "Zehong Wang"
                    },
                    {
                        "name": "Xiaoye Qian"
                    },
                    {
                        "name": "Feifan Bai"
                    },
                    {
                        "name": "Yifan Ding"
                    },
                    {
                        "name": "Xuwei Luo"
                    },
                    {
                        "name": "Shinan Zhang"
                    },
                    {
                        "name": "Keerthiram Murugesan"
                    },
                    {
                        "name": "Chuxu Zhang"
                    },
                    {
                        "name": "Yanfang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Yanfang Ye"
                },
                "author": "Yanfang Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13544v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13544v2",
                "updated": "2025-05-21T01:34:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    1,
                    34,
                    19,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-19T02:09:41Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    2,
                    9,
                    41,
                    0,
                    139,
                    0
                ],
                "title": "Multi-head Temporal Latent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Temporal Latent Attention"
                },
                "summary": "While Transformer self-attention offers strong parallelism, the Key-Value\n(KV) cache grows linearly with sequence length and becomes a bottleneck for\ninference efficiency. Multi-head latent attention was recently developed to\ncompress the KV cache into a low-rank latent space. This paper proposes\nMulti-head Temporal Latent Attention (MTLA), which further reduces the KV cache\nsize along the temporal dimension, greatly lowering the memory footprint of\nself-attention inference. MTLA employs a hyper-network to dynamically merge\ntemporally adjacent KV cache vectors. To address the mismatch between the\ncompressed KV cache and processed sequence lengths, a stride-aware causal mask\nis proposed to ensure efficient parallel training and consistency with\ninference behaviour. Experiments across tasks, including speech translation,\nspeech recognition, speech understanding and text summarisation, demonstrate\nthat MTLA achieves competitive performance compared to standard Multi-Head\nAttention (MHA), while greatly improving inference speed and GPU memory usage.\nFor example, on a English-German speech translation task, MTLA achieves a 5.3x\nspeedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,\nwhile maintaining translation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer self-attention offers strong parallelism, the Key-Value\n(KV) cache grows linearly with sequence length and becomes a bottleneck for\ninference efficiency. Multi-head latent attention was recently developed to\ncompress the KV cache into a low-rank latent space. This paper proposes\nMulti-head Temporal Latent Attention (MTLA), which further reduces the KV cache\nsize along the temporal dimension, greatly lowering the memory footprint of\nself-attention inference. MTLA employs a hyper-network to dynamically merge\ntemporally adjacent KV cache vectors. To address the mismatch between the\ncompressed KV cache and processed sequence lengths, a stride-aware causal mask\nis proposed to ensure efficient parallel training and consistency with\ninference behaviour. Experiments across tasks, including speech translation,\nspeech recognition, speech understanding and text summarisation, demonstrate\nthat MTLA achieves competitive performance compared to standard Multi-Head\nAttention (MHA), while greatly improving inference speed and GPU memory usage.\nFor example, on a English-German speech translation task, MTLA achieves a 5.3x\nspeedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,\nwhile maintaining translation quality."
                },
                "authors": [
                    {
                        "name": "Keqi Deng"
                    },
                    {
                        "name": "Philip C. Woodland"
                    }
                ],
                "author_detail": {
                    "name": "Philip C. Woodland"
                },
                "author": "Philip C. Woodland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13544v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14992v1",
                "updated": "2025-05-21T00:40:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    0,
                    40,
                    5,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T00:40:05Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    0,
                    40,
                    5,
                    2,
                    141,
                    0
                ],
                "title": "Effective and Efficient Schema-aware Information Extraction Using\n  On-Device Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective and Efficient Schema-aware Information Extraction Using\n  On-Device Large Language Models"
                },
                "summary": "Information extraction (IE) plays a crucial role in natural language\nprocessing (NLP) by converting unstructured text into structured knowledge.\nDeploying computationally intensive large language models (LLMs) on\nresource-constrained devices for information extraction is challenging,\nparticularly due to issues like hallucinations, limited context length, and\nhigh latency-especially when handling diverse extraction schemas. To address\nthese challenges, we propose a two-stage information extraction approach\nadapted for on-device LLMs, called Dual-LoRA with Incremental Schema Caching\n(DLISC), which enhances both schema identification and schema-aware extraction\nin terms of effectiveness and efficiency. In particular, DLISC adopts an\nIdentification LoRA module for retrieving the most relevant schemas to a given\nquery, and an Extraction LoRA module for performing information extraction\nbased on the previously selected schemas. To accelerate extraction inference,\nIncremental Schema Caching is incorporated to reduce redundant computation,\nsubstantially improving efficiency. Extensive experiments across multiple\ninformation extraction datasets demonstrate notable improvements in both\neffectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information extraction (IE) plays a crucial role in natural language\nprocessing (NLP) by converting unstructured text into structured knowledge.\nDeploying computationally intensive large language models (LLMs) on\nresource-constrained devices for information extraction is challenging,\nparticularly due to issues like hallucinations, limited context length, and\nhigh latency-especially when handling diverse extraction schemas. To address\nthese challenges, we propose a two-stage information extraction approach\nadapted for on-device LLMs, called Dual-LoRA with Incremental Schema Caching\n(DLISC), which enhances both schema identification and schema-aware extraction\nin terms of effectiveness and efficiency. In particular, DLISC adopts an\nIdentification LoRA module for retrieving the most relevant schemas to a given\nquery, and an Extraction LoRA module for performing information extraction\nbased on the previously selected schemas. To accelerate extraction inference,\nIncremental Schema Caching is incorporated to reduce redundant computation,\nsubstantially improving efficiency. Extensive experiments across multiple\ninformation extraction datasets demonstrate notable improvements in both\neffectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Zhihao Wen"
                    },
                    {
                        "name": "Sheng Liang"
                    },
                    {
                        "name": "Yaxiong Wu"
                    },
                    {
                        "name": "Yongyue Zhang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14969v1",
                "updated": "2025-05-20T23:12:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    23,
                    12,
                    16,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T23:12:16Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    23,
                    12,
                    16,
                    1,
                    140,
                    0
                ],
                "title": "STree: Speculative Tree Decoding for Hybrid State-Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STree: Speculative Tree Decoding for Hybrid State-Space Models"
                },
                "summary": "Speculative decoding is a technique to leverage hardware concurrency to\nimprove the efficiency of large-scale autoregressive (AR) Transformer models by\nenabling multiple steps of token generation in a single forward pass.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead to\ncurrent SSM state update implementations. With the algorithm, we describe a\nhardware-aware implementation that improves naive application of AR Transformer\ntree-based speculative decoding methods to SSMs. Furthermore, we outperform\nvanilla speculative decoding with SSMs even with a baseline drafting model and\ntree structure on three different benchmarks, opening up opportunities for\nfurther speed up with SSM and hybrid model inference. Code will be released\nupon paper acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a technique to leverage hardware concurrency to\nimprove the efficiency of large-scale autoregressive (AR) Transformer models by\nenabling multiple steps of token generation in a single forward pass.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead to\ncurrent SSM state update implementations. With the algorithm, we describe a\nhardware-aware implementation that improves naive application of AR Transformer\ntree-based speculative decoding methods to SSMs. Furthermore, we outperform\nvanilla speculative decoding with SSMs even with a baseline drafting model and\ntree structure on three different benchmarks, opening up opportunities for\nfurther speed up with SSM and hybrid model inference. Code will be released\nupon paper acceptance."
                },
                "authors": [
                    {
                        "name": "Yangchao Wu"
                    },
                    {
                        "name": "Zongyue Qin"
                    },
                    {
                        "name": "Alex Wong"
                    },
                    {
                        "name": "Stefano Soatto"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Soatto"
                },
                "author": "Stefano Soatto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18001v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18001v3",
                "updated": "2025-05-20T18:49:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    18,
                    49,
                    30,
                    1,
                    140,
                    0
                ],
                "published": "2025-04-25T01:10:49Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    1,
                    10,
                    49,
                    4,
                    115,
                    0
                ],
                "title": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data"
                },
                "summary": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc."
                },
                "authors": [
                    {
                        "name": "Daniel Zavorotny"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "arxiv_comment": "11 pages, 11 figures, EGPGV25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18001v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18001v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15364v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15364v3",
                "updated": "2025-05-20T17:50:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    50,
                    11,
                    1,
                    140,
                    0
                ],
                "published": "2025-04-21T18:12:46Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    18,
                    12,
                    46,
                    0,
                    111,
                    0
                ],
                "title": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments"
                },
                "summary": "We demonstrate that geometrically distinctive keys during LLM inference tend\nto have high attention scores. Based on the phenomenon we propose KeyDiff, a\ntraining-free KV cache eviction method based solely on key similarity. Unlike\nother KV cache eviction methods, KeyDiff can process arbitrarily long prompts\nwithin strict resource constraints and efficiently generate responses. We\nprovide a theoretical basis for KeyDiff by relating key diversity with\nattention scores. These results imply KeyDiff can efficiently identify the most\nimportant tokens to retain. Notably KeyDiff does not rely on attention scores,\nallowing the use of optimized attention mechanisms like FlashAttention. Under a\nstrict memory allowance, we demonstrate the effectiveness of KeyDiff for the\nLlama and Qwen model families by observing a performance gap of less than 0.04%\nwith 8K cache budget ($\\sim$23% KV cache reduction) from the non-evicting\nbaseline on LongBench for Llama 3.1-8B and Llama 3.2-3B. We also observe near\nbaseline performance for Deepseek-R1-Distill-Llama-8B on the Math500 reasoning\nbenchmark and decrease end-to-end inference latency by up to 30% compared to\nthe other token-eviction methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate that geometrically distinctive keys during LLM inference tend\nto have high attention scores. Based on the phenomenon we propose KeyDiff, a\ntraining-free KV cache eviction method based solely on key similarity. Unlike\nother KV cache eviction methods, KeyDiff can process arbitrarily long prompts\nwithin strict resource constraints and efficiently generate responses. We\nprovide a theoretical basis for KeyDiff by relating key diversity with\nattention scores. These results imply KeyDiff can efficiently identify the most\nimportant tokens to retain. Notably KeyDiff does not rely on attention scores,\nallowing the use of optimized attention mechanisms like FlashAttention. Under a\nstrict memory allowance, we demonstrate the effectiveness of KeyDiff for the\nLlama and Qwen model families by observing a performance gap of less than 0.04%\nwith 8K cache budget ($\\sim$23% KV cache reduction) from the non-evicting\nbaseline on LongBench for Llama 3.1-8B and Llama 3.2-3B. We also observe near\nbaseline performance for Deepseek-R1-Distill-Llama-8B on the Math500 reasoning\nbenchmark and decrease end-to-end inference latency by up to 30% compared to\nthe other token-eviction methods."
                },
                "authors": [
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew J Morse"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15364v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15364v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v4",
                "updated": "2025-05-20T16:29:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    29,
                    52,
                    1,
                    140,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose a novel batching and scheduling algorithm\nthat minimizes inference latency while effectively managing the KV cache's\nmemory.\n  More specifically, we make the following contributions. First, to evaluate\nthe performance of online algorithms for scheduling in LLM inference, we\nintroduce a hindsight optimal benchmark, formulated as an integer program that\ncomputes the minimum total inference latency under full future information.\nSecond, we prove that no deterministic online algorithm can achieve a constant\ncompetitive ratio when the arrival process is arbitrary. Third, motivated by\nthe computational intractability of solving the integer program at scale, we\npropose a polynomial-time online scheduling algorithm and show that under\ncertain conditions it can achieve a constant competitive ratio. We also\ndemonstrate our algorithm's strong empirical performance by comparing it to the\nhindsight optimal in a synthetic dataset. Finally, we conduct empirical\nevaluations on a real-world public LLM inference dataset, simulating the\nLlama2-70B model on A100 GPUs, and show that our algorithm significantly\noutperforms the benchmark algorithms. Overall, our results offer a path toward\nmore sustainable and cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose a novel batching and scheduling algorithm\nthat minimizes inference latency while effectively managing the KV cache's\nmemory.\n  More specifically, we make the following contributions. First, to evaluate\nthe performance of online algorithms for scheduling in LLM inference, we\nintroduce a hindsight optimal benchmark, formulated as an integer program that\ncomputes the minimum total inference latency under full future information.\nSecond, we prove that no deterministic online algorithm can achieve a constant\ncompetitive ratio when the arrival process is arbitrary. Third, motivated by\nthe computational intractability of solving the integer program at scale, we\npropose a polynomial-time online scheduling algorithm and show that under\ncertain conditions it can achieve a constant competitive ratio. We also\ndemonstrate our algorithm's strong empirical performance by comparing it to the\nhindsight optimal in a synthetic dataset. Finally, we conduct empirical\nevaluations on a real-world public LLM inference dataset, simulating the\nLlama2-70B model on A100 GPUs, and show that our algorithm significantly\noutperforms the benchmark algorithms. Overall, our results offer a path toward\nmore sustainable and cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Konstantina Mellou"
                    },
                    {
                        "name": "Marco Molinaro"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14427v1",
                "updated": "2025-05-20T14:38:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    38,
                    34,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:38:34Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    38,
                    34,
                    1,
                    140,
                    0
                ],
                "title": "SkyMemory: A LEO Edge Cache for Transformer Inference Optimization and\n  Scale Out",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyMemory: A LEO Edge Cache for Transformer Inference Optimization and\n  Scale Out"
                },
                "summary": "We expand the scope of cache memory to include LEO constellations, which are\nhighly distributed systems with thousands of satellites connected with\nfree-space optics inter-satellite links (ISL) always only one hop from any\npoint on earth. We show how to increase the number of cache hits and improve\nthe speed of inference for the important use case of LLMs. These benefits apply\nnot only to LLMs, both terrestrially hosted and on satellites, but also\ngeneralize to any cache distributed over multiple locations that needs to be\naccessed in a timely manner. We show the benefit of our key value cache (KVC)\nprotocol in simulations and present a proof-of-concept implementation of the\nprotocol for KVCs on a testbed comprising 5 Intel NUC Linux mini PCs hosting a\n19x5 constellation, with an NVIDIA Jetson Nano 8GB GPU hosting the LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We expand the scope of cache memory to include LEO constellations, which are\nhighly distributed systems with thousands of satellites connected with\nfree-space optics inter-satellite links (ISL) always only one hop from any\npoint on earth. We show how to increase the number of cache hits and improve\nthe speed of inference for the important use case of LLMs. These benefits apply\nnot only to LLMs, both terrestrially hosted and on satellites, but also\ngeneralize to any cache distributed over multiple locations that needs to be\naccessed in a timely manner. We show the benefit of our key value cache (KVC)\nprotocol in simulations and present a proof-of-concept implementation of the\nprotocol for KVCs on a testbed comprising 5 Intel NUC Linux mini PCs hosting a\n19x5 constellation, with an NVIDIA Jetson Nano 8GB GPU hosting the LLM."
                },
                "authors": [
                    {
                        "name": "Thomas Sandholm"
                    },
                    {
                        "name": "Sayandev Mukherjee"
                    },
                    {
                        "name": "Lin Cheng"
                    },
                    {
                        "name": "Bernardo A. Huberman"
                    }
                ],
                "author_detail": {
                    "name": "Bernardo A. Huberman"
                },
                "author": "Bernardo A. Huberman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14398v1",
                "updated": "2025-05-20T14:14:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    14,
                    38,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:14:38Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    14,
                    38,
                    1,
                    140,
                    0
                ],
                "title": "Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable\n  Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable\n  Computation"
                },
                "summary": "While humans naturally learn and adapt from past experiences, large language\nmodels (LLMs) and their agentic counterparts struggle to retain reasoning from\nprevious tasks and apply them in future contexts. To address this limitation,\nwe propose a novel framework, log-augmented generation (LAG) that directly\nreuses prior computation and reasoning from past logs at test time to enhance\nmodel's ability to learn from previous tasks and perform better on new, unseen\nchallenges, all while keeping the system efficient and scalable. Specifically,\nour system represents task logs using key-value (KV) caches, encoding the full\nreasoning context of prior tasks while storing KV caches for only a selected\nsubset of tokens. When a new task arises, LAG retrieves the KV values from\nrelevant logs to augment generation. Our approach differs from reflection-based\nmemory mechanisms by directly reusing prior reasoning and computations without\nrequiring additional steps for knowledge extraction or distillation. Our method\nalso goes beyond existing KV caching techniques, which primarily target\nefficiency gains rather than improving accuracy. Experiments on knowledge- and\nreasoning-intensive datasets demonstrate that our method significantly\noutperforms standard agentic systems that do not utilize logs, as well as\nexisting solutions based on reflection and KV cache techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While humans naturally learn and adapt from past experiences, large language\nmodels (LLMs) and their agentic counterparts struggle to retain reasoning from\nprevious tasks and apply them in future contexts. To address this limitation,\nwe propose a novel framework, log-augmented generation (LAG) that directly\nreuses prior computation and reasoning from past logs at test time to enhance\nmodel's ability to learn from previous tasks and perform better on new, unseen\nchallenges, all while keeping the system efficient and scalable. Specifically,\nour system represents task logs using key-value (KV) caches, encoding the full\nreasoning context of prior tasks while storing KV caches for only a selected\nsubset of tokens. When a new task arises, LAG retrieves the KV values from\nrelevant logs to augment generation. Our approach differs from reflection-based\nmemory mechanisms by directly reusing prior reasoning and computations without\nrequiring additional steps for knowledge extraction or distillation. Our method\nalso goes beyond existing KV caching techniques, which primarily target\nefficiency gains rather than improving accuracy. Experiments on knowledge- and\nreasoning-intensive datasets demonstrate that our method significantly\noutperforms standard agentic systems that do not utilize logs, as well as\nexisting solutions based on reflection and KV cache techniques."
                },
                "authors": [
                    {
                        "name": "Peter Baile Chen"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Samuel Madden"
                    },
                    {
                        "name": "Jacob Andreas"
                    },
                    {
                        "name": "Michael Cafarella"
                    }
                ],
                "author_detail": {
                    "name": "Michael Cafarella"
                },
                "author": "Michael Cafarella",
                "arxiv_comment": "Data and code are available at https://peterbaile.github.io/lag/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14085v1",
                "updated": "2025-05-20T08:46:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    8,
                    46,
                    23,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T08:46:23Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    8,
                    46,
                    23,
                    1,
                    140,
                    0
                ],
                "title": "CE-LSLM: Efficient Large-Small Language Model Inference and\n  Communication via Cloud-Edge Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CE-LSLM: Efficient Large-Small Language Model Inference and\n  Communication via Cloud-Edge Collaboration"
                },
                "summary": "Emerging intelligent service scenarios in 6G communication impose stringent\nrequirements for low latency, high reliability, and privacy preservation.\nGenerative large language models (LLMs) are gradually becoming key enablers for\nthe integration of semantic communication and computation. However, due to the\nlimited computational resources of edge devices and the increasing complexity\nof heterogeneous terminal access, existing centralized inference approaches\nfail to meet the dual demands of response efficiency and data privacy in\nedge-side inference tasks. To address these challenges, this paper proposes a\nnovel collaborative inference architecture that integrates cloud-based LLMs\nwith edge-deployed small language models (SLMs), enabling dynamic scheduling\nand sharing of semantic-level intermediate states, and establishing a unified\ncomputation-communication paradigm tailored for 6G networks. Specifically, a\nkey-value (KV) cache reuse mechanism is introduced to enhance the semantic\nunderstanding of edge models through contextual guidance from the cloud, while\nsignificantly reducing edge-side computational and storage overhead.\nFurthermore, a cross-node parallel scheduling mechanism is proposed to achieve\nasynchronous coordination between model state loading and decoding computation,\nthereby improving edge responsiveness. In addition, we investigate layer\nalignment and representation compression strategies between heterogeneous\nmodels to alleviate the communication burden on the edge. Experimental results\ndemonstrate that the proposed architecture exhibits superior adaptability and\nscalability in terms of inference latency, system stability, and concurrent\nprocessing capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging intelligent service scenarios in 6G communication impose stringent\nrequirements for low latency, high reliability, and privacy preservation.\nGenerative large language models (LLMs) are gradually becoming key enablers for\nthe integration of semantic communication and computation. However, due to the\nlimited computational resources of edge devices and the increasing complexity\nof heterogeneous terminal access, existing centralized inference approaches\nfail to meet the dual demands of response efficiency and data privacy in\nedge-side inference tasks. To address these challenges, this paper proposes a\nnovel collaborative inference architecture that integrates cloud-based LLMs\nwith edge-deployed small language models (SLMs), enabling dynamic scheduling\nand sharing of semantic-level intermediate states, and establishing a unified\ncomputation-communication paradigm tailored for 6G networks. Specifically, a\nkey-value (KV) cache reuse mechanism is introduced to enhance the semantic\nunderstanding of edge models through contextual guidance from the cloud, while\nsignificantly reducing edge-side computational and storage overhead.\nFurthermore, a cross-node parallel scheduling mechanism is proposed to achieve\nasynchronous coordination between model state loading and decoding computation,\nthereby improving edge responsiveness. In addition, we investigate layer\nalignment and representation compression strategies between heterogeneous\nmodels to alleviate the communication burden on the edge. Experimental results\ndemonstrate that the proposed architecture exhibits superior adaptability and\nscalability in terms of inference latency, system stability, and concurrent\nprocessing capacity."
                },
                "authors": [
                    {
                        "name": "Pengyan Zhu"
                    },
                    {
                        "name": "Tingting Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yang"
                },
                "author": "Tingting Yang",
                "arxiv_comment": "14 pages, 7 figures including subplots",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13002v2",
                "updated": "2025-05-20T07:34:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    7,
                    34,
                    45,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-19T11:41:21Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    11,
                    41,
                    21,
                    0,
                    139,
                    0
                ],
                "title": "PIM-malloc: A Fast and Scalable Dynamic Memory Allocator for\n  Processing-In-Memory (PIM) Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM-malloc: A Fast and Scalable Dynamic Memory Allocator for\n  Processing-In-Memory (PIM) Architectures"
                },
                "summary": "Dynamic memory allocation is essential in modern programming but remains\nunder-supported in current PIM devices. In this work, we first conduct a design\nspace exploration of PIM memory allocators, examining optimal metadata\nplacement and management strategies. Building on these insights, we propose\nPIM-malloc, a fast and scalable allocator for real PIM hardware, improving\nallocation performance by $66\\times$. We further enhance this design with a\nlightweight, per-PIM core hardware cache for dynamic allocation, achieving an\nadditional $31\\%$ performance gain. Finally, we demonstrate the effectiveness\nof PIM-malloc using a dynamic graph update workload, achieving a $28\\times$\nthroughput increase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic memory allocation is essential in modern programming but remains\nunder-supported in current PIM devices. In this work, we first conduct a design\nspace exploration of PIM memory allocators, examining optimal metadata\nplacement and management strategies. Building on these insights, we propose\nPIM-malloc, a fast and scalable allocator for real PIM hardware, improving\nallocation performance by $66\\times$. We further enhance this design with a\nlightweight, per-PIM core hardware cache for dynamic allocation, achieving an\nadditional $31\\%$ performance gain. Finally, we demonstrate the effectiveness\nof PIM-malloc using a dynamic graph update workload, achieving a $28\\times$\nthroughput increase."
                },
                "authors": [
                    {
                        "name": "Dongjae Lee"
                    },
                    {
                        "name": "Bongjoon Hyun"
                    },
                    {
                        "name": "Youngjin Kwon"
                    },
                    {
                        "name": "Minsoo Rhu"
                    }
                ],
                "author_detail": {
                    "name": "Minsoo Rhu"
                },
                "author": "Minsoo Rhu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14010v1",
                "updated": "2025-05-20T07:04:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    7,
                    4,
                    34,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T07:04:34Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    7,
                    4,
                    34,
                    1,
                    140,
                    0
                ],
                "title": "UHD Image Dehazing via anDehazeFormer with Atmospheric-aware KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UHD Image Dehazing via anDehazeFormer with Atmospheric-aware KV Cache"
                },
                "summary": "In this paper, we propose an efficient visual transformer framework for\nultra-high-definition (UHD) image dehazing that addresses the key challenges of\nslow training speed and high memory consumption for existing methods. Our\napproach introduces two key innovations: 1) an \\textbf{a}daptive\n\\textbf{n}ormalization mechanism inspired by the nGPT architecture that enables\nultra-fast and stable training with a network with a restricted range of\nparameter expressions; and 2) we devise an atmospheric scattering-aware KV\ncaching mechanism that dynamically optimizes feature preservation based on the\nphysical haze formation model. The proposed architecture improves the training\nconvergence speed by \\textbf{5 $\\times$} while reducing memory overhead,\nenabling real-time processing of 50 high-resolution images per second on an\nRTX4090 GPU. Experimental results show that our approach maintains\nstate-of-the-art dehazing quality while significantly improving computational\nefficiency for 4K/8K image restoration tasks. Furthermore, we provide a new\ndehazing image interpretable method with the help of an integrated gradient\nattribution map. Our code can be found here:\nhttps://anonymous.4open.science/r/anDehazeFormer-632E/README.md.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose an efficient visual transformer framework for\nultra-high-definition (UHD) image dehazing that addresses the key challenges of\nslow training speed and high memory consumption for existing methods. Our\napproach introduces two key innovations: 1) an \\textbf{a}daptive\n\\textbf{n}ormalization mechanism inspired by the nGPT architecture that enables\nultra-fast and stable training with a network with a restricted range of\nparameter expressions; and 2) we devise an atmospheric scattering-aware KV\ncaching mechanism that dynamically optimizes feature preservation based on the\nphysical haze formation model. The proposed architecture improves the training\nconvergence speed by \\textbf{5 $\\times$} while reducing memory overhead,\nenabling real-time processing of 50 high-resolution images per second on an\nRTX4090 GPU. Experimental results show that our approach maintains\nstate-of-the-art dehazing quality while significantly improving computational\nefficiency for 4K/8K image restoration tasks. Furthermore, we provide a new\ndehazing image interpretable method with the help of an integrated gradient\nattribution map. Our code can be found here:\nhttps://anonymous.4open.science/r/anDehazeFormer-632E/README.md."
                },
                "authors": [
                    {
                        "name": "Pu Wang"
                    },
                    {
                        "name": "Pengwen Dai"
                    },
                    {
                        "name": "Chen Wu"
                    },
                    {
                        "name": "Yeying Jin"
                    },
                    {
                        "name": "Dianjie Lu"
                    },
                    {
                        "name": "Guijuan Zhang"
                    },
                    {
                        "name": "Youshan Zhang"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhuoran Zheng"
                },
                "author": "Zhuoran Zheng",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01281v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01281v3",
                "updated": "2025-05-20T04:52:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    4,
                    52,
                    21,
                    1,
                    140,
                    0
                ],
                "published": "2025-04-02T01:16:10Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    16,
                    10,
                    2,
                    92,
                    0
                ],
                "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding"
                },
                "summary": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Akash Das"
                    },
                    {
                        "name": "Shivam Gupta"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01281v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01281v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.22664v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22664v1",
                "updated": "2025-05-28T17:59:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    59,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:59:59Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    59,
                    2,
                    148,
                    0
                ],
                "title": "Zero-Shot Vision Encoder Grafting via LLM Surrogates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Vision Encoder Grafting via LLM Surrogates"
                },
                "summary": "Vision language models (VLMs) typically pair a modestly sized vision encoder\nwith a large language model (LLM), e.g., Llama-70B, making the decoder the\nprimary computational burden during training. To reduce costs, a potential\npromising strategy is to first train the vision encoder using a small language\nmodel before transferring it to the large one. We construct small \"surrogate\nmodels\" that share the same embedding space and representation language as the\nlarge target LLM by directly inheriting its shallow layers. Vision encoders\ntrained on the surrogate can then be directly transferred to the larger model,\na process we call zero-shot grafting -- when plugged directly into the\nfull-size target LLM, the grafted pair surpasses the encoder-surrogate pair\nand, on some benchmarks, even performs on par with full decoder training with\nthe target LLM. Furthermore, our surrogate training approach reduces overall\nVLM training costs by ~45% when using Llama-70B as the decoder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision language models (VLMs) typically pair a modestly sized vision encoder\nwith a large language model (LLM), e.g., Llama-70B, making the decoder the\nprimary computational burden during training. To reduce costs, a potential\npromising strategy is to first train the vision encoder using a small language\nmodel before transferring it to the large one. We construct small \"surrogate\nmodels\" that share the same embedding space and representation language as the\nlarge target LLM by directly inheriting its shallow layers. Vision encoders\ntrained on the surrogate can then be directly transferred to the larger model,\na process we call zero-shot grafting -- when plugged directly into the\nfull-size target LLM, the grafted pair surpasses the encoder-surrogate pair\nand, on some benchmarks, even performs on par with full decoder training with\nthe target LLM. Furthermore, our surrogate training approach reduces overall\nVLM training costs by ~45% when using Llama-70B as the decoder."
                },
                "authors": [
                    {
                        "name": "Kaiyu Yue"
                    },
                    {
                        "name": "Vasu Singla"
                    },
                    {
                        "name": "Menglin Jia"
                    },
                    {
                        "name": "John Kirchenbauer"
                    },
                    {
                        "name": "Rifaa Qadri"
                    },
                    {
                        "name": "Zikui Cai"
                    },
                    {
                        "name": "Abhinav Bhatele"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Tom Goldstein"
                    }
                ],
                "author_detail": {
                    "name": "Tom Goldstein"
                },
                "author": "Tom Goldstein",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22664v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22664v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22663v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22663v1",
                "updated": "2025-05-28T17:59:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    57,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:59:57Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    57,
                    2,
                    148,
                    0
                ],
                "title": "Training Free Stylized Abstraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Free Stylized Abstraction"
                },
                "summary": "Stylized abstraction synthesizes visually exaggerated yet semantically\nfaithful representations of subjects, balancing recognizability with perceptual\ndistortion. Unlike image-to-image translation, which prioritizes structural\nfidelity, stylized abstraction demands selective retention of identity cues\nwhile embracing stylistic divergence, especially challenging for\nout-of-distribution individuals. We propose a training-free framework that\ngenerates stylized abstractions from a single image using inference-time\nscaling in vision-language models (VLLMs) to extract identity-relevant\nfeatures, and a novel cross-domain rectified flow inversion strategy that\nreconstructs structure based on style-dependent priors. Our method adapts\nstructural restoration dynamically through style-aware temporal scheduling,\nenabling high-fidelity reconstructions that honor both subject and style. It\nsupports multi-round abstraction-aware generation without fine-tuning. To\nevaluate this task, we introduce StyleBench, a GPT-based human-aligned metric\nsuited for abstract styles where pixel-level similarity fails. Experiments\nacross diverse abstraction (e.g., LEGO, knitted dolls, South Park) show strong\ngeneralization to unseen identities and styles in a fully open-source setup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stylized abstraction synthesizes visually exaggerated yet semantically\nfaithful representations of subjects, balancing recognizability with perceptual\ndistortion. Unlike image-to-image translation, which prioritizes structural\nfidelity, stylized abstraction demands selective retention of identity cues\nwhile embracing stylistic divergence, especially challenging for\nout-of-distribution individuals. We propose a training-free framework that\ngenerates stylized abstractions from a single image using inference-time\nscaling in vision-language models (VLLMs) to extract identity-relevant\nfeatures, and a novel cross-domain rectified flow inversion strategy that\nreconstructs structure based on style-dependent priors. Our method adapts\nstructural restoration dynamically through style-aware temporal scheduling,\nenabling high-fidelity reconstructions that honor both subject and style. It\nsupports multi-round abstraction-aware generation without fine-tuning. To\nevaluate this task, we introduce StyleBench, a GPT-based human-aligned metric\nsuited for abstract styles where pixel-level similarity fails. Experiments\nacross diverse abstraction (e.g., LEGO, knitted dolls, South Park) show strong\ngeneralization to unseen identities and styles in a fully open-source setup."
                },
                "authors": [
                    {
                        "name": "Aimon Rahman"
                    },
                    {
                        "name": "Kartik Narayan"
                    },
                    {
                        "name": "Vishal M. Patel"
                    }
                ],
                "author_detail": {
                    "name": "Vishal M. Patel"
                },
                "author": "Vishal M. Patel",
                "arxiv_comment": "Project Page: https://kartik-3004.github.io/TF-SA/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22663v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22663v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22662v1",
                "updated": "2025-05-28T17:59:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    53,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:59:53Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    53,
                    2,
                    148,
                    0
                ],
                "title": "AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models"
                },
                "summary": "The reasoning-capable large language models (LLMs) demonstrate strong\nperformance on complex reasoning tasks but often suffer from overthinking,\ngenerating unnecessarily long chain-of-thought (CoT) reasoning paths for easy\nreasoning questions, thereby increasing inference cost and latency. Recent\napproaches attempt to address this challenge by manually deciding when to apply\nlong or short reasoning. However, they lack the flexibility to adapt CoT length\ndynamically based on question complexity. In this paper, we propose Auto\nLong-Short Reasoning (AutoL2S), a dynamic and model-agnostic framework that\nenables LLMs to dynamically compress their generated reasoning path based on\nthe complexity of the reasoning question. AutoL2S enables a learned paradigm,\nin which LLMs themselves can decide when longer reasoning is necessary and when\nshorter reasoning suffices, by training on data annotated with our proposed\nmethod, which includes both long and short CoT paths and a special <EASY>\ntoken. We then use <EASY> token to indicate when the model can skip generating\nlengthy CoT reasoning. This proposed annotation strategy can enhance the LLMs'\nability to generate shorter CoT reasoning paths with improved quality after\ntraining. Extensive evaluation results show that AutoL2S reduces the length of\nreasoning generation by up to 57% without compromising performance,\ndemonstrating the effectiveness of AutoL2S for scalable and efficient LLM\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning-capable large language models (LLMs) demonstrate strong\nperformance on complex reasoning tasks but often suffer from overthinking,\ngenerating unnecessarily long chain-of-thought (CoT) reasoning paths for easy\nreasoning questions, thereby increasing inference cost and latency. Recent\napproaches attempt to address this challenge by manually deciding when to apply\nlong or short reasoning. However, they lack the flexibility to adapt CoT length\ndynamically based on question complexity. In this paper, we propose Auto\nLong-Short Reasoning (AutoL2S), a dynamic and model-agnostic framework that\nenables LLMs to dynamically compress their generated reasoning path based on\nthe complexity of the reasoning question. AutoL2S enables a learned paradigm,\nin which LLMs themselves can decide when longer reasoning is necessary and when\nshorter reasoning suffices, by training on data annotated with our proposed\nmethod, which includes both long and short CoT paths and a special <EASY>\ntoken. We then use <EASY> token to indicate when the model can skip generating\nlengthy CoT reasoning. This proposed annotation strategy can enhance the LLMs'\nability to generate shorter CoT reasoning paths with improved quality after\ntraining. Extensive evaluation results show that AutoL2S reduces the length of\nreasoning generation by up to 57% without compromising performance,\ndemonstrating the effectiveness of AutoL2S for scalable and efficient LLM\nreasoning."
                },
                "authors": [
                    {
                        "name": "Feng Luo"
                    },
                    {
                        "name": "Yu-Neng Chuang"
                    },
                    {
                        "name": "Guanchu Wang"
                    },
                    {
                        "name": "Hoang Anh Duy Le"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Hongyi Liu"
                    },
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Vladimir Braverman"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22661v1",
                "updated": "2025-05-28T17:59:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    43,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:59:43Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    43,
                    2,
                    148,
                    0
                ],
                "title": "GuessArena: Guess Who I Am? A Self-Adaptive Framework for Evaluating\n  LLMs in Domain-Specific Knowledge and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GuessArena: Guess Who I Am? A Self-Adaptive Framework for Evaluating\n  LLMs in Domain-Specific Knowledge and Reasoning"
                },
                "summary": "The evaluation of large language models (LLMs) has traditionally relied on\nstatic benchmarks, a paradigm that poses two major limitations: (1) predefined\ntest sets lack adaptability to diverse application domains, and (2)\nstandardized evaluation protocols often fail to capture fine-grained\nassessments of domain-specific knowledge and contextual reasoning abilities. To\novercome these challenges, we propose GuessArena, an adaptive evaluation\nframework grounded in adversarial game-based interactions. Inspired by the\ninteractive structure of the Guess Who I Am? game, our framework seamlessly\nintegrates dynamic domain knowledge modeling with progressive reasoning\nassessment to improve evaluation fidelity. Empirical studies across five\nvertical domains-finance, healthcare, manufacturing, information technology,\nand education-demonstrate that GuessArena effectively distinguishes LLMs in\nterms of domain knowledge coverage and reasoning chain completeness. Compared\nto conventional benchmarks, our method provides substantial advantages in\ninterpretability, scalability, and scenario adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of large language models (LLMs) has traditionally relied on\nstatic benchmarks, a paradigm that poses two major limitations: (1) predefined\ntest sets lack adaptability to diverse application domains, and (2)\nstandardized evaluation protocols often fail to capture fine-grained\nassessments of domain-specific knowledge and contextual reasoning abilities. To\novercome these challenges, we propose GuessArena, an adaptive evaluation\nframework grounded in adversarial game-based interactions. Inspired by the\ninteractive structure of the Guess Who I Am? game, our framework seamlessly\nintegrates dynamic domain knowledge modeling with progressive reasoning\nassessment to improve evaluation fidelity. Empirical studies across five\nvertical domains-finance, healthcare, manufacturing, information technology,\nand education-demonstrate that GuessArena effectively distinguishes LLMs in\nterms of domain knowledge coverage and reasoning chain completeness. Compared\nto conventional benchmarks, our method provides substantial advantages in\ninterpretability, scalability, and scenario adaptability."
                },
                "authors": [
                    {
                        "name": "Qingchen Yu"
                    },
                    {
                        "name": "Zifan Zheng"
                    },
                    {
                        "name": "Ding Chen"
                    },
                    {
                        "name": "Simin Niu"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "arxiv_comment": "Accepted by ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22659v1",
                "updated": "2025-05-28T17:59:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    29,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:59:29Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    29,
                    2,
                    148,
                    0
                ],
                "title": "Network Generating Processes With Self Exciting Arrival Times",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network Generating Processes With Self Exciting Arrival Times"
                },
                "summary": "In this paper, we propose a novel modeling framework for time-evolving\nnetworks allowing for long-term dependence in network features that update in\ncontinuous time. Dynamic network growth is functionally parameterized via the\nconditional intensity of a marked point process. This characterization enables\nflexible modeling of both the time of updates and the network updates\nthemselves, dependent on the entire left-continuous sample path. We propose a\npath-dependent nonlinear marked Hawkes process as an expressive platform for\nmodeling such data; its dynamic mark space embeds the time-evolving network. We\nestablish stability conditions, demonstrate simulation and subsequent feasible\nlikelihood-based inference through numerical study, and present an application\nto conference attendee social network data. The resulting methodology serves as\na general framework that can be readily adapted to a wide range of network\ntopologies and point process model specifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a novel modeling framework for time-evolving\nnetworks allowing for long-term dependence in network features that update in\ncontinuous time. Dynamic network growth is functionally parameterized via the\nconditional intensity of a marked point process. This characterization enables\nflexible modeling of both the time of updates and the network updates\nthemselves, dependent on the entire left-continuous sample path. We propose a\npath-dependent nonlinear marked Hawkes process as an expressive platform for\nmodeling such data; its dynamic mark space embeds the time-evolving network. We\nestablish stability conditions, demonstrate simulation and subsequent feasible\nlikelihood-based inference through numerical study, and present an application\nto conference attendee social network data. The resulting methodology serves as\na general framework that can be readily adapted to a wide range of network\ntopologies and point process model specifications."
                },
                "authors": [
                    {
                        "name": "Duncan A Clark"
                    },
                    {
                        "name": "Conor J. Kresin"
                    },
                    {
                        "name": "Charlotte M. Jones-Todd"
                    }
                ],
                "author_detail": {
                    "name": "Charlotte M. Jones-Todd"
                },
                "author": "Charlotte M. Jones-Todd",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22657v1",
                "updated": "2025-05-28T17:59:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    13,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:59:13Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    13,
                    2,
                    148,
                    0
                ],
                "title": "3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large\n  Language Model"
                },
                "summary": "Humans excel at performing complex tasks by leveraging long-term memory\nacross temporal and spatial experiences. In contrast, current Large Language\nModels (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D\nenvironments. We posit that part of this limitation is due to the lack of\nproper 3D spatial-temporal memory modeling in LLMs. To address this, we first\nintroduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000\ntrajectories and 2,892 embodied tasks, question-answering and captioning,\ndesigned to evaluate an agent's ability to reason over long-term memory in 3D\nenvironments. Second, we propose 3DLLM-Mem, a novel dynamic memory management\nand fusion model for embodied spatial-temporal reasoning and actions in LLMs.\nOur model uses working memory tokens, which represents current observations, as\nqueries to selectively attend to and fuse the most useful spatial and temporal\nfeatures from episodic memory, which stores past observations and interactions.\nOur approach allows the agent to focus on task-relevant information while\nmaintaining memory efficiency in complex, long-horizon environments.\nExperimental results demonstrate that 3DLLM-Mem achieves state-of-the-art\nperformance across various tasks, outperforming the strongest baselines by\n16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans excel at performing complex tasks by leveraging long-term memory\nacross temporal and spatial experiences. In contrast, current Large Language\nModels (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D\nenvironments. We posit that part of this limitation is due to the lack of\nproper 3D spatial-temporal memory modeling in LLMs. To address this, we first\nintroduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000\ntrajectories and 2,892 embodied tasks, question-answering and captioning,\ndesigned to evaluate an agent's ability to reason over long-term memory in 3D\nenvironments. Second, we propose 3DLLM-Mem, a novel dynamic memory management\nand fusion model for embodied spatial-temporal reasoning and actions in LLMs.\nOur model uses working memory tokens, which represents current observations, as\nqueries to selectively attend to and fuse the most useful spatial and temporal\nfeatures from episodic memory, which stores past observations and interactions.\nOur approach allows the agent to focus on task-relevant information while\nmaintaining memory efficiency in complex, long-horizon environments.\nExperimental results demonstrate that 3DLLM-Mem achieves state-of-the-art\nperformance across various tasks, outperforming the strongest baselines by\n16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied\ntasks."
                },
                "authors": [
                    {
                        "name": "Wenbo Hu"
                    },
                    {
                        "name": "Yining Hong"
                    },
                    {
                        "name": "Yanjun Wang"
                    },
                    {
                        "name": "Leison Gao"
                    },
                    {
                        "name": "Zibu Wei"
                    },
                    {
                        "name": "Xingcheng Yao"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Yonatan Bitton"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Wei Chang"
                },
                "author": "Kai-Wei Chang",
                "arxiv_comment": "demos at: https://3dllm-mem.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22654v1",
                "updated": "2025-05-28T17:59:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    8,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:59:08Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    8,
                    2,
                    148,
                    0
                ],
                "title": "VScan: Rethinking Visual Token Reduction for Efficient Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VScan: Rethinking Visual Token Reduction for Efficient Large\n  Vision-Language Models"
                },
                "summary": "Recent Large Vision-Language Models (LVLMs) have advanced multi-modal\nunderstanding by incorporating finer-grained visual perception and encoding.\nHowever, such methods incur significant computational costs due to longer\nvisual token sequences, posing challenges for real-time deployment. To mitigate\nthis, prior studies have explored pruning unimportant visual tokens either at\nthe output layer of the visual encoder or at the early layers of the language\nmodel. In this work, we revisit these design choices and reassess their\neffectiveness through comprehensive empirical studies of how visual tokens are\nprocessed throughout the visual encoding and language decoding stages. Guided\nby these insights, we propose VScan, a two-stage visual token reduction\nframework that addresses token redundancy by: (1) integrating complementary\nglobal and local scans with token merging during visual encoding, and (2)\nintroducing pruning at intermediate layers of the language model. Extensive\nexperimental results across four LVLMs validate the effectiveness of VScan in\naccelerating inference and demonstrate its superior performance over current\nstate-of-the-arts on sixteen benchmarks. Notably, when applied to\nLLaVA-NeXT-7B, VScan achieves a 2.91$\\times$ speedup in prefilling and a\n10$\\times$ reduction in FLOPs, while retaining 95.4% of the original\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Large Vision-Language Models (LVLMs) have advanced multi-modal\nunderstanding by incorporating finer-grained visual perception and encoding.\nHowever, such methods incur significant computational costs due to longer\nvisual token sequences, posing challenges for real-time deployment. To mitigate\nthis, prior studies have explored pruning unimportant visual tokens either at\nthe output layer of the visual encoder or at the early layers of the language\nmodel. In this work, we revisit these design choices and reassess their\neffectiveness through comprehensive empirical studies of how visual tokens are\nprocessed throughout the visual encoding and language decoding stages. Guided\nby these insights, we propose VScan, a two-stage visual token reduction\nframework that addresses token redundancy by: (1) integrating complementary\nglobal and local scans with token merging during visual encoding, and (2)\nintroducing pruning at intermediate layers of the language model. Extensive\nexperimental results across four LVLMs validate the effectiveness of VScan in\naccelerating inference and demonstrate its superior performance over current\nstate-of-the-arts on sixteen benchmarks. Notably, when applied to\nLLaVA-NeXT-7B, VScan achieves a 2.91$\\times$ speedup in prefilling and a\n10$\\times$ reduction in FLOPs, while retaining 95.4% of the original\nperformance."
                },
                "authors": [
                    {
                        "name": "Ce Zhang"
                    },
                    {
                        "name": "Kaixin Ma"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Yaqi Xie"
                    },
                    {
                        "name": "Katia Sycara"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22655v1",
                "updated": "2025-05-28T17:59:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    8,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:59:08Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    8,
                    2,
                    148,
                    0
                ],
                "title": "Position: Uncertainty Quantification Needs Reassessment for\n  Large-language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: Uncertainty Quantification Needs Reassessment for\n  Large-language Model Agents"
                },
                "summary": "Large-language models (LLMs) and chatbot agents are known to provide wrong\noutputs at times, and it was recently found that this can never be fully\nprevented. Hence, uncertainty quantification plays a crucial role, aiming to\nquantify the level of ambiguity in either one overall number or two numbers for\naleatoric and epistemic uncertainty. This position paper argues that this\ntraditional dichotomy of uncertainties is too limited for the open and\ninteractive setup that LLM agents operate in when communicating with a user,\nand that we need to research avenues that enrich uncertainties in this novel\nscenario. We review the literature and find that popular definitions of\naleatoric and epistemic uncertainties directly contradict each other and lose\ntheir meaning in interactive LLM agent settings. Hence, we propose three novel\nresearch directions that focus on uncertainties in such human-computer\ninteractions: Underspecification uncertainties, for when users do not provide\nall information or define the exact task at the first go, interactive learning,\nto ask follow-up questions and reduce the uncertainty about the current\ncontext, and output uncertainties, to utilize the rich language and speech\nspace to express uncertainties as more than mere numbers. We expect that these\nnew ways of dealing with and communicating uncertainties will lead to LLM agent\ninteractions that are more transparent, trustworthy, and intuitive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-language models (LLMs) and chatbot agents are known to provide wrong\noutputs at times, and it was recently found that this can never be fully\nprevented. Hence, uncertainty quantification plays a crucial role, aiming to\nquantify the level of ambiguity in either one overall number or two numbers for\naleatoric and epistemic uncertainty. This position paper argues that this\ntraditional dichotomy of uncertainties is too limited for the open and\ninteractive setup that LLM agents operate in when communicating with a user,\nand that we need to research avenues that enrich uncertainties in this novel\nscenario. We review the literature and find that popular definitions of\naleatoric and epistemic uncertainties directly contradict each other and lose\ntheir meaning in interactive LLM agent settings. Hence, we propose three novel\nresearch directions that focus on uncertainties in such human-computer\ninteractions: Underspecification uncertainties, for when users do not provide\nall information or define the exact task at the first go, interactive learning,\nto ask follow-up questions and reduce the uncertainty about the current\ncontext, and output uncertainties, to utilize the rich language and speech\nspace to express uncertainties as more than mere numbers. We expect that these\nnew ways of dealing with and communicating uncertainties will lead to LLM agent\ninteractions that are more transparent, trustworthy, and intuitive."
                },
                "authors": [
                    {
                        "name": "Michael Kirchhof"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Enkelejda Kasneci"
                },
                "author": "Enkelejda Kasneci",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22653v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22653v1",
                "updated": "2025-05-28T17:59:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    3,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:59:03Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    3,
                    2,
                    148,
                    0
                ],
                "title": "The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in\n  Learning to Reason",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in\n  Learning to Reason"
                },
                "summary": "Recent studies on post-training large language models (LLMs) for reasoning\nthrough reinforcement learning (RL) typically focus on tasks that can be\naccurately verified and rewarded, such as solving math problems. In contrast,\nour research investigates the impact of reward noise, a more practical\nconsideration for real-world scenarios involving the post-training of LLMs\nusing reward models. We found that LLMs demonstrate strong robustness to\nsubstantial reward noise. For example, manually flipping 40% of the reward\nfunction's outputs in math tasks still allows a Qwen-2.5-7B model to achieve\nrapid convergence, improving its performance on math tasks from 5% to 72%,\ncompared to the 75% accuracy achieved by a model trained with noiseless\nrewards. Surprisingly, by only rewarding the appearance of key reasoning\nphrases (namely reasoning pattern reward, RPR), such as ``first, I need\nto''-without verifying the correctness of answers, the model achieved peak\ndownstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models\ntrained with strict correctness verification and accurate rewards. Recognizing\nthe importance of the reasoning process over the final results, we combined RPR\nwith noisy reward models. RPR helped calibrate the noisy reward models,\nmitigating potential false negatives and enhancing the LLM's performance on\nopen-ended tasks. These findings suggest the importance of improving models'\nfoundational abilities during the pre-training phase while providing insights\nfor advancing post-training techniques. Our code and scripts are available at\nhttps://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies on post-training large language models (LLMs) for reasoning\nthrough reinforcement learning (RL) typically focus on tasks that can be\naccurately verified and rewarded, such as solving math problems. In contrast,\nour research investigates the impact of reward noise, a more practical\nconsideration for real-world scenarios involving the post-training of LLMs\nusing reward models. We found that LLMs demonstrate strong robustness to\nsubstantial reward noise. For example, manually flipping 40% of the reward\nfunction's outputs in math tasks still allows a Qwen-2.5-7B model to achieve\nrapid convergence, improving its performance on math tasks from 5% to 72%,\ncompared to the 75% accuracy achieved by a model trained with noiseless\nrewards. Surprisingly, by only rewarding the appearance of key reasoning\nphrases (namely reasoning pattern reward, RPR), such as ``first, I need\nto''-without verifying the correctness of answers, the model achieved peak\ndownstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models\ntrained with strict correctness verification and accurate rewards. Recognizing\nthe importance of the reasoning process over the final results, we combined RPR\nwith noisy reward models. RPR helped calibrate the noisy reward models,\nmitigating potential false negatives and enhancing the LLM's performance on\nopen-ended tasks. These findings suggest the importance of improving models'\nfoundational abilities during the pre-training phase while providing insights\nfor advancing post-training techniques. Our code and scripts are available at\nhttps://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason."
                },
                "authors": [
                    {
                        "name": "Ang Lv"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22653v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22653v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22650v1",
                "updated": "2025-05-28T17:57:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    57,
                    29,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:57:29Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    57,
                    29,
                    2,
                    148,
                    0
                ],
                "title": "On Learning Verifiers for Chain-of-Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Learning Verifiers for Chain-of-Thought Reasoning"
                },
                "summary": "Chain-of-Thought reasoning has emerged as a powerful approach for solving\ncomplex mathematical and logical problems. However, it can often veer off track\nthrough incorrect or unsubstantiated inferences. Formal mathematical reasoning,\nwhich can be checked with a formal verifier, is one approach to addressing this\nissue. However, currently LLMs are simply not good enough to solve complex\nproblems in a formal way, and even just formalizing an informal problem\nstatement can be challenging. Motivated by this fact, in this work we consider\nthe problem of learning reliable verifiers for natural language\nChain-of-Thought reasoning. That is, given a problem statement and step-by-step\nsolution in natural language, the aim of the verifier is to output [Yes] if the\nreasoning steps in the solution are all valid, and [No] otherwise. In this work\nwe give a formal PAC-learning framework for studying this problem. We propose\nand analyze several natural verification goals, at different levels of\nstrength, in this framework. We provide sample complexity upper-bounds for\nlearning verifiers satisfying these goals, as well as lower-bound and\nimpossibility results for learning other natural verification objectives\nwithout additional assumptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought reasoning has emerged as a powerful approach for solving\ncomplex mathematical and logical problems. However, it can often veer off track\nthrough incorrect or unsubstantiated inferences. Formal mathematical reasoning,\nwhich can be checked with a formal verifier, is one approach to addressing this\nissue. However, currently LLMs are simply not good enough to solve complex\nproblems in a formal way, and even just formalizing an informal problem\nstatement can be challenging. Motivated by this fact, in this work we consider\nthe problem of learning reliable verifiers for natural language\nChain-of-Thought reasoning. That is, given a problem statement and step-by-step\nsolution in natural language, the aim of the verifier is to output [Yes] if the\nreasoning steps in the solution are all valid, and [No] otherwise. In this work\nwe give a formal PAC-learning framework for studying this problem. We propose\nand analyze several natural verification goals, at different levels of\nstrength, in this framework. We provide sample complexity upper-bounds for\nlearning verifiers satisfying these goals, as well as lower-bound and\nimpossibility results for learning other natural verification objectives\nwithout additional assumptions."
                },
                "authors": [
                    {
                        "name": "Maria-Florina Balcan"
                    },
                    {
                        "name": "Avrim Blum"
                    },
                    {
                        "name": "Zhiyuan Li"
                    },
                    {
                        "name": "Dravyansh Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Dravyansh Sharma"
                },
                "author": "Dravyansh Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22646v1",
                "updated": "2025-05-28T17:56:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    56,
                    53,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:56:53Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    56,
                    53,
                    2,
                    148,
                    0
                ],
                "title": "Path-Dependent SDEs: Solutions and Parameter Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Path-Dependent SDEs: Solutions and Parameter Estimation"
                },
                "summary": "We develop a consistent method for estimating the parameters of a rich class\nof path-dependent SDEs, called signature SDEs, which can model general\npath-dependent phenomena. Path signatures are iterated integrals of a given\npath with the property that any sufficiently nice function of the path can be\napproximated by a linear functional of its signatures. This is why we model the\ndrift and diffusion of our signature SDE as linear functions of path\nsignatures. We provide conditions that ensure the existence and uniqueness of\nsolutions to a general signature SDE. We then introduce the Expected Signature\nMatching Method (ESMM) for linear signature SDEs, which enables inference of\nthe signature-dependent drift and diffusion coefficients from observed\ntrajectories. Furthermore, we prove that ESMM is consistent: given sufficiently\nmany samples and Picard iterations used by the method, the parameters estimated\nby the ESMM approach the true parameter with arbitrary precision. Finally, we\ndemonstrate on a variety of empirical simulations that our ESMM accurately\ninfers the drift and diffusion parameters from observed trajectories. While\nparameter estimation is often restricted by the need for a suitable parametric\nmodel, this work makes progress toward a completely general framework for SDE\nparameter estimation, using signature terms to model arbitrary path-independent\nand path-dependent processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a consistent method for estimating the parameters of a rich class\nof path-dependent SDEs, called signature SDEs, which can model general\npath-dependent phenomena. Path signatures are iterated integrals of a given\npath with the property that any sufficiently nice function of the path can be\napproximated by a linear functional of its signatures. This is why we model the\ndrift and diffusion of our signature SDE as linear functions of path\nsignatures. We provide conditions that ensure the existence and uniqueness of\nsolutions to a general signature SDE. We then introduce the Expected Signature\nMatching Method (ESMM) for linear signature SDEs, which enables inference of\nthe signature-dependent drift and diffusion coefficients from observed\ntrajectories. Furthermore, we prove that ESMM is consistent: given sufficiently\nmany samples and Picard iterations used by the method, the parameters estimated\nby the ESMM approach the true parameter with arbitrary precision. Finally, we\ndemonstrate on a variety of empirical simulations that our ESMM accurately\ninfers the drift and diffusion parameters from observed trajectories. While\nparameter estimation is often restricted by the need for a suitable parametric\nmodel, this work makes progress toward a completely general framework for SDE\nparameter estimation, using signature terms to model arbitrary path-independent\nand path-dependent processes."
                },
                "authors": [
                    {
                        "name": "Pardis Semnani"
                    },
                    {
                        "name": "Vincent Guan"
                    },
                    {
                        "name": "Elina Robeva"
                    },
                    {
                        "name": "Darrick Lee"
                    }
                ],
                "author_detail": {
                    "name": "Darrick Lee"
                },
                "author": "Darrick Lee",
                "arxiv_comment": "41 pages, 4 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60L20, 60L90, 62M99, 62M09",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22645v1",
                "updated": "2025-05-28T17:56:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    56,
                    49,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:56:49Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    56,
                    49,
                    2,
                    148,
                    0
                ],
                "title": "Characterizing Bias: Benchmarking Large Language Models in Simplified\n  versus Traditional Chinese",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing Bias: Benchmarking Large Language Models in Simplified\n  versus Traditional Chinese"
                },
                "summary": "While the capabilities of Large Language Models (LLMs) have been studied in\nboth Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit\ndifferential performance when prompted in these two variants of written\nChinese. This understanding is critical, as disparities in the quality of LLM\nresponses can perpetuate representational harms by ignoring the different\ncultural contexts underlying Simplified versus Traditional Chinese, and can\nexacerbate downstream harms in LLM-facilitated decision-making in domains such\nas education or hiring. To investigate potential LLM performance disparities,\nwe design two benchmark tasks that reflect real-world scenarios: regional term\nchoice (prompting the LLM to name a described item which is referred to\ndifferently in Mainland China and Taiwan), and regional name choice (prompting\nthe LLM to choose who to hire from a list of names in both Simplified and\nTraditional Chinese). For both tasks, we audit the performance of 11 leading\ncommercial LLM services and open-sourced models -- spanning those primarily\ntrained on English, Simplified Chinese, or Traditional Chinese. Our analyses\nindicate that biases in LLM responses are dependent on both the task and\nprompting language: while most LLMs disproportionately favored Simplified\nChinese responses in the regional term choice task, they surprisingly favored\nTraditional Chinese names in the regional name choice task. We find that these\ndisparities may arise from differences in training data representation, written\ncharacter preferences, and tokenization of Simplified and Traditional Chinese.\nThese findings highlight the need for further analysis of LLM biases; as such,\nwe provide an open-sourced benchmark dataset to foster reproducible evaluations\nof future LLM behavior across Chinese language variants\n(https://github.com/brucelyu17/SC-TC-Bench).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the capabilities of Large Language Models (LLMs) have been studied in\nboth Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit\ndifferential performance when prompted in these two variants of written\nChinese. This understanding is critical, as disparities in the quality of LLM\nresponses can perpetuate representational harms by ignoring the different\ncultural contexts underlying Simplified versus Traditional Chinese, and can\nexacerbate downstream harms in LLM-facilitated decision-making in domains such\nas education or hiring. To investigate potential LLM performance disparities,\nwe design two benchmark tasks that reflect real-world scenarios: regional term\nchoice (prompting the LLM to name a described item which is referred to\ndifferently in Mainland China and Taiwan), and regional name choice (prompting\nthe LLM to choose who to hire from a list of names in both Simplified and\nTraditional Chinese). For both tasks, we audit the performance of 11 leading\ncommercial LLM services and open-sourced models -- spanning those primarily\ntrained on English, Simplified Chinese, or Traditional Chinese. Our analyses\nindicate that biases in LLM responses are dependent on both the task and\nprompting language: while most LLMs disproportionately favored Simplified\nChinese responses in the regional term choice task, they surprisingly favored\nTraditional Chinese names in the regional name choice task. We find that these\ndisparities may arise from differences in training data representation, written\ncharacter preferences, and tokenization of Simplified and Traditional Chinese.\nThese findings highlight the need for further analysis of LLM biases; as such,\nwe provide an open-sourced benchmark dataset to foster reproducible evaluations\nof future LLM behavior across Chinese language variants\n(https://github.com/brucelyu17/SC-TC-Bench)."
                },
                "authors": [
                    {
                        "name": "Hanjia Lyu"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Jian Kang"
                    },
                    {
                        "name": "Allison Koenecke"
                    }
                ],
                "author_detail": {
                    "name": "Allison Koenecke"
                },
                "author": "Allison Koenecke",
                "arxiv_comment": "To appear in the 2025 ACM Conference on Fairness, Accountability, and\n  Transparency (FAccT '25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22637v1",
                "updated": "2025-05-28T17:53:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    53,
                    31,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:53:31Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    53,
                    31,
                    2,
                    148,
                    0
                ],
                "title": "Understanding (Un)Reliability of Steering Vectors in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding (Un)Reliability of Steering Vectors in Language Models"
                },
                "summary": "Steering vectors are a lightweight method to control language model behavior\nby adding a learned bias to the activations at inference time. Although\nsteering demonstrates promising performance, recent work shows that it can be\nunreliable or even counterproductive in some cases. This paper studies the\ninfluence of prompt types and the geometry of activation differences on\nsteering reliability. First, we find that all seven prompt types used in our\nexperiments produce a net positive steering effect, but exhibit high variance\nacross samples, and often give an effect opposite of the desired one. No prompt\ntype clearly outperforms the others, and yet the steering vectors resulting\nfrom the different prompt types often differ directionally (as measured by\ncosine similarity). Second, we show that higher cosine similarity between\ntraining set activation differences predicts more effective steering. Finally,\nwe observe that datasets where positive and negative activations are better\nseparated are more steerable. Our results suggest that vector steering is\nunreliable when the target behavior is not represented by a coherent direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering vectors are a lightweight method to control language model behavior\nby adding a learned bias to the activations at inference time. Although\nsteering demonstrates promising performance, recent work shows that it can be\nunreliable or even counterproductive in some cases. This paper studies the\ninfluence of prompt types and the geometry of activation differences on\nsteering reliability. First, we find that all seven prompt types used in our\nexperiments produce a net positive steering effect, but exhibit high variance\nacross samples, and often give an effect opposite of the desired one. No prompt\ntype clearly outperforms the others, and yet the steering vectors resulting\nfrom the different prompt types often differ directionally (as measured by\ncosine similarity). Second, we show that higher cosine similarity between\ntraining set activation differences predicts more effective steering. Finally,\nwe observe that datasets where positive and negative activations are better\nseparated are more steerable. Our results suggest that vector steering is\nunreliable when the target behavior is not represented by a coherent direction."
                },
                "authors": [
                    {
                        "name": "Joschka Braun"
                    },
                    {
                        "name": "Carsten Eickhoff"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Seyed Ali Bahrainian"
                    },
                    {
                        "name": "Dmitrii Krasheninnikov"
                    }
                ],
                "author_detail": {
                    "name": "Dmitrii Krasheninnikov"
                },
                "author": "Dmitrii Krasheninnikov",
                "arxiv_comment": "17 pages, 10 figures. Presented at the ICLR 2025 Workshop on\n  Foundation Models in the Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22636v1",
                "updated": "2025-05-28T17:51:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    51,
                    17,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:51:17Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    51,
                    17,
                    2,
                    148,
                    0
                ],
                "title": "ObjectClear: Complete Object Removal via Object-Effect Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ObjectClear: Complete Object Removal via Object-Effect Attention"
                },
                "summary": "Object removal requires eliminating not only the target object but also its\neffects, such as shadows and reflections. However, diffusion-based inpainting\nmethods often produce artifacts, hallucinate content, alter background, and\nstruggle to remove object effects accurately. To address this challenge, we\nintroduce a new dataset for OBject-Effect Removal, named OBER, which provides\npaired images with and without object effects, along with precise masks for\nboth objects and their associated visual artifacts. The dataset comprises\nhigh-quality captured and simulated data, covering diverse object categories\nand complex multi-object scenes. Building on OBER, we propose a novel\nframework, ObjectClear, which incorporates an object-effect attention mechanism\nto guide the model toward the foreground removal regions by learning attention\nmasks, effectively decoupling foreground removal from background\nreconstruction. Furthermore, the predicted attention map enables an\nattention-guided fusion strategy during inference, greatly preserving\nbackground details. Extensive experiments demonstrate that ObjectClear\noutperforms existing methods, achieving improved object-effect removal quality\nand background fidelity, especially in complex scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object removal requires eliminating not only the target object but also its\neffects, such as shadows and reflections. However, diffusion-based inpainting\nmethods often produce artifacts, hallucinate content, alter background, and\nstruggle to remove object effects accurately. To address this challenge, we\nintroduce a new dataset for OBject-Effect Removal, named OBER, which provides\npaired images with and without object effects, along with precise masks for\nboth objects and their associated visual artifacts. The dataset comprises\nhigh-quality captured and simulated data, covering diverse object categories\nand complex multi-object scenes. Building on OBER, we propose a novel\nframework, ObjectClear, which incorporates an object-effect attention mechanism\nto guide the model toward the foreground removal regions by learning attention\nmasks, effectively decoupling foreground removal from background\nreconstruction. Furthermore, the predicted attention map enables an\nattention-guided fusion strategy during inference, greatly preserving\nbackground details. Extensive experiments demonstrate that ObjectClear\noutperforms existing methods, achieving improved object-effect removal quality\nand background fidelity, especially in complex scenarios."
                },
                "authors": [
                    {
                        "name": "Jixin Zhao"
                    },
                    {
                        "name": "Shangchen Zhou"
                    },
                    {
                        "name": "Zhouxia Wang"
                    },
                    {
                        "name": "Peiqing Yang"
                    },
                    {
                        "name": "Chen Change Loy"
                    }
                ],
                "author_detail": {
                    "name": "Chen Change Loy"
                },
                "author": "Chen Change Loy",
                "arxiv_comment": "Project page: https://zjx0101.github.io/projects/ObjectClear/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22635v1",
                "updated": "2025-05-28T17:51:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    51,
                    10,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:51:10Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    51,
                    10,
                    2,
                    148,
                    0
                ],
                "title": "Learning Composable Chains-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Composable Chains-of-Thought"
                },
                "summary": "A common approach for teaching large language models (LLMs) to reason is to\ntrain on chain-of-thought (CoT) traces of in-distribution reasoning problems,\nbut such annotated data is costly to obtain for every problem of interest. We\nwant reasoning models to generalize beyond their training distribution, and\nideally to generalize compositionally: combine atomic reasoning skills to solve\nharder, unseen reasoning tasks. We take a step towards compositional\ngeneralization of reasoning skills when addressing a target compositional task\nthat has no labeled CoT data. We find that simply training models on CoT data\nof atomic tasks leads to limited generalization, but minimally modifying CoT\nformats of constituent atomic tasks to be composable can lead to improvements.\nWe can train \"atomic CoT\" models on the atomic tasks with Composable CoT data\nand combine them with multitask learning or model merging for better zero-shot\nperformance on the target compositional task. Such a combined model can be\nfurther bootstrapped on a small amount of compositional data using rejection\nsampling fine-tuning (RFT). Results on string operations and natural language\nskill compositions show that training LLMs on Composable CoT outperforms\nmultitask learning and continued fine-tuning baselines within a given training\ndata budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common approach for teaching large language models (LLMs) to reason is to\ntrain on chain-of-thought (CoT) traces of in-distribution reasoning problems,\nbut such annotated data is costly to obtain for every problem of interest. We\nwant reasoning models to generalize beyond their training distribution, and\nideally to generalize compositionally: combine atomic reasoning skills to solve\nharder, unseen reasoning tasks. We take a step towards compositional\ngeneralization of reasoning skills when addressing a target compositional task\nthat has no labeled CoT data. We find that simply training models on CoT data\nof atomic tasks leads to limited generalization, but minimally modifying CoT\nformats of constituent atomic tasks to be composable can lead to improvements.\nWe can train \"atomic CoT\" models on the atomic tasks with Composable CoT data\nand combine them with multitask learning or model merging for better zero-shot\nperformance on the target compositional task. Such a combined model can be\nfurther bootstrapped on a small amount of compositional data using rejection\nsampling fine-tuning (RFT). Results on string operations and natural language\nskill compositions show that training LLMs on Composable CoT outperforms\nmultitask learning and continued fine-tuning baselines within a given training\ndata budget."
                },
                "authors": [
                    {
                        "name": "Fangcong Yin"
                    },
                    {
                        "name": "Zeyu Leo Liu"
                    },
                    {
                        "name": "Liu Leqi"
                    },
                    {
                        "name": "Xi Ye"
                    },
                    {
                        "name": "Greg Durrett"
                    }
                ],
                "author_detail": {
                    "name": "Greg Durrett"
                },
                "author": "Greg Durrett",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22632v1",
                "updated": "2025-05-28T17:50:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    50,
                    20,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:50:20Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    50,
                    20,
                    2,
                    148,
                    0
                ],
                "title": "Towards the Efficient Inference by Incorporating Automated Computational\n  Phenotypes under Covariate Shift",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards the Efficient Inference by Incorporating Automated Computational\n  Phenotypes under Covariate Shift"
                },
                "summary": "Collecting gold-standard phenotype data via manual extraction is typically\nlabor-intensive and slow, whereas automated computational phenotypes (ACPs)\noffer a systematic and much faster alternative. However, simply replacing the\ngold-standard with ACPs, without acknowledging their differences, could lead to\nbiased results and misleading conclusions. Motivated by the complexity of\nincorporating ACPs while maintaining the validity of downstream analyses, in\nthis paper, we consider a semi-supervised learning setting that consists of\nboth labeled data (with gold-standard) and unlabeled data (without\ngold-standard), under the covariate shift framework. We develop doubly robust\nand semiparametrically efficient estimators that leverage ACPs for general\ntarget parameters in the unlabeled and combined populations. In addition, we\ncarefully analyze the efficiency gains achieved by incorporating ACPs,\ncomparing scenarios with and without their inclusion. Notably, we identify that\nACPs for the unlabeled data, instead of for the labeled data, drive the\nenhanced efficiency gains. To validate our theoretical findings, we conduct\ncomprehensive synthetic experiments and apply our method to multiple real-world\ndatasets, confirming the practical advantages of our approach.\n\\hfill{\\texttt{Code}:\n\\href{https://github.com/brucejunjin/ICML2025-ACPCS}{\\faGithub}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collecting gold-standard phenotype data via manual extraction is typically\nlabor-intensive and slow, whereas automated computational phenotypes (ACPs)\noffer a systematic and much faster alternative. However, simply replacing the\ngold-standard with ACPs, without acknowledging their differences, could lead to\nbiased results and misleading conclusions. Motivated by the complexity of\nincorporating ACPs while maintaining the validity of downstream analyses, in\nthis paper, we consider a semi-supervised learning setting that consists of\nboth labeled data (with gold-standard) and unlabeled data (without\ngold-standard), under the covariate shift framework. We develop doubly robust\nand semiparametrically efficient estimators that leverage ACPs for general\ntarget parameters in the unlabeled and combined populations. In addition, we\ncarefully analyze the efficiency gains achieved by incorporating ACPs,\ncomparing scenarios with and without their inclusion. Notably, we identify that\nACPs for the unlabeled data, instead of for the labeled data, drive the\nenhanced efficiency gains. To validate our theoretical findings, we conduct\ncomprehensive synthetic experiments and apply our method to multiple real-world\ndatasets, confirming the practical advantages of our approach.\n\\hfill{\\texttt{Code}:\n\\href{https://github.com/brucejunjin/ICML2025-ACPCS}{\\faGithub}}"
                },
                "authors": [
                    {
                        "name": "Chao Ying"
                    },
                    {
                        "name": "Jun Jin"
                    },
                    {
                        "name": "Yi Guo"
                    },
                    {
                        "name": "Xiudi Li"
                    },
                    {
                        "name": "Muxuan Liang"
                    },
                    {
                        "name": "Jiwei Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jiwei Zhao"
                },
                "author": "Jiwei Zhao",
                "arxiv_journal_ref": "2025 ICML",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22630v1",
                "updated": "2025-05-28T17:47:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    47,
                    52,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:47:52Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    47,
                    52,
                    2,
                    148,
                    0
                ],
                "title": "Stochastic Chameleons: Irrelevant Context Hallucinations Reveal\n  Class-Based (Mis)Generalization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Chameleons: Irrelevant Context Hallucinations Reveal\n  Class-Based (Mis)Generalization in LLMs"
                },
                "summary": "The widespread success of large language models (LLMs) on NLP benchmarks has\nbeen accompanied by concerns that LLMs function primarily as stochastic parrots\nthat reproduce texts similar to what they saw during pre-training, often\nerroneously. But what is the nature of their errors, and do these errors\nexhibit any regularities? In this work, we examine irrelevant context\nhallucinations, in which models integrate misleading contextual cues into their\npredictions. Through behavioral analysis, we show that these errors result from\na structured yet flawed mechanism that we term class-based (mis)generalization,\nin which models combine abstract class cues with features extracted from the\nquery or context to derive answers. Furthermore, mechanistic interpretability\nexperiments on Llama-3, Mistral, and Pythia across 39 factual recall relation\ntypes reveal that this behavior is reflected in the model's internal\ncomputations: (i) abstract class representations are constructed in lower\nlayers before being refined into specific answers in higher layers, (ii)\nfeature selection is governed by two competing circuits -- one prioritizing\ndirect query-based reasoning, the other incorporating contextual cues -- whose\nrelative influences determine the final output. Our findings provide a more\nnuanced perspective on the stochastic parrot argument: through form-based\ntraining, LLMs can exhibit generalization leveraging abstractions, albeit in\nunreliable ways based on contextual cues -- what we term stochastic chameleons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread success of large language models (LLMs) on NLP benchmarks has\nbeen accompanied by concerns that LLMs function primarily as stochastic parrots\nthat reproduce texts similar to what they saw during pre-training, often\nerroneously. But what is the nature of their errors, and do these errors\nexhibit any regularities? In this work, we examine irrelevant context\nhallucinations, in which models integrate misleading contextual cues into their\npredictions. Through behavioral analysis, we show that these errors result from\na structured yet flawed mechanism that we term class-based (mis)generalization,\nin which models combine abstract class cues with features extracted from the\nquery or context to derive answers. Furthermore, mechanistic interpretability\nexperiments on Llama-3, Mistral, and Pythia across 39 factual recall relation\ntypes reveal that this behavior is reflected in the model's internal\ncomputations: (i) abstract class representations are constructed in lower\nlayers before being refined into specific answers in higher layers, (ii)\nfeature selection is governed by two competing circuits -- one prioritizing\ndirect query-based reasoning, the other incorporating contextual cues -- whose\nrelative influences determine the final output. Our findings provide a more\nnuanced perspective on the stochastic parrot argument: through form-based\ntraining, LLMs can exhibit generalization leveraging abstractions, albeit in\nunreliable ways based on contextual cues -- what we term stochastic chameleons."
                },
                "authors": [
                    {
                        "name": "Ziling Cheng"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Marc-Antoine Rondeau"
                    },
                    {
                        "name": "Jackie Chi Kit Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Jackie Chi Kit Cheung"
                },
                "author": "Jackie Chi Kit Cheung",
                "arxiv_comment": "Accepted to ACL 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12764v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12764v3",
                "updated": "2025-05-28T17:47:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    47,
                    46,
                    2,
                    148,
                    0
                ],
                "published": "2025-04-17T09:01:16Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    9,
                    1,
                    16,
                    3,
                    107,
                    0
                ],
                "title": "GraphOmni: A Comprehensive and Extendable Benchmark Framework for Large\n  Language Models on Graph-theoretic Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphOmni: A Comprehensive and Extendable Benchmark Framework for Large\n  Language Models on Graph-theoretic Tasks"
                },
                "summary": "This paper introduces GraphOmni, a comprehensive benchmark designed to\nevaluate the reasoning capabilities of LLMs on graph-theoretic tasks\narticulated in natural language. GraphOmni encompasses diverse graph types,\nserialization formats, and prompting schemes, significantly exceeding prior\nefforts in both scope and depth. Through extensive systematic evaluation, we\nidentify critical interactions among these dimensions, demonstrating their\nsubstantial impact on model performance. Our experiments reveal that\nstate-of-the-art models like Claude-3.5 and o4-mini consistently outperform\nother models, yet even these leading models exhibit substantial room for\nimprovement. Performance variability is evident depending on the specific\ncombinations of factors we considered, underscoring the necessity of\ncomprehensive evaluations across these interconnected dimensions. Additionally,\nwe observe distinct impacts of serialization and prompting strategies between\nopen-source and closed-source models, encouraging the development of tailored\napproaches. Motivated by the findings, we also propose a reinforcement\nlearning-inspired framework that adaptively selects the optimal factors\ninfluencing LLM reasoning capabilities. This flexible and extendable benchmark\nnot only deepens our understanding of LLM performance on structured tasks but\nalso provides a robust foundation for advancing research in LLM-based graph\nreasoning. The code and datasets are available at\nhttps://github.com/GAI-Community/GraphOmni.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces GraphOmni, a comprehensive benchmark designed to\nevaluate the reasoning capabilities of LLMs on graph-theoretic tasks\narticulated in natural language. GraphOmni encompasses diverse graph types,\nserialization formats, and prompting schemes, significantly exceeding prior\nefforts in both scope and depth. Through extensive systematic evaluation, we\nidentify critical interactions among these dimensions, demonstrating their\nsubstantial impact on model performance. Our experiments reveal that\nstate-of-the-art models like Claude-3.5 and o4-mini consistently outperform\nother models, yet even these leading models exhibit substantial room for\nimprovement. Performance variability is evident depending on the specific\ncombinations of factors we considered, underscoring the necessity of\ncomprehensive evaluations across these interconnected dimensions. Additionally,\nwe observe distinct impacts of serialization and prompting strategies between\nopen-source and closed-source models, encouraging the development of tailored\napproaches. Motivated by the findings, we also propose a reinforcement\nlearning-inspired framework that adaptively selects the optimal factors\ninfluencing LLM reasoning capabilities. This flexible and extendable benchmark\nnot only deepens our understanding of LLM performance on structured tasks but\nalso provides a robust foundation for advancing research in LLM-based graph\nreasoning. The code and datasets are available at\nhttps://github.com/GAI-Community/GraphOmni."
                },
                "authors": [
                    {
                        "name": "Hao Xu"
                    },
                    {
                        "name": "Xiangru Jian"
                    },
                    {
                        "name": "Xinjian Zhao"
                    },
                    {
                        "name": "Wei Pang"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Suyuchen Wang"
                    },
                    {
                        "name": "Qixin Zhang"
                    },
                    {
                        "name": "Zhengyuan Dong"
                    },
                    {
                        "name": "Joao Monteiro"
                    },
                    {
                        "name": "Bang Liu"
                    },
                    {
                        "name": "Qiuzhuang Sun"
                    },
                    {
                        "name": "Tianshu Yu"
                    }
                ],
                "author_detail": {
                    "name": "Tianshu Yu"
                },
                "author": "Tianshu Yu",
                "arxiv_comment": "Project Page: https://gai-community.github.io/Graph-Omni/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12764v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12764v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22618v1",
                "updated": "2025-05-28T17:39:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:39:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding"
                },
                "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22617v1",
                "updated": "2025-05-28T17:38:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    38,
                    45,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:38:45Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    38,
                    45,
                    2,
                    148,
                    0
                ],
                "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language\n  Models"
                },
                "summary": "This paper aims to overcome a major obstacle in scaling RL for reasoning with\nLLMs, namely the collapse of policy entropy. Such phenomenon is consistently\nobserved across vast RL runs without entropy intervention, where the policy\nentropy dropped sharply at the early training stage, this diminished\nexploratory ability is always accompanied with the saturation of policy\nperformance. In practice, we establish a transformation equation R=-a*e^H+b\nbetween entropy H and downstream performance R. This empirical law strongly\nindicates that, the policy performance is traded from policy entropy, thus\nbottlenecked by its exhaustion, and the ceiling is fully predictable H=0,\nR=-a+b. Our finding necessitates entropy management for continuous exploration\ntoward scaling compute for RL. To this end, we investigate entropy dynamics\nboth theoretically and empirically. Our derivation highlights that, the change\nin policy entropy is driven by the covariance between action probability and\nthe change in logits, which is proportional to its advantage when using Policy\nGradient-like algorithms. Empirical study shows that, the values of covariance\nterm and entropy differences matched exactly, supporting the theoretical\nconclusion. Moreover, the covariance term stays mostly positive throughout\ntraining, further explaining why policy entropy would decrease monotonically.\nThrough understanding the mechanism behind entropy dynamics, we motivate to\ncontrol entropy by restricting the update of high-covariance tokens.\nSpecifically, we propose two simple yet effective techniques, namely Clip-Cov\nand KL-Cov, which clip and apply KL penalty to tokens with high covariances\nrespectively. Experiments show that these methods encourage exploration, thus\nhelping policy escape entropy collapse and achieve better downstream\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper aims to overcome a major obstacle in scaling RL for reasoning with\nLLMs, namely the collapse of policy entropy. Such phenomenon is consistently\nobserved across vast RL runs without entropy intervention, where the policy\nentropy dropped sharply at the early training stage, this diminished\nexploratory ability is always accompanied with the saturation of policy\nperformance. In practice, we establish a transformation equation R=-a*e^H+b\nbetween entropy H and downstream performance R. This empirical law strongly\nindicates that, the policy performance is traded from policy entropy, thus\nbottlenecked by its exhaustion, and the ceiling is fully predictable H=0,\nR=-a+b. Our finding necessitates entropy management for continuous exploration\ntoward scaling compute for RL. To this end, we investigate entropy dynamics\nboth theoretically and empirically. Our derivation highlights that, the change\nin policy entropy is driven by the covariance between action probability and\nthe change in logits, which is proportional to its advantage when using Policy\nGradient-like algorithms. Empirical study shows that, the values of covariance\nterm and entropy differences matched exactly, supporting the theoretical\nconclusion. Moreover, the covariance term stays mostly positive throughout\ntraining, further explaining why policy entropy would decrease monotonically.\nThrough understanding the mechanism behind entropy dynamics, we motivate to\ncontrol entropy by restricting the update of high-covariance tokens.\nSpecifically, we propose two simple yet effective techniques, namely Clip-Cov\nand KL-Cov, which clip and apply KL penalty to tokens with high covariances\nrespectively. Experiments show that these methods encourage exploration, thus\nhelping policy escape entropy collapse and achieve better downstream\nperformance."
                },
                "authors": [
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Jiacheng Chen"
                    },
                    {
                        "name": "Lifan Yuan"
                    },
                    {
                        "name": "Zhi Wang"
                    },
                    {
                        "name": "Yuxin Zuo"
                    },
                    {
                        "name": "Haozhan Li"
                    },
                    {
                        "name": "Yuchen Fan"
                    },
                    {
                        "name": "Huayu Chen"
                    },
                    {
                        "name": "Weize Chen"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Bowen Zhou"
                    },
                    {
                        "name": "Ning Ding"
                    }
                ],
                "author_detail": {
                    "name": "Ning Ding"
                },
                "author": "Ning Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11326v2",
                "updated": "2025-05-28T17:37:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    37,
                    39,
                    2,
                    148,
                    0
                ],
                "published": "2024-08-21T04:19:52Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    19,
                    52,
                    2,
                    234,
                    0
                ],
                "title": "Automating Thought of Search: A Journey Towards Soundness and\n  Completeness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Thought of Search: A Journey Towards Soundness and\n  Completeness"
                },
                "summary": "Large language models (LLMs) are being used to solve planning problems that\nrequire search. Most of the literature uses LLMs as world models to define the\nsearch space, forgoing soundness for the sake of flexibility. A recent work,\nThought of Search (ToS), proposed defining the search space with code, having\nLLMs produce that code. ToS requires a human in the loop, collaboratively\nproducing a sound successor function and goal test. The result, however, is\nworth the effort: all the tested datasets were solved with 100% accuracy.\nConsequently, there is great potential to automate the ToS process. We take a\nfirst major step towards automating ToS (AutoToS), taking the human out of the\nloop of interactions with the language model. AutoToS guides the language model\nstep by step towards the generation of sound and complete search components,\nthrough feedback from both generic and domain specific unit tests. We show that\nAutoToS is able to achieve 100% accuracy on all the evaluated domains with a\nsmall number of LLM calls.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are being used to solve planning problems that\nrequire search. Most of the literature uses LLMs as world models to define the\nsearch space, forgoing soundness for the sake of flexibility. A recent work,\nThought of Search (ToS), proposed defining the search space with code, having\nLLMs produce that code. ToS requires a human in the loop, collaboratively\nproducing a sound successor function and goal test. The result, however, is\nworth the effort: all the tested datasets were solved with 100% accuracy.\nConsequently, there is great potential to automate the ToS process. We take a\nfirst major step towards automating ToS (AutoToS), taking the human out of the\nloop of interactions with the language model. AutoToS guides the language model\nstep by step towards the generation of sound and complete search components,\nthrough feedback from both generic and domain specific unit tests. We show that\nAutoToS is able to achieve 100% accuracy on all the evaluated domains with a\nsmall number of LLM calls."
                },
                "authors": [
                    {
                        "name": "Daniel Cao"
                    },
                    {
                        "name": "Michael Katz"
                    },
                    {
                        "name": "Harsha Kokel"
                    },
                    {
                        "name": "Kavitha Srinivas"
                    },
                    {
                        "name": "Shirin Sohrabi"
                    }
                ],
                "author_detail": {
                    "name": "Shirin Sohrabi"
                },
                "author": "Shirin Sohrabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18116v2",
                "updated": "2025-05-28T17:31:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    31,
                    37,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-23T17:17:40Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    17,
                    17,
                    40,
                    4,
                    143,
                    0
                ],
                "title": "Bridging Supervised Learning and Reinforcement Learning in Math\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Supervised Learning and Reinforcement Learning in Math\n  Reasoning"
                },
                "summary": "Reinforcement Learning (RL) has played a central role in the recent surge of\nLLMs' math abilities by enabling self-improvement through binary verifier\nsignals. In contrast, Supervised Learning (SL) is rarely considered for such\nverification-driven training, largely due to its heavy reliance on reference\nanswers and inability to reflect on mistakes. In this work, we challenge the\nprevailing notion that self-improvement is exclusive to RL and propose\nNegative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to\nreflect on their failures and improve autonomously with no external teachers.\nIn online training, instead of throwing away self-generated negative answers,\nNFT constructs an implicit negative policy to model them. This implicit policy\nis parameterized with the same positive LLM we target to optimize on positive\ndata, enabling direct policy optimization on all LLMs' generations. We conduct\nexperiments on 7B and 32B models in math reasoning tasks. Results consistently\nshow that through the additional leverage of negative feedback, NFT\nsignificantly improves over SL baselines like Rejection sampling Fine-Tuning,\nmatching or even surpassing leading RL algorithms like GRPO and DAPO.\nFurthermore, we demonstrate that NFT and GRPO are actually equivalent in\nstrict-on-policy training, even though they originate from entirely different\ntheoretical foundations. Our experiments and theoretical findings bridge the\ngap between SL and RL methods in binary-feedback learning systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has played a central role in the recent surge of\nLLMs' math abilities by enabling self-improvement through binary verifier\nsignals. In contrast, Supervised Learning (SL) is rarely considered for such\nverification-driven training, largely due to its heavy reliance on reference\nanswers and inability to reflect on mistakes. In this work, we challenge the\nprevailing notion that self-improvement is exclusive to RL and propose\nNegative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to\nreflect on their failures and improve autonomously with no external teachers.\nIn online training, instead of throwing away self-generated negative answers,\nNFT constructs an implicit negative policy to model them. This implicit policy\nis parameterized with the same positive LLM we target to optimize on positive\ndata, enabling direct policy optimization on all LLMs' generations. We conduct\nexperiments on 7B and 32B models in math reasoning tasks. Results consistently\nshow that through the additional leverage of negative feedback, NFT\nsignificantly improves over SL baselines like Rejection sampling Fine-Tuning,\nmatching or even surpassing leading RL algorithms like GRPO and DAPO.\nFurthermore, we demonstrate that NFT and GRPO are actually equivalent in\nstrict-on-policy training, even though they originate from entirely different\ntheoretical foundations. Our experiments and theoretical findings bridge the\ngap between SL and RL methods in binary-feedback learning systems."
                },
                "authors": [
                    {
                        "name": "Huayu Chen"
                    },
                    {
                        "name": "Kaiwen Zheng"
                    },
                    {
                        "name": "Qinsheng Zhang"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Yin Cui"
                    },
                    {
                        "name": "Haotian Ye"
                    },
                    {
                        "name": "Tsung-Yi Lin"
                    },
                    {
                        "name": "Ming-Yu Liu"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Haoxiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoxiang Wang"
                },
                "author": "Haoxiang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00134v2",
                "updated": "2025-05-28T17:18:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    18,
                    49,
                    2,
                    148,
                    0
                ],
                "published": "2025-02-28T19:25:04Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    19,
                    25,
                    4,
                    4,
                    59,
                    0
                ],
                "title": "Personalized Causal Graph Reasoning for LLMs: A Case Study on Dietary\n  Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Causal Graph Reasoning for LLMs: A Case Study on Dietary\n  Recommendations"
                },
                "summary": "Large Language Models (LLMs) effectively leverage common-sense knowledge for\ngeneral reasoning, yet they struggle with personalized reasoning when tasked\nwith interpreting multifactor personal data. This limitation restricts their\napplicability in domains that require context-aware decision-making tailored to\nindividuals. This paper introduces Personalized Causal Graph Reasoning as an\nagentic framework that enhances LLM reasoning by incorporating personal causal\ngraphs derived from data of individuals. These graphs provide a foundation that\nguides the LLM's reasoning process. We evaluate it on a case study on\nnutrient-oriented dietary recommendations, which requires personal reasoning\ndue to the implicit unique dietary effects. We propose a counterfactual\nevaluation to estimate the efficiency of LLM-recommended foods for glucose\nmanagement. Results demonstrate that the proposed method efficiently provides\npersonalized dietary recommendations to reduce average glucose iAUC across\nthree time windows, which outperforms the previous approach. LLM-as-a-judge\nevaluation results indicate that our proposed method enhances personalization\nin the reasoning process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) effectively leverage common-sense knowledge for\ngeneral reasoning, yet they struggle with personalized reasoning when tasked\nwith interpreting multifactor personal data. This limitation restricts their\napplicability in domains that require context-aware decision-making tailored to\nindividuals. This paper introduces Personalized Causal Graph Reasoning as an\nagentic framework that enhances LLM reasoning by incorporating personal causal\ngraphs derived from data of individuals. These graphs provide a foundation that\nguides the LLM's reasoning process. We evaluate it on a case study on\nnutrient-oriented dietary recommendations, which requires personal reasoning\ndue to the implicit unique dietary effects. We propose a counterfactual\nevaluation to estimate the efficiency of LLM-recommended foods for glucose\nmanagement. Results demonstrate that the proposed method efficiently provides\npersonalized dietary recommendations to reduce average glucose iAUC across\nthree time windows, which outperforms the previous approach. LLM-as-a-judge\nevaluation results indicate that our proposed method enhances personalization\nin the reasoning process."
                },
                "authors": [
                    {
                        "name": "Zhongqi Yang"
                    },
                    {
                        "name": "Amir Rahmani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Rahmani"
                },
                "author": "Amir Rahmani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17284v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17284v5",
                "updated": "2025-05-28T17:16:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    16,
                    40,
                    2,
                    148,
                    0
                ],
                "published": "2024-11-26T10:13:39Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    13,
                    39,
                    1,
                    331,
                    0
                ],
                "title": "AutoElicit: Using Large Language Models for Expert Prior Elicitation in\n  Predictive Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoElicit: Using Large Language Models for Expert Prior Elicitation in\n  Predictive Modelling"
                },
                "summary": "Large language models (LLMs) acquire a breadth of information across various\ndomains. However, their computational complexity, cost, and lack of\ntransparency often hinder their direct application for predictive tasks where\nprivacy and interpretability are paramount. In fields such as healthcare,\nbiology, and finance, specialised and interpretable linear models still hold\nconsiderable value. In such domains, labelled data may be scarce or expensive\nto obtain. Well-specified prior distributions over model parameters can reduce\nthe sample complexity of learning through Bayesian inference; however,\neliciting expert priors can be time-consuming. We therefore introduce\nAutoElicit to extract knowledge from LLMs and construct priors for predictive\nmodels. We show these priors are informative and can be refined using natural\nlanguage. We perform a careful study contrasting AutoElicit with in-context\nlearning and demonstrate how to perform model selection between the two\nmethods. We find that AutoElicit yields priors that can substantially reduce\nerror over uninformative priors, using fewer labels, and consistently\noutperform in-context learning. We show that AutoElicit saves over 6 months of\nlabelling effort when building a new predictive model for urinary tract\ninfections from sensor recordings of people living with dementia.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) acquire a breadth of information across various\ndomains. However, their computational complexity, cost, and lack of\ntransparency often hinder their direct application for predictive tasks where\nprivacy and interpretability are paramount. In fields such as healthcare,\nbiology, and finance, specialised and interpretable linear models still hold\nconsiderable value. In such domains, labelled data may be scarce or expensive\nto obtain. Well-specified prior distributions over model parameters can reduce\nthe sample complexity of learning through Bayesian inference; however,\neliciting expert priors can be time-consuming. We therefore introduce\nAutoElicit to extract knowledge from LLMs and construct priors for predictive\nmodels. We show these priors are informative and can be refined using natural\nlanguage. We perform a careful study contrasting AutoElicit with in-context\nlearning and demonstrate how to perform model selection between the two\nmethods. We find that AutoElicit yields priors that can substantially reduce\nerror over uninformative priors, using fewer labels, and consistently\noutperform in-context learning. We show that AutoElicit saves over 6 months of\nlabelling effort when building a new predictive model for urinary tract\ninfections from sensor recordings of people living with dementia."
                },
                "authors": [
                    {
                        "name": "Alexander Capstick"
                    },
                    {
                        "name": "Rahul G. Krishnan"
                    },
                    {
                        "name": "Payam Barnaghi"
                    }
                ],
                "author_detail": {
                    "name": "Payam Barnaghi"
                },
                "author": "Payam Barnaghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17284v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17284v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03561v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03561v2",
                "updated": "2025-05-28T17:03:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    3,
                    17,
                    2,
                    148,
                    0
                ],
                "published": "2025-04-04T16:10:57Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    10,
                    57,
                    4,
                    94,
                    0
                ],
                "title": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge\n  Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge\n  Refinement"
                },
                "summary": "In the interaction between agents and their environments, agents expand their\ncapabilities by planning and executing actions. However, LLM-based agents face\nsubstantial challenges when deployed in novel environments or required to\nnavigate unconventional action spaces. To empower agents to autonomously\nexplore environments, optimize workflows, and enhance their understanding of\nactions, we propose SynWorld, a framework that allows agents to synthesize\npossible scenarios with multi-step action invocation within the action space\nand perform Monte Carlo Tree Search (MCTS) exploration to effectively refine\ntheir action knowledge in the current environment. Our experiments demonstrate\nthat SynWorld is an effective and general approach to learning action knowledge\nin new environments. Code is available at https://github.com/zjunlp/SynWorld.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the interaction between agents and their environments, agents expand their\ncapabilities by planning and executing actions. However, LLM-based agents face\nsubstantial challenges when deployed in novel environments or required to\nnavigate unconventional action spaces. To empower agents to autonomously\nexplore environments, optimize workflows, and enhance their understanding of\nactions, we propose SynWorld, a framework that allows agents to synthesize\npossible scenarios with multi-step action invocation within the action space\nand perform Monte Carlo Tree Search (MCTS) exploration to effectively refine\ntheir action knowledge in the current environment. Our experiments demonstrate\nthat SynWorld is an effective and general approach to learning action knowledge\nin new environments. Code is available at https://github.com/zjunlp/SynWorld."
                },
                "authors": [
                    {
                        "name": "Runnan Fang"
                    },
                    {
                        "name": "Xiaobin Wang"
                    },
                    {
                        "name": "Yuan Liang"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zekun Xi"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03561v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03561v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22591v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22591v1",
                "updated": "2025-05-28T17:02:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    2,
                    47,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:02:47Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    2,
                    47,
                    2,
                    148,
                    0
                ],
                "title": "Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical\n  Reasoning"
                },
                "summary": "Although large language models demonstrate strong performance across various\ndomains, they still struggle with numerous bad cases in mathematical reasoning.\nPrevious approaches to learning from errors synthesize training data by solely\nextrapolating from isolated bad cases, thereby failing to generalize the\nextensive patterns inherent within these cases. This paper presents\nSelf-Error-Instruct (SEI), a framework that addresses these model weaknesses\nand synthesizes more generalized targeted training data. Specifically, we\nexplore a target model on two mathematical datasets, GSM8K and MATH, to\npinpoint bad cases. Then, we generate error keyphrases for these cases based on\nthe instructor model's (GPT-4o) analysis and identify error types by clustering\nthese keyphrases. Next, we sample a few bad cases during each generation for\neach identified error type and input them into the instructor model, which\nsynthesizes additional training data using a self-instruct approach. This new\ndata is refined through a one-shot learning process to ensure that only the\nmost effective examples are kept. Finally, we use these curated data to\nfine-tune the target model, iteratively repeating the process to enhance\nperformance. We apply our framework to various models and observe improvements\nin their reasoning abilities across both in-domain and out-of-domain\nmathematics datasets. These results demonstrate the effectiveness of self-error\ninstruction in improving LLMs' mathematical reasoning through error\ngeneralization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models demonstrate strong performance across various\ndomains, they still struggle with numerous bad cases in mathematical reasoning.\nPrevious approaches to learning from errors synthesize training data by solely\nextrapolating from isolated bad cases, thereby failing to generalize the\nextensive patterns inherent within these cases. This paper presents\nSelf-Error-Instruct (SEI), a framework that addresses these model weaknesses\nand synthesizes more generalized targeted training data. Specifically, we\nexplore a target model on two mathematical datasets, GSM8K and MATH, to\npinpoint bad cases. Then, we generate error keyphrases for these cases based on\nthe instructor model's (GPT-4o) analysis and identify error types by clustering\nthese keyphrases. Next, we sample a few bad cases during each generation for\neach identified error type and input them into the instructor model, which\nsynthesizes additional training data using a self-instruct approach. This new\ndata is refined through a one-shot learning process to ensure that only the\nmost effective examples are kept. Finally, we use these curated data to\nfine-tune the target model, iteratively repeating the process to enhance\nperformance. We apply our framework to various models and observe improvements\nin their reasoning abilities across both in-domain and out-of-domain\nmathematics datasets. These results demonstrate the effectiveness of self-error\ninstruction in improving LLMs' mathematical reasoning through error\ngeneralization."
                },
                "authors": [
                    {
                        "name": "Erxin Yu"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Ming Liao"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Boyang Xue"
                    },
                    {
                        "name": "Minghui Xu"
                    },
                    {
                        "name": "Baojun Wang"
                    },
                    {
                        "name": "Lanqing Hong"
                    },
                    {
                        "name": "Fei Mi"
                    },
                    {
                        "name": "Lifeng Shang"
                    }
                ],
                "author_detail": {
                    "name": "Lifeng Shang"
                },
                "author": "Lifeng Shang",
                "arxiv_comment": "16 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22591v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22591v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00197v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00197v2",
                "updated": "2025-05-28T16:59:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    59,
                    10,
                    2,
                    148,
                    0
                ],
                "published": "2024-09-30T19:51:41Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    19,
                    51,
                    41,
                    0,
                    274,
                    0
                ],
                "title": "More buck-per-shot: Why learning trumps mitigation in noisy quantum\n  sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More buck-per-shot: Why learning trumps mitigation in noisy quantum\n  sensing"
                },
                "summary": "Quantum sensing is one of the most promising applications for quantum\ntechnologies. However, reaching the ultimate sensitivities enabled by the laws\nof quantum mechanics can be a challenging task in realistic scenarios where\nnoise is present. While several strategies have been proposed to deal with the\ndetrimental effects of noise, these come at the cost of an extra shot budget.\nGiven that shots are a precious resource for sensing -- as infinite\nmeasurements could lead to infinite precision -- care must be taken to truly\nguarantee that any shot not being used for sensing is actually leading to some\nmetrological improvement. In this work, we study whether investing shots in\nerror-mitigation, inference techniques, or combinations thereof, can improve\nthe sensitivity of a noisy quantum sensor on a (shot) budget. We present a\ndetailed bias-variance error analysis for various sensing protocols. Our\nresults show that the costs of zero-noise extrapolation techniques outweigh\ntheir benefits. We also find that pre-characterizing a quantum sensor via\ninference techniques leads to the best performance, under the assumption that\nthe sensor is sufficiently stable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum sensing is one of the most promising applications for quantum\ntechnologies. However, reaching the ultimate sensitivities enabled by the laws\nof quantum mechanics can be a challenging task in realistic scenarios where\nnoise is present. While several strategies have been proposed to deal with the\ndetrimental effects of noise, these come at the cost of an extra shot budget.\nGiven that shots are a precious resource for sensing -- as infinite\nmeasurements could lead to infinite precision -- care must be taken to truly\nguarantee that any shot not being used for sensing is actually leading to some\nmetrological improvement. In this work, we study whether investing shots in\nerror-mitigation, inference techniques, or combinations thereof, can improve\nthe sensitivity of a noisy quantum sensor on a (shot) budget. We present a\ndetailed bias-variance error analysis for various sensing protocols. Our\nresults show that the costs of zero-noise extrapolation techniques outweigh\ntheir benefits. We also find that pre-characterizing a quantum sensor via\ninference techniques leads to the best performance, under the assumption that\nthe sensor is sufficiently stable."
                },
                "authors": [
                    {
                        "name": "Aroosa Ijaz"
                    },
                    {
                        "name": "C. Huerta Alderete"
                    },
                    {
                        "name": "Frédéric Sauvage"
                    },
                    {
                        "name": "Lukasz Cincio"
                    },
                    {
                        "name": "M. Cerezo"
                    },
                    {
                        "name": "Matthew L. Goh"
                    }
                ],
                "author_detail": {
                    "name": "Matthew L. Goh"
                },
                "author": "Matthew L. Goh",
                "arxiv_doi": "10.1016/j.mtquan.2025.100042",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.mtquan.2025.100042",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.00197v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00197v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "27+21 pages, 9+2 figures, 1+1 tables, updated to published version",
                "arxiv_journal_ref": "Materials Today Quantum 6, 100042 (2025)",
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22587v1",
                "updated": "2025-05-28T16:59:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    59,
                    9,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T16:59:09Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    59,
                    9,
                    2,
                    148,
                    0
                ],
                "title": "Bayesian Non-Parametric Inference for Lévy Measures in State-Space\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Non-Parametric Inference for Lévy Measures in State-Space\n  Models"
                },
                "summary": "L\\'evy processes, known for their ability to model complex dynamics with\nskewness, heavy tails and discontinuities, play a critical role in stochastic\nmodeling across various domains. However, inference for most L\\'evy processes,\nwhether in parametric or non-parametric settings, remains a significant\nchallenge. In this work, we present a novel Bayesian non-parametric inference\nframework for inferring the L\\'evy measures of subordinators and normal\nvariance-mean (NVM) processes within a linear L\\'evy state space model, a setup\nthat significantly extends existing methodologies. We employ the Dirichlet\nprocess which further results in a Student-t mixture representation to enable\ninference for the L\\'evy measures. An efficient augmented Markov Chain Monte\nCarlo algorithm is developed for this problem that ensures both accuracy and\ncomputational feasibility. The effectiveness of the method is demonstrated on\nsynthetic and tick-level (high-frequency) financial datasets, and we show the\npractical utility of the inference results using the forecasting performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L\\'evy processes, known for their ability to model complex dynamics with\nskewness, heavy tails and discontinuities, play a critical role in stochastic\nmodeling across various domains. However, inference for most L\\'evy processes,\nwhether in parametric or non-parametric settings, remains a significant\nchallenge. In this work, we present a novel Bayesian non-parametric inference\nframework for inferring the L\\'evy measures of subordinators and normal\nvariance-mean (NVM) processes within a linear L\\'evy state space model, a setup\nthat significantly extends existing methodologies. We employ the Dirichlet\nprocess which further results in a Student-t mixture representation to enable\ninference for the L\\'evy measures. An efficient augmented Markov Chain Monte\nCarlo algorithm is developed for this problem that ensures both accuracy and\ncomputational feasibility. The effectiveness of the method is demonstrated on\nsynthetic and tick-level (high-frequency) financial datasets, and we show the\npractical utility of the inference results using the forecasting performance."
                },
                "authors": [
                    {
                        "name": "Bill Z. Lin"
                    },
                    {
                        "name": "Simon Godsill"
                    }
                ],
                "author_detail": {
                    "name": "Simon Godsill"
                },
                "author": "Simon Godsill",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19318v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19318v3",
                "updated": "2025-05-29T02:06:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    2,
                    6,
                    32,
                    3,
                    149,
                    0
                ],
                "published": "2025-01-31T17:15:33Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    15,
                    33,
                    4,
                    31,
                    0
                ],
                "title": "MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented\n  Reinforcement in Embodied Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented\n  Reinforcement in Embodied Systems"
                },
                "summary": "While large language models (LLMs) have shown promising capabilities as\nzero-shot planners for embodied agents, their inability to learn from\nexperience and build persistent mental models limits their robustness in\ncomplex open-world environments like Minecraft. We introduce MINDSTORES, an\nexperience-augmented planning framework that enables embodied agents to build\nand leverage mental models through natural interaction with their environment.\nDrawing inspiration from how humans construct and refine cognitive mental\nmodels, our approach extends existing zero-shot LLM planning by maintaining a\ndatabase of past experiences that informs future planning iterations. The key\ninnovation is representing accumulated experiences as natural language\nembeddings of (state, task, plan, outcome) tuples, which can then be\nefficiently retrieved and reasoned over by an LLM planner to generate insights\nand guide plan refinement for novel states and tasks. Through extensive\nexperiments in the MineDojo environment, a simulation environment for agents in\nMinecraft that provides low-level controls for Minecraft, we find that\nMINDSTORES learns and applies its knowledge significantly better than existing\nmemory-based LLM planners while maintaining the flexibility and generalization\nbenefits of zero-shot approaches, representing an important step toward more\ncapable embodied AI systems that can learn continuously through natural\nexperience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have shown promising capabilities as\nzero-shot planners for embodied agents, their inability to learn from\nexperience and build persistent mental models limits their robustness in\ncomplex open-world environments like Minecraft. We introduce MINDSTORES, an\nexperience-augmented planning framework that enables embodied agents to build\nand leverage mental models through natural interaction with their environment.\nDrawing inspiration from how humans construct and refine cognitive mental\nmodels, our approach extends existing zero-shot LLM planning by maintaining a\ndatabase of past experiences that informs future planning iterations. The key\ninnovation is representing accumulated experiences as natural language\nembeddings of (state, task, plan, outcome) tuples, which can then be\nefficiently retrieved and reasoned over by an LLM planner to generate insights\nand guide plan refinement for novel states and tasks. Through extensive\nexperiments in the MineDojo environment, a simulation environment for agents in\nMinecraft that provides low-level controls for Minecraft, we find that\nMINDSTORES learns and applies its knowledge significantly better than existing\nmemory-based LLM planners while maintaining the flexibility and generalization\nbenefits of zero-shot approaches, representing an important step toward more\ncapable embodied AI systems that can learn continuously through natural\nexperience."
                },
                "authors": [
                    {
                        "name": "Anirudh Chari"
                    },
                    {
                        "name": "Suraj Reddy"
                    },
                    {
                        "name": "Aditya Tiwari"
                    },
                    {
                        "name": "Richard Lian"
                    },
                    {
                        "name": "Brian Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Brian Zhou"
                },
                "author": "Brian Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19318v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19318v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22586v1",
                "updated": "2025-05-28T16:58:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    58,
                    23,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T16:58:23Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    58,
                    23,
                    2,
                    148,
                    0
                ],
                "title": "Precise In-Parameter Concept Erasure in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precise In-Parameter Concept Erasure in Large Language Models"
                },
                "summary": "Large language models (LLMs) often acquire knowledge during pretraining that\nis undesirable in downstream deployments, e.g., sensitive information or\ncopyrighted content. Existing approaches for removing such knowledge rely on\nfine-tuning, training low-rank adapters or fact-level editing, but these are\neither too coarse, too shallow, or ineffective. In this work, we propose PISCES\n(Precise In-parameter Suppression for Concept EraSure), a novel framework for\nprecisely erasing entire concepts from model parameters by directly editing\ndirections that encode them in parameter space. PISCES uses a disentangler\nmodel to decompose MLP vectors into interpretable features, identifies those\nassociated with a target concept using automated interpretability techniques,\nand removes them from model parameters. Experiments on Gemma 2 and Llama 3.1\nover various concepts show that PISCES achieves modest gains in efficacy over\nleading erasure methods, reducing accuracy on the target concept to as low as\n7.7%, while dramatically improving erasure specificity (by up to 31%) and\nrobustness (by up to 38%). Overall, these results demonstrate that\nfeature-based in-parameter editing enables a more precise and reliable approach\nfor removing conceptual knowledge in language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often acquire knowledge during pretraining that\nis undesirable in downstream deployments, e.g., sensitive information or\ncopyrighted content. Existing approaches for removing such knowledge rely on\nfine-tuning, training low-rank adapters or fact-level editing, but these are\neither too coarse, too shallow, or ineffective. In this work, we propose PISCES\n(Precise In-parameter Suppression for Concept EraSure), a novel framework for\nprecisely erasing entire concepts from model parameters by directly editing\ndirections that encode them in parameter space. PISCES uses a disentangler\nmodel to decompose MLP vectors into interpretable features, identifies those\nassociated with a target concept using automated interpretability techniques,\nand removes them from model parameters. Experiments on Gemma 2 and Llama 3.1\nover various concepts show that PISCES achieves modest gains in efficacy over\nleading erasure methods, reducing accuracy on the target concept to as low as\n7.7%, while dramatically improving erasure specificity (by up to 31%) and\nrobustness (by up to 38%). Overall, these results demonstrate that\nfeature-based in-parameter editing enables a more precise and reliable approach\nfor removing conceptual knowledge in language models."
                },
                "authors": [
                    {
                        "name": "Yoav Gur-Arieh"
                    },
                    {
                        "name": "Clara Suslik"
                    },
                    {
                        "name": "Yihuai Hong"
                    },
                    {
                        "name": "Fazl Barez"
                    },
                    {
                        "name": "Mor Geva"
                    }
                ],
                "author_detail": {
                    "name": "Mor Geva"
                },
                "author": "Mor Geva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19255v2",
                "updated": "2025-05-28T16:58:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    58,
                    13,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-25T18:23:39Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    18,
                    23,
                    39,
                    6,
                    145,
                    0
                ],
                "title": "VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on\n  Multimodal Tool Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on\n  Multimodal Tool Use"
                },
                "summary": "Reinforcement Learning Finetuning (RFT) has significantly advanced the\nreasoning capabilities of large language models (LLMs) by enabling long chains\nof thought, self-correction, and effective tool use. While recent works attempt\nto extend RFT to vision-language models (VLMs), these efforts largely produce\ntext-only reasoning conditioned on static image inputs, falling short of true\nmultimodal reasoning in the response. In contrast, test-time methods like\nVisual Sketchpad incorporate visual steps but lack training mechanisms.\n  We introduce VTool-R1, the first framework that trains VLMs to generate\nmultimodal chains of thought by interleaving text and intermediate visual\nreasoning steps. VTool-R1 integrates Python-based visual editing tools into the\nRFT process, enabling VLMs to learn when and how to generate visual reasoning\nsteps that benefit final reasoning. Trained with outcome-based rewards tied to\ntask accuracy, our approach elicits strategic visual tool use for reasoning\nwithout relying on process-based supervision. Experiments on structured visual\nquestion answering over charts and tables show that VTool-R1 enhances reasoning\nperformance by teaching VLMs to \"think with images\" and generate multimodal\nchain of thoughts with tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning Finetuning (RFT) has significantly advanced the\nreasoning capabilities of large language models (LLMs) by enabling long chains\nof thought, self-correction, and effective tool use. While recent works attempt\nto extend RFT to vision-language models (VLMs), these efforts largely produce\ntext-only reasoning conditioned on static image inputs, falling short of true\nmultimodal reasoning in the response. In contrast, test-time methods like\nVisual Sketchpad incorporate visual steps but lack training mechanisms.\n  We introduce VTool-R1, the first framework that trains VLMs to generate\nmultimodal chains of thought by interleaving text and intermediate visual\nreasoning steps. VTool-R1 integrates Python-based visual editing tools into the\nRFT process, enabling VLMs to learn when and how to generate visual reasoning\nsteps that benefit final reasoning. Trained with outcome-based rewards tied to\ntask accuracy, our approach elicits strategic visual tool use for reasoning\nwithout relying on process-based supervision. Experiments on structured visual\nquestion answering over charts and tables show that VTool-R1 enhances reasoning\nperformance by teaching VLMs to \"think with images\" and generate multimodal\nchain of thoughts with tools."
                },
                "authors": [
                    {
                        "name": "Mingyuan Wu"
                    },
                    {
                        "name": "Jingcheng Yang"
                    },
                    {
                        "name": "Jize Jiang"
                    },
                    {
                        "name": "Meitang Li"
                    },
                    {
                        "name": "Kaizhuo Yan"
                    },
                    {
                        "name": "Hanchao Yu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Chengxiang Zhai"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    }
                ],
                "author_detail": {
                    "name": "Klara Nahrstedt"
                },
                "author": "Klara Nahrstedt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22584v1",
                "updated": "2025-05-28T16:56:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    56,
                    41,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T16:56:41Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    56,
                    41,
                    2,
                    148,
                    0
                ],
                "title": "DocReRank: Single-Page Hard Negative Query Generation for Training\n  Multi-Modal RAG Rerankers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DocReRank: Single-Page Hard Negative Query Generation for Training\n  Multi-Modal RAG Rerankers"
                },
                "summary": "Rerankers play a critical role in multimodal Retrieval-Augmented Generation\n(RAG) by refining ranking of an initial set of retrieved documents. Rerankers\nare typically trained using hard negative mining, whose goal is to select pages\nfor each query which rank high, but are actually irrelevant. However, this\nselection process is typically passive and restricted to what the retriever can\nfind in the available corpus, leading to several inherent limitations. These\ninclude: limited diversity, negative examples which are often not hard enough,\nlow controllability, and frequent false negatives which harm training. Our\npaper proposes an alternative approach: Single-Page Hard Negative Query\nGeneration, which goes the other way around. Instead of retrieving negative\npages per query, we generate hard negative queries per page. Using an automated\nLLM-VLM pipeline, and given a page and its positive query, we create hard\nnegatives by rephrasing the query to be as similar as possible in form and\ncontext, yet not answerable from the page. This paradigm enables fine-grained\ncontrol over the generated queries, resulting in diverse, hard, and targeted\nnegatives. It also supports efficient false negative verification. Our\nexperiments show that rerankers trained with data generated using our approach\noutperform existing models and significantly improve retrieval performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rerankers play a critical role in multimodal Retrieval-Augmented Generation\n(RAG) by refining ranking of an initial set of retrieved documents. Rerankers\nare typically trained using hard negative mining, whose goal is to select pages\nfor each query which rank high, but are actually irrelevant. However, this\nselection process is typically passive and restricted to what the retriever can\nfind in the available corpus, leading to several inherent limitations. These\ninclude: limited diversity, negative examples which are often not hard enough,\nlow controllability, and frequent false negatives which harm training. Our\npaper proposes an alternative approach: Single-Page Hard Negative Query\nGeneration, which goes the other way around. Instead of retrieving negative\npages per query, we generate hard negative queries per page. Using an automated\nLLM-VLM pipeline, and given a page and its positive query, we create hard\nnegatives by rephrasing the query to be as similar as possible in form and\ncontext, yet not answerable from the page. This paradigm enables fine-grained\ncontrol over the generated queries, resulting in diverse, hard, and targeted\nnegatives. It also supports efficient false negative verification. Our\nexperiments show that rerankers trained with data generated using our approach\noutperform existing models and significantly improve retrieval performance."
                },
                "authors": [
                    {
                        "name": "Navve Wasserman"
                    },
                    {
                        "name": "Oliver Heinimann"
                    },
                    {
                        "name": "Yuval Golbari"
                    },
                    {
                        "name": "Tal Zimbalist"
                    },
                    {
                        "name": "Eli Schwartz"
                    },
                    {
                        "name": "Michal Irani"
                    }
                ],
                "author_detail": {
                    "name": "Michal Irani"
                },
                "author": "Michal Irani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22582v1",
                "updated": "2025-05-28T16:54:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    54,
                    53,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T16:54:53Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    54,
                    53,
                    2,
                    148,
                    0
                ],
                "title": "Less, but Better: Efficient Multilingual Expansion for LLMs via\n  Layer-wise Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less, but Better: Efficient Multilingual Expansion for LLMs via\n  Layer-wise Mixture-of-Experts"
                },
                "summary": "Continually expanding new languages for existing large language models (LLMs)\nis a promising yet challenging approach to building powerful multilingual LLMs.\nThe biggest challenge is to make the model continuously learn new languages\nwhile preserving the proficient ability of old languages. To achieve this,\nrecent work utilizes the Mixture-of-Experts (MoE) architecture to expand new\nlanguages by adding new experts and avoid catastrophic forgetting of old\nlanguages by routing corresponding tokens to the original model backbone (old\nexperts). Although intuitive, this kind of method is parameter-costly when\nexpanding new languages and still inevitably impacts the performance of old\nlanguages. To address these limitations, we analyze the language\ncharacteristics of different layers in LLMs and propose a layer-wise expert\nallocation algorithm (LayerMoE) to determine the appropriate number of new\nexperts for each layer. Specifically, we find different layers in LLMs exhibit\ndifferent representation similarities between languages and then utilize the\nsimilarity as the indicator to allocate experts for each layer, i.e., the\nhigher similarity, the fewer experts. Additionally, to further mitigate the\nforgetting of old languages, we add a classifier in front of the router network\non the layers with higher similarity to guide the routing of old language\ntokens. Experimental results show that our method outperforms the previous\nstate-of-the-art baseline with 60% fewer experts in the single-expansion\nsetting and with 33.3% fewer experts in the lifelong-expansion setting,\ndemonstrating the effectiveness of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continually expanding new languages for existing large language models (LLMs)\nis a promising yet challenging approach to building powerful multilingual LLMs.\nThe biggest challenge is to make the model continuously learn new languages\nwhile preserving the proficient ability of old languages. To achieve this,\nrecent work utilizes the Mixture-of-Experts (MoE) architecture to expand new\nlanguages by adding new experts and avoid catastrophic forgetting of old\nlanguages by routing corresponding tokens to the original model backbone (old\nexperts). Although intuitive, this kind of method is parameter-costly when\nexpanding new languages and still inevitably impacts the performance of old\nlanguages. To address these limitations, we analyze the language\ncharacteristics of different layers in LLMs and propose a layer-wise expert\nallocation algorithm (LayerMoE) to determine the appropriate number of new\nexperts for each layer. Specifically, we find different layers in LLMs exhibit\ndifferent representation similarities between languages and then utilize the\nsimilarity as the indicator to allocate experts for each layer, i.e., the\nhigher similarity, the fewer experts. Additionally, to further mitigate the\nforgetting of old languages, we add a classifier in front of the router network\non the layers with higher similarity to guide the routing of old language\ntokens. Experimental results show that our method outperforms the previous\nstate-of-the-art baseline with 60% fewer experts in the single-expansion\nsetting and with 33.3% fewer experts in the lifelong-expansion setting,\ndemonstrating the effectiveness of our method."
                },
                "authors": [
                    {
                        "name": "Xue Zhang"
                    },
                    {
                        "name": "Yunlong Liang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Songming Zhang"
                    },
                    {
                        "name": "Yufeng Chen"
                    },
                    {
                        "name": "Jinan Xu"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "arxiv_comment": "ACL 2025 (Main), 16 pages, 5 figures, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22574v1",
                "updated": "2025-05-28T16:47:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    47,
                    2,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T16:47:02Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    47,
                    2,
                    2,
                    148,
                    0
                ],
                "title": "Attention-based Neural Network Emulators for Multi-Probe Data Vectors\n  Part III: Modeling The Next Generation Surveys",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-based Neural Network Emulators for Multi-Probe Data Vectors\n  Part III: Modeling The Next Generation Surveys"
                },
                "summary": "Machine learning can accelerate cosmological inferences that involve many\nsequential evaluations of computationally expensive data vectors. Previous\nworks in this series have examined how machine learning architectures impact\nemulator accuracy and training time for optical shear and galaxy clustering\n2-point function. In this final manuscript, we explore neural network\nperformance when emulating Cosmic Microwave Background temperature and\npolarization power spectra. We maximize the volume of applicability in the\nparameter space of our emulators within the standard $\\Lambda$-cold-dark-matter\nmodel while ensuring that errors are below cosmic variance. Relative to\nstandard multi-layer perceptron architectures, we find the\ndot-product-attention mechanism reduces the number of outliers among testing\ncosmologies, defined as the fraction of testing points with $\\Delta \\chi^2 >\n0.2$ relative to \\textsc{CAMB} outputs, for a wide range of training set sizes.\nSuch precision enables attention-based emulators to be directly applied to real\ndata without requiring any additional correction via importance sampling.\nCombined with pre-processing techniques and optimized activation and loss\nfunctions, attention-based models can meet the precision criteria set by\ncurrent and future CMB and lensing experiments. For each of Planck, Simons\nObservatory, CMB S4, and CMB HD, we find the fraction of outlier points to be\nless than $10\\%$ with around $2\\times10^5$ to $4\\times10^5$ training data\nvectors. We further explore the applications of these methods to supernova\ndistance, weak lensing, and galaxy clustering, as well as alternative\narchitectures and pre-processing techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning can accelerate cosmological inferences that involve many\nsequential evaluations of computationally expensive data vectors. Previous\nworks in this series have examined how machine learning architectures impact\nemulator accuracy and training time for optical shear and galaxy clustering\n2-point function. In this final manuscript, we explore neural network\nperformance when emulating Cosmic Microwave Background temperature and\npolarization power spectra. We maximize the volume of applicability in the\nparameter space of our emulators within the standard $\\Lambda$-cold-dark-matter\nmodel while ensuring that errors are below cosmic variance. Relative to\nstandard multi-layer perceptron architectures, we find the\ndot-product-attention mechanism reduces the number of outliers among testing\ncosmologies, defined as the fraction of testing points with $\\Delta \\chi^2 >\n0.2$ relative to \\textsc{CAMB} outputs, for a wide range of training set sizes.\nSuch precision enables attention-based emulators to be directly applied to real\ndata without requiring any additional correction via importance sampling.\nCombined with pre-processing techniques and optimized activation and loss\nfunctions, attention-based models can meet the precision criteria set by\ncurrent and future CMB and lensing experiments. For each of Planck, Simons\nObservatory, CMB S4, and CMB HD, we find the fraction of outlier points to be\nless than $10\\%$ with around $2\\times10^5$ to $4\\times10^5$ training data\nvectors. We further explore the applications of these methods to supernova\ndistance, weak lensing, and galaxy clustering, as well as alternative\narchitectures and pre-processing techniques."
                },
                "authors": [
                    {
                        "name": "Yijie Zhu"
                    },
                    {
                        "name": "Evan Saraivanov"
                    },
                    {
                        "name": "Joshua A. Kable"
                    },
                    {
                        "name": "Artemis Sofia Giannakopoulou"
                    },
                    {
                        "name": "Amritpal Nijjar"
                    },
                    {
                        "name": "Vivian Miranda"
                    },
                    {
                        "name": "Marco Bonici"
                    },
                    {
                        "name": "Tim Eifler"
                    },
                    {
                        "name": "Elisabeth Krause"
                    }
                ],
                "author_detail": {
                    "name": "Elisabeth Krause"
                },
                "author": "Elisabeth Krause",
                "arxiv_comment": "27 pages, 24 figures, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22573v1",
                "updated": "2025-05-28T16:46:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    46,
                    56,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T16:46:56Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    46,
                    56,
                    2,
                    148,
                    0
                ],
                "title": "FNOPE: Simulation-based inference on function spaces with Fourier Neural\n  Operators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FNOPE: Simulation-based inference on function spaces with Fourier Neural\n  Operators"
                },
                "summary": "Simulation-based inference (SBI) is an established approach for performing\nBayesian inference on scientific simulators. SBI so far works best on\nlow-dimensional parametric models. However, it is difficult to infer\nfunction-valued parameters, which frequently occur in disciplines that model\nspatiotemporal processes such as the climate and earth sciences. Here, we\nintroduce an approach for efficient posterior estimation, using a Fourier\nNeural Operator (FNO) architecture with a flow matching objective. We show that\nour approach, FNOPE, can perform inference of function-valued parameters at a\nfraction of the simulation budget of state of the art methods. In addition,\nFNOPE supports posterior evaluation at arbitrary discretizations of the domain,\nas well as simultaneous estimation of vector-valued parameters. We demonstrate\nthe effectiveness of our approach on several benchmark tasks and a challenging\nspatial inference task from glaciology. FNOPE extends the applicability of SBI\nmethods to new scientific domains by enabling the inference of function-valued\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based inference (SBI) is an established approach for performing\nBayesian inference on scientific simulators. SBI so far works best on\nlow-dimensional parametric models. However, it is difficult to infer\nfunction-valued parameters, which frequently occur in disciplines that model\nspatiotemporal processes such as the climate and earth sciences. Here, we\nintroduce an approach for efficient posterior estimation, using a Fourier\nNeural Operator (FNO) architecture with a flow matching objective. We show that\nour approach, FNOPE, can perform inference of function-valued parameters at a\nfraction of the simulation budget of state of the art methods. In addition,\nFNOPE supports posterior evaluation at arbitrary discretizations of the domain,\nas well as simultaneous estimation of vector-valued parameters. We demonstrate\nthe effectiveness of our approach on several benchmark tasks and a challenging\nspatial inference task from glaciology. FNOPE extends the applicability of SBI\nmethods to new scientific domains by enabling the inference of function-valued\nparameters."
                },
                "authors": [
                    {
                        "name": "Guy Moss"
                    },
                    {
                        "name": "Leah Sophie Muhle"
                    },
                    {
                        "name": "Reinhard Drews"
                    },
                    {
                        "name": "Jakob H. Macke"
                    },
                    {
                        "name": "Cornelius Schröder"
                    }
                ],
                "author_detail": {
                    "name": "Cornelius Schröder"
                },
                "author": "Cornelius Schröder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22572v1",
                "updated": "2025-05-28T16:46:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    46,
                    55,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T16:46:55Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    46,
                    55,
                    2,
                    148,
                    0
                ],
                "title": "Fusion Steering: Prompt-Specific Activation Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusion Steering: Prompt-Specific Activation Control"
                },
                "summary": "We present Fusion Steering, an activation steering methodology that improves\nfactual accuracy in large language models (LLMs) for question-answering (QA)\ntasks. This approach introduces flexible steering configurations, including\nfull-layer steering and segmented steering. Unlike traditional methods\nconstrained to single-layer or fixed-layer operations, Fusion Steering employs\ndynamic injection of prompt-specific activation deltas across all transformer\nlayers. These activation deltas are derived from reference completions that\ncombine the ground-truth answer with a model-generated explanation to\nfacilitate semantically enriched, example-specific steering. The injection\nweights are optimized per prompt using Optuna, targeting a joint objective that\nbalances token overlap (factual alignment) and perplexity (fluency proxy).\nEvaluation employs a composite score integrating token overlap and LLM-graded\nquality, encompassing factual accuracy, coherence, and relevance. Empirical\nresults on 260 SimpleQA prompts (selected from 500 where the baseline failed)\nshowcase the efficacy of segmented steering. Using Gemma-2-2B-IT with 8-bit\nquantization, segmented steering achieves an accuracy of 25.4% (outputs scoring\n$\\geq 0.6$), outperforming the baseline at 3.5% and full-layer steering at\n16.2%. Under the stricter SimpleQA rubric, segmented steering boosts fully\ncorrect responses from 0.0% to 13.1%. These findings highlight the strengths of\nsegmented, dynamic intervention strategies and the promise of per-prompt,\nfull-network activation control. Fusion Steering is also amenable to sparse\nrepresentations, such as Neuronpedia or sparse crosscoders, suggesting a\npromising direction for interpretable and scalable activation-level control in\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Fusion Steering, an activation steering methodology that improves\nfactual accuracy in large language models (LLMs) for question-answering (QA)\ntasks. This approach introduces flexible steering configurations, including\nfull-layer steering and segmented steering. Unlike traditional methods\nconstrained to single-layer or fixed-layer operations, Fusion Steering employs\ndynamic injection of prompt-specific activation deltas across all transformer\nlayers. These activation deltas are derived from reference completions that\ncombine the ground-truth answer with a model-generated explanation to\nfacilitate semantically enriched, example-specific steering. The injection\nweights are optimized per prompt using Optuna, targeting a joint objective that\nbalances token overlap (factual alignment) and perplexity (fluency proxy).\nEvaluation employs a composite score integrating token overlap and LLM-graded\nquality, encompassing factual accuracy, coherence, and relevance. Empirical\nresults on 260 SimpleQA prompts (selected from 500 where the baseline failed)\nshowcase the efficacy of segmented steering. Using Gemma-2-2B-IT with 8-bit\nquantization, segmented steering achieves an accuracy of 25.4% (outputs scoring\n$\\geq 0.6$), outperforming the baseline at 3.5% and full-layer steering at\n16.2%. Under the stricter SimpleQA rubric, segmented steering boosts fully\ncorrect responses from 0.0% to 13.1%. These findings highlight the strengths of\nsegmented, dynamic intervention strategies and the promise of per-prompt,\nfull-network activation control. Fusion Steering is also amenable to sparse\nrepresentations, such as Neuronpedia or sparse crosscoders, suggesting a\npromising direction for interpretable and scalable activation-level control in\nLLMs."
                },
                "authors": [
                    {
                        "name": "Waldemar Chang"
                    },
                    {
                        "name": "Alhassan Yasin"
                    }
                ],
                "author_detail": {
                    "name": "Alhassan Yasin"
                },
                "author": "Alhassan Yasin",
                "arxiv_comment": "14 pages, 4 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22571v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22571v2",
                "updated": "2025-05-29T01:52:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    1,
                    52,
                    23,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-28T16:46:31Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    46,
                    31,
                    2,
                    148,
                    0
                ],
                "title": "Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified\n  Retrieval-Augmented Generation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified\n  Retrieval-Augmented Generation Systems"
                },
                "summary": "This paper presents a novel approach for unified retrieval-augmented\ngeneration (RAG) systems using the recent emerging large language model (LLM)\nagent concept. Specifically, Agent LLM, which utilizes LLM as fundamental\ncontrollers, has become a promising approach to enable the interpretability of\nRAG tasks, especially for complex reasoning question-answering systems (e.g.,\nmulti-hop queries). Nonetheless, previous works mainly focus on solving RAG\nsystems with either single-hop or multi-hop approaches separately, which limits\nthe application of those approaches to real-world applications. In this study,\nwe propose a trainable agent framework called Agent-UniRAG for unified\nretrieval-augmented LLM systems, which enhances the effectiveness and\ninterpretability of RAG systems. The main idea is to design an LLM agent\nframework to solve RAG tasks step-by-step based on the complexity of the\ninputs, simultaneously including single-hop and multi-hop queries in an\nend-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset\nto enable the proposed agent framework for small open-source LLMs (e.g.,\nLlama-3-8B). The results show comparable performances with closed-source and\nlarger open-source LLMs across various RAG benchmarks. Our source code and\ndataset are publicly available for further exploitation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach for unified retrieval-augmented\ngeneration (RAG) systems using the recent emerging large language model (LLM)\nagent concept. Specifically, Agent LLM, which utilizes LLM as fundamental\ncontrollers, has become a promising approach to enable the interpretability of\nRAG tasks, especially for complex reasoning question-answering systems (e.g.,\nmulti-hop queries). Nonetheless, previous works mainly focus on solving RAG\nsystems with either single-hop or multi-hop approaches separately, which limits\nthe application of those approaches to real-world applications. In this study,\nwe propose a trainable agent framework called Agent-UniRAG for unified\nretrieval-augmented LLM systems, which enhances the effectiveness and\ninterpretability of RAG systems. The main idea is to design an LLM agent\nframework to solve RAG tasks step-by-step based on the complexity of the\ninputs, simultaneously including single-hop and multi-hop queries in an\nend-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset\nto enable the proposed agent framework for small open-source LLMs (e.g.,\nLlama-3-8B). The results show comparable performances with closed-source and\nlarger open-source LLMs across various RAG benchmarks. Our source code and\ndataset are publicly available for further exploitation."
                },
                "authors": [
                    {
                        "name": "Hoang Pham"
                    },
                    {
                        "name": "Thuy-Duong Nguyen"
                    },
                    {
                        "name": "Khac-Hoai Nam Bui"
                    }
                ],
                "author_detail": {
                    "name": "Khac-Hoai Nam Bui"
                },
                "author": "Khac-Hoai Nam Bui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22571v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22571v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22566v1",
                "updated": "2025-05-28T16:43:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    43,
                    1,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T16:43:01Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    43,
                    1,
                    2,
                    148,
                    0
                ],
                "title": "Universal Visuo-Tactile Video Understanding for Embodied Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Visuo-Tactile Video Understanding for Embodied Interaction"
                },
                "summary": "Tactile perception is essential for embodied agents to understand physical\nattributes of objects that cannot be determined through visual inspection\nalone. While existing approaches have made progress in visual and language\nmodalities for physical understanding, they fail to effectively incorporate\ntactile information that provides crucial haptic feedback for real-world\ninteraction. In this paper, we present VTV-LLM, the first multi-modal large\nlanguage model for universal Visuo-Tactile Video (VTV) understanding that\nbridges the gap between tactile perception and natural language. To address the\nchallenges of cross-sensor and cross-modal integration, we contribute VTV150K,\na comprehensive dataset comprising 150,000 video frames from 100 diverse\nobjects captured across three different tactile sensors (GelSight Mini, DIGIT,\nand Tac3D), annotated with four fundamental tactile attributes (hardness,\nprotrusion, elasticity, and friction). We develop a novel three-stage training\nparadigm that includes VTV enhancement for robust visuo-tactile representation,\nVTV-text alignment for cross-modal correspondence, and text prompt finetuning\nfor natural language generation. Our framework enables sophisticated tactile\nreasoning capabilities including feature assessment, comparative analysis,\nscenario-based decision making and so on. Experimental evaluations demonstrate\nthat VTV-LLM achieves superior performance in tactile video understanding\ntasks, establishing a foundation for more intuitive human-machine interaction\nin tactile domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tactile perception is essential for embodied agents to understand physical\nattributes of objects that cannot be determined through visual inspection\nalone. While existing approaches have made progress in visual and language\nmodalities for physical understanding, they fail to effectively incorporate\ntactile information that provides crucial haptic feedback for real-world\ninteraction. In this paper, we present VTV-LLM, the first multi-modal large\nlanguage model for universal Visuo-Tactile Video (VTV) understanding that\nbridges the gap between tactile perception and natural language. To address the\nchallenges of cross-sensor and cross-modal integration, we contribute VTV150K,\na comprehensive dataset comprising 150,000 video frames from 100 diverse\nobjects captured across three different tactile sensors (GelSight Mini, DIGIT,\nand Tac3D), annotated with four fundamental tactile attributes (hardness,\nprotrusion, elasticity, and friction). We develop a novel three-stage training\nparadigm that includes VTV enhancement for robust visuo-tactile representation,\nVTV-text alignment for cross-modal correspondence, and text prompt finetuning\nfor natural language generation. Our framework enables sophisticated tactile\nreasoning capabilities including feature assessment, comparative analysis,\nscenario-based decision making and so on. Experimental evaluations demonstrate\nthat VTV-LLM achieves superior performance in tactile video understanding\ntasks, establishing a foundation for more intuitive human-machine interaction\nin tactile domains."
                },
                "authors": [
                    {
                        "name": "Yifan Xie"
                    },
                    {
                        "name": "Mingyang Li"
                    },
                    {
                        "name": "Shoujie Li"
                    },
                    {
                        "name": "Xingting Li"
                    },
                    {
                        "name": "Guangyu Chen"
                    },
                    {
                        "name": "Fei Ma"
                    },
                    {
                        "name": "Fei Richard Yu"
                    },
                    {
                        "name": "Wenbo Ding"
                    }
                ],
                "author_detail": {
                    "name": "Wenbo Ding"
                },
                "author": "Wenbo Ding",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12051v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12051v3",
                "updated": "2025-05-28T16:40:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    40,
                    24,
                    2,
                    148,
                    0
                ],
                "published": "2025-03-15T08:54:25Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    8,
                    54,
                    25,
                    5,
                    74,
                    0
                ],
                "title": "TLUE: A Tibetan Language Understanding Evaluation Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TLUE: A Tibetan Language Understanding Evaluation Benchmark"
                },
                "summary": "Large language models (LLMs) have made tremendous progress in recent years,\nbut low-resource languages, such as Tibetan, remain significantly\nunderrepresented in their evaluation. Despite Tibetan being spoken by over\nseven million people, it has largely been neglected in the development and\nassessment of LLMs. To address this gap, we present TLUE (A Tibetan Language\nUnderstanding Evaluation Benchmark), the first large-scale benchmark for\nassessing LLMs' capabilities in Tibetan. TLUE comprises two major components:\n(1) a comprehensive multi-task understanding benchmark spanning 5 domains and\n67 subdomains, and (2) a safety benchmark covering 7 subdomains. We evaluate a\ndiverse set of state-of-the-art LLMs. Experimental results demonstrate that\nmost LLMs perform below the random baseline, highlighting the considerable\nchallenges LLMs face in processing Tibetan, a low-resource language. TLUE\nprovides an essential foundation for driving future research and progress in\nTibetan language understanding and underscores the need for greater inclusivity\nin LLM development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made tremendous progress in recent years,\nbut low-resource languages, such as Tibetan, remain significantly\nunderrepresented in their evaluation. Despite Tibetan being spoken by over\nseven million people, it has largely been neglected in the development and\nassessment of LLMs. To address this gap, we present TLUE (A Tibetan Language\nUnderstanding Evaluation Benchmark), the first large-scale benchmark for\nassessing LLMs' capabilities in Tibetan. TLUE comprises two major components:\n(1) a comprehensive multi-task understanding benchmark spanning 5 domains and\n67 subdomains, and (2) a safety benchmark covering 7 subdomains. We evaluate a\ndiverse set of state-of-the-art LLMs. Experimental results demonstrate that\nmost LLMs perform below the random baseline, highlighting the considerable\nchallenges LLMs face in processing Tibetan, a low-resource language. TLUE\nprovides an essential foundation for driving future research and progress in\nTibetan language understanding and underscores the need for greater inclusivity\nin LLM development."
                },
                "authors": [
                    {
                        "name": "Fan Gao"
                    },
                    {
                        "name": "Cheng Huang"
                    },
                    {
                        "name": "Nyima Tashi"
                    },
                    {
                        "name": "Xiangxiang Wang"
                    },
                    {
                        "name": "Thupten Tsering"
                    },
                    {
                        "name": "Ban Ma-bao"
                    },
                    {
                        "name": "Renzeg Duojie"
                    },
                    {
                        "name": "Gadeng Luosang"
                    },
                    {
                        "name": "Rinchen Dongrub"
                    },
                    {
                        "name": "Dorje Tashi"
                    },
                    {
                        "name": "Hao Wang Xiao Feng"
                    },
                    {
                        "name": "Yongbin Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Yu"
                },
                "author": "Yongbin Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12051v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22563v1",
                "updated": "2025-05-28T16:40:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    40,
                    6,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T16:40:06Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    40,
                    6,
                    2,
                    148,
                    0
                ],
                "title": "Do Large Language Models Think Like the Brain? Sentence-Level Evidence\n  from fMRI and Hierarchical Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Think Like the Brain? Sentence-Level Evidence\n  from fMRI and Hierarchical Embeddings"
                },
                "summary": "Understanding whether large language models (LLMs) and the human brain\nconverge on similar computational principles remains a fundamental and\nimportant question in cognitive neuroscience and AI. Do the brain-like patterns\nobserved in LLMs emerge simply from scaling, or do they reflect deeper\nalignment with the architecture of human language processing? This study\nfocuses on the sentence-level neural mechanisms of language models,\nsystematically investigating how hierarchical representations in LLMs align\nwith the dynamic neural responses during human sentence comprehension. By\ncomparing hierarchical embeddings from 14 publicly available LLMs with fMRI\ndata collected from participants, who were exposed to a naturalistic narrative\nstory, we constructed sentence-level neural prediction models to precisely\nidentify the model layers most significantly correlated with brain region\nactivations. Results show that improvements in model performance drive the\nevolution of representational architectures toward brain-like hierarchies,\nparticularly achieving stronger functional and anatomical correspondence at\nhigher semantic abstraction levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding whether large language models (LLMs) and the human brain\nconverge on similar computational principles remains a fundamental and\nimportant question in cognitive neuroscience and AI. Do the brain-like patterns\nobserved in LLMs emerge simply from scaling, or do they reflect deeper\nalignment with the architecture of human language processing? This study\nfocuses on the sentence-level neural mechanisms of language models,\nsystematically investigating how hierarchical representations in LLMs align\nwith the dynamic neural responses during human sentence comprehension. By\ncomparing hierarchical embeddings from 14 publicly available LLMs with fMRI\ndata collected from participants, who were exposed to a naturalistic narrative\nstory, we constructed sentence-level neural prediction models to precisely\nidentify the model layers most significantly correlated with brain region\nactivations. Results show that improvements in model performance drive the\nevolution of representational architectures toward brain-like hierarchies,\nparticularly achieving stronger functional and anatomical correspondence at\nhigher semantic abstraction levels."
                },
                "authors": [
                    {
                        "name": "Yu Lei"
                    },
                    {
                        "name": "Xingyang Ge"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Yiming Yang"
                    },
                    {
                        "name": "Bolei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Bolei Ma"
                },
                "author": "Bolei Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22552v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22552v1",
                "updated": "2025-05-28T16:34:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    34,
                    14,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T16:34:14Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    34,
                    14,
                    2,
                    148,
                    0
                ],
                "title": "ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation\n  with Lightweight Specialized LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation\n  with Lightweight Specialized LLM"
                },
                "summary": "Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of\nlarge language models (LLMs) is an emerging research challenge in claim\nverification. While KGs provide structured, semantically rich representations\nwell-suited for reasoning, most existing verification methods rely on\nunstructured text corpora, limiting their ability to effectively leverage KGs.\nAdditionally, despite possessing strong reasoning abilities, modern LLMs\nstruggle with multi-step modular pipelines and reasoning over KGs without\nadaptation. To address these challenges, we propose ClaimPKG, an end-to-end\nframework that seamlessly integrates LLM reasoning with structured knowledge\nfrom KGs. Specifically, the main idea of ClaimPKG is to employ a lightweight,\nspecialized LLM to represent the input claim as pseudo-subgraphs, guiding a\ndedicated subgraph retrieval module to identify relevant KG subgraphs. These\nretrieved subgraphs are then processed by a general-purpose LLM to produce the\nfinal verdict and justification. Extensive experiments on the FactKG dataset\ndemonstrate that ClaimPKG achieves state-of-the-art performance, outperforming\nstrong baselines in this research field by 9%-12% accuracy points across\nmultiple categories. Furthermore, ClaimPKG exhibits zero-shot generalizability\nto unstructured datasets such as HoVer and FEVEROUS, effectively combining\nstructured knowledge from KGs with LLM reasoning across various LLM backbones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of\nlarge language models (LLMs) is an emerging research challenge in claim\nverification. While KGs provide structured, semantically rich representations\nwell-suited for reasoning, most existing verification methods rely on\nunstructured text corpora, limiting their ability to effectively leverage KGs.\nAdditionally, despite possessing strong reasoning abilities, modern LLMs\nstruggle with multi-step modular pipelines and reasoning over KGs without\nadaptation. To address these challenges, we propose ClaimPKG, an end-to-end\nframework that seamlessly integrates LLM reasoning with structured knowledge\nfrom KGs. Specifically, the main idea of ClaimPKG is to employ a lightweight,\nspecialized LLM to represent the input claim as pseudo-subgraphs, guiding a\ndedicated subgraph retrieval module to identify relevant KG subgraphs. These\nretrieved subgraphs are then processed by a general-purpose LLM to produce the\nfinal verdict and justification. Extensive experiments on the FactKG dataset\ndemonstrate that ClaimPKG achieves state-of-the-art performance, outperforming\nstrong baselines in this research field by 9%-12% accuracy points across\nmultiple categories. Furthermore, ClaimPKG exhibits zero-shot generalizability\nto unstructured datasets such as HoVer and FEVEROUS, effectively combining\nstructured knowledge from KGs with LLM reasoning across various LLM backbones."
                },
                "authors": [
                    {
                        "name": "Hoang Pham"
                    },
                    {
                        "name": "Thanh-Do Nguyen"
                    },
                    {
                        "name": "Khac-Hoai Nam Bui"
                    }
                ],
                "author_detail": {
                    "name": "Khac-Hoai Nam Bui"
                },
                "author": "Khac-Hoai Nam Bui",
                "arxiv_comment": "Accepted by ACL 2025 findings",
                "arxiv_journal_ref": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22552v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22552v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22548v1",
                "updated": "2025-05-28T16:32:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    32,
                    16,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T16:32:16Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    32,
                    16,
                    2,
                    148,
                    0
                ],
                "title": "Emotion-o1: Adaptive Long Reasoning for Emotion Understanding in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotion-o1: Adaptive Long Reasoning for Emotion Understanding in LLMs"
                },
                "summary": "Emotion understanding includes basic tasks (e.g., sentiment/emotion\nclassification) and advanced tasks (e.g., sarcasm/humor detection). Current\nmethods rely on fixed-length CoT reasoning, failing to adapt to the varying\ncomplexity of emotions. We propose a task-adaptive reasoning framework that\nemploys DeepSeek-R1 to generate variable-length reasoning chains for different\nemotion tasks. By combining fine-tuning with reinforcement learning, we design\na composite reward function that balances four objectives: prediction accuracy,\nadaptive reasoning depth control, structural diversity in reasoning paths, and\nsuppression of repetitive logic. This approach achieves dynamic\ncontext-sensitive inference while enabling LLMs to autonomously develop deep\nreasoning capabilities. Experimental results demonstrate consistent\nimprovements in both Acc and F1 scores across four tasks: emotion, sentiment,\nhumor, and sarcasm. Notably, peak enhancements reached 3.56% F1 (2.76% Acc) for\nbasic tasks and 37.95% F1 (23.14% Acc) for advanced tasks. Our work bridges\nrigid CoT reasoning and emotional complexity through adaptive-depth analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotion understanding includes basic tasks (e.g., sentiment/emotion\nclassification) and advanced tasks (e.g., sarcasm/humor detection). Current\nmethods rely on fixed-length CoT reasoning, failing to adapt to the varying\ncomplexity of emotions. We propose a task-adaptive reasoning framework that\nemploys DeepSeek-R1 to generate variable-length reasoning chains for different\nemotion tasks. By combining fine-tuning with reinforcement learning, we design\na composite reward function that balances four objectives: prediction accuracy,\nadaptive reasoning depth control, structural diversity in reasoning paths, and\nsuppression of repetitive logic. This approach achieves dynamic\ncontext-sensitive inference while enabling LLMs to autonomously develop deep\nreasoning capabilities. Experimental results demonstrate consistent\nimprovements in both Acc and F1 scores across four tasks: emotion, sentiment,\nhumor, and sarcasm. Notably, peak enhancements reached 3.56% F1 (2.76% Acc) for\nbasic tasks and 37.95% F1 (23.14% Acc) for advanced tasks. Our work bridges\nrigid CoT reasoning and emotional complexity through adaptive-depth analysis."
                },
                "authors": [
                    {
                        "name": "Changhao Song"
                    },
                    {
                        "name": "Yazhou Zhang"
                    },
                    {
                        "name": "Peng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Zhang"
                },
                "author": "Peng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22541v1",
                "updated": "2025-05-28T16:23:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    23,
                    48,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T16:23:48Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    23,
                    48,
                    2,
                    148,
                    0
                ],
                "title": "A Human-Centric Approach to Explainable AI for Personalized Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Human-Centric Approach to Explainable AI for Personalized Education"
                },
                "summary": "Deep neural networks form the backbone of artificial intelligence research,\nwith potential to transform the human experience in areas ranging from\nautonomous driving to personal assistants, healthcare to education. However,\ntheir integration into the daily routines of real-world classrooms remains\nlimited. It is not yet common for a teacher to assign students individualized\nhomework targeting their specific weaknesses, provide students with instant\nfeedback, or simulate student responses to a new exam question. While these\nmodels excel in predictive performance, this lack of adoption can be attributed\nto a significant weakness: the lack of explainability of model decisions,\nleading to a lack of trust from students, parents, and teachers. This thesis\naims to bring human needs to the forefront of eXplainable AI (XAI) research,\ngrounded in the concrete use case of personalized learning and teaching. We\nframe the contributions along two verticals: technical advances in XAI and\ntheir aligned human studies. We investigate explainability in AI for education,\nrevealing systematic disagreements between post-hoc explainers and identifying\na need for inherently interpretable model architectures. We propose four novel\ntechnical contributions in interpretability with a multimodal modular\narchitecture (MultiModN), an interpretable mixture-of-experts model\n(InterpretCC), adversarial training for explainer stability, and a\ntheory-driven LLM-XAI framework to present explanations to students\n(iLLuMinaTE), which we evaluate in diverse settings with professors, teachers,\nlearning scientists, and university students. By combining empirical\nevaluations of existing explainers with novel architectural designs and human\nstudies, our work lays a foundation for human-centric AI systems that balance\nstate-of-the-art performance with built-in transparency and trust.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks form the backbone of artificial intelligence research,\nwith potential to transform the human experience in areas ranging from\nautonomous driving to personal assistants, healthcare to education. However,\ntheir integration into the daily routines of real-world classrooms remains\nlimited. It is not yet common for a teacher to assign students individualized\nhomework targeting their specific weaknesses, provide students with instant\nfeedback, or simulate student responses to a new exam question. While these\nmodels excel in predictive performance, this lack of adoption can be attributed\nto a significant weakness: the lack of explainability of model decisions,\nleading to a lack of trust from students, parents, and teachers. This thesis\naims to bring human needs to the forefront of eXplainable AI (XAI) research,\ngrounded in the concrete use case of personalized learning and teaching. We\nframe the contributions along two verticals: technical advances in XAI and\ntheir aligned human studies. We investigate explainability in AI for education,\nrevealing systematic disagreements between post-hoc explainers and identifying\na need for inherently interpretable model architectures. We propose four novel\ntechnical contributions in interpretability with a multimodal modular\narchitecture (MultiModN), an interpretable mixture-of-experts model\n(InterpretCC), adversarial training for explainer stability, and a\ntheory-driven LLM-XAI framework to present explanations to students\n(iLLuMinaTE), which we evaluate in diverse settings with professors, teachers,\nlearning scientists, and university students. By combining empirical\nevaluations of existing explainers with novel architectural designs and human\nstudies, our work lays a foundation for human-centric AI systems that balance\nstate-of-the-art performance with built-in transparency and trust."
                },
                "authors": [
                    {
                        "name": "Vinitra Swamy"
                    }
                ],
                "author_detail": {
                    "name": "Vinitra Swamy"
                },
                "author": "Vinitra Swamy",
                "arxiv_comment": "PhD Thesis, EPFL (Computer Science)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22523v1",
                "updated": "2025-05-28T16:09:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    9,
                    33,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T16:09:33Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    9,
                    33,
                    2,
                    148,
                    0
                ],
                "title": "PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image\n  Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image\n  Generative Models"
                },
                "summary": "Generating high-quality, multi-layer transparent images from text prompts can\nunlock a new level of creative control, allowing users to edit each layer as\neffortlessly as editing text outputs from LLMs. However, the development of\nmulti-layer generative models lags behind that of conventional text-to-image\nmodels due to the absence of a large, high-quality corpus of multi-layer\ntransparent data. In this paper, we address this fundamental challenge by: (i)\nreleasing the first open, ultra-high-fidelity PrismLayers (PrismLayersPro)\ndataset of 200K (20K) multilayer transparent images with accurate alpha mattes,\n(ii) introducing a trainingfree synthesis pipeline that generates such data on\ndemand using off-the-shelf diffusion models, and (iii) delivering a strong,\nopen-source multi-layer generation model, ART+, which matches the aesthetics of\nmodern text-to-image generation models. The key technical contributions\ninclude: LayerFLUX, which excels at generating high-quality single transparent\nlayers with accurate alpha mattes, and MultiLayerFLUX, which composes multiple\nLayerFLUX outputs into complete images, guided by human-annotated semantic\nlayout. To ensure higher quality, we apply a rigorous filtering stage to remove\nartifacts and semantic mismatches, followed by human selection. Fine-tuning the\nstate-of-the-art ART model on our synthetic PrismLayersPro yields ART+, which\noutperforms the original ART in 60% of head-to-head user study comparisons and\neven matches the visual quality of images generated by the FLUX.1-[dev] model.\nWe anticipate that our work will establish a solid dataset foundation for the\nmulti-layer transparent image generation task, enabling research and\napplications that require precise, editable, and visually compelling layered\nimagery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating high-quality, multi-layer transparent images from text prompts can\nunlock a new level of creative control, allowing users to edit each layer as\neffortlessly as editing text outputs from LLMs. However, the development of\nmulti-layer generative models lags behind that of conventional text-to-image\nmodels due to the absence of a large, high-quality corpus of multi-layer\ntransparent data. In this paper, we address this fundamental challenge by: (i)\nreleasing the first open, ultra-high-fidelity PrismLayers (PrismLayersPro)\ndataset of 200K (20K) multilayer transparent images with accurate alpha mattes,\n(ii) introducing a trainingfree synthesis pipeline that generates such data on\ndemand using off-the-shelf diffusion models, and (iii) delivering a strong,\nopen-source multi-layer generation model, ART+, which matches the aesthetics of\nmodern text-to-image generation models. The key technical contributions\ninclude: LayerFLUX, which excels at generating high-quality single transparent\nlayers with accurate alpha mattes, and MultiLayerFLUX, which composes multiple\nLayerFLUX outputs into complete images, guided by human-annotated semantic\nlayout. To ensure higher quality, we apply a rigorous filtering stage to remove\nartifacts and semantic mismatches, followed by human selection. Fine-tuning the\nstate-of-the-art ART model on our synthetic PrismLayersPro yields ART+, which\noutperforms the original ART in 60% of head-to-head user study comparisons and\neven matches the visual quality of images generated by the FLUX.1-[dev] model.\nWe anticipate that our work will establish a solid dataset foundation for the\nmulti-layer transparent image generation task, enabling research and\napplications that require precise, editable, and visually compelling layered\nimagery."
                },
                "authors": [
                    {
                        "name": "Junwen Chen"
                    },
                    {
                        "name": "Heyang Jiang"
                    },
                    {
                        "name": "Yanbin Wang"
                    },
                    {
                        "name": "Keming Wu"
                    },
                    {
                        "name": "Ji Li"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Keiji Yanai"
                    },
                    {
                        "name": "Dong Chen"
                    },
                    {
                        "name": "Yuhui Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Yuhui Yuan"
                },
                "author": "Yuhui Yuan",
                "arxiv_comment": "Homepage: https://prism-layers.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19641v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19641v3",
                "updated": "2025-05-28T16:04:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    4,
                    3,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-26T07:59:36Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    59,
                    36,
                    0,
                    146,
                    0
                ],
                "title": "SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning\n  Logical Reasoning and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning\n  Logical Reasoning and Beyond"
                },
                "summary": "Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the\npotential of Reinforcement Learning (RL) to enhance reasoning abilities in\nLarge Language Models (LLMs). While open-source replication efforts have\nprimarily focused on mathematical and coding domains, methods and resources for\ndeveloping general reasoning capabilities remain underexplored. This gap is\npartly due to the challenge of collecting diverse and verifiable reasoning data\nsuitable for RL. We hypothesize that logical reasoning is critical for\ndeveloping general reasoning capabilities, as logic forms a fundamental\nbuilding block of reasoning. In this work, we present SynLogic, a data\nsynthesis framework and dataset that generates diverse logical reasoning data\nat scale, encompassing 35 diverse logical reasoning tasks. The SynLogic\napproach enables controlled synthesis of data with adjustable difficulty and\nquantity. Importantly, all examples can be verified by simple rules, making\nthem ideally suited for RL with verifiable rewards. In our experiments, we\nvalidate the effectiveness of RL training on the SynLogic dataset based on 7B\nand 32B models. SynLogic leads to state-of-the-art logical reasoning\nperformance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B\nby 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and\ncoding tasks improves the training efficiency of these domains and\nsignificantly enhances reasoning generalization. Notably, our mixed training\nmodel outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These\nfindings position SynLogic as a valuable resource for advancing the broader\nreasoning capabilities of LLMs. We open-source both the data synthesis pipeline\nand the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the\npotential of Reinforcement Learning (RL) to enhance reasoning abilities in\nLarge Language Models (LLMs). While open-source replication efforts have\nprimarily focused on mathematical and coding domains, methods and resources for\ndeveloping general reasoning capabilities remain underexplored. This gap is\npartly due to the challenge of collecting diverse and verifiable reasoning data\nsuitable for RL. We hypothesize that logical reasoning is critical for\ndeveloping general reasoning capabilities, as logic forms a fundamental\nbuilding block of reasoning. In this work, we present SynLogic, a data\nsynthesis framework and dataset that generates diverse logical reasoning data\nat scale, encompassing 35 diverse logical reasoning tasks. The SynLogic\napproach enables controlled synthesis of data with adjustable difficulty and\nquantity. Importantly, all examples can be verified by simple rules, making\nthem ideally suited for RL with verifiable rewards. In our experiments, we\nvalidate the effectiveness of RL training on the SynLogic dataset based on 7B\nand 32B models. SynLogic leads to state-of-the-art logical reasoning\nperformance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B\nby 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and\ncoding tasks improves the training efficiency of these domains and\nsignificantly enhances reasoning generalization. Notably, our mixed training\nmodel outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These\nfindings position SynLogic as a valuable resource for advancing the broader\nreasoning capabilities of LLMs. We open-source both the data synthesis pipeline\nand the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic."
                },
                "authors": [
                    {
                        "name": "Junteng Liu"
                    },
                    {
                        "name": "Yuanxiang Fan"
                    },
                    {
                        "name": "Zhuo Jiang"
                    },
                    {
                        "name": "Han Ding"
                    },
                    {
                        "name": "Yongyi Hu"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Yiqi Shi"
                    },
                    {
                        "name": "Shitong Weng"
                    },
                    {
                        "name": "Aili Chen"
                    },
                    {
                        "name": "Shiqi Chen"
                    },
                    {
                        "name": "Yunan Huang"
                    },
                    {
                        "name": "Mozhi Zhang"
                    },
                    {
                        "name": "Pengyu Zhao"
                    },
                    {
                        "name": "Junjie Yan"
                    },
                    {
                        "name": "Junxian He"
                    }
                ],
                "author_detail": {
                    "name": "Junxian He"
                },
                "author": "Junxian He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19641v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19641v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22514v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22514v1",
                "updated": "2025-05-28T16:02:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    2,
                    57,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T16:02:57Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    2,
                    57,
                    2,
                    148,
                    0
                ],
                "title": "Closing the Quantum-Classical Scaling Gap in Approximate Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Closing the Quantum-Classical Scaling Gap in Approximate Optimization"
                },
                "summary": "In a recent study (Ref. [1]), quantum annealing was reported to exhibit a\nscaling advantage for approximately solving Quadratic Unconstrained Binary\nOptimization (QUBO). However, this claim critically depends on the choice of\nclassical reference algorithm -- Parallel Tempering with Isoenergetic Cluster\nMoves (PT-ICM). Here, we reassess these findings with different classical\nparadigm -- Simulated Bifurcation Machine (SBM) -- that harnesses nonlinear\nHamiltonian dynamics. By leveraging chaotic behavior rather than thermal\nfluctuations, SBM achieves comparable or superior scaling performance,\neffectively closing the previously reported quantum-classical gap. We show that\nsmall problem sizes analyzed in [1] are insufficient for inferring asymptotic\nscaling, due to sensitivity to runtime and hardware-specific factors. By\nextending the benchmark to larger instances -- beyond current quantum annealing\ncapabilities -- we establish strong classical scaling behavior. And as a\nresult, we conclude that it is unlikely that current generation of quantum\nannealers, can demonstrate supremacy in discrete approximate optimization under\noperationally meaningful conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a recent study (Ref. [1]), quantum annealing was reported to exhibit a\nscaling advantage for approximately solving Quadratic Unconstrained Binary\nOptimization (QUBO). However, this claim critically depends on the choice of\nclassical reference algorithm -- Parallel Tempering with Isoenergetic Cluster\nMoves (PT-ICM). Here, we reassess these findings with different classical\nparadigm -- Simulated Bifurcation Machine (SBM) -- that harnesses nonlinear\nHamiltonian dynamics. By leveraging chaotic behavior rather than thermal\nfluctuations, SBM achieves comparable or superior scaling performance,\neffectively closing the previously reported quantum-classical gap. We show that\nsmall problem sizes analyzed in [1] are insufficient for inferring asymptotic\nscaling, due to sensitivity to runtime and hardware-specific factors. By\nextending the benchmark to larger instances -- beyond current quantum annealing\ncapabilities -- we establish strong classical scaling behavior. And as a\nresult, we conclude that it is unlikely that current generation of quantum\nannealers, can demonstrate supremacy in discrete approximate optimization under\noperationally meaningful conditions."
                },
                "authors": [
                    {
                        "name": "J. Pawlowski"
                    },
                    {
                        "name": "P. Tarasiuk"
                    },
                    {
                        "name": "J. Tuziemski"
                    },
                    {
                        "name": "L. Pawela"
                    },
                    {
                        "name": "B. Gardas"
                    }
                ],
                "author_detail": {
                    "name": "B. Gardas"
                },
                "author": "B. Gardas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22514v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22514v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20201v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20201v2",
                "updated": "2025-05-28T15:55:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    55,
                    49,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-26T16:42:02Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    16,
                    42,
                    2,
                    0,
                    146,
                    0
                ],
                "title": "Reasoning Is Not All You Need: Examining LLMs for Multi-Turn Mental\n  Health Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Is Not All You Need: Examining LLMs for Multi-Turn Mental\n  Health Conversations"
                },
                "summary": "Limited access to mental healthcare, extended wait times, and increasing\ncapabilities of Large Language Models (LLMs) has led individuals to turn to\nLLMs for fulfilling their mental health needs. However, examining the\nmulti-turn mental health conversation capabilities of LLMs remains\nunder-explored. Existing evaluation frameworks typically focus on diagnostic\naccuracy and win-rates and often overlook alignment with patient-specific\ngoals, values, and personalities required for meaningful conversations. To\naddress this, we introduce MedAgent, a novel framework for synthetically\ngenerating realistic, multi-turn mental health sensemaking conversations and\nuse it to create the Mental Health Sensemaking Dialogue (MHSD) dataset,\ncomprising over 2,200 patient-LLM conversations. Additionally, we present\nMultiSenseEval, a holistic framework to evaluate the multi-turn conversation\nabilities of LLMs in healthcare settings using human-centric criteria. Our\nfindings reveal that frontier reasoning models yield below-par performance for\npatient-centric communication and struggle at advanced diagnostic capabilities\nwith average score of 31%. Additionally, we observed variation in model\nperformance based on patient's persona and performance drop with increasing\nturns in the conversation. Our work provides a comprehensive synthetic data\ngeneration framework, a dataset and evaluation framework for assessing LLMs in\nmulti-turn mental health conversations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limited access to mental healthcare, extended wait times, and increasing\ncapabilities of Large Language Models (LLMs) has led individuals to turn to\nLLMs for fulfilling their mental health needs. However, examining the\nmulti-turn mental health conversation capabilities of LLMs remains\nunder-explored. Existing evaluation frameworks typically focus on diagnostic\naccuracy and win-rates and often overlook alignment with patient-specific\ngoals, values, and personalities required for meaningful conversations. To\naddress this, we introduce MedAgent, a novel framework for synthetically\ngenerating realistic, multi-turn mental health sensemaking conversations and\nuse it to create the Mental Health Sensemaking Dialogue (MHSD) dataset,\ncomprising over 2,200 patient-LLM conversations. Additionally, we present\nMultiSenseEval, a holistic framework to evaluate the multi-turn conversation\nabilities of LLMs in healthcare settings using human-centric criteria. Our\nfindings reveal that frontier reasoning models yield below-par performance for\npatient-centric communication and struggle at advanced diagnostic capabilities\nwith average score of 31%. Additionally, we observed variation in model\nperformance based on patient's persona and performance drop with increasing\nturns in the conversation. Our work provides a comprehensive synthetic data\ngeneration framework, a dataset and evaluation framework for assessing LLMs in\nmulti-turn mental health conversations."
                },
                "authors": [
                    {
                        "name": "Mohit Chandra"
                    },
                    {
                        "name": "Siddharth Sriraman"
                    },
                    {
                        "name": "Harneet Singh Khanuja"
                    },
                    {
                        "name": "Yiqiao Jin"
                    },
                    {
                        "name": "Munmun De Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Munmun De Choudhury"
                },
                "author": "Munmun De Choudhury",
                "arxiv_comment": "34 pages, 5 figures, 30 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20201v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20201v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22504v1",
                "updated": "2025-05-28T15:52:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    52,
                    22,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T15:52:22Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    52,
                    22,
                    2,
                    148,
                    0
                ],
                "title": "Geometric GNNs for Charged Particle Tracking at GlueX",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric GNNs for Charged Particle Tracking at GlueX"
                },
                "summary": "Nuclear physics experiments are aimed at uncovering the fundamental building\nblocks of matter. The experiments involve high-energy collisions that produce\ncomplex events with many particle trajectories. Tracking charged particles\nresulting from collisions in the presence of a strong magnetic field is\ncritical to enable the reconstruction of particle trajectories and precise\ndetermination of interactions. It is traditionally achieved through\ncombinatorial approaches that scale worse than linearly as the number of hits\ngrows. Since particle hit data naturally form a 3-dimensional point cloud and\ncan be structured as graphs, Graph Neural Networks (GNNs) emerge as an\nintuitive and effective choice for this task. In this study, we evaluate the\nGNN model for track finding on the data from the GlueX experiment at Jefferson\nLab. We use simulation data to train the model and test on both simulation and\nreal GlueX measurements. We demonstrate that GNN-based track finding\noutperforms the currently used traditional method at GlueX in terms of\nsegment-based efficiency at a fixed purity while providing faster inferences.\nWe show that the GNN model can achieve significant speedup by processing\nmultiple events in batches, which exploits the parallel computation capability\nof Graphical Processing Units (GPUs). Finally, we compare the GNN\nimplementation on GPU and FPGA and describe the trade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nuclear physics experiments are aimed at uncovering the fundamental building\nblocks of matter. The experiments involve high-energy collisions that produce\ncomplex events with many particle trajectories. Tracking charged particles\nresulting from collisions in the presence of a strong magnetic field is\ncritical to enable the reconstruction of particle trajectories and precise\ndetermination of interactions. It is traditionally achieved through\ncombinatorial approaches that scale worse than linearly as the number of hits\ngrows. Since particle hit data naturally form a 3-dimensional point cloud and\ncan be structured as graphs, Graph Neural Networks (GNNs) emerge as an\nintuitive and effective choice for this task. In this study, we evaluate the\nGNN model for track finding on the data from the GlueX experiment at Jefferson\nLab. We use simulation data to train the model and test on both simulation and\nreal GlueX measurements. We demonstrate that GNN-based track finding\noutperforms the currently used traditional method at GlueX in terms of\nsegment-based efficiency at a fixed purity while providing faster inferences.\nWe show that the GNN model can achieve significant speedup by processing\nmultiple events in batches, which exploits the parallel computation capability\nof Graphical Processing Units (GPUs). Finally, we compare the GNN\nimplementation on GPU and FPGA and describe the trade-off."
                },
                "authors": [
                    {
                        "name": "Ahmed Hossam Mohammed"
                    },
                    {
                        "name": "Kishansingh Rajput"
                    },
                    {
                        "name": "Simon Taylor"
                    },
                    {
                        "name": "Denis Furletov"
                    },
                    {
                        "name": "Sergey Furletov"
                    },
                    {
                        "name": "Malachi Schram"
                    }
                ],
                "author_detail": {
                    "name": "Malachi Schram"
                },
                "author": "Malachi Schram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22503v1",
                "updated": "2025-05-28T15:51:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    51,
                    13,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T15:51:13Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    51,
                    13,
                    2,
                    148,
                    0
                ],
                "title": "From Strangers to Assistants: Fast Desire Alignment for Embodied\n  Agent-User Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Strangers to Assistants: Fast Desire Alignment for Embodied\n  Agent-User Adaptation"
                },
                "summary": "While embodied agents have made significant progress in performing complex\nphysical tasks, real-world applications demand more than pure task execution.\nThe agents must collaborate with unfamiliar agents and human users, whose goals\nare often vague and implicit. In such settings, interpreting ambiguous\ninstructions and uncovering underlying desires is essential for effective\nassistance. Therefore, fast and accurate desire alignment becomes a critical\ncapability for embodied agents. In this work, we first develop a home\nassistance simulation environment HA-Desire that integrates an LLM-driven human\nuser agent exhibiting realistic value-driven goal selection and communication.\nThe ego agent must interact with this proxy user to infer and adapt to the\nuser's latent desires. To achieve this, we present a novel framework FAMER for\nfast desire alignment, which introduces a desire-based mental reasoning\nmechanism to identify user intent and filter desire-irrelevant actions. We\nfurther design a reflection-based communication module that reduces redundant\ninquiries, and incorporate goal-relevant information extraction with memory\npersistence to improve information reuse and reduce unnecessary exploration.\nExtensive experiments demonstrate that our framework significantly enhances\nboth task execution and communication efficiency, enabling embodied agents to\nquickly adapt to user-specific desires in complex embodied environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While embodied agents have made significant progress in performing complex\nphysical tasks, real-world applications demand more than pure task execution.\nThe agents must collaborate with unfamiliar agents and human users, whose goals\nare often vague and implicit. In such settings, interpreting ambiguous\ninstructions and uncovering underlying desires is essential for effective\nassistance. Therefore, fast and accurate desire alignment becomes a critical\ncapability for embodied agents. In this work, we first develop a home\nassistance simulation environment HA-Desire that integrates an LLM-driven human\nuser agent exhibiting realistic value-driven goal selection and communication.\nThe ego agent must interact with this proxy user to infer and adapt to the\nuser's latent desires. To achieve this, we present a novel framework FAMER for\nfast desire alignment, which introduces a desire-based mental reasoning\nmechanism to identify user intent and filter desire-irrelevant actions. We\nfurther design a reflection-based communication module that reduces redundant\ninquiries, and incorporate goal-relevant information extraction with memory\npersistence to improve information reuse and reduce unnecessary exploration.\nExtensive experiments demonstrate that our framework significantly enhances\nboth task execution and communication efficiency, enabling embodied agents to\nquickly adapt to user-specific desires in complex embodied environments."
                },
                "authors": [
                    {
                        "name": "Yuanfei Wang"
                    },
                    {
                        "name": "Xinju Huang"
                    },
                    {
                        "name": "Fangwei Zhong"
                    },
                    {
                        "name": "Yaodong Yang"
                    },
                    {
                        "name": "Yizhou Wang"
                    },
                    {
                        "name": "Yuanpei Chen"
                    },
                    {
                        "name": "Hao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Hao Dong"
                },
                "author": "Hao Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22501v1",
                "updated": "2025-05-28T15:50:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    50,
                    48,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T15:50:48Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    50,
                    48,
                    2,
                    148,
                    0
                ],
                "title": "EvolveSearch: An Iterative Self-Evolving Search Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvolveSearch: An Iterative Self-Evolving Search Agent"
                },
                "summary": "The rapid advancement of large language models (LLMs) has transformed the\nlandscape of agentic information seeking capabilities through the integration\nof tools such as search engines and web browsers. However, current mainstream\napproaches for enabling LLM web search proficiency face significant challenges:\nsupervised fine-tuning struggles with data production in open-search domains,\nwhile RL converges quickly, limiting their data utilization efficiency. To\naddress these issues, we propose EvolveSearch, a novel iterative self-evolution\nframework that combines SFT and RL to enhance agentic web search capabilities\nwithout any external human-annotated reasoning data. Extensive experiments on\nseven multi-hop question-answering (MHQA) benchmarks demonstrate that\nEvolveSearch consistently improves performance across iterations, ultimately\nachieving an average improvement of 4.7\\% over the current state-of-the-art\nacross seven benchmarks, opening the door to self-evolution agentic\ncapabilities in open web search domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has transformed the\nlandscape of agentic information seeking capabilities through the integration\nof tools such as search engines and web browsers. However, current mainstream\napproaches for enabling LLM web search proficiency face significant challenges:\nsupervised fine-tuning struggles with data production in open-search domains,\nwhile RL converges quickly, limiting their data utilization efficiency. To\naddress these issues, we propose EvolveSearch, a novel iterative self-evolution\nframework that combines SFT and RL to enhance agentic web search capabilities\nwithout any external human-annotated reasoning data. Extensive experiments on\nseven multi-hop question-answering (MHQA) benchmarks demonstrate that\nEvolveSearch consistently improves performance across iterations, ultimately\nachieving an average improvement of 4.7\\% over the current state-of-the-art\nacross seven benchmarks, opening the door to self-evolution agentic\ncapabilities in open web search domains."
                },
                "authors": [
                    {
                        "name": "Dingchu Zhang"
                    },
                    {
                        "name": "Yida Zhao"
                    },
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Baixuan Li"
                    },
                    {
                        "name": "Wenbiao Yin"
                    },
                    {
                        "name": "Liwen Zhang"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Yufeng Li"
                    },
                    {
                        "name": "Kewei Tu"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Huang"
                },
                "author": "Fei Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.10208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.10208v2",
                "updated": "2025-05-28T15:42:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    42,
                    47,
                    2,
                    148,
                    0
                ],
                "published": "2024-03-15T11:16:47Z",
                "published_parsed": [
                    2024,
                    3,
                    15,
                    11,
                    16,
                    47,
                    4,
                    75,
                    0
                ],
                "title": "Irrational Random Utility Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Irrational Random Utility Models"
                },
                "summary": "The Random Utility Model (RUM) is the leading model to represent the\naggregate choices of a heterogeneous population of preference maximizers. We\nshow that if (and only if) preferences are sufficiently uncorrelated, RUM\nchoices can also be generated by a population of decision makers who do not\nmaximize any preference. In proving this result, we also characterize the\ngeneral class of choices generated by such irrational populations, with\napplications beyond the RUM framework. We discuss the relevance of our results\nfor the falsifiability of the rational interpretation of RUMs, the inference of\nindividual rationality from aggregate choices, and the nature of welfare\njudgments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Random Utility Model (RUM) is the leading model to represent the\naggregate choices of a heterogeneous population of preference maximizers. We\nshow that if (and only if) preferences are sufficiently uncorrelated, RUM\nchoices can also be generated by a population of decision makers who do not\nmaximize any preference. In proving this result, we also characterize the\ngeneral class of choices generated by such irrational populations, with\napplications beyond the RUM framework. We discuss the relevance of our results\nfor the falsifiability of the rational interpretation of RUMs, the inference of\nindividual rationality from aggregate choices, and the nature of welfare\njudgments."
                },
                "authors": [
                    {
                        "name": "Daniele Caliari"
                    },
                    {
                        "name": "Henrik Petri"
                    }
                ],
                "author_detail": {
                    "name": "Henrik Petri"
                },
                "author": "Henrik Petri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.10208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.10208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.TH",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13171v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13171v2",
                "updated": "2025-05-28T15:39:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    39,
                    49,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-19T14:28:35Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    28,
                    35,
                    0,
                    139,
                    0
                ],
                "title": "Positional Fragility in LLMs: How Offset Effects Reshape Our\n  Understanding of Memorization Risks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Positional Fragility in LLMs: How Offset Effects Reshape Our\n  Understanding of Memorization Risks"
                },
                "summary": "Large language models are known to memorize parts of their training data,\nposing risk of copyright violations. To systematically examine this risk, we\npretrain language models (1B/3B/8B) from scratch on 83B tokens, mixing\nweb-scale data with public domain books used to simulate copyrighted content at\ncontrolled frequencies at lengths at least ten times longer than prior work. We\nthereby identified the offset effect, a phenomenon characterized by two key\nfindings: (1) verbatim memorization is most strongly triggered by short\nprefixes drawn from the beginning of the context window, with memorization\ndecreasing counterintuitively as prefix length increases; and (2) a sharp\ndecline in verbatim recall when prefix begins offset from the initial tokens of\nthe context window. We attribute this to positional fragility: models rely\ndisproportionately on the earliest tokens in their context window as retrieval\nanchors, making them sensitive to even slight shifts. We further observe that\nwhen the model fails to retrieve memorized content, it often produces\ndegenerated text. Leveraging these findings, we show that shifting sensitive\ndata deeper into the context window suppresses both extractable memorization\nand degeneration. Our results suggest that positional offset is a critical and\npreviously overlooked axis for evaluating memorization risks, since prior work\nimplicitly assumed uniformity by probing only from the beginning of training\nsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are known to memorize parts of their training data,\nposing risk of copyright violations. To systematically examine this risk, we\npretrain language models (1B/3B/8B) from scratch on 83B tokens, mixing\nweb-scale data with public domain books used to simulate copyrighted content at\ncontrolled frequencies at lengths at least ten times longer than prior work. We\nthereby identified the offset effect, a phenomenon characterized by two key\nfindings: (1) verbatim memorization is most strongly triggered by short\nprefixes drawn from the beginning of the context window, with memorization\ndecreasing counterintuitively as prefix length increases; and (2) a sharp\ndecline in verbatim recall when prefix begins offset from the initial tokens of\nthe context window. We attribute this to positional fragility: models rely\ndisproportionately on the earliest tokens in their context window as retrieval\nanchors, making them sensitive to even slight shifts. We further observe that\nwhen the model fails to retrieve memorized content, it often produces\ndegenerated text. Leveraging these findings, we show that shifting sensitive\ndata deeper into the context window suppresses both extractable memorization\nand degeneration. Our results suggest that positional offset is a critical and\npreviously overlooked axis for evaluating memorization risks, since prior work\nimplicitly assumed uniformity by probing only from the beginning of training\nsequences."
                },
                "authors": [
                    {
                        "name": "Yixuan Xu"
                    },
                    {
                        "name": "Antoni-Joan Solergibert i Llaquet"
                    },
                    {
                        "name": "Antoine Bosselut"
                    },
                    {
                        "name": "Imanol Schlag"
                    }
                ],
                "author_detail": {
                    "name": "Imanol Schlag"
                },
                "author": "Imanol Schlag",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13171v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13171v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01747v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01747v3",
                "updated": "2025-05-28T15:29:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    29,
                    3,
                    2,
                    148,
                    0
                ],
                "published": "2025-03-03T17:15:17Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    17,
                    15,
                    17,
                    0,
                    62,
                    0
                ],
                "title": "Position: Don't Use the CLT in LLM Evals With Fewer Than a Few Hundred\n  Datapoints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: Don't Use the CLT in LLM Evals With Fewer Than a Few Hundred\n  Datapoints"
                },
                "summary": "Rigorous statistical evaluations of large language models (LLMs), including\nvalid error bars and significance testing, are essential for meaningful and\nreliable performance assessment. Currently, when such statistical measures are\nreported, they typically rely on the Central Limit Theorem (CLT). In this\nposition paper, we argue that while CLT-based methods for uncertainty\nquantification are appropriate when benchmarks consist of thousands of\nexamples, they fail to provide adequate uncertainty estimates for LLM\nevaluations that rely on smaller, highly specialized benchmarks. In these\nsmall-data settings, we demonstrate that CLT-based methods perform very poorly,\nusually dramatically underestimating uncertainty (i.e. producing error bars\nthat are too small). We give recommendations for alternative frequentist and\nBayesian methods that are both easy to implement and more appropriate in these\nincreasingly common scenarios. We provide a simple Python library for these\nBayesian methods at https://github.com/sambowyer/bayes_evals .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rigorous statistical evaluations of large language models (LLMs), including\nvalid error bars and significance testing, are essential for meaningful and\nreliable performance assessment. Currently, when such statistical measures are\nreported, they typically rely on the Central Limit Theorem (CLT). In this\nposition paper, we argue that while CLT-based methods for uncertainty\nquantification are appropriate when benchmarks consist of thousands of\nexamples, they fail to provide adequate uncertainty estimates for LLM\nevaluations that rely on smaller, highly specialized benchmarks. In these\nsmall-data settings, we demonstrate that CLT-based methods perform very poorly,\nusually dramatically underestimating uncertainty (i.e. producing error bars\nthat are too small). We give recommendations for alternative frequentist and\nBayesian methods that are both easy to implement and more appropriate in these\nincreasingly common scenarios. We provide a simple Python library for these\nBayesian methods at https://github.com/sambowyer/bayes_evals ."
                },
                "authors": [
                    {
                        "name": "Sam Bowyer"
                    },
                    {
                        "name": "Laurence Aitchison"
                    },
                    {
                        "name": "Desi R. Ivanova"
                    }
                ],
                "author_detail": {
                    "name": "Desi R. Ivanova"
                },
                "author": "Desi R. Ivanova",
                "arxiv_comment": "42 pages, 39 figures. ICML 2025 Spotlight Position Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01747v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01747v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13913v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13913v2",
                "updated": "2025-05-28T15:25:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    25,
                    29,
                    2,
                    148,
                    0
                ],
                "published": "2025-02-19T17:46:30Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    46,
                    30,
                    2,
                    50,
                    0
                ],
                "title": "How Do LLMs Perform Two-Hop Reasoning in Context?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Do LLMs Perform Two-Hop Reasoning in Context?"
                },
                "summary": "``Socrates is human. All humans are mortal. Therefore, Socrates is mortal.''\nThis form of argument illustrates a typical pattern of two-hop reasoning.\nFormally, two-hop reasoning refers to the process of inferring a conclusion by\nmaking two logical steps, each connecting adjacent concepts, such that the\nfinal conclusion depends on the integration of both steps. It is one of the\nmost fundamental components of human reasoning and plays a crucial role in both\nformal logic and everyday decision-making. Despite recent progress in large\nlanguage models (LLMs), we surprisingly find that they can fail at solving\nsimple two-hop reasoning problems when distractors are present. We observe on a\nsynthetic dataset that pre-trained LLMs often resort to random guessing among\nall plausible conclusions. However, after few steps of fine-tuning, models\nachieve near-perfect accuracy and exhibit strong length generalization. To\nunderstand the underlying mechanisms, we train a 3-layer Transformer from\nscratch on a synthetic two-hop reasoning task and reverse-engineer its internal\ninformation flow. We observe a clear progression in the attention logits\nthroughout training. This pictures a sharp phase transition from an initial\nstage of random guessing to the emergence of a structured sequential query\nmechanism, where the model first retrieves the preceding and the bridge\nconcepts in the early layers and then uses them to infer the final answer.\nFinally, we show that these dynamics can be captured by a minimal\nthree-parameter attention-only network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "``Socrates is human. All humans are mortal. Therefore, Socrates is mortal.''\nThis form of argument illustrates a typical pattern of two-hop reasoning.\nFormally, two-hop reasoning refers to the process of inferring a conclusion by\nmaking two logical steps, each connecting adjacent concepts, such that the\nfinal conclusion depends on the integration of both steps. It is one of the\nmost fundamental components of human reasoning and plays a crucial role in both\nformal logic and everyday decision-making. Despite recent progress in large\nlanguage models (LLMs), we surprisingly find that they can fail at solving\nsimple two-hop reasoning problems when distractors are present. We observe on a\nsynthetic dataset that pre-trained LLMs often resort to random guessing among\nall plausible conclusions. However, after few steps of fine-tuning, models\nachieve near-perfect accuracy and exhibit strong length generalization. To\nunderstand the underlying mechanisms, we train a 3-layer Transformer from\nscratch on a synthetic two-hop reasoning task and reverse-engineer its internal\ninformation flow. We observe a clear progression in the attention logits\nthroughout training. This pictures a sharp phase transition from an initial\nstage of random guessing to the emergence of a structured sequential query\nmechanism, where the model first retrieves the preceding and the bridge\nconcepts in the early layers and then uses them to infer the final answer.\nFinally, we show that these dynamics can be captured by a minimal\nthree-parameter attention-only network."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Hanlin Zhu"
                    },
                    {
                        "name": "Ruiqi Zhang"
                    },
                    {
                        "name": "Jiantao Jiao"
                    },
                    {
                        "name": "Song Mei"
                    },
                    {
                        "name": "Michael I. Jordan"
                    },
                    {
                        "name": "Stuart Russell"
                    }
                ],
                "author_detail": {
                    "name": "Stuart Russell"
                },
                "author": "Stuart Russell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13913v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13913v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11180v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11180v3",
                "updated": "2025-05-29T03:37:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    3,
                    37,
                    40,
                    3,
                    149,
                    0
                ],
                "published": "2024-12-15T13:18:56Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    13,
                    18,
                    56,
                    6,
                    350,
                    0
                ],
                "title": "TINED: GNNs-to-MLPs by Teacher Injection and Dirichlet Energy\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TINED: GNNs-to-MLPs by Teacher Injection and Dirichlet Energy\n  Distillation"
                },
                "summary": "Graph Neural Networks (GNNs) are pivotal in graph-based learning,\nparticularly excelling in node classification. However, their scalability is\nhindered by the need for multi-hop data during inference, limiting their\napplication in latency-sensitive scenarios. Recent efforts to distill GNNs into\nmulti-layer perceptrons (MLPs) for faster inference often underutilize the\nlayer-level insights of GNNs. In this paper, we present TINED, a novel approach\nthat distills GNNs to MLPs on a layer-by-layer basis using Teacher Injection\nand Dirichlet Energy Distillation techniques. We focus on two key operations in\nGNN layers: feature transformation (FT) and graph propagation (GP). We\nrecognize that FT is computationally equivalent to a fully-connected (FC) layer\nin MLPs. Thus, we propose directly transferring teacher parameters from an FT\nin a GNN to an FC layer in the student MLP, enhanced by fine-tuning. In TINED,\nthe FC layers in an MLP replicate the sequence of FTs and GPs in the GNN. We\nalso establish a theoretical bound for GP approximation. Furthermore, we note\nthat FT and GP operations in GNN layers often exhibit opposing smoothing\neffects: GP is aggressive, while FT is conservative. Using Dirichlet energy, we\ndevelop a DE ratio to measure these effects and propose Dirichlet Energy\nDistillation to convey these characteristics from GNN layers to MLP layers.\nExtensive experiments show that TINED outperforms GNNs and leading distillation\nmethods across various settings and seven datasets. Source code are available\nat https://github.com/scottjiao/TINED_ICML25/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are pivotal in graph-based learning,\nparticularly excelling in node classification. However, their scalability is\nhindered by the need for multi-hop data during inference, limiting their\napplication in latency-sensitive scenarios. Recent efforts to distill GNNs into\nmulti-layer perceptrons (MLPs) for faster inference often underutilize the\nlayer-level insights of GNNs. In this paper, we present TINED, a novel approach\nthat distills GNNs to MLPs on a layer-by-layer basis using Teacher Injection\nand Dirichlet Energy Distillation techniques. We focus on two key operations in\nGNN layers: feature transformation (FT) and graph propagation (GP). We\nrecognize that FT is computationally equivalent to a fully-connected (FC) layer\nin MLPs. Thus, we propose directly transferring teacher parameters from an FT\nin a GNN to an FC layer in the student MLP, enhanced by fine-tuning. In TINED,\nthe FC layers in an MLP replicate the sequence of FTs and GPs in the GNN. We\nalso establish a theoretical bound for GP approximation. Furthermore, we note\nthat FT and GP operations in GNN layers often exhibit opposing smoothing\neffects: GP is aggressive, while FT is conservative. Using Dirichlet energy, we\ndevelop a DE ratio to measure these effects and propose Dirichlet Energy\nDistillation to convey these characteristics from GNN layers to MLP layers.\nExtensive experiments show that TINED outperforms GNNs and leading distillation\nmethods across various settings and seven datasets. Source code are available\nat https://github.com/scottjiao/TINED_ICML25/."
                },
                "authors": [
                    {
                        "name": "Ziang Zhou"
                    },
                    {
                        "name": "Zhihao Ding"
                    },
                    {
                        "name": "Jieming Shi"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Shiqi Shen"
                    }
                ],
                "author_detail": {
                    "name": "Shiqi Shen"
                },
                "author": "Shiqi Shen",
                "arxiv_comment": "17 pages, published as a conference paper at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11180v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11180v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11926v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11926v3",
                "updated": "2025-05-28T15:24:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    24,
                    43,
                    2,
                    148,
                    0
                ],
                "published": "2025-02-17T15:39:50Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    15,
                    39,
                    50,
                    0,
                    48,
                    0
                ],
                "title": "BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion\n  Recognition Datasets for 28 Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion\n  Recognition Datasets for 28 Languages"
                },
                "summary": "People worldwide use language in subtle and complex ways to express emotions.\nAlthough emotion recognition--an umbrella term for several NLP tasks--impacts\nvarious applications within NLP and beyond, most work in this area has focused\non high-resource languages. This has led to significant disparities in research\nefforts and proposed solutions, particularly for under-resourced languages,\nwhich often lack high-quality annotated datasets. In this paper, we present\nBRIGHTER--a collection of multilabeled, emotion-annotated datasets in 28\ndifferent languages and across several domains. BRIGHTER primarily covers\nlow-resource languages from Africa, Asia, Eastern Europe, and Latin America,\nwith instances labeled by fluent speakers. We highlight the challenges related\nto the data collection and annotation processes, and then report experimental\nresults for monolingual and crosslingual multi-label emotion identification, as\nwell as emotion intensity recognition. We analyse the variability in\nperformance across languages and text domains, both with and without the use of\nLLMs, and show that the BRIGHTER datasets represent a meaningful step towards\naddressing the gap in text-based emotion recognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "People worldwide use language in subtle and complex ways to express emotions.\nAlthough emotion recognition--an umbrella term for several NLP tasks--impacts\nvarious applications within NLP and beyond, most work in this area has focused\non high-resource languages. This has led to significant disparities in research\nefforts and proposed solutions, particularly for under-resourced languages,\nwhich often lack high-quality annotated datasets. In this paper, we present\nBRIGHTER--a collection of multilabeled, emotion-annotated datasets in 28\ndifferent languages and across several domains. BRIGHTER primarily covers\nlow-resource languages from Africa, Asia, Eastern Europe, and Latin America,\nwith instances labeled by fluent speakers. We highlight the challenges related\nto the data collection and annotation processes, and then report experimental\nresults for monolingual and crosslingual multi-label emotion identification, as\nwell as emotion intensity recognition. We analyse the variability in\nperformance across languages and text domains, both with and without the use of\nLLMs, and show that the BRIGHTER datasets represent a meaningful step towards\naddressing the gap in text-based emotion recognition."
                },
                "authors": [
                    {
                        "name": "Shamsuddeen Hassan Muhammad"
                    },
                    {
                        "name": "Nedjma Ousidhoum"
                    },
                    {
                        "name": "Idris Abdulmumin"
                    },
                    {
                        "name": "Jan Philip Wahle"
                    },
                    {
                        "name": "Terry Ruas"
                    },
                    {
                        "name": "Meriem Beloucif"
                    },
                    {
                        "name": "Christine de Kock"
                    },
                    {
                        "name": "Nirmal Surange"
                    },
                    {
                        "name": "Daniela Teodorescu"
                    },
                    {
                        "name": "Ibrahim Said Ahmad"
                    },
                    {
                        "name": "David Ifeoluwa Adelani"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    },
                    {
                        "name": "Felermino D. M. A. Ali"
                    },
                    {
                        "name": "Ilseyar Alimova"
                    },
                    {
                        "name": "Vladimir Araujo"
                    },
                    {
                        "name": "Nikolay Babakov"
                    },
                    {
                        "name": "Naomi Baes"
                    },
                    {
                        "name": "Ana-Maria Bucur"
                    },
                    {
                        "name": "Andiswa Bukula"
                    },
                    {
                        "name": "Guanqun Cao"
                    },
                    {
                        "name": "Rodrigo Tufino Cardenas"
                    },
                    {
                        "name": "Rendi Chevi"
                    },
                    {
                        "name": "Chiamaka Ijeoma Chukwuneke"
                    },
                    {
                        "name": "Alexandra Ciobotaru"
                    },
                    {
                        "name": "Daryna Dementieva"
                    },
                    {
                        "name": "Murja Sani Gadanya"
                    },
                    {
                        "name": "Robert Geislinger"
                    },
                    {
                        "name": "Bela Gipp"
                    },
                    {
                        "name": "Oumaima Hourrane"
                    },
                    {
                        "name": "Oana Ignat"
                    },
                    {
                        "name": "Falalu Ibrahim Lawan"
                    },
                    {
                        "name": "Rooweither Mabuya"
                    },
                    {
                        "name": "Rahmad Mahendra"
                    },
                    {
                        "name": "Vukosi Marivate"
                    },
                    {
                        "name": "Alexander Panchenko"
                    },
                    {
                        "name": "Andrew Piper"
                    },
                    {
                        "name": "Charles Henrique Porto Ferreira"
                    },
                    {
                        "name": "Vitaly Protasov"
                    },
                    {
                        "name": "Samuel Rutunda"
                    },
                    {
                        "name": "Manish Shrivastava"
                    },
                    {
                        "name": "Aura Cristina Udrea"
                    },
                    {
                        "name": "Lilian Diana Awuor Wanzare"
                    },
                    {
                        "name": "Sophie Wu"
                    },
                    {
                        "name": "Florian Valentin Wunderlich"
                    },
                    {
                        "name": "Hanif Muhammad Zhafran"
                    },
                    {
                        "name": "Tianhui Zhang"
                    },
                    {
                        "name": "Yi Zhou"
                    },
                    {
                        "name": "Saif M. Mohammad"
                    }
                ],
                "author_detail": {
                    "name": "Saif M. Mohammad"
                },
                "author": "Saif M. Mohammad",
                "arxiv_comment": "Accepted at ACL2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11926v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11926v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22469v1",
                "updated": "2025-05-28T15:22:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    22,
                    15,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T15:22:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    22,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "CPINN-ABPI: Physics-Informed Neural Networks for Accurate Power\n  Estimation in MPSoCs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CPINN-ABPI: Physics-Informed Neural Networks for Accurate Power\n  Estimation in MPSoCs"
                },
                "summary": "Efficient thermal and power management in modern multiprocessor\nsystems-on-chip (MPSoCs) demands accurate power consumption estimation. One of\nthe state-of-the-art approaches, Alternative Blind Power Identification (ABPI),\ntheoretically eliminates the dependence on steady-state temperatures,\naddressing a major shortcoming of previous approaches. However, ABPI\nperformance has remained unverified in actual hardware implementations. In this\nstudy, we conduct the first empirical validation of ABPI on commercial hardware\nusing the NVIDIA Jetson Xavier AGX platform. Our findings reveal that, while\nABPI provides computational efficiency and independence from steady-state\ntemperature, it exhibits considerable accuracy deficiencies in real-world\nscenarios. To overcome these limitations, we introduce a novel approach that\nintegrates Custom Physics-Informed Neural Networks (CPINNs) with the underlying\nthermal model of ABPI. Our approach employs a specialized loss function that\nharmonizes physical principles with data-driven learning, complemented by\nmulti-objective genetic algorithm optimization to balance estimation accuracy\nand computational cost. In experimental validation, CPINN-ABPI achieves a\nreduction of 84.7\\% CPU and 73.9\\% GPU in the mean absolute error (MAE)\nrelative to ABPI, with the weighted mean absolute percentage error (WMAPE)\nimproving from 47\\%--81\\% to $\\sim$12\\%. The method maintains real-time\nperformance with 195.3~$\\mu$s of inference time, with similar 85\\%--99\\%\naccuracy gains across heterogeneous SoCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient thermal and power management in modern multiprocessor\nsystems-on-chip (MPSoCs) demands accurate power consumption estimation. One of\nthe state-of-the-art approaches, Alternative Blind Power Identification (ABPI),\ntheoretically eliminates the dependence on steady-state temperatures,\naddressing a major shortcoming of previous approaches. However, ABPI\nperformance has remained unverified in actual hardware implementations. In this\nstudy, we conduct the first empirical validation of ABPI on commercial hardware\nusing the NVIDIA Jetson Xavier AGX platform. Our findings reveal that, while\nABPI provides computational efficiency and independence from steady-state\ntemperature, it exhibits considerable accuracy deficiencies in real-world\nscenarios. To overcome these limitations, we introduce a novel approach that\nintegrates Custom Physics-Informed Neural Networks (CPINNs) with the underlying\nthermal model of ABPI. Our approach employs a specialized loss function that\nharmonizes physical principles with data-driven learning, complemented by\nmulti-objective genetic algorithm optimization to balance estimation accuracy\nand computational cost. In experimental validation, CPINN-ABPI achieves a\nreduction of 84.7\\% CPU and 73.9\\% GPU in the mean absolute error (MAE)\nrelative to ABPI, with the weighted mean absolute percentage error (WMAPE)\nimproving from 47\\%--81\\% to $\\sim$12\\%. The method maintains real-time\nperformance with 195.3~$\\mu$s of inference time, with similar 85\\%--99\\%\naccuracy gains across heterogeneous SoCs."
                },
                "authors": [
                    {
                        "name": "Mohamed R. Elshamy"
                    },
                    {
                        "name": "Mehdi Elahi"
                    },
                    {
                        "name": "Ahmad Patooghy"
                    },
                    {
                        "name": "Abdel-Hameed A. Badawy"
                    }
                ],
                "author_detail": {
                    "name": "Abdel-Hameed A. Badawy"
                },
                "author": "Abdel-Hameed A. Badawy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22467v2",
                "updated": "2025-05-29T04:17:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    4,
                    17,
                    13,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-28T15:20:09Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    20,
                    9,
                    2,
                    148,
                    0
                ],
                "title": "Topological Structure Learning Should Be A Research Priority for\n  LLM-Based Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topological Structure Learning Should Be A Research Priority for\n  LLM-Based Multi-Agent Systems"
                },
                "summary": "Large Language Model-based Multi-Agent Systems (MASs) have emerged as a\npowerful paradigm for tackling complex tasks through collaborative\nintelligence. Nevertheless, the question of how agents should be structurally\norganized for optimal cooperation remains largely unexplored. In this position\npaper, we aim to gently redirect the focus of the MAS research community toward\nthis critical dimension: develop topology-aware MASs for specific tasks.\nSpecifically, the system consists of three core components - agents,\ncommunication links, and communication patterns - that collectively shape its\ncoordination performance and efficiency. To this end, we introduce a\nsystematic, three-stage framework: agent selection, structure profiling, and\ntopology synthesis. Each stage would trigger new research opportunities in\nareas such as language models, reinforcement learning, graph learning, and\ngenerative modeling; together, they could unleash the full potential of MASs in\ncomplicated real-world applications. Then, we discuss the potential challenges\nand opportunities in the evaluation of multiple systems. We hope our\nperspective and framework can offer critical new insights in the era of agentic\nAI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-based Multi-Agent Systems (MASs) have emerged as a\npowerful paradigm for tackling complex tasks through collaborative\nintelligence. Nevertheless, the question of how agents should be structurally\norganized for optimal cooperation remains largely unexplored. In this position\npaper, we aim to gently redirect the focus of the MAS research community toward\nthis critical dimension: develop topology-aware MASs for specific tasks.\nSpecifically, the system consists of three core components - agents,\ncommunication links, and communication patterns - that collectively shape its\ncoordination performance and efficiency. To this end, we introduce a\nsystematic, three-stage framework: agent selection, structure profiling, and\ntopology synthesis. Each stage would trigger new research opportunities in\nareas such as language models, reinforcement learning, graph learning, and\ngenerative modeling; together, they could unleash the full potential of MASs in\ncomplicated real-world applications. Then, we discuss the potential challenges\nand opportunities in the evaluation of multiple systems. We hope our\nperspective and framework can offer critical new insights in the era of agentic\nAI."
                },
                "authors": [
                    {
                        "name": "Jiaxi Yang"
                    },
                    {
                        "name": "Mengqi Zhang"
                    },
                    {
                        "name": "Yiqiao Jin"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Lu Lin"
                    },
                    {
                        "name": "Yi He"
                    },
                    {
                        "name": "Weijie Xu"
                    },
                    {
                        "name": "James Evans"
                    },
                    {
                        "name": "Jindong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jindong Wang"
                },
                "author": "Jindong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00777v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00777v3",
                "updated": "2025-05-28T15:18:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    18,
                    7,
                    2,
                    148,
                    0
                ],
                "published": "2025-01-01T09:00:10Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    9,
                    0,
                    10,
                    2,
                    1,
                    0
                ],
                "title": "FitCF: A Framework for Automatic Feature Importance-guided\n  Counterfactual Example Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FitCF: A Framework for Automatic Feature Importance-guided\n  Counterfactual Example Generation"
                },
                "summary": "Counterfactual examples are widely used in natural language processing (NLP)\nas valuable data to improve models, and in explainable artificial intelligence\n(XAI) to understand model behavior. The automated generation of counterfactual\nexamples remains a challenging task even for large language models (LLMs),\ndespite their impressive performance on many tasks. In this paper, we first\nintroduce ZeroCF, a faithful approach for leveraging important words derived\nfrom feature attribution methods to generate counterfactual examples in a\nzero-shot setting. Second, we present a new framework, FitCF, which further\nverifies aforementioned counterfactuals by label flip verification and then\ninserts them as demonstrations for few-shot prompting, outperforming two\nstate-of-the-art baselines. Through ablation studies, we identify the\nimportance of each of FitCF's core components in improving the quality of\ncounterfactuals, as assessed through flip rate, perplexity, and similarity\nmeasures. Furthermore, we show the effectiveness of LIME and Integrated\nGradients as backbone attribution methods for FitCF and find that the number of\ndemonstrations has the largest effect on performance. Finally, we reveal a\nstrong correlation between the faithfulness of feature attribution scores and\nthe quality of generated counterfactuals, which we hope will serve as an\nimportant finding for future research in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual examples are widely used in natural language processing (NLP)\nas valuable data to improve models, and in explainable artificial intelligence\n(XAI) to understand model behavior. The automated generation of counterfactual\nexamples remains a challenging task even for large language models (LLMs),\ndespite their impressive performance on many tasks. In this paper, we first\nintroduce ZeroCF, a faithful approach for leveraging important words derived\nfrom feature attribution methods to generate counterfactual examples in a\nzero-shot setting. Second, we present a new framework, FitCF, which further\nverifies aforementioned counterfactuals by label flip verification and then\ninserts them as demonstrations for few-shot prompting, outperforming two\nstate-of-the-art baselines. Through ablation studies, we identify the\nimportance of each of FitCF's core components in improving the quality of\ncounterfactuals, as assessed through flip rate, perplexity, and similarity\nmeasures. Furthermore, we show the effectiveness of LIME and Integrated\nGradients as backbone attribution methods for FitCF and find that the number of\ndemonstrations has the largest effect on performance. Finally, we reveal a\nstrong correlation between the faithfulness of feature attribution scores and\nthe quality of generated counterfactuals, which we hope will serve as an\nimportant finding for future research in this direction."
                },
                "authors": [
                    {
                        "name": "Qianli Wang"
                    },
                    {
                        "name": "Nils Feldhus"
                    },
                    {
                        "name": "Simon Ostermann"
                    },
                    {
                        "name": "Luis Felipe Villa-Arenas"
                    },
                    {
                        "name": "Sebastian Möller"
                    },
                    {
                        "name": "Vera Schmitt"
                    }
                ],
                "author_detail": {
                    "name": "Vera Schmitt"
                },
                "author": "Vera Schmitt",
                "arxiv_comment": "ACL 2025 Findings; camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00777v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00777v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2205.09386v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2205.09386v2",
                "updated": "2025-05-28T15:17:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    17,
                    36,
                    2,
                    148,
                    0
                ],
                "published": "2022-05-19T08:31:14Z",
                "published_parsed": [
                    2022,
                    5,
                    19,
                    8,
                    31,
                    14,
                    3,
                    139,
                    0
                ],
                "title": "On the Distortion of Multi-winner Election Using Single-Candidate\n  Ballots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Distortion of Multi-winner Election Using Single-Candidate\n  Ballots"
                },
                "summary": "In this paper, we study the distortion bounds for voting mechanisms in\nmulti-winner elections in general metric spaces. Our study pertains to the case\nin which each voter only reports her favorite candidate amongst $m$ possible\nchoices. Given that candidates' locations are undisclosed to the mechanism, the\nmechanism has to form a $w-$winner committee based solely on the number of\nvotes received by candidates. We establish distortion bounds for both truthful\nand non-truthful mechanisms. Our research highlights the significance of the\n$\\sigma$ parameter, which represents the ratio between maximum and minimum\ndistances among all candidate pairs. We show that the distortion is linear in\n$\\sigma$. First, we demonstrate that all mechanisms possess a distortion\ngreater than $1+\\frac{w-1}{w+1}(\\sigma-1)$. To give an upper bound, we study\nthe Single Non-Transferable Vote (SNTV) mechanism, whose distortion is at most\n$1+2\\sigma$. Second, we retrieve the upper bounds for strategyproof mechanisms.\nIn particular, we infer an upper bound by examining the Random Sequential\nDictator mechanism that achieves a distortion less than $1+4\\sigma$ when $w=2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study the distortion bounds for voting mechanisms in\nmulti-winner elections in general metric spaces. Our study pertains to the case\nin which each voter only reports her favorite candidate amongst $m$ possible\nchoices. Given that candidates' locations are undisclosed to the mechanism, the\nmechanism has to form a $w-$winner committee based solely on the number of\nvotes received by candidates. We establish distortion bounds for both truthful\nand non-truthful mechanisms. Our research highlights the significance of the\n$\\sigma$ parameter, which represents the ratio between maximum and minimum\ndistances among all candidate pairs. We show that the distortion is linear in\n$\\sigma$. First, we demonstrate that all mechanisms possess a distortion\ngreater than $1+\\frac{w-1}{w+1}(\\sigma-1)$. To give an upper bound, we study\nthe Single Non-Transferable Vote (SNTV) mechanism, whose distortion is at most\n$1+2\\sigma$. Second, we retrieve the upper bounds for strategyproof mechanisms.\nIn particular, we infer an upper bound by examining the Random Sequential\nDictator mechanism that achieves a distortion less than $1+4\\sigma$ when $w=2$."
                },
                "authors": [
                    {
                        "name": "Gennaro Auricchio"
                    },
                    {
                        "name": "Zeyu Ren"
                    },
                    {
                        "name": "Zihe Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2205.09386v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2205.09386v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22461v1",
                "updated": "2025-05-28T15:16:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    16,
                    15,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T15:16:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    16,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "SHTOcc: Effective 3D Occupancy Prediction with Sparse Head and Tail\n  Voxels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SHTOcc: Effective 3D Occupancy Prediction with Sparse Head and Tail\n  Voxels"
                },
                "summary": "3D occupancy prediction has attracted much attention in the field of\nautonomous driving due to its powerful geometric perception and object\nrecognition capabilities. However, existing methods have not explored the most\nessential distribution patterns of voxels, resulting in unsatisfactory results.\nThis paper first explores the inter-class distribution and geometric\ndistribution of voxels, thereby solving the long-tail problem caused by the\ninter-class distribution and the poor performance caused by the geometric\ndistribution. Specifically, this paper proposes SHTOcc (Sparse Head-Tail\nOccupancy), which uses sparse head-tail voxel construction to accurately\nidentify and balance key voxels in the head and tail classes, while using\ndecoupled learning to reduce the model's bias towards the dominant (head)\ncategory and enhance the focus on the tail class. Experiments show that\nsignificant improvements have been made on multiple baselines: SHTOcc reduces\nGPU memory usage by 42.2%, increases inference speed by 58.6%, and improves\naccuracy by about 7%, verifying its effectiveness and efficiency. The code is\navailable at https://github.com/ge95net/SHTOcc",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D occupancy prediction has attracted much attention in the field of\nautonomous driving due to its powerful geometric perception and object\nrecognition capabilities. However, existing methods have not explored the most\nessential distribution patterns of voxels, resulting in unsatisfactory results.\nThis paper first explores the inter-class distribution and geometric\ndistribution of voxels, thereby solving the long-tail problem caused by the\ninter-class distribution and the poor performance caused by the geometric\ndistribution. Specifically, this paper proposes SHTOcc (Sparse Head-Tail\nOccupancy), which uses sparse head-tail voxel construction to accurately\nidentify and balance key voxels in the head and tail classes, while using\ndecoupled learning to reduce the model's bias towards the dominant (head)\ncategory and enhance the focus on the tail class. Experiments show that\nsignificant improvements have been made on multiple baselines: SHTOcc reduces\nGPU memory usage by 42.2%, increases inference speed by 58.6%, and improves\naccuracy by about 7%, verifying its effectiveness and efficiency. The code is\navailable at https://github.com/ge95net/SHTOcc"
                },
                "authors": [
                    {
                        "name": "Qiucheng Yu"
                    },
                    {
                        "name": "Yuan Xie"
                    },
                    {
                        "name": "Xin Tan"
                    }
                ],
                "author_detail": {
                    "name": "Xin Tan"
                },
                "author": "Xin Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11418v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11418v2",
                "updated": "2025-05-28T15:14:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    14,
                    24,
                    2,
                    148,
                    0
                ],
                "published": "2024-12-16T03:34:40Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    3,
                    34,
                    40,
                    0,
                    351,
                    0
                ],
                "title": "ConKE: Conceptualization-Augmented Knowledge Editing in Large Language\n  Models for Commonsense Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConKE: Conceptualization-Augmented Knowledge Editing in Large Language\n  Models for Commonsense Reasoning"
                },
                "summary": "Knowledge Editing (KE) aims to adjust a Large Language Model's (LLM) internal\nrepresentations and parameters to correct inaccuracies and improve output\nconsistency without incurring the computational expense of re-training the\nentire model. However, editing commonsense knowledge still faces difficulties,\nincluding limited knowledge coverage in existing resources, the infeasibility\nof annotating labels for an overabundance of commonsense knowledge, and the\nstrict knowledge formats of current editing methods. In this paper, we address\nthese challenges by presenting ConceptEdit, a framework that integrates\nconceptualization and instantiation into the KE pipeline for LLMs to enhance\ntheir commonsense reasoning capabilities. ConceptEdit dynamically diagnoses\nimplausible commonsense knowledge within an LLM using another verifier LLM and\naugments the source knowledge to be edited with conceptualization for stronger\ngeneralizability. Experimental results demonstrate that LLMs enhanced with\nConceptEdit successfully generate commonsense knowledge with improved\nplausibility compared to other baselines and achieve stronger performance\nacross multiple question answering benchmarks. Our data, code, and models are\npublicly available at https://github.com/HKUST-KnowComp/ConKE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Editing (KE) aims to adjust a Large Language Model's (LLM) internal\nrepresentations and parameters to correct inaccuracies and improve output\nconsistency without incurring the computational expense of re-training the\nentire model. However, editing commonsense knowledge still faces difficulties,\nincluding limited knowledge coverage in existing resources, the infeasibility\nof annotating labels for an overabundance of commonsense knowledge, and the\nstrict knowledge formats of current editing methods. In this paper, we address\nthese challenges by presenting ConceptEdit, a framework that integrates\nconceptualization and instantiation into the KE pipeline for LLMs to enhance\ntheir commonsense reasoning capabilities. ConceptEdit dynamically diagnoses\nimplausible commonsense knowledge within an LLM using another verifier LLM and\naugments the source knowledge to be edited with conceptualization for stronger\ngeneralizability. Experimental results demonstrate that LLMs enhanced with\nConceptEdit successfully generate commonsense knowledge with improved\nplausibility compared to other baselines and achieve stronger performance\nacross multiple question answering benchmarks. Our data, code, and models are\npublicly available at https://github.com/HKUST-KnowComp/ConKE."
                },
                "authors": [
                    {
                        "name": "Liyu Zhang"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "Findings of ACL2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11418v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11418v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22457v1",
                "updated": "2025-05-28T15:13:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    13,
                    34,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T15:13:34Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    13,
                    34,
                    2,
                    148,
                    0
                ],
                "title": "Fostering Video Reasoning via Next-Event Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fostering Video Reasoning via Next-Event Prediction"
                },
                "summary": "Next-token prediction serves as the foundational learning task enabling\nreasoning in LLMs. But what should the learning task be when aiming to equip\nMLLMs with temporal reasoning capabilities over video inputs? Existing tasks\nsuch as video question answering often rely on annotations from humans or much\nstronger MLLMs, while video captioning tends to entangle temporal reasoning\nwith spatial information. To address this gap, we propose next-event prediction\n(NEP), a learning task that harnesses future video segments as a rich,\nself-supervised signal to foster temporal reasoning. We segment each video into\npast and future frames: the MLLM takes the past frames as input and predicts a\nsummary of events derived from the future frames, thereby encouraging the model\nto reason temporally in order to complete the task. To support this task, we\ncurate V1-33K, a dataset comprising 33,000 automatically extracted video\nsegments spanning diverse real-world scenarios. We further explore a range of\nvideo instruction-tuning strategies to study their effects on temporal\nreasoning. To evaluate progress, we introduce FutureBench to assess coherence\nin predicting unseen future events. Experiments validate that NEP offers a\nscalable and effective training paradigm for fostering temporal reasoning in\nMLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-token prediction serves as the foundational learning task enabling\nreasoning in LLMs. But what should the learning task be when aiming to equip\nMLLMs with temporal reasoning capabilities over video inputs? Existing tasks\nsuch as video question answering often rely on annotations from humans or much\nstronger MLLMs, while video captioning tends to entangle temporal reasoning\nwith spatial information. To address this gap, we propose next-event prediction\n(NEP), a learning task that harnesses future video segments as a rich,\nself-supervised signal to foster temporal reasoning. We segment each video into\npast and future frames: the MLLM takes the past frames as input and predicts a\nsummary of events derived from the future frames, thereby encouraging the model\nto reason temporally in order to complete the task. To support this task, we\ncurate V1-33K, a dataset comprising 33,000 automatically extracted video\nsegments spanning diverse real-world scenarios. We further explore a range of\nvideo instruction-tuning strategies to study their effects on temporal\nreasoning. To evaluate progress, we introduce FutureBench to assess coherence\nin predicting unseen future events. Experiments validate that NEP offers a\nscalable and effective training paradigm for fostering temporal reasoning in\nMLLMs."
                },
                "authors": [
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Hongfu Liu"
                    },
                    {
                        "name": "Xiangyan Liu"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Kenji Kawaguchi"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Tianyu Pang"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Pang"
                },
                "author": "Tianyu Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22453v1",
                "updated": "2025-05-28T15:11:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    11,
                    16,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T15:11:16Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    11,
                    16,
                    2,
                    148,
                    0
                ],
                "title": "Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO"
                },
                "summary": "Improving Multi-modal Large Language Models (MLLMs) in the post-training\nstage typically relies on supervised fine-tuning (SFT) or reinforcement\nlearning (RL). However, these supervised methods require expensive and manually\nannotated multi-modal data--an ultimately unsustainable resource. While recent\nefforts have explored unsupervised post-training, their methods are complex and\ndifficult to iterate. In this work, we are the first to investigate the use of\nGRPO, a stable and scalable online RL algorithm, for enabling continual\nself-improvement without any external supervision. We propose MM-UPT, a simple\nyet effective framework for unsupervised post-training of MLLMs. MM-UPT builds\nupon GRPO, replacing traditional reward signals with a self-rewarding mechanism\nbased on majority voting over multiple sampled responses. Our experiments\ndemonstrate that MM-UPT significantly improves the reasoning ability of\nQwen2.5-VL-7B (e.g., 66.3 %$\\rightarrow$72.9 % on MathVista, 62.9\n%$\\rightarrow$68.7 % on We-Math), using standard dataset without ground truth\nlabels. MM-UPT also outperforms prior unsupervised baselines and even\napproaches the results of supervised GRPO. Furthermore, we show that\nincorporating synthetic questions, generated solely by MLLM itself, can boost\nperformance as well, highlighting a promising approach for scalable\nself-improvement. Overall, MM-UPT offers a new paradigm for continual,\nautonomous enhancement of MLLMs in the absence of external supervision. Our\ncode is available at https://github.com/waltonfuture/MM-UPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Multi-modal Large Language Models (MLLMs) in the post-training\nstage typically relies on supervised fine-tuning (SFT) or reinforcement\nlearning (RL). However, these supervised methods require expensive and manually\nannotated multi-modal data--an ultimately unsustainable resource. While recent\nefforts have explored unsupervised post-training, their methods are complex and\ndifficult to iterate. In this work, we are the first to investigate the use of\nGRPO, a stable and scalable online RL algorithm, for enabling continual\nself-improvement without any external supervision. We propose MM-UPT, a simple\nyet effective framework for unsupervised post-training of MLLMs. MM-UPT builds\nupon GRPO, replacing traditional reward signals with a self-rewarding mechanism\nbased on majority voting over multiple sampled responses. Our experiments\ndemonstrate that MM-UPT significantly improves the reasoning ability of\nQwen2.5-VL-7B (e.g., 66.3 %$\\rightarrow$72.9 % on MathVista, 62.9\n%$\\rightarrow$68.7 % on We-Math), using standard dataset without ground truth\nlabels. MM-UPT also outperforms prior unsupervised baselines and even\napproaches the results of supervised GRPO. Furthermore, we show that\nincorporating synthetic questions, generated solely by MLLM itself, can boost\nperformance as well, highlighting a promising approach for scalable\nself-improvement. Overall, MM-UPT offers a new paradigm for continual,\nautonomous enhancement of MLLMs in the absence of external supervision. Our\ncode is available at https://github.com/waltonfuture/MM-UPT."
                },
                "authors": [
                    {
                        "name": "Lai Wei"
                    },
                    {
                        "name": "Yuting Li"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Weiran Huang"
                    },
                    {
                        "name": "Lichao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lichao Sun"
                },
                "author": "Lichao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22447v1",
                "updated": "2025-05-28T15:09:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    9,
                    56,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T15:09:56Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    9,
                    56,
                    2,
                    148,
                    0
                ],
                "title": "Privacy-preserving Prompt Personalization in Federated Learning for\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-preserving Prompt Personalization in Federated Learning for\n  Multimodal Large Language Models"
                },
                "summary": "Prompt learning is a crucial technique for adapting pre-trained multimodal\nlanguage models (MLLMs) to user tasks. Federated prompt personalization (FPP)\nis further developed to address data heterogeneity and local overfitting,\nhowever, it exposes personalized prompts - valuable intellectual assets - to\nprivacy risks like prompt stealing or membership inference attacks.\nWidely-adopted techniques like differential privacy add noise to prompts,\nwhereas degrading personalization performance. We propose SecFPP, a secure FPP\nprotocol harmonizing generalization, personalization, and privacy guarantees.\nSecFPP employs hierarchical prompt adaptation with domain-level and class-level\ncomponents to handle multi-granular data imbalance. For privacy, it uses a\nnovel secret-sharing-based adaptive clustering algorithm for domain-level\nadaptation while keeping class-level components private. While theoretically\nand empirically secure, SecFPP achieves state-of-the-art accuracy under severe\nheterogeneity in data distribution. Extensive experiments show it significantly\noutperforms both non-private and privacy-preserving baselines, offering a\nsuperior privacy-performance trade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt learning is a crucial technique for adapting pre-trained multimodal\nlanguage models (MLLMs) to user tasks. Federated prompt personalization (FPP)\nis further developed to address data heterogeneity and local overfitting,\nhowever, it exposes personalized prompts - valuable intellectual assets - to\nprivacy risks like prompt stealing or membership inference attacks.\nWidely-adopted techniques like differential privacy add noise to prompts,\nwhereas degrading personalization performance. We propose SecFPP, a secure FPP\nprotocol harmonizing generalization, personalization, and privacy guarantees.\nSecFPP employs hierarchical prompt adaptation with domain-level and class-level\ncomponents to handle multi-granular data imbalance. For privacy, it uses a\nnovel secret-sharing-based adaptive clustering algorithm for domain-level\nadaptation while keeping class-level components private. While theoretically\nand empirically secure, SecFPP achieves state-of-the-art accuracy under severe\nheterogeneity in data distribution. Extensive experiments show it significantly\noutperforms both non-private and privacy-preserving baselines, offering a\nsuperior privacy-performance trade-off."
                },
                "authors": [
                    {
                        "name": "Sizai Hou"
                    },
                    {
                        "name": "Songze Li"
                    },
                    {
                        "name": "Baturalp Buyukates"
                    }
                ],
                "author_detail": {
                    "name": "Baturalp Buyukates"
                },
                "author": "Baturalp Buyukates",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22442v1",
                "updated": "2025-05-28T15:07:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    7,
                    24,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T15:07:24Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    7,
                    24,
                    2,
                    148,
                    0
                ],
                "title": "SOReL and TOReL: Two Methods for Fully Offline Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOReL and TOReL: Two Methods for Fully Offline Reinforcement Learning"
                },
                "summary": "Sample efficiency remains a major obstacle for real world adoption of\nreinforcement learning (RL): success has been limited to settings where\nsimulators provide access to essentially unlimited environment interactions,\nwhich in reality are typically costly or dangerous to obtain. Offline RL in\nprinciple offers a solution by exploiting offline data to learn a near-optimal\npolicy before deployment. In practice, however, current offline RL methods rely\non extensive online interactions for hyperparameter tuning, and have no\nreliable bound on their initial online performance. To address these two\nissues, we introduce two algorithms. Firstly, SOReL: an algorithm for safe\noffline reinforcement learning. Using only offline data, our Bayesian approach\ninfers a posterior over environment dynamics to obtain a reliable estimate of\nthe online performance via the posterior predictive uncertainty. Crucially, all\nhyperparameters are also tuned fully offline. Secondly, we introduce TOReL: a\ntuning for offline reinforcement learning algorithm that extends our\ninformation rate based offline hyperparameter tuning methods to general offline\nRL approaches. Our empirical evaluation confirms SOReL's ability to accurately\nestimate regret in the Bayesian setting whilst TOReL's offline hyperparameter\ntuning achieves competitive performance with the best online hyperparameter\ntuning methods using only offline data. Thus, SOReL and TOReL make a\nsignificant step towards safe and reliable offline RL, unlocking the potential\nfor RL in the real world. Our implementations are publicly available:\nhttps://github.com/CWibault/sorel\\_torel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sample efficiency remains a major obstacle for real world adoption of\nreinforcement learning (RL): success has been limited to settings where\nsimulators provide access to essentially unlimited environment interactions,\nwhich in reality are typically costly or dangerous to obtain. Offline RL in\nprinciple offers a solution by exploiting offline data to learn a near-optimal\npolicy before deployment. In practice, however, current offline RL methods rely\non extensive online interactions for hyperparameter tuning, and have no\nreliable bound on their initial online performance. To address these two\nissues, we introduce two algorithms. Firstly, SOReL: an algorithm for safe\noffline reinforcement learning. Using only offline data, our Bayesian approach\ninfers a posterior over environment dynamics to obtain a reliable estimate of\nthe online performance via the posterior predictive uncertainty. Crucially, all\nhyperparameters are also tuned fully offline. Secondly, we introduce TOReL: a\ntuning for offline reinforcement learning algorithm that extends our\ninformation rate based offline hyperparameter tuning methods to general offline\nRL approaches. Our empirical evaluation confirms SOReL's ability to accurately\nestimate regret in the Bayesian setting whilst TOReL's offline hyperparameter\ntuning achieves competitive performance with the best online hyperparameter\ntuning methods using only offline data. Thus, SOReL and TOReL make a\nsignificant step towards safe and reliable offline RL, unlocking the potential\nfor RL in the real world. Our implementations are publicly available:\nhttps://github.com/CWibault/sorel\\_torel."
                },
                "authors": [
                    {
                        "name": "Mattie Fellows"
                    },
                    {
                        "name": "Clarisse Wibault"
                    },
                    {
                        "name": "Uljad Berdica"
                    },
                    {
                        "name": "Johannes Forkel"
                    },
                    {
                        "name": "Jakob N. Foerster"
                    },
                    {
                        "name": "Michael A. Osborne"
                    }
                ],
                "author_detail": {
                    "name": "Michael A. Osborne"
                },
                "author": "Michael A. Osborne",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.07651v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.07651v2",
                "updated": "2025-05-28T15:07:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    7,
                    8,
                    2,
                    148,
                    0
                ],
                "published": "2023-06-13T09:43:32Z",
                "published_parsed": [
                    2023,
                    6,
                    13,
                    9,
                    43,
                    32,
                    1,
                    164,
                    0
                ],
                "title": "Variational Positive-incentive Noise: How Noise Benefits Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Positive-incentive Noise: How Noise Benefits Models"
                },
                "summary": "A large number of works aim to alleviate the impact of noise due to an\nunderlying conventional assumption of the negative role of noise. However, some\nexisting works show that the assumption does not always hold. In this paper, we\ninvestigate how to benefit the classical models by random noise under the\nframework of Positive-incentive Noise (Pi-Noise). Since the ideal objective of\nPi-Noise is intractable, we propose to optimize its variational bound instead,\nnamely variational Pi-Noise (VPN). With the variational inference, a VPN\ngenerator implemented by neural networks is designed for enhancing base models\nand simplifying the inference of base models, without changing the architecture\nof base models. Benefiting from the independent design of base models and VPN\ngenerators, the VPN generator can work with most existing models. From the\nexperiments, it is shown that the proposed VPN generator can improve the base\nmodels. It is appealing that the trained variational VPN generator prefers to\nblur the irrelevant ingredients in complicated images, which meets our\nexpectations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A large number of works aim to alleviate the impact of noise due to an\nunderlying conventional assumption of the negative role of noise. However, some\nexisting works show that the assumption does not always hold. In this paper, we\ninvestigate how to benefit the classical models by random noise under the\nframework of Positive-incentive Noise (Pi-Noise). Since the ideal objective of\nPi-Noise is intractable, we propose to optimize its variational bound instead,\nnamely variational Pi-Noise (VPN). With the variational inference, a VPN\ngenerator implemented by neural networks is designed for enhancing base models\nand simplifying the inference of base models, without changing the architecture\nof base models. Benefiting from the independent design of base models and VPN\ngenerators, the VPN generator can work with most existing models. From the\nexperiments, it is shown that the proposed VPN generator can improve the base\nmodels. It is appealing that the trained variational VPN generator prefers to\nblur the irrelevant ingredients in complicated images, which meets our\nexpectations."
                },
                "authors": [
                    {
                        "name": "Hongyuan Zhang"
                    },
                    {
                        "name": "Sida Huang"
                    },
                    {
                        "name": "Yubin Guo"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "arxiv_comment": "Acceptted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.07651v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.07651v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06365v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06365v2",
                "updated": "2025-05-28T15:06:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    6,
                    30,
                    2,
                    148,
                    0
                ],
                "published": "2025-01-10T22:07:56Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    22,
                    7,
                    56,
                    4,
                    10,
                    0
                ],
                "title": "Gender-Neutral Large Language Models for Medical Applications: Reducing\n  Bias in PubMed Abstracts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gender-Neutral Large Language Models for Medical Applications: Reducing\n  Bias in PubMed Abstracts"
                },
                "summary": "This paper presents a pipeline for mitigating gender bias in large language\nmodels (LLMs) used in medical literature by neutralizing gendered occupational\npronouns. A dataset of 379,000 PubMed abstracts from 1965-1980 was processed to\nidentify and modify pronouns tied to professions. We developed a BERT-based\nmodel, \"Modern Occupational Bias Elimination with Refined Training,\" or\n\"MOBERT,\" trained on these neutralized abstracts, and compared its performance\nwith \"1965BERT,\" trained on the original dataset. MOBERT achieved a 70%\ninclusive replacement rate, while 1965BERT reached only 4%. A further analysis\nof MOBERT revealed that pronoun replacement accuracy correlated with the\nfrequency of occupational terms in the training data. We propose expanding the\ndataset and refining the pipeline to improve performance and ensure more\nequitable language modeling in medical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a pipeline for mitigating gender bias in large language\nmodels (LLMs) used in medical literature by neutralizing gendered occupational\npronouns. A dataset of 379,000 PubMed abstracts from 1965-1980 was processed to\nidentify and modify pronouns tied to professions. We developed a BERT-based\nmodel, \"Modern Occupational Bias Elimination with Refined Training,\" or\n\"MOBERT,\" trained on these neutralized abstracts, and compared its performance\nwith \"1965BERT,\" trained on the original dataset. MOBERT achieved a 70%\ninclusive replacement rate, while 1965BERT reached only 4%. A further analysis\nof MOBERT revealed that pronoun replacement accuracy correlated with the\nfrequency of occupational terms in the training data. We propose expanding the\ndataset and refining the pipeline to improve performance and ensure more\nequitable language modeling in medical applications."
                },
                "authors": [
                    {
                        "name": "Elizabeth Schaefer"
                    },
                    {
                        "name": "Kirk Roberts"
                    }
                ],
                "author_detail": {
                    "name": "Kirk Roberts"
                },
                "author": "Kirk Roberts",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06365v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06365v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22441v1",
                "updated": "2025-05-28T15:04:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    4,
                    46,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T15:04:46Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    4,
                    46,
                    2,
                    148,
                    0
                ],
                "title": "Can NeRFs See without Cameras?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can NeRFs See without Cameras?"
                },
                "summary": "Neural Radiance Fields (NeRFs) have been remarkably successful at\nsynthesizing novel views of 3D scenes by optimizing a volumetric scene\nfunction. This scene function models how optical rays bring color information\nfrom a 3D object to the camera pixels. Radio frequency (RF) or audio signals\ncan also be viewed as a vehicle for delivering information about the\nenvironment to a sensor. However, unlike camera pixels, an RF/audio sensor\nreceives a mixture of signals that contain many environmental reflections (also\ncalled \"multipath\"). Is it still possible to infer the environment using such\nmultipath signals? We show that with redesign, NeRFs can be taught to learn\nfrom multipath signals, and thereby \"see\" the environment. As a grounding\napplication, we aim to infer the indoor floorplan of a home from sparse WiFi\nmeasurements made at multiple locations inside the home. Although a difficult\ninverse problem, our implicitly learnt floorplans look promising, and enables\nforward applications, such as indoor signal prediction and basic ray tracing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Radiance Fields (NeRFs) have been remarkably successful at\nsynthesizing novel views of 3D scenes by optimizing a volumetric scene\nfunction. This scene function models how optical rays bring color information\nfrom a 3D object to the camera pixels. Radio frequency (RF) or audio signals\ncan also be viewed as a vehicle for delivering information about the\nenvironment to a sensor. However, unlike camera pixels, an RF/audio sensor\nreceives a mixture of signals that contain many environmental reflections (also\ncalled \"multipath\"). Is it still possible to infer the environment using such\nmultipath signals? We show that with redesign, NeRFs can be taught to learn\nfrom multipath signals, and thereby \"see\" the environment. As a grounding\napplication, we aim to infer the indoor floorplan of a home from sparse WiFi\nmeasurements made at multiple locations inside the home. Although a difficult\ninverse problem, our implicitly learnt floorplans look promising, and enables\nforward applications, such as indoor signal prediction and basic ray tracing."
                },
                "authors": [
                    {
                        "name": "Chaitanya Amballa"
                    },
                    {
                        "name": "Sattwik Basu"
                    },
                    {
                        "name": "Yu-Lin Wei"
                    },
                    {
                        "name": "Zhijian Yang"
                    },
                    {
                        "name": "Mehmet Ergezer"
                    },
                    {
                        "name": "Romit Roy Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Romit Roy Choudhury"
                },
                "author": "Romit Roy Choudhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22438v1",
                "updated": "2025-05-28T15:03:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    3,
                    27,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T15:03:27Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    3,
                    27,
                    2,
                    148,
                    0
                ],
                "title": "Synonymous Variational Inference for Perceptual Image Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synonymous Variational Inference for Perceptual Image Compression"
                },
                "summary": "Recent contributions of semantic information theory reveal the set-element\nrelationship between semantic and syntactic information, represented as\nsynonymous relationships. In this paper, we propose a synonymous variational\ninference (SVI) method based on this synonymity viewpoint to re-analyze the\nperceptual image compression problem. It takes perceptual similarity as a\ntypical synonymous criterion to build an ideal synonymous set (Synset), and\napproximate the posterior of its latent synonymous representation with a\nparametric density by minimizing a partial semantic KL divergence. This\nanalysis theoretically proves that the optimization direction of perception\nimage compression follows a triple tradeoff that can cover the existing\nrate-distortion-perception schemes. Additionally, we introduce synonymous image\ncompression (SIC), a new image compression scheme that corresponds to the\nanalytical process of SVI, and implement a progressive SIC codec to fully\nleverage the model's capabilities. Experimental results demonstrate comparable\nrate-distortion-perception performance using a single progressive SIC codec,\nthus verifying the effectiveness of our proposed analysis method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent contributions of semantic information theory reveal the set-element\nrelationship between semantic and syntactic information, represented as\nsynonymous relationships. In this paper, we propose a synonymous variational\ninference (SVI) method based on this synonymity viewpoint to re-analyze the\nperceptual image compression problem. It takes perceptual similarity as a\ntypical synonymous criterion to build an ideal synonymous set (Synset), and\napproximate the posterior of its latent synonymous representation with a\nparametric density by minimizing a partial semantic KL divergence. This\nanalysis theoretically proves that the optimization direction of perception\nimage compression follows a triple tradeoff that can cover the existing\nrate-distortion-perception schemes. Additionally, we introduce synonymous image\ncompression (SIC), a new image compression scheme that corresponds to the\nanalytical process of SVI, and implement a progressive SIC codec to fully\nleverage the model's capabilities. Experimental results demonstrate comparable\nrate-distortion-perception performance using a single progressive SIC codec,\nthus verifying the effectiveness of our proposed analysis method."
                },
                "authors": [
                    {
                        "name": "Zijian Liang"
                    },
                    {
                        "name": "Kai Niu"
                    },
                    {
                        "name": "Changshuo Wang"
                    },
                    {
                        "name": "Jin Xu"
                    },
                    {
                        "name": "Ping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Zhang"
                },
                "author": "Ping Zhang",
                "arxiv_comment": "31 pages, 20 figures. This paper is accepted by Proceedings of the\n  42nd International Conference on Machine Learning (ICML 2025) Poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06999v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06999v2",
                "updated": "2025-05-28T14:58:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    58,
                    36,
                    2,
                    148,
                    0
                ],
                "published": "2025-02-10T19:49:54Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    19,
                    49,
                    54,
                    0,
                    41,
                    0
                ],
                "title": "Outsourced diffusion sampling: Efficient posterior inference in latent\n  spaces of generative models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outsourced diffusion sampling: Efficient posterior inference in latent\n  spaces of generative models"
                },
                "summary": "Any well-behaved generative model over a variable $\\mathbf{x}$ can be\nexpressed as a deterministic transformation of an exogenous ('outsourced')\nGaussian noise variable $\\mathbf{z}$: $\\mathbf{x}=f_\\theta(\\mathbf{z})$. In\nsuch a model (\\eg, a VAE, GAN, or continuous-time flow-based model), sampling\nof the target variable $\\mathbf{x} \\sim p_\\theta(\\mathbf{x})$ is\nstraightforward, but sampling from a posterior distribution of the form\n$p(\\mathbf{x}\\mid\\mathbf{y}) \\propto\np_\\theta(\\mathbf{x})r(\\mathbf{x},\\mathbf{y})$, where $r$ is a constraint\nfunction depending on an auxiliary variable $\\mathbf{y}$, is generally\nintractable. We propose to amortize the cost of sampling from such posterior\ndistributions with diffusion models that sample a distribution in the noise\nspace ($\\mathbf{z}$). These diffusion samplers are trained by reinforcement\nlearning algorithms to enforce that the transformed samples\n$f_\\theta(\\mathbf{z})$ are distributed according to the posterior in the data\nspace ($\\mathbf{x}$). For many models and constraints, the posterior in noise\nspace is smoother than in data space, making it more suitable for amortized\ninference. Our method enables conditional sampling under unconditional GAN,\n(H)VAE, and flow-based priors, comparing favorably with other inference\nmethods. We demonstrate the proposed outsourced diffusion sampling in several\nexperiments with large pretrained prior models: conditional image generation,\nreinforcement learning with human feedback, and protein structure generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Any well-behaved generative model over a variable $\\mathbf{x}$ can be\nexpressed as a deterministic transformation of an exogenous ('outsourced')\nGaussian noise variable $\\mathbf{z}$: $\\mathbf{x}=f_\\theta(\\mathbf{z})$. In\nsuch a model (\\eg, a VAE, GAN, or continuous-time flow-based model), sampling\nof the target variable $\\mathbf{x} \\sim p_\\theta(\\mathbf{x})$ is\nstraightforward, but sampling from a posterior distribution of the form\n$p(\\mathbf{x}\\mid\\mathbf{y}) \\propto\np_\\theta(\\mathbf{x})r(\\mathbf{x},\\mathbf{y})$, where $r$ is a constraint\nfunction depending on an auxiliary variable $\\mathbf{y}$, is generally\nintractable. We propose to amortize the cost of sampling from such posterior\ndistributions with diffusion models that sample a distribution in the noise\nspace ($\\mathbf{z}$). These diffusion samplers are trained by reinforcement\nlearning algorithms to enforce that the transformed samples\n$f_\\theta(\\mathbf{z})$ are distributed according to the posterior in the data\nspace ($\\mathbf{x}$). For many models and constraints, the posterior in noise\nspace is smoother than in data space, making it more suitable for amortized\ninference. Our method enables conditional sampling under unconditional GAN,\n(H)VAE, and flow-based priors, comparing favorably with other inference\nmethods. We demonstrate the proposed outsourced diffusion sampling in several\nexperiments with large pretrained prior models: conditional image generation,\nreinforcement learning with human feedback, and protein structure generation."
                },
                "authors": [
                    {
                        "name": "Siddarth Venkatraman"
                    },
                    {
                        "name": "Mohsin Hasan"
                    },
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Luca Scimeca"
                    },
                    {
                        "name": "Marcin Sendera"
                    },
                    {
                        "name": "Yoshua Bengio"
                    },
                    {
                        "name": "Glen Berseth"
                    },
                    {
                        "name": "Nikolay Malkin"
                    }
                ],
                "author_detail": {
                    "name": "Nikolay Malkin"
                },
                "author": "Nikolay Malkin",
                "arxiv_comment": "ICML 2025; code:\n  https://github.com/HyperPotatoNeo/Outsourced_Diffusion_Sampling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06999v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06999v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22435v1",
                "updated": "2025-05-28T14:58:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    58,
                    29,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T14:58:29Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    58,
                    29,
                    2,
                    148,
                    0
                ],
                "title": "Does Johnny Get the Message? Evaluating Cybersecurity Notifications for\n  Everyday Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Johnny Get the Message? Evaluating Cybersecurity Notifications for\n  Everyday Users"
                },
                "summary": "Due to the increasing presence of networked devices in everyday life, not\nonly cybersecurity specialists but also end users benefit from security\napplications such as firewalls, vulnerability scanners, and intrusion detection\nsystems. Recent approaches use large language models (LLMs) to rewrite brief,\ntechnical security alerts into intuitive language and suggest actionable\nmeasures, helping everyday users understand and respond appropriately to\nsecurity risks. However, it remains an open question how well such alerts are\nexplained to users. LLM outputs can also be hallucinated, inconsistent, or\nmisleading. In this work, we introduce the Human-Centered Security Alert\nEvaluation Framework (HCSAEF). HCSAEF assesses LLM-generated cybersecurity\nnotifications to support researchers who want to compare notifications\ngenerated for everyday users, improve them, or analyze the capabilities of\ndifferent LLMs in explaining cybersecurity issues. We demonstrate HCSAEF\nthrough three use cases, which allow us to quantify the impact of prompt\ndesign, model selection, and output consistency. Our findings indicate that\nHCSAEF effectively differentiates generated notifications along dimensions such\nas intuitiveness, urgency, and correctness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the increasing presence of networked devices in everyday life, not\nonly cybersecurity specialists but also end users benefit from security\napplications such as firewalls, vulnerability scanners, and intrusion detection\nsystems. Recent approaches use large language models (LLMs) to rewrite brief,\ntechnical security alerts into intuitive language and suggest actionable\nmeasures, helping everyday users understand and respond appropriately to\nsecurity risks. However, it remains an open question how well such alerts are\nexplained to users. LLM outputs can also be hallucinated, inconsistent, or\nmisleading. In this work, we introduce the Human-Centered Security Alert\nEvaluation Framework (HCSAEF). HCSAEF assesses LLM-generated cybersecurity\nnotifications to support researchers who want to compare notifications\ngenerated for everyday users, improve them, or analyze the capabilities of\ndifferent LLMs in explaining cybersecurity issues. We demonstrate HCSAEF\nthrough three use cases, which allow us to quantify the impact of prompt\ndesign, model selection, and output consistency. Our findings indicate that\nHCSAEF effectively differentiates generated notifications along dimensions such\nas intuitiveness, urgency, and correctness."
                },
                "authors": [
                    {
                        "name": "Victor Jüttner"
                    },
                    {
                        "name": "Erik Buchmann"
                    }
                ],
                "author_detail": {
                    "name": "Erik Buchmann"
                },
                "author": "Erik Buchmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17773v2",
                "updated": "2025-05-28T14:57:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    57,
                    51,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-23T11:44:02Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    11,
                    44,
                    2,
                    4,
                    143,
                    0
                ],
                "title": "C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in\n  Large Language Models"
                },
                "summary": "Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning\nlarge language models (LLMs), but it often produces overconfident predictions\nin data-scarce few-shot settings. To address this issue, several classical\nstatistical learning approaches have been repurposed for scalable\nuncertainty-aware LoRA fine-tuning. However, these approaches neglect how input\ncharacteristics affect the predictive uncertainty estimates. To address this\nlimitation, we propose Contextual Low-Rank Adaptation (\\textbf{C-LoRA}) as a\nnovel uncertainty-aware and parameter efficient fine-tuning approach, by\ndeveloping new lightweight LoRA modules contextualized to each input data\nsample to dynamically adapt uncertainty estimates. Incorporating data-driven\ncontexts into the parameter posteriors, C-LoRA mitigates overfitting, achieves\nwell-calibrated uncertainties, and yields robust predictions. Extensive\nexperiments demonstrate that C-LoRA consistently outperforms the\nstate-of-the-art uncertainty-aware LoRA methods in both uncertainty\nquantification and model generalization. Ablation studies further confirm the\ncritical role of our contextual modules in capturing sample-specific\nuncertainties. C-LoRA sets a new standard for robust, uncertainty-aware LLM\nfine-tuning in few-shot regimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning\nlarge language models (LLMs), but it often produces overconfident predictions\nin data-scarce few-shot settings. To address this issue, several classical\nstatistical learning approaches have been repurposed for scalable\nuncertainty-aware LoRA fine-tuning. However, these approaches neglect how input\ncharacteristics affect the predictive uncertainty estimates. To address this\nlimitation, we propose Contextual Low-Rank Adaptation (\\textbf{C-LoRA}) as a\nnovel uncertainty-aware and parameter efficient fine-tuning approach, by\ndeveloping new lightweight LoRA modules contextualized to each input data\nsample to dynamically adapt uncertainty estimates. Incorporating data-driven\ncontexts into the parameter posteriors, C-LoRA mitigates overfitting, achieves\nwell-calibrated uncertainties, and yields robust predictions. Extensive\nexperiments demonstrate that C-LoRA consistently outperforms the\nstate-of-the-art uncertainty-aware LoRA methods in both uncertainty\nquantification and model generalization. Ablation studies further confirm the\ncritical role of our contextual modules in capturing sample-specific\nuncertainties. C-LoRA sets a new standard for robust, uncertainty-aware LLM\nfine-tuning in few-shot regimes."
                },
                "authors": [
                    {
                        "name": "Amir Hossein Rahmati"
                    },
                    {
                        "name": "Sanket Jantre"
                    },
                    {
                        "name": "Weifeng Zhang"
                    },
                    {
                        "name": "Yucheng Wang"
                    },
                    {
                        "name": "Byung-Jun Yoon"
                    },
                    {
                        "name": "Nathan M. Urban"
                    },
                    {
                        "name": "Xiaoning Qian"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoning Qian"
                },
                "author": "Xiaoning Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14191v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14191v4",
                "updated": "2025-05-28T14:56:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    56,
                    35,
                    2,
                    148,
                    0
                ],
                "published": "2024-09-21T16:38:22Z",
                "published_parsed": [
                    2024,
                    9,
                    21,
                    16,
                    38,
                    22,
                    5,
                    265,
                    0
                ],
                "title": "Addressing and Visualizing Misalignments in Human Task-Solving\n  Trajectories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing and Visualizing Misalignments in Human Task-Solving\n  Trajectories"
                },
                "summary": "Understanding misalignments in human task-solving trajectories is crucial for\nenhancing AI models trained to closely mimic human reasoning. This study\ncategorizes such misalignments into three types: (1) lack of functions to\nexpress intent, (2) inefficient action sequences, and (3) incorrect intentions\nthat cannot solve the task. To address these issues, we first formalize and\ndefine these three misalignment types in a unified framework. We then propose a\nheuristic algorithm to detect misalignments in ARCTraj trajectories and analyze\ntheir impact hierarchically and quantitatively. We also present an intention\nestimation method based on our formalism that infers missing alignment between\nuser actions and intentions. Through trajectory alignment, we experimentally\ndemonstrate that AI models trained on human task-solving trajectories improve\nperformance in mimicking human reasoning. Based on hierarchical analysis and\nexperiments, we highlight the importance of trajectory-intention alignment and\ndemonstrate the effectiveness of intention-aligned training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding misalignments in human task-solving trajectories is crucial for\nenhancing AI models trained to closely mimic human reasoning. This study\ncategorizes such misalignments into three types: (1) lack of functions to\nexpress intent, (2) inefficient action sequences, and (3) incorrect intentions\nthat cannot solve the task. To address these issues, we first formalize and\ndefine these three misalignment types in a unified framework. We then propose a\nheuristic algorithm to detect misalignments in ARCTraj trajectories and analyze\ntheir impact hierarchically and quantitatively. We also present an intention\nestimation method based on our formalism that infers missing alignment between\nuser actions and intentions. Through trajectory alignment, we experimentally\ndemonstrate that AI models trained on human task-solving trajectories improve\nperformance in mimicking human reasoning. Based on hierarchical analysis and\nexperiments, we highlight the importance of trajectory-intention alignment and\ndemonstrate the effectiveness of intention-aligned training."
                },
                "authors": [
                    {
                        "name": "Sejin Kim"
                    },
                    {
                        "name": "Hosung Lee"
                    },
                    {
                        "name": "Sundong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Sundong Kim"
                },
                "author": "Sundong Kim",
                "arxiv_comment": "KDD 2025 accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14191v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14191v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22430v1",
                "updated": "2025-05-28T14:55:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    55,
                    33,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T14:55:33Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    55,
                    33,
                    2,
                    148,
                    0
                ],
                "title": "RAG-Zeval: Towards Robust and Interpretable Evaluation on RAG Responses\n  through End-to-End Rule-Guided Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-Zeval: Towards Robust and Interpretable Evaluation on RAG Responses\n  through End-to-End Rule-Guided Reasoning"
                },
                "summary": "Robust evaluation is critical for deploying trustworthy retrieval-augmented\ngeneration (RAG) systems. However, current LLM-based evaluation frameworks\npredominantly rely on directly prompting resource-intensive models with complex\nmulti-stage prompts, underutilizing models' reasoning capabilities and\nintroducing significant computational cost. In this paper, we present RAG-Zeval\n(RAG-Zero Evaluator), a novel end-to-end framework that formulates faithfulness\nand correctness evaluation as a rule-guided reasoning task. Our approach trains\nevaluators with reinforcement learning, facilitating compact models to generate\ncomprehensive and sound assessments with detailed explanation in one-pass. We\nintroduce a ranking-based outcome reward mechanism, using preference judgments\nrather than absolute scores, to address the challenge of obtaining precise\npointwise reward signals. To this end, we synthesize the ranking references by\ngenerating quality-controlled responses with zero human annotation. Experiments\ndemonstrate RAG-Zeval's superior performance, achieving the strongest\ncorrelation with human judgments and outperforming baselines that rely on LLMs\nwith 10-100 times more parameters. Our approach also exhibits superior\ninterpretability in response evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust evaluation is critical for deploying trustworthy retrieval-augmented\ngeneration (RAG) systems. However, current LLM-based evaluation frameworks\npredominantly rely on directly prompting resource-intensive models with complex\nmulti-stage prompts, underutilizing models' reasoning capabilities and\nintroducing significant computational cost. In this paper, we present RAG-Zeval\n(RAG-Zero Evaluator), a novel end-to-end framework that formulates faithfulness\nand correctness evaluation as a rule-guided reasoning task. Our approach trains\nevaluators with reinforcement learning, facilitating compact models to generate\ncomprehensive and sound assessments with detailed explanation in one-pass. We\nintroduce a ranking-based outcome reward mechanism, using preference judgments\nrather than absolute scores, to address the challenge of obtaining precise\npointwise reward signals. To this end, we synthesize the ranking references by\ngenerating quality-controlled responses with zero human annotation. Experiments\ndemonstrate RAG-Zeval's superior performance, achieving the strongest\ncorrelation with human judgments and outperforming baselines that rely on LLMs\nwith 10-100 times more parameters. Our approach also exhibits superior\ninterpretability in response evaluation."
                },
                "authors": [
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Yunxiang Li"
                    },
                    {
                        "name": "Tianhua Zhang"
                    },
                    {
                        "name": "Hongyin Luo"
                    },
                    {
                        "name": "Xixin Wu"
                    },
                    {
                        "name": "James Glass"
                    },
                    {
                        "name": "Helen Meng"
                    }
                ],
                "author_detail": {
                    "name": "Helen Meng"
                },
                "author": "Helen Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20538v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20538v2",
                "updated": "2025-05-28T14:54:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    54,
                    54,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-26T21:49:18Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    21,
                    49,
                    18,
                    0,
                    146,
                    0
                ],
                "title": "AstroVisBench: A Code Benchmark for Scientific Computing and\n  Visualization in Astronomy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AstroVisBench: A Code Benchmark for Scientific Computing and\n  Visualization in Astronomy"
                },
                "summary": "Large Language Models (LLMs) are being explored for applications in\nscientific research, including their capabilities to synthesize literature,\nanswer research questions, generate research ideas, and even conduct\ncomputational experiments. Ultimately, our goal is for these to help scientists\nderive novel scientific insights. In many areas of science, such insights often\narise from processing and visualizing data to understand its patterns. However,\nevaluating whether an LLM-mediated scientific workflow produces outputs\nconveying the correct scientific insights is challenging to evaluate and has\nnot been addressed in past work. We introduce AstroVisBench, the first\nbenchmark for both scientific computing and visualization in the astronomy\ndomain. AstroVisBench judges a language model's ability to both (1) create\nastronomy-specific workflows to process and analyze data and (2) visualize the\nresults of these workflows through complex plots. Our evaluation of\nvisualizations uses a novel LLM-as-a-judge workflow, which is validated against\nannotation by five professional astronomers. Using AstroVisBench we present an\nevaluation of state-of-the-art language models, showing a significant gap in\ntheir ability to engage in astronomy research as useful assistants. This\nevaluation provides a strong end-to-end evaluation for AI scientists that\noffers a path forward for the development of visualization-based workflows,\nwhich are central to a broad range of domains from physics to biology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are being explored for applications in\nscientific research, including their capabilities to synthesize literature,\nanswer research questions, generate research ideas, and even conduct\ncomputational experiments. Ultimately, our goal is for these to help scientists\nderive novel scientific insights. In many areas of science, such insights often\narise from processing and visualizing data to understand its patterns. However,\nevaluating whether an LLM-mediated scientific workflow produces outputs\nconveying the correct scientific insights is challenging to evaluate and has\nnot been addressed in past work. We introduce AstroVisBench, the first\nbenchmark for both scientific computing and visualization in the astronomy\ndomain. AstroVisBench judges a language model's ability to both (1) create\nastronomy-specific workflows to process and analyze data and (2) visualize the\nresults of these workflows through complex plots. Our evaluation of\nvisualizations uses a novel LLM-as-a-judge workflow, which is validated against\nannotation by five professional astronomers. Using AstroVisBench we present an\nevaluation of state-of-the-art language models, showing a significant gap in\ntheir ability to engage in astronomy research as useful assistants. This\nevaluation provides a strong end-to-end evaluation for AI scientists that\noffers a path forward for the development of visualization-based workflows,\nwhich are central to a broad range of domains from physics to biology."
                },
                "authors": [
                    {
                        "name": "Sebastian Antony Joseph"
                    },
                    {
                        "name": "Syed Murtaza Husain"
                    },
                    {
                        "name": "Stella S. R. Offner"
                    },
                    {
                        "name": "Stéphanie Juneau"
                    },
                    {
                        "name": "Paul Torrey"
                    },
                    {
                        "name": "Adam S. Bolton"
                    },
                    {
                        "name": "Juan P. Farias"
                    },
                    {
                        "name": "Niall Gaffney"
                    },
                    {
                        "name": "Greg Durrett"
                    },
                    {
                        "name": "Junyi Jessy Li"
                    }
                ],
                "author_detail": {
                    "name": "Junyi Jessy Li"
                },
                "author": "Junyi Jessy Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20538v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20538v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22429v1",
                "updated": "2025-05-28T14:53:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    53,
                    53,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T14:53:53Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    53,
                    53,
                    2,
                    148,
                    0
                ],
                "title": "Zero-Shot 3D Visual Grounding from Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot 3D Visual Grounding from Vision-Language Models"
                },
                "summary": "3D Visual Grounding (3DVG) seeks to locate target objects in 3D scenes using\nnatural language descriptions, enabling downstream applications such as\naugmented reality and robotics. Existing approaches typically rely on labeled\n3D data and predefined categories, limiting scalability to open-world settings.\nWe present SeeGround, a zero-shot 3DVG framework that leverages 2D\nVision-Language Models (VLMs) to bypass the need for 3D-specific training. To\nbridge the modality gap, we introduce a hybrid input format that pairs\nquery-aligned rendered views with spatially enriched textual descriptions. Our\nframework incorporates two core components: a Perspective Adaptation Module\nthat dynamically selects optimal viewpoints based on the query, and a Fusion\nAlignment Module that integrates visual and spatial signals to enhance\nlocalization precision. Extensive evaluations on ScanRefer and Nr3D confirm\nthat SeeGround achieves substantial improvements over existing zero-shot\nbaselines -- outperforming them by 7.7% and 7.1%, respectively -- and even\nrivals fully supervised alternatives, demonstrating strong generalization under\nchallenging conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Visual Grounding (3DVG) seeks to locate target objects in 3D scenes using\nnatural language descriptions, enabling downstream applications such as\naugmented reality and robotics. Existing approaches typically rely on labeled\n3D data and predefined categories, limiting scalability to open-world settings.\nWe present SeeGround, a zero-shot 3DVG framework that leverages 2D\nVision-Language Models (VLMs) to bypass the need for 3D-specific training. To\nbridge the modality gap, we introduce a hybrid input format that pairs\nquery-aligned rendered views with spatially enriched textual descriptions. Our\nframework incorporates two core components: a Perspective Adaptation Module\nthat dynamically selects optimal viewpoints based on the query, and a Fusion\nAlignment Module that integrates visual and spatial signals to enhance\nlocalization precision. Extensive evaluations on ScanRefer and Nr3D confirm\nthat SeeGround achieves substantial improvements over existing zero-shot\nbaselines -- outperforming them by 7.7% and 7.1%, respectively -- and even\nrivals fully supervised alternatives, demonstrating strong generalization under\nchallenging conditions."
                },
                "authors": [
                    {
                        "name": "Rong Li"
                    },
                    {
                        "name": "Shijie Li"
                    },
                    {
                        "name": "Lingdong Kong"
                    },
                    {
                        "name": "Xulei Yang"
                    },
                    {
                        "name": "Junwei Liang"
                    }
                ],
                "author_detail": {
                    "name": "Junwei Liang"
                },
                "author": "Junwei Liang",
                "arxiv_comment": "3D-LLM/VLA @ CVPR 2025; Project Page at https://seeground.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01002v2",
                "updated": "2025-05-28T14:52:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    52,
                    38,
                    2,
                    148,
                    0
                ],
                "published": "2025-04-01T17:40:12Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    40,
                    12,
                    1,
                    91,
                    0
                ],
                "title": "Token embeddings violate the manifold hypothesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token embeddings violate the manifold hypothesis"
                },
                "summary": "A full understanding of the behavior of a large language model (LLM) requires\nour understanding of its input token space. If this space differs from our\nassumptions, our understanding of and conclusions about the LLM will likely be\nflawed. We elucidate the structure of the token embeddings both empirically and\ntheoretically. We present a novel statistical test assuming that the\nneighborhood around each token has a relatively flat and smooth structure as\nthe null hypothesis. Failing to reject the null is uninformative, but rejecting\nit at a specific token $\\psi$ implies an irregularity in the token subspace in\na $\\psi$-neighborhood, $B(\\psi)$. The structure assumed in the null is a\ngeneralization of a manifold with boundary called a \\emph{smooth fiber bundle}\n(which can be split into two spatial regimes -- small and large radius), so we\ndenote our new hypothesis test as the ``fiber bundle hypothesis.'' Failure to\nreject the null hypothesis is uninformative, but rejecting it at $\\psi$\nindicates a statistically significant irregularity at $B(\\psi)$. By running our\ntest over several open-source LLMs, each with unique token embeddings, we find\nthat the null is frequently rejected, and so the evidence suggests that the\ntoken subspace is not a fiber bundle and hence also not a manifold. As a\nconsequence of our findings, when an LLM is presented with two semantically\nequivalent prompts, if one prompt contains a token implicated by our test, the\nresponse to that prompt will likely exhibit less stability than the other.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A full understanding of the behavior of a large language model (LLM) requires\nour understanding of its input token space. If this space differs from our\nassumptions, our understanding of and conclusions about the LLM will likely be\nflawed. We elucidate the structure of the token embeddings both empirically and\ntheoretically. We present a novel statistical test assuming that the\nneighborhood around each token has a relatively flat and smooth structure as\nthe null hypothesis. Failing to reject the null is uninformative, but rejecting\nit at a specific token $\\psi$ implies an irregularity in the token subspace in\na $\\psi$-neighborhood, $B(\\psi)$. The structure assumed in the null is a\ngeneralization of a manifold with boundary called a \\emph{smooth fiber bundle}\n(which can be split into two spatial regimes -- small and large radius), so we\ndenote our new hypothesis test as the ``fiber bundle hypothesis.'' Failure to\nreject the null hypothesis is uninformative, but rejecting it at $\\psi$\nindicates a statistically significant irregularity at $B(\\psi)$. By running our\ntest over several open-source LLMs, each with unique token embeddings, we find\nthat the null is frequently rejected, and so the evidence suggests that the\ntoken subspace is not a fiber bundle and hence also not a manifold. As a\nconsequence of our findings, when an LLM is presented with two semantically\nequivalent prompts, if one prompt contains a token implicated by our test, the\nresponse to that prompt will likely exhibit less stability than the other."
                },
                "authors": [
                    {
                        "name": "Michael Robinson"
                    },
                    {
                        "name": "Sourya Dey"
                    },
                    {
                        "name": "Tony Chiang"
                    }
                ],
                "author_detail": {
                    "name": "Tony Chiang"
                },
                "author": "Tony Chiang",
                "arxiv_comment": "27 pages, 6 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "53Z50, 62H15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22425v1",
                "updated": "2025-05-28T14:52:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    52,
                    15,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T14:52:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    52,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Scaling Reasoning without Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Reasoning without Attention"
                },
                "summary": "Large language models (LLMs) have made significant advances in complex\nreasoning tasks, yet they remain bottlenecked by two core challenges:\narchitectural inefficiency due to reliance on Transformers, and a lack of\nstructured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an\nattention-free language model that addresses both issues through architectural\nand data-centric innovations. Built on the state space dual (SSD) layers of\nMamba-2, our model eliminates the need for self-attention and key-value\ncaching, enabling fixed-memory, constant-time inference. To train it for\ncomplex reasoning, we propose a two-phase curriculum fine-tuning strategy based\non the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically\nstructured problems via abstract concept selection and rationale-guided\ngeneration. On benchmark evaluations, \\ourmodel-7B outperforms strong\nTransformer and hybrid models of comparable scale, and even surpasses the much\nlarger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on\nLivecodebench. These results highlight the potential of state space models as\nefficient and scalable alternatives to attention-based architectures for\nhigh-capacity reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant advances in complex\nreasoning tasks, yet they remain bottlenecked by two core challenges:\narchitectural inefficiency due to reliance on Transformers, and a lack of\nstructured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an\nattention-free language model that addresses both issues through architectural\nand data-centric innovations. Built on the state space dual (SSD) layers of\nMamba-2, our model eliminates the need for self-attention and key-value\ncaching, enabling fixed-memory, constant-time inference. To train it for\ncomplex reasoning, we propose a two-phase curriculum fine-tuning strategy based\non the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically\nstructured problems via abstract concept selection and rationale-guided\ngeneration. On benchmark evaluations, \\ourmodel-7B outperforms strong\nTransformer and hybrid models of comparable scale, and even surpasses the much\nlarger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on\nLivecodebench. These results highlight the potential of state space models as\nefficient and scalable alternatives to attention-based architectures for\nhigh-capacity reasoning."
                },
                "authors": [
                    {
                        "name": "Xueliang Zhao"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Lingpeng Kong"
                    }
                ],
                "author_detail": {
                    "name": "Lingpeng Kong"
                },
                "author": "Lingpeng Kong",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22418v1",
                "updated": "2025-05-28T14:43:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    43,
                    55,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T14:43:55Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    43,
                    55,
                    2,
                    148,
                    0
                ],
                "title": "AI Trust Reshaping Administrative Burdens: Understanding Trust-Burden\n  Dynamics in LLM-Assisted Benefits Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Trust Reshaping Administrative Burdens: Understanding Trust-Burden\n  Dynamics in LLM-Assisted Benefits Systems"
                },
                "summary": "Supplemental Nutrition Assistance Program (SNAP) is an essential benefit\nsupport system provided by the US administration to 41 million federally\ndetermined low-income applicants. Through interviews with such applicants\nacross a diverse set of experiences with the SNAP system, our findings reveal\nthat new AI technologies like LLMs can alleviate traditional burdens but also\nintroduce new burdens. We introduce new types of learning, compliance, and\npsychological costs that transform the administrative burden on applicants. We\nalso identify how trust in AI across three dimensions--competence, integrity,\nand benevolence--is perceived to reduce administrative burdens, which may stem\nfrom unintended and untoward overt trust in the system. We discuss calibrating\nappropriate levels of user trust in LLM-based administrative systems,\nmitigating newly introduced burdens. In particular, our findings suggest that\nevidence-based information disclosure is necessary in benefits administration\nand propose directions for future research on trust-burden dynamics in\nAI-assisted administration systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supplemental Nutrition Assistance Program (SNAP) is an essential benefit\nsupport system provided by the US administration to 41 million federally\ndetermined low-income applicants. Through interviews with such applicants\nacross a diverse set of experiences with the SNAP system, our findings reveal\nthat new AI technologies like LLMs can alleviate traditional burdens but also\nintroduce new burdens. We introduce new types of learning, compliance, and\npsychological costs that transform the administrative burden on applicants. We\nalso identify how trust in AI across three dimensions--competence, integrity,\nand benevolence--is perceived to reduce administrative burdens, which may stem\nfrom unintended and untoward overt trust in the system. We discuss calibrating\nappropriate levels of user trust in LLM-based administrative systems,\nmitigating newly introduced burdens. In particular, our findings suggest that\nevidence-based information disclosure is necessary in benefits administration\nand propose directions for future research on trust-burden dynamics in\nAI-assisted administration systems."
                },
                "authors": [
                    {
                        "name": "Jeongwon Jo"
                    },
                    {
                        "name": "He Zhang"
                    },
                    {
                        "name": "Jie Cai"
                    },
                    {
                        "name": "Nitesh Goyal"
                    }
                ],
                "author_detail": {
                    "name": "Nitesh Goyal"
                },
                "author": "Nitesh Goyal",
                "arxiv_doi": "10.1145/3715275.3732077",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3715275.3732077",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.22418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "FAccT 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19618v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19618v2",
                "updated": "2025-05-28T14:42:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    42,
                    9,
                    2,
                    148,
                    0
                ],
                "published": "2025-03-25T13:03:09Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    3,
                    9,
                    1,
                    84,
                    0
                ],
                "title": "Beyond Verifiable Rewards: Scaling Reinforcement Learning for Language\n  Models to Unverifiable Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Verifiable Rewards: Scaling Reinforcement Learning for Language\n  Models to Unverifiable Data"
                },
                "summary": "We propose to scale RL to unverifiable data with a novel algorithm JEPO\n(Jensen's Evidence lower bound Policy Optimization). While most prior efforts\non scaling RL for LLMs focus on verifiable data where ground truth answers are\ntypically short-form and can be matched easily; we investigate the case where\nsuch assumptions are less valid (e.g., when answers are long-form such as\nmathematical proofs). To scale RL training to unverifiable data with\ncontemporary training constraints, we propose JEPO. JEPO applies Jensen's\nevidence lower bound, a pragmatic simplification of the evidence lower bound\nwhich views chain-of-thought as a latent variable in the generative process. We\nshow that on verifiable data (math), JEPO is as effective as RL with verifiable\nrewards; on semi-verifiable data (numina), JEPO improves on soft-match based\nevaluations compared to RL with verifiable rewards which can only leverage a\nsubset of the data source; finally, on unverifiable data (numina-proof), JEPO\noutperforms SFT and a few ablation baselines on likelihood evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose to scale RL to unverifiable data with a novel algorithm JEPO\n(Jensen's Evidence lower bound Policy Optimization). While most prior efforts\non scaling RL for LLMs focus on verifiable data where ground truth answers are\ntypically short-form and can be matched easily; we investigate the case where\nsuch assumptions are less valid (e.g., when answers are long-form such as\nmathematical proofs). To scale RL training to unverifiable data with\ncontemporary training constraints, we propose JEPO. JEPO applies Jensen's\nevidence lower bound, a pragmatic simplification of the evidence lower bound\nwhich views chain-of-thought as a latent variable in the generative process. We\nshow that on verifiable data (math), JEPO is as effective as RL with verifiable\nrewards; on semi-verifiable data (numina), JEPO improves on soft-match based\nevaluations compared to RL with verifiable rewards which can only leverage a\nsubset of the data source; finally, on unverifiable data (numina-proof), JEPO\noutperforms SFT and a few ablation baselines on likelihood evaluations."
                },
                "authors": [
                    {
                        "name": "Yunhao Tang"
                    },
                    {
                        "name": "Sid Wang"
                    },
                    {
                        "name": "Lovish Madaan"
                    },
                    {
                        "name": "Rémi Munos"
                    }
                ],
                "author_detail": {
                    "name": "Rémi Munos"
                },
                "author": "Rémi Munos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19618v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19618v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22411v1",
                "updated": "2025-05-28T14:39:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    39,
                    26,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T14:39:26Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    39,
                    26,
                    2,
                    148,
                    0
                ],
                "title": "Mitigating Overthinking in Large Reasoning Models via Manifold Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Overthinking in Large Reasoning Models via Manifold Steering"
                },
                "summary": "Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable\ncapabilities in solving complex tasks such as mathematics and coding. However,\nthese models frequently exhibit a phenomenon known as overthinking during\ninference, characterized by excessive validation loops and redundant\ndeliberation, leading to substantial computational overheads. In this paper, we\naim to mitigate overthinking by investigating the underlying mechanisms from\nthe perspective of mechanistic interpretability. We first showcase that the\ntendency of overthinking can be effectively captured by a single direction in\nthe model's activation space and the issue can be eased by intervening the\nactivations along this direction. However, this efficacy soon reaches a plateau\nand even deteriorates as the intervention strength increases. We therefore\nsystematically explore the activation space and find that the overthinking\nphenomenon is actually tied to a low-dimensional manifold, which indicates that\nthe limited effect stems from the noises introduced by the high-dimensional\nsteering direction. Based on this insight, we propose Manifold Steering, a\nnovel approach that elegantly projects the steering direction onto the\nlow-dimensional activation manifold given the theoretical approximation of the\ninterference noise. Extensive experiments on DeepSeek-R1 distilled models\nvalidate that our method reduces output tokens by up to 71% while maintaining\nand even improving the accuracy on several mathematical benchmarks. Our method\nalso exhibits robust cross-domain transferability, delivering consistent token\nreduction performance in code generation and knowledge-based QA tasks. Code is\navailable at: https://github.com/Aries-iai/Manifold_Steering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable\ncapabilities in solving complex tasks such as mathematics and coding. However,\nthese models frequently exhibit a phenomenon known as overthinking during\ninference, characterized by excessive validation loops and redundant\ndeliberation, leading to substantial computational overheads. In this paper, we\naim to mitigate overthinking by investigating the underlying mechanisms from\nthe perspective of mechanistic interpretability. We first showcase that the\ntendency of overthinking can be effectively captured by a single direction in\nthe model's activation space and the issue can be eased by intervening the\nactivations along this direction. However, this efficacy soon reaches a plateau\nand even deteriorates as the intervention strength increases. We therefore\nsystematically explore the activation space and find that the overthinking\nphenomenon is actually tied to a low-dimensional manifold, which indicates that\nthe limited effect stems from the noises introduced by the high-dimensional\nsteering direction. Based on this insight, we propose Manifold Steering, a\nnovel approach that elegantly projects the steering direction onto the\nlow-dimensional activation manifold given the theoretical approximation of the\ninterference noise. Extensive experiments on DeepSeek-R1 distilled models\nvalidate that our method reduces output tokens by up to 71% while maintaining\nand even improving the accuracy on several mathematical benchmarks. Our method\nalso exhibits robust cross-domain transferability, delivering consistent token\nreduction performance in code generation and knowledge-based QA tasks. Code is\navailable at: https://github.com/Aries-iai/Manifold_Steering."
                },
                "authors": [
                    {
                        "name": "Yao Huang"
                    },
                    {
                        "name": "Huanran Chen"
                    },
                    {
                        "name": "Shouwei Ruan"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Xingxing Wei"
                    },
                    {
                        "name": "Yinpeng Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yinpeng Dong"
                },
                "author": "Yinpeng Dong",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22407v1",
                "updated": "2025-05-28T14:37:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    37,
                    21,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T14:37:21Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    37,
                    21,
                    2,
                    148,
                    0
                ],
                "title": "Self-Reflective Reinforcement Learning for Diffusion-based Image\n  Reasoning Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Reflective Reinforcement Learning for Diffusion-based Image\n  Reasoning Generation"
                },
                "summary": "Diffusion models have recently demonstrated exceptional performance in image\ngeneration task. However, existing image generation methods still significantly\nsuffer from the dilemma of image reasoning, especially in logic-centered image\ngeneration tasks. Inspired by the success of Chain of Thought (CoT) and\nReinforcement Learning (RL) in LLMs, we propose SRRL, a self-reflective RL\nalgorithm for diffusion models to achieve reasoning generation of logical\nimages by performing reflection and iteration across generation trajectories.\nThe intermediate samples in the denoising process carry noise, making accurate\nreward evaluation difficult. To address this challenge, SRRL treats the entire\ndenoising trajectory as a CoT step with multi-round reflective denoising\nprocess and introduces condition guided forward process, which allows for\nreflective iteration between CoT steps. Through SRRL-based iterative diffusion\ntraining, we introduce image reasoning through CoT into generation tasks\nadhering to physical laws and unconventional physical phenomena for the first\ntime. Notably, experimental results of case study exhibit that the superior\nperformance of our SRRL algorithm even compared with GPT-4o. The project page\nis https://jadenpan0.github.io/srrl.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have recently demonstrated exceptional performance in image\ngeneration task. However, existing image generation methods still significantly\nsuffer from the dilemma of image reasoning, especially in logic-centered image\ngeneration tasks. Inspired by the success of Chain of Thought (CoT) and\nReinforcement Learning (RL) in LLMs, we propose SRRL, a self-reflective RL\nalgorithm for diffusion models to achieve reasoning generation of logical\nimages by performing reflection and iteration across generation trajectories.\nThe intermediate samples in the denoising process carry noise, making accurate\nreward evaluation difficult. To address this challenge, SRRL treats the entire\ndenoising trajectory as a CoT step with multi-round reflective denoising\nprocess and introduces condition guided forward process, which allows for\nreflective iteration between CoT steps. Through SRRL-based iterative diffusion\ntraining, we introduce image reasoning through CoT into generation tasks\nadhering to physical laws and unconventional physical phenomena for the first\ntime. Notably, experimental results of case study exhibit that the superior\nperformance of our SRRL algorithm even compared with GPT-4o. The project page\nis https://jadenpan0.github.io/srrl.github.io/."
                },
                "authors": [
                    {
                        "name": "Jiadong Pan"
                    },
                    {
                        "name": "Zhiyuan Ma"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19623v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19623v2",
                "updated": "2025-05-28T14:32:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    32,
                    56,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-26T07:45:11Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    45,
                    11,
                    0,
                    146,
                    0
                ],
                "title": "AgentRecBench: Benchmarking LLM Agent-based Personalized Recommender\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentRecBench: Benchmarking LLM Agent-based Personalized Recommender\n  Systems"
                },
                "summary": "The emergence of agentic recommender systems powered by Large Language Models\n(LLMs) represents a paradigm shift in personalized recommendations, leveraging\nLLMs' advanced reasoning and role-playing capabilities to enable autonomous,\nadaptive decision-making. Unlike traditional recommendation approaches, agentic\nrecommender systems can dynamically gather and interpret user-item interactions\nfrom complex environments, generating robust recommendation strategies that\ngeneralize across diverse scenarios. However, the field currently lacks\nstandardized evaluation protocols to systematically assess these methods. To\naddress this critical gap, we propose: (1) an interactive textual\nrecommendation simulator incorporating rich user and item metadata and three\ntypical evaluation scenarios (classic, evolving-interest, and cold-start\nrecommendation tasks); (2) a unified modular framework for developing and\nstudying agentic recommender systems; and (3) the first comprehensive benchmark\ncomparing 10 classical and agentic recommendation methods. Our findings\ndemonstrate the superiority of agentic systems and establish actionable design\nguidelines for their core components. The benchmark environment has been\nrigorously validated through an open challenge and remains publicly available\nwith a continuously maintained\nleaderboard~\\footnote[2]{https://tsinghua-fib-lab.github.io/AgentSocietyChallenge/pages/overview.html},\nfostering ongoing community engagement and reproducible research. The benchmark\nis available at:\n\\hyperlink{https://huggingface.co/datasets/SGJQovo/AgentRecBench}{https://huggingface.co/datasets/SGJQovo/AgentRecBench}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of agentic recommender systems powered by Large Language Models\n(LLMs) represents a paradigm shift in personalized recommendations, leveraging\nLLMs' advanced reasoning and role-playing capabilities to enable autonomous,\nadaptive decision-making. Unlike traditional recommendation approaches, agentic\nrecommender systems can dynamically gather and interpret user-item interactions\nfrom complex environments, generating robust recommendation strategies that\ngeneralize across diverse scenarios. However, the field currently lacks\nstandardized evaluation protocols to systematically assess these methods. To\naddress this critical gap, we propose: (1) an interactive textual\nrecommendation simulator incorporating rich user and item metadata and three\ntypical evaluation scenarios (classic, evolving-interest, and cold-start\nrecommendation tasks); (2) a unified modular framework for developing and\nstudying agentic recommender systems; and (3) the first comprehensive benchmark\ncomparing 10 classical and agentic recommendation methods. Our findings\ndemonstrate the superiority of agentic systems and establish actionable design\nguidelines for their core components. The benchmark environment has been\nrigorously validated through an open challenge and remains publicly available\nwith a continuously maintained\nleaderboard~\\footnote[2]{https://tsinghua-fib-lab.github.io/AgentSocietyChallenge/pages/overview.html},\nfostering ongoing community engagement and reproducible research. The benchmark\nis available at:\n\\hyperlink{https://huggingface.co/datasets/SGJQovo/AgentRecBench}{https://huggingface.co/datasets/SGJQovo/AgentRecBench}."
                },
                "authors": [
                    {
                        "name": "Yu Shang"
                    },
                    {
                        "name": "Peijie Liu"
                    },
                    {
                        "name": "Yuwei Yan"
                    },
                    {
                        "name": "Zijing Wu"
                    },
                    {
                        "name": "Leheng Sheng"
                    },
                    {
                        "name": "Yuanqing Yu"
                    },
                    {
                        "name": "Chumeng Jiang"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19623v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19623v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05242v2",
                "updated": "2025-05-28T14:27:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    27,
                    44,
                    2,
                    148,
                    0
                ],
                "published": "2025-02-07T13:25:33Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    25,
                    33,
                    4,
                    38,
                    0
                ],
                "title": "Beyond External Monitors: Enhancing Transparency of Large Language\n  Models for Easier Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond External Monitors: Enhancing Transparency of Large Language\n  Models for Easier Monitoring"
                },
                "summary": "Large language models (LLMs) are becoming increasingly capable, but the\nmechanisms of their thinking and decision-making process remain unclear.\nChain-of-thoughts (CoTs) have been commonly utilized to monitor LLMs, but this\nstrategy fails to accurately reflect LLMs' thinking process. Techniques based\non LLMs' hidden representations provide an inner perspective to monitor their\nlatent thinking. However, previous methods only try to develop external\nmonitors instead of making LLMs themselves easier to monitor. In this paper, we\npropose a novel method TELLME, improving the transparency of LLMs and helping\nmonitors identify unsuitable and sensitive behaviors. Furthermore, we showcase\nthe applications of TELLME on trustworthiness tasks (\\eg, safety risks\nmonitoring tasks and detoxification tasks), where LLMs achieve consistent\nimprovement in transparency and task performance. More crucially, we\ntheoretically analyze the improvement of TELLME on LLMs' generalization ability\nthrough optimal transport theory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are becoming increasingly capable, but the\nmechanisms of their thinking and decision-making process remain unclear.\nChain-of-thoughts (CoTs) have been commonly utilized to monitor LLMs, but this\nstrategy fails to accurately reflect LLMs' thinking process. Techniques based\non LLMs' hidden representations provide an inner perspective to monitor their\nlatent thinking. However, previous methods only try to develop external\nmonitors instead of making LLMs themselves easier to monitor. In this paper, we\npropose a novel method TELLME, improving the transparency of LLMs and helping\nmonitors identify unsuitable and sensitive behaviors. Furthermore, we showcase\nthe applications of TELLME on trustworthiness tasks (\\eg, safety risks\nmonitoring tasks and detoxification tasks), where LLMs achieve consistent\nimprovement in transparency and task performance. More crucially, we\ntheoretically analyze the improvement of TELLME on LLMs' generalization ability\nthrough optimal transport theory."
                },
                "authors": [
                    {
                        "name": "Guanxu Chen"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Tao Luo"
                    },
                    {
                        "name": "Lijie Hu"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "arxiv_comment": "25 pages,6 figures,13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12339v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12339v2",
                "updated": "2025-05-28T14:24:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    24,
                    12,
                    2,
                    148,
                    0
                ],
                "published": "2025-04-15T01:44:56Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    1,
                    44,
                    56,
                    1,
                    105,
                    0
                ],
                "title": "GOAT-TTS: Expressive and Realistic Speech Generation via A Dual-Branch\n  LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GOAT-TTS: Expressive and Realistic Speech Generation via A Dual-Branch\n  LLM"
                },
                "summary": "While large language models (LLMs) have revolutionized text-to-speech (TTS)\nsynthesis through discrete tokenization paradigms, current architectures\nexhibit fundamental tensions between three critical dimensions: 1) irreversible\nloss of acoustic characteristics caused by quantization of speech prompts; 2)\nstringent dependence on precisely aligned prompt speech-text pairs that limit\nreal-world deployment; and 3) catastrophic forgetting of the LLM's native text\ncomprehension during optimization for speech token generation. To address these\nchallenges, we propose an LLM-based text-to-speech Generation approach\nOptimized via a novel dual-branch ArchiTecture (GOAT-TTS). Our framework\nintroduces two key innovations: (1) The modality-alignment branch combines a\nspeech encoder and projector to capture continuous acoustic embeddings,\nenabling bidirectional correlation between paralinguistic features (language,\ntimbre, emotion) and semantic text representations without transcript\ndependency; (2) The speech-generation branch employs modular fine-tuning on\ntop-k layers of an LLM for speech token prediction while freezing the bottom-n\nlayers to preserve foundational linguistic knowledge. Moreover, multi-token\nprediction is introduced to support real-time streaming TTS synthesis.\nExperimental results demonstrate that our GOAT-TTS achieves performance\ncomparable to state-of-the-art TTS models while validating the efficacy of\nsynthesized dialect speech data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have revolutionized text-to-speech (TTS)\nsynthesis through discrete tokenization paradigms, current architectures\nexhibit fundamental tensions between three critical dimensions: 1) irreversible\nloss of acoustic characteristics caused by quantization of speech prompts; 2)\nstringent dependence on precisely aligned prompt speech-text pairs that limit\nreal-world deployment; and 3) catastrophic forgetting of the LLM's native text\ncomprehension during optimization for speech token generation. To address these\nchallenges, we propose an LLM-based text-to-speech Generation approach\nOptimized via a novel dual-branch ArchiTecture (GOAT-TTS). Our framework\nintroduces two key innovations: (1) The modality-alignment branch combines a\nspeech encoder and projector to capture continuous acoustic embeddings,\nenabling bidirectional correlation between paralinguistic features (language,\ntimbre, emotion) and semantic text representations without transcript\ndependency; (2) The speech-generation branch employs modular fine-tuning on\ntop-k layers of an LLM for speech token prediction while freezing the bottom-n\nlayers to preserve foundational linguistic knowledge. Moreover, multi-token\nprediction is introduced to support real-time streaming TTS synthesis.\nExperimental results demonstrate that our GOAT-TTS achieves performance\ncomparable to state-of-the-art TTS models while validating the efficacy of\nsynthesized dialect speech data."
                },
                "authors": [
                    {
                        "name": "Yaodong Song"
                    },
                    {
                        "name": "Hongjie Chen"
                    },
                    {
                        "name": "Jie Lian"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Guangmin Xia"
                    },
                    {
                        "name": "Zehan Li"
                    },
                    {
                        "name": "Genliang Zhao"
                    },
                    {
                        "name": "Jian Kang"
                    },
                    {
                        "name": "Jie Li"
                    },
                    {
                        "name": "Yongxiang Li"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12339v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12339v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22394v1",
                "updated": "2025-05-28T14:23:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    23,
                    30,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T14:23:30Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    23,
                    30,
                    2,
                    148,
                    0
                ],
                "title": "PacTure: Efficient PBR Texture Generation on Packed Views with Visual\n  Autoregressive Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PacTure: Efficient PBR Texture Generation on Packed Views with Visual\n  Autoregressive Models"
                },
                "summary": "We present PacTure, a novel framework for generating physically-based\nrendering (PBR) material textures from an untextured 3D mesh, a text\ndescription, and an optional image prompt. Early 2D generation-based texturing\napproaches generate textures sequentially from different views, resulting in\nlong inference times and globally inconsistent textures. More recent approaches\nadopt multi-view generation with cross-view attention to enhance global\nconsistency, which, however, limits the resolution for each view. In response\nto these weaknesses, we first introduce view packing, a novel technique that\nsignificantly increases the effective resolution for each view during\nmulti-view generation without imposing additional inference cost, by\nformulating the arrangement of multi-view maps as a 2D rectangle bin packing\nproblem. In contrast to UV mapping, it preserves the spatial proximity\nessential for image generation and maintains full compatibility with current 2D\ngenerative models. To further reduce the inference cost, we enable fine-grained\ncontrol and multi-domain generation within the next-scale prediction\nautoregressive framework to create an efficient multi-view multi-domain\ngenerative backbone. Extensive experiments show that PacTure outperforms\nstate-of-the-art methods in both quality of generated PBR textures and\nefficiency in training and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present PacTure, a novel framework for generating physically-based\nrendering (PBR) material textures from an untextured 3D mesh, a text\ndescription, and an optional image prompt. Early 2D generation-based texturing\napproaches generate textures sequentially from different views, resulting in\nlong inference times and globally inconsistent textures. More recent approaches\nadopt multi-view generation with cross-view attention to enhance global\nconsistency, which, however, limits the resolution for each view. In response\nto these weaknesses, we first introduce view packing, a novel technique that\nsignificantly increases the effective resolution for each view during\nmulti-view generation without imposing additional inference cost, by\nformulating the arrangement of multi-view maps as a 2D rectangle bin packing\nproblem. In contrast to UV mapping, it preserves the spatial proximity\nessential for image generation and maintains full compatibility with current 2D\ngenerative models. To further reduce the inference cost, we enable fine-grained\ncontrol and multi-domain generation within the next-scale prediction\nautoregressive framework to create an efficient multi-view multi-domain\ngenerative backbone. Extensive experiments show that PacTure outperforms\nstate-of-the-art methods in both quality of generated PBR textures and\nefficiency in training and inference."
                },
                "authors": [
                    {
                        "name": "Fan Fei"
                    },
                    {
                        "name": "Jiajun Tang"
                    },
                    {
                        "name": "Fei-Peng Tian"
                    },
                    {
                        "name": "Boxin Shi"
                    },
                    {
                        "name": "Ping Tan"
                    }
                ],
                "author_detail": {
                    "name": "Ping Tan"
                },
                "author": "Ping Tan",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19385v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19385v4",
                "updated": "2025-05-28T14:20:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    20,
                    39,
                    2,
                    148,
                    0
                ],
                "published": "2025-03-25T06:30:45Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    30,
                    45,
                    1,
                    84,
                    0
                ],
                "title": "Inference-Time Scaling for Flow Models via Stochastic Generation and\n  Rollover Budget Forcing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Scaling for Flow Models via Stochastic Generation and\n  Rollover Budget Forcing"
                },
                "summary": "We propose an inference-time scaling approach for pretrained flow models.\nRecently, inference-time scaling has gained significant attention in LLMs and\ndiffusion models, improving sample quality or better aligning outputs with user\npreferences by leveraging additional computation. For diffusion models,\nparticle sampling has allowed more efficient scaling due to the stochasticity\nat intermediate denoising steps. On the contrary, while flow models have gained\npopularity as an alternative to diffusion models--offering faster generation\nand high-quality outputs in state-of-the-art image and video generative\nmodels--efficient inference-time scaling methods used for diffusion models\ncannot be directly applied due to their deterministic generative process. To\nenable efficient inference-time scaling for flow models, we propose three key\nideas: 1) SDE-based generation, enabling particle sampling in flow models, 2)\nInterpolant conversion, broadening the search space and enhancing sample\ndiversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of\ncomputational resources across timesteps to maximize budget utilization. Our\nexperiments show that SDE-based generation, particularly variance-preserving\n(VP) interpolant-based generation, improves the performance of particle\nsampling methods for inference-time scaling in flow models. Additionally, we\ndemonstrate that RBF with VP-SDE achieves the best performance, outperforming\nall previous inference-time scaling approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an inference-time scaling approach for pretrained flow models.\nRecently, inference-time scaling has gained significant attention in LLMs and\ndiffusion models, improving sample quality or better aligning outputs with user\npreferences by leveraging additional computation. For diffusion models,\nparticle sampling has allowed more efficient scaling due to the stochasticity\nat intermediate denoising steps. On the contrary, while flow models have gained\npopularity as an alternative to diffusion models--offering faster generation\nand high-quality outputs in state-of-the-art image and video generative\nmodels--efficient inference-time scaling methods used for diffusion models\ncannot be directly applied due to their deterministic generative process. To\nenable efficient inference-time scaling for flow models, we propose three key\nideas: 1) SDE-based generation, enabling particle sampling in flow models, 2)\nInterpolant conversion, broadening the search space and enhancing sample\ndiversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of\ncomputational resources across timesteps to maximize budget utilization. Our\nexperiments show that SDE-based generation, particularly variance-preserving\n(VP) interpolant-based generation, improves the performance of particle\nsampling methods for inference-time scaling in flow models. Additionally, we\ndemonstrate that RBF with VP-SDE achieves the best performance, outperforming\nall previous inference-time scaling approaches."
                },
                "authors": [
                    {
                        "name": "Jaihoon Kim"
                    },
                    {
                        "name": "Taehoon Yoon"
                    },
                    {
                        "name": "Jisung Hwang"
                    },
                    {
                        "name": "Minhyuk Sung"
                    }
                ],
                "author_detail": {
                    "name": "Minhyuk Sung"
                },
                "author": "Minhyuk Sung",
                "arxiv_comment": "Project page: https://flow-inference-time-scaling.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19385v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19385v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17999v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17999v2",
                "updated": "2025-05-28T14:16:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    16,
                    45,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-23T15:04:16Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    15,
                    4,
                    16,
                    4,
                    143,
                    0
                ],
                "title": "Revisiting Feature Interactions from the Perspective of Quadratic Neural\n  Networks for Click-through Rate Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Feature Interactions from the Perspective of Quadratic Neural\n  Networks for Click-through Rate Prediction"
                },
                "summary": "Hadamard Product (HP) has long been a cornerstone in click-through rate (CTR)\nprediction tasks due to its simplicity, effectiveness, and ability to capture\nfeature interactions without additional parameters. However, the underlying\nreasons for its effectiveness remain unclear. In this paper, we revisit HP from\nthe perspective of Quadratic Neural Networks (QNN), which leverage quadratic\ninteraction terms to model complex feature relationships. We further reveal\nQNN's ability to expand the feature space and provide smooth nonlinear\napproximations without relying on activation functions. Meanwhile, we find that\ntraditional post-activation does not further improve the performance of the\nQNN. Instead, mid-activation is a more suitable alternative. Through\ntheoretical analysis and empirical evaluation of 25 QNN neuron formats, we\nidentify a good-performing variant and make further enhancements on it.\nSpecifically, we propose the Multi-Head Khatri-Rao Product as a superior\nalternative to HP and a Self-Ensemble Loss with dynamic ensemble capability\nwithin the same network to enhance computational efficiency and performance.\nUltimately, we propose a novel neuron format, QNN-alpha, which is tailored for\nCTR prediction tasks. Experimental results show that QNN-alpha achieves new\nstate-of-the-art performance on six public datasets while maintaining low\ninference latency, good scalability, and excellent compatibility. The code,\nrunning logs, and detailed hyperparameter configurations are available at:\nhttps://github.com/salmon1802/QNN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hadamard Product (HP) has long been a cornerstone in click-through rate (CTR)\nprediction tasks due to its simplicity, effectiveness, and ability to capture\nfeature interactions without additional parameters. However, the underlying\nreasons for its effectiveness remain unclear. In this paper, we revisit HP from\nthe perspective of Quadratic Neural Networks (QNN), which leverage quadratic\ninteraction terms to model complex feature relationships. We further reveal\nQNN's ability to expand the feature space and provide smooth nonlinear\napproximations without relying on activation functions. Meanwhile, we find that\ntraditional post-activation does not further improve the performance of the\nQNN. Instead, mid-activation is a more suitable alternative. Through\ntheoretical analysis and empirical evaluation of 25 QNN neuron formats, we\nidentify a good-performing variant and make further enhancements on it.\nSpecifically, we propose the Multi-Head Khatri-Rao Product as a superior\nalternative to HP and a Self-Ensemble Loss with dynamic ensemble capability\nwithin the same network to enhance computational efficiency and performance.\nUltimately, we propose a novel neuron format, QNN-alpha, which is tailored for\nCTR prediction tasks. Experimental results show that QNN-alpha achieves new\nstate-of-the-art performance on six public datasets while maintaining low\ninference latency, good scalability, and excellent compatibility. The code,\nrunning logs, and detailed hyperparameter configurations are available at:\nhttps://github.com/salmon1802/QNN."
                },
                "authors": [
                    {
                        "name": "Honghao Li"
                    },
                    {
                        "name": "Yiwen Zhang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Lei Sang"
                    },
                    {
                        "name": "Jieming Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jieming Zhu"
                },
                "author": "Jieming Zhu",
                "arxiv_comment": "KDD'25 accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17999v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17999v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21277v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21277v2",
                "updated": "2025-05-28T14:16:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    16,
                    10,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-27T14:48:44Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    48,
                    44,
                    1,
                    147,
                    0
                ],
                "title": "Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks\n  through Expanding Strategy Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks\n  through Expanding Strategy Space"
                },
                "summary": "Large Language Models (LLMs), despite advanced general capabilities, still\nsuffer from numerous safety risks, especially jailbreak attacks that bypass\nsafety protocols. Understanding these vulnerabilities through black-box\njailbreak attacks, which better reflect real-world scenarios, offers critical\ninsights into model robustness. While existing methods have shown improvements\nthrough various prompt engineering techniques, their success remains limited\nagainst safety-aligned models, overlooking a more fundamental problem: the\neffectiveness is inherently bounded by the predefined strategy spaces. However,\nexpanding this space presents significant challenges in both systematically\ncapturing essential attack patterns and efficiently navigating the increased\ncomplexity. To better explore the potential of expanding the strategy space, we\naddress these challenges through a novel framework that decomposes jailbreak\nstrategies into essential components based on the Elaboration Likelihood Model\n(ELM) theory and develops genetic-based optimization with intention evaluation\nmechanisms. To be striking, our experiments reveal unprecedented jailbreak\ncapabilities by expanding the strategy space: we achieve over 90% success rate\non Claude-3.5 where prior methods completely fail, while demonstrating strong\ncross-model transferability and surpassing specialized safeguard models in\nevaluation accuracy. The code is open-sourced at:\nhttps://github.com/Aries-iai/CL-GSO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), despite advanced general capabilities, still\nsuffer from numerous safety risks, especially jailbreak attacks that bypass\nsafety protocols. Understanding these vulnerabilities through black-box\njailbreak attacks, which better reflect real-world scenarios, offers critical\ninsights into model robustness. While existing methods have shown improvements\nthrough various prompt engineering techniques, their success remains limited\nagainst safety-aligned models, overlooking a more fundamental problem: the\neffectiveness is inherently bounded by the predefined strategy spaces. However,\nexpanding this space presents significant challenges in both systematically\ncapturing essential attack patterns and efficiently navigating the increased\ncomplexity. To better explore the potential of expanding the strategy space, we\naddress these challenges through a novel framework that decomposes jailbreak\nstrategies into essential components based on the Elaboration Likelihood Model\n(ELM) theory and develops genetic-based optimization with intention evaluation\nmechanisms. To be striking, our experiments reveal unprecedented jailbreak\ncapabilities by expanding the strategy space: we achieve over 90% success rate\non Claude-3.5 where prior methods completely fail, while demonstrating strong\ncross-model transferability and surpassing specialized safeguard models in\nevaluation accuracy. The code is open-sourced at:\nhttps://github.com/Aries-iai/CL-GSO."
                },
                "authors": [
                    {
                        "name": "Yao Huang"
                    },
                    {
                        "name": "Yitong Sun"
                    },
                    {
                        "name": "Shouwei Ruan"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Xingxing Wei"
                    }
                ],
                "author_detail": {
                    "name": "Xingxing Wei"
                },
                "author": "Xingxing Wei",
                "arxiv_comment": "19 pages, 20 figures, accepted by ACL 2025, Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21277v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21277v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22389v1",
                "updated": "2025-05-28T14:14:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    14,
                    19,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T14:14:19Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    14,
                    19,
                    2,
                    148,
                    0
                ],
                "title": "Train with Perturbation, Infer after Merging: A Two-Stage Framework for\n  Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Train with Perturbation, Infer after Merging: A Two-Stage Framework for\n  Continual Learning"
                },
                "summary": "Continual Learning (CL) aims to enable models to continuously acquire new\nknowledge from a sequence of tasks with avoiding the forgetting of learned\ninformation. However, existing CL methods only rely on the parameters of the\nmost recent task for inference, which makes them susceptible to catastrophic\nforgetting. Inspired by the recent success of model merging techniques, we\npropose \\textbf{Perturb-and-Merge (P\\&M)}, a novel continual learning framework\nthat integrates model merging into the CL paradigm to mitigate forgetting.\nSpecifically, after training on each task, P\\&M constructs a new model by\nforming a convex combination of the previous model and the newly trained\ntask-specific model. Through theoretical analysis, we minimize the total loss\nincrease across all tasks and derive an analytical solution for the optimal\nmerging coefficient. To further improve the performance of the merged model, we\nobserve that the degradation introduced during merging can be alleviated by a\nregularization term composed of the task vector and the Hessian matrix of the\nloss function. Interestingly, we show that this term can be efficiently\napproximated using second-order symmetric finite differences, and a stochastic\nperturbation strategy along the task vector direction is accordingly devised\nwhich incurs no additional forward or backward passes while providing an\neffective approximation of the regularization term. Finally, we combine P\\&M\nwith LoRA, a parameter-efficient fine-tuning method, to reduce memory overhead.\nOur proposed approach achieves state-of-the-art performance on several\ncontinual learning benchmark datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Learning (CL) aims to enable models to continuously acquire new\nknowledge from a sequence of tasks with avoiding the forgetting of learned\ninformation. However, existing CL methods only rely on the parameters of the\nmost recent task for inference, which makes them susceptible to catastrophic\nforgetting. Inspired by the recent success of model merging techniques, we\npropose \\textbf{Perturb-and-Merge (P\\&M)}, a novel continual learning framework\nthat integrates model merging into the CL paradigm to mitigate forgetting.\nSpecifically, after training on each task, P\\&M constructs a new model by\nforming a convex combination of the previous model and the newly trained\ntask-specific model. Through theoretical analysis, we minimize the total loss\nincrease across all tasks and derive an analytical solution for the optimal\nmerging coefficient. To further improve the performance of the merged model, we\nobserve that the degradation introduced during merging can be alleviated by a\nregularization term composed of the task vector and the Hessian matrix of the\nloss function. Interestingly, we show that this term can be efficiently\napproximated using second-order symmetric finite differences, and a stochastic\nperturbation strategy along the task vector direction is accordingly devised\nwhich incurs no additional forward or backward passes while providing an\neffective approximation of the regularization term. Finally, we combine P\\&M\nwith LoRA, a parameter-efficient fine-tuning method, to reduce memory overhead.\nOur proposed approach achieves state-of-the-art performance on several\ncontinual learning benchmark datasets."
                },
                "authors": [
                    {
                        "name": "Haomiao Qiu"
                    },
                    {
                        "name": "Miao Zhang"
                    },
                    {
                        "name": "Ziyue Qiao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "17 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22388v1",
                "updated": "2025-05-28T14:14:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    14,
                    14,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T14:14:14Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    14,
                    14,
                    2,
                    148,
                    0
                ],
                "title": "A Synthetic Business Cycle Approach to Counterfactual Analysis with\n  Nonstationary Macroeconomic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Synthetic Business Cycle Approach to Counterfactual Analysis with\n  Nonstationary Macroeconomic Data"
                },
                "summary": "This paper investigates the use of synthetic control methods for causal\ninference in macroeconomic settings when dealing with possibly nonstationary\ndata. While the synthetic control approach has gained popularity for estimating\ncounterfactual outcomes, we caution researchers against assuming a common\nnonstationary trend factor across units for macroeconomic outcomes, as doing so\nmay result in misleading causal estimation-a pitfall we refer to as the\nspurious synthetic control problem. To address this issue, we propose a\nsynthetic business cycle framework that explicitly separates trend and cyclical\ncomponents. By leveraging the treated unit's historical data to forecast its\ntrend and using control units only for cyclical fluctuations, our\ndivide-and-conquer strategy eliminates spurious correlations and improves the\nrobustness of counterfactual prediction in macroeconomic applications. As\nempirical illustrations, we examine the cases of German reunification and the\nhandover of Hong Kong, demonstrating the advantages of the proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the use of synthetic control methods for causal\ninference in macroeconomic settings when dealing with possibly nonstationary\ndata. While the synthetic control approach has gained popularity for estimating\ncounterfactual outcomes, we caution researchers against assuming a common\nnonstationary trend factor across units for macroeconomic outcomes, as doing so\nmay result in misleading causal estimation-a pitfall we refer to as the\nspurious synthetic control problem. To address this issue, we propose a\nsynthetic business cycle framework that explicitly separates trend and cyclical\ncomponents. By leveraging the treated unit's historical data to forecast its\ntrend and using control units only for cyclical fluctuations, our\ndivide-and-conquer strategy eliminates spurious correlations and improves the\nrobustness of counterfactual prediction in macroeconomic applications. As\nempirical illustrations, we examine the cases of German reunification and the\nhandover of Hong Kong, demonstrating the advantages of the proposed approach."
                },
                "authors": [
                    {
                        "name": "Zhentao Shi"
                    },
                    {
                        "name": "Jin Xi"
                    },
                    {
                        "name": "Haitian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Haitian Xie"
                },
                "author": "Haitian Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22375v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22375v2",
                "updated": "2025-05-29T01:59:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    1,
                    59,
                    0,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-28T14:03:02Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    3,
                    2,
                    2,
                    148,
                    0
                ],
                "title": "Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition"
                },
                "summary": "This work presents Pangu Embedded, an efficient Large Language Model (LLM)\nreasoner developed on Ascend Neural Processing Units (NPUs), featuring flexible\nfast and slow thinking capabilities. Pangu Embedded addresses the significant\ncomputational costs and inference latency challenges prevalent in existing\nreasoning-optimized LLMs. We propose a two-stage training framework for its\nconstruction. In Stage 1, the model is finetuned via an iterative distillation\nprocess, incorporating inter-iteration model merging to effectively aggregate\ncomplementary knowledge. This is followed by reinforcement learning on Ascend\nclusters, optimized by a latency-tolerant scheduler that combines stale\nsynchronous parallelism with prioritized data queues. The RL process is guided\nby a Multi-source Adaptive Reward System (MARS), which generates dynamic,\ntask-specific reward signals using deterministic metrics and lightweight LLM\nevaluators for mathematics, coding, and general problem-solving tasks. Stage 2\nintroduces a dual-system framework, endowing Pangu Embedded with a \"fast\" mode\nfor routine queries and a deeper \"slow\" mode for complex inference. This\nframework offers both manual mode switching for user control and an automatic,\ncomplexity-aware mode selection mechanism that dynamically allocates\ncomputational resources to balance latency and reasoning depth. Experimental\nresults on benchmarks including AIME 2024, GPQA, and LiveCodeBench demonstrate\nthat Pangu Embedded with 7B parameters, outperforms similar-size models like\nQwen3-8B and GLM4-9B. It delivers rapid responses and state-of-the-art\nreasoning quality within a single, unified model architecture, highlighting a\npromising direction for developing powerful yet practically deployable LLM\nreasoners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents Pangu Embedded, an efficient Large Language Model (LLM)\nreasoner developed on Ascend Neural Processing Units (NPUs), featuring flexible\nfast and slow thinking capabilities. Pangu Embedded addresses the significant\ncomputational costs and inference latency challenges prevalent in existing\nreasoning-optimized LLMs. We propose a two-stage training framework for its\nconstruction. In Stage 1, the model is finetuned via an iterative distillation\nprocess, incorporating inter-iteration model merging to effectively aggregate\ncomplementary knowledge. This is followed by reinforcement learning on Ascend\nclusters, optimized by a latency-tolerant scheduler that combines stale\nsynchronous parallelism with prioritized data queues. The RL process is guided\nby a Multi-source Adaptive Reward System (MARS), which generates dynamic,\ntask-specific reward signals using deterministic metrics and lightweight LLM\nevaluators for mathematics, coding, and general problem-solving tasks. Stage 2\nintroduces a dual-system framework, endowing Pangu Embedded with a \"fast\" mode\nfor routine queries and a deeper \"slow\" mode for complex inference. This\nframework offers both manual mode switching for user control and an automatic,\ncomplexity-aware mode selection mechanism that dynamically allocates\ncomputational resources to balance latency and reasoning depth. Experimental\nresults on benchmarks including AIME 2024, GPQA, and LiveCodeBench demonstrate\nthat Pangu Embedded with 7B parameters, outperforms similar-size models like\nQwen3-8B and GLM4-9B. It delivers rapid responses and state-of-the-art\nreasoning quality within a single, unified model architecture, highlighting a\npromising direction for developing powerful yet practically deployable LLM\nreasoners."
                },
                "authors": [
                    {
                        "name": "Hanting Chen"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Zhenni Bi"
                    },
                    {
                        "name": "Jinpeng Li"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Fei Mi"
                    },
                    {
                        "name": "Mingjian Zhu"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Kaikai Song"
                    },
                    {
                        "name": "Yifei Fu"
                    },
                    {
                        "name": "Xu He"
                    },
                    {
                        "name": "Yu Luo"
                    },
                    {
                        "name": "Chong Zhu"
                    },
                    {
                        "name": "Quan He"
                    },
                    {
                        "name": "Xueyu Wu"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Hailin Hu"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Dacheng Tao"
                    },
                    {
                        "name": "Xinghao Chen"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "arxiv_affiliation": "and Other Contributors",
                "author": "Yunhe Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22375v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08820v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08820v3",
                "updated": "2025-05-28T13:59:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    59,
                    23,
                    2,
                    148,
                    0
                ],
                "published": "2024-10-11T14:02:42Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    2,
                    42,
                    4,
                    285,
                    0
                ],
                "title": "Which Demographics do LLMs Default to During Annotation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Demographics do LLMs Default to During Annotation?"
                },
                "summary": "Demographics and cultural background of annotators influence the labels they\nassign in text annotation -- for instance, an elderly woman might find it\noffensive to read a message addressed to a \"bro\", but a male teenager might\nfind it appropriate. It is therefore important to acknowledge label variations\nto not under-represent members of a society. Two research directions developed\nout of this observation in the context of using large language models (LLM) for\ndata annotations, namely (1) studying biases and inherent knowledge of LLMs and\n(2) injecting diversity in the output by manipulating the prompt with\ndemographic information. We combine these two strands of research and ask the\nquestion to which demographics an LLM resorts to when no demographics is given.\nTo answer this question, we evaluate which attributes of human annotators LLMs\ninherently mimic. Furthermore, we compare non-demographic conditioned prompts\nand placebo-conditioned prompts (e.g., \"you are an annotator who lives in house\nnumber 5\") to demographics-conditioned prompts (\"You are a 45 year old man and\nan expert on politeness annotation. How do you rate {instance}\"). We study\nthese questions for politeness and offensiveness annotations on the POPQUORN\ndata set, a corpus created in a controlled manner to investigate human label\nvariations based on demographics which has not been used for LLM-based analyses\nso far. We observe notable influences related to gender, race, and age in\ndemographic prompting, which contrasts with previous studies that found no such\neffects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demographics and cultural background of annotators influence the labels they\nassign in text annotation -- for instance, an elderly woman might find it\noffensive to read a message addressed to a \"bro\", but a male teenager might\nfind it appropriate. It is therefore important to acknowledge label variations\nto not under-represent members of a society. Two research directions developed\nout of this observation in the context of using large language models (LLM) for\ndata annotations, namely (1) studying biases and inherent knowledge of LLMs and\n(2) injecting diversity in the output by manipulating the prompt with\ndemographic information. We combine these two strands of research and ask the\nquestion to which demographics an LLM resorts to when no demographics is given.\nTo answer this question, we evaluate which attributes of human annotators LLMs\ninherently mimic. Furthermore, we compare non-demographic conditioned prompts\nand placebo-conditioned prompts (e.g., \"you are an annotator who lives in house\nnumber 5\") to demographics-conditioned prompts (\"You are a 45 year old man and\nan expert on politeness annotation. How do you rate {instance}\"). We study\nthese questions for politeness and offensiveness annotations on the POPQUORN\ndata set, a corpus created in a controlled manner to investigate human label\nvariations based on demographics which has not been used for LLM-based analyses\nso far. We observe notable influences related to gender, race, and age in\ndemographic prompting, which contrasts with previous studies that found no such\neffects."
                },
                "authors": [
                    {
                        "name": "Johannes Schäfer"
                    },
                    {
                        "name": "Aidan Combs"
                    },
                    {
                        "name": "Christopher Bagdon"
                    },
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Nadine Probol"
                    },
                    {
                        "name": "Lynn Greschner"
                    },
                    {
                        "name": "Sean Papay"
                    },
                    {
                        "name": "Yarik Menchaca Resendiz"
                    },
                    {
                        "name": "Aswathy Velutharambath"
                    },
                    {
                        "name": "Amelie Wührl"
                    },
                    {
                        "name": "Sabine Weber"
                    },
                    {
                        "name": "Roman Klinger"
                    }
                ],
                "author_detail": {
                    "name": "Roman Klinger"
                },
                "author": "Roman Klinger",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08820v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08820v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22371v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22371v2",
                "updated": "2025-05-29T07:22:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    7,
                    22,
                    57,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-28T13:58:20Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    58,
                    20,
                    2,
                    148,
                    0
                ],
                "title": "Adaptive tail index estimation: minimal assumptions and non-asymptotic\n  guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive tail index estimation: minimal assumptions and non-asymptotic\n  guarantees"
                },
                "summary": "A notoriously difficult challenge in extreme value theory is the choice of\nthe number $k\\ll n$, where $n$ is the total sample size, of extreme data points\nto consider for inference of tail quantities. Existing theoretical guarantees\nfor adaptive methods typically require second-order assumptions or von Mises\nassumptions that are difficult to verify and often come with tuning parameters\nthat are challenging to calibrate. This paper revisits the problem of adaptive\nselection of $k$ for the Hill estimator. Our goal is not an `optimal' $k$ but\none that is `good enough', in the sense that we strive for non-asymptotic\nguarantees that might be sub-optimal but are explicit and require minimal\nconditions. We propose a transparent adaptive rule that does not require\npreliminary calibration of constants, inspired by `adaptive validation'\ndeveloped in high-dimensional statistics. A key feature of our approach is the\nconsideration of a grid for $k$ of size $ \\ll n $, which aligns with common\npractice among practitioners but has remained unexplored in theoretical\nanalysis. Our rule only involves an explicit expression of a variance-type\nterm; in particular, it does not require controlling or estimating a biasterm.\nOur theoretical analysis is valid for all heavy-tailed distributions,\nspecifically for all regularly varying survival functions. Furthermore, when\nvon Mises conditions hold, our method achieves `almost' minimax optimality with\na rate of $\\sqrt{\\log \\log n}~ n^{-|\\rho|/(1+2|\\rho|)}$ when the grid size is\nof order $\\log n$, in contrast to the $ (\\log \\log (n)/n)^{|\\rho|/(1+2|\\rho|)}\n$ rate in existing work. Our simulations show that our approach performs\nparticularly well for ill-behaved distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A notoriously difficult challenge in extreme value theory is the choice of\nthe number $k\\ll n$, where $n$ is the total sample size, of extreme data points\nto consider for inference of tail quantities. Existing theoretical guarantees\nfor adaptive methods typically require second-order assumptions or von Mises\nassumptions that are difficult to verify and often come with tuning parameters\nthat are challenging to calibrate. This paper revisits the problem of adaptive\nselection of $k$ for the Hill estimator. Our goal is not an `optimal' $k$ but\none that is `good enough', in the sense that we strive for non-asymptotic\nguarantees that might be sub-optimal but are explicit and require minimal\nconditions. We propose a transparent adaptive rule that does not require\npreliminary calibration of constants, inspired by `adaptive validation'\ndeveloped in high-dimensional statistics. A key feature of our approach is the\nconsideration of a grid for $k$ of size $ \\ll n $, which aligns with common\npractice among practitioners but has remained unexplored in theoretical\nanalysis. Our rule only involves an explicit expression of a variance-type\nterm; in particular, it does not require controlling or estimating a biasterm.\nOur theoretical analysis is valid for all heavy-tailed distributions,\nspecifically for all regularly varying survival functions. Furthermore, when\nvon Mises conditions hold, our method achieves `almost' minimax optimality with\na rate of $\\sqrt{\\log \\log n}~ n^{-|\\rho|/(1+2|\\rho|)}$ when the grid size is\nof order $\\log n$, in contrast to the $ (\\log \\log (n)/n)^{|\\rho|/(1+2|\\rho|)}\n$ rate in existing work. Our simulations show that our approach performs\nparticularly well for ill-behaved distributions."
                },
                "authors": [
                    {
                        "name": "Johannes Lederer"
                    },
                    {
                        "name": "Anne Sabourin"
                    },
                    {
                        "name": "Mahsa Taheri"
                    }
                ],
                "author_detail": {
                    "name": "Mahsa Taheri"
                },
                "author": "Mahsa Taheri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22371v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22371v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.OT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22369v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22369v1",
                "updated": "2025-05-28T13:56:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    56,
                    48,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T13:56:48Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    56,
                    48,
                    2,
                    148,
                    0
                ],
                "title": "Model-independent cosmological inference after the DESI DR2 data with\n  improved inverse distance ladder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-independent cosmological inference after the DESI DR2 data with\n  improved inverse distance ladder"
                },
                "summary": "Recently, the baryon acoustic oscillations (BAO) measurements from the DESI\nsurvey have suggested hints of dynamical dark energy, challenging the standard\n$\\Lambda $CDM model. In this work, we adopt an improved inverse distance ladder\napproach based on the latest cosmological data to provide a model-independent\nperspective, employing a global parametrization based on cosmic age (PAge). Our\nanalysis incorporates DESI DR2 BAO measurements, cosmic chronometer (CC) data,\nand type Ia supernovae (SNe) observations from either the DESY5 or PantheonPlus\ndatasets. For the DESY5+DESI DR2+CC datasets, we obtain $H_0 = 67.91 \\pm\n2.33~\\mathrm{km~s^{-1}~Mpc^{-1}}$. This value is consistent with the Planck\n2018 result, while shows $2.0 \\sigma$ tension with the SH0ES measurement.\nFurthermore, by mapping specific cosmological models into PAge approximation\nparameter space $(p_{\\mathrm{age}}, \\eta)$, our model-independent analysis\nreveals a notable deviation from the $\\Lambda \\mathrm{CDM}$ model, as indicated\nby the DESY5 and DESI DR2 datasets. Finally, DESY5+DESI DR2+CC datasets provide\nnearly decisive evidence favoring the PAge model over the standard $\\Lambda\n\\mathrm{CDM}$ model. These findings highlight the need for further\ninvestigation into the expansion history to better understand the deviations\nfrom the $\\Lambda \\mathrm{CDM}$ model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the baryon acoustic oscillations (BAO) measurements from the DESI\nsurvey have suggested hints of dynamical dark energy, challenging the standard\n$\\Lambda $CDM model. In this work, we adopt an improved inverse distance ladder\napproach based on the latest cosmological data to provide a model-independent\nperspective, employing a global parametrization based on cosmic age (PAge). Our\nanalysis incorporates DESI DR2 BAO measurements, cosmic chronometer (CC) data,\nand type Ia supernovae (SNe) observations from either the DESY5 or PantheonPlus\ndatasets. For the DESY5+DESI DR2+CC datasets, we obtain $H_0 = 67.91 \\pm\n2.33~\\mathrm{km~s^{-1}~Mpc^{-1}}$. This value is consistent with the Planck\n2018 result, while shows $2.0 \\sigma$ tension with the SH0ES measurement.\nFurthermore, by mapping specific cosmological models into PAge approximation\nparameter space $(p_{\\mathrm{age}}, \\eta)$, our model-independent analysis\nreveals a notable deviation from the $\\Lambda \\mathrm{CDM}$ model, as indicated\nby the DESY5 and DESI DR2 datasets. Finally, DESY5+DESI DR2+CC datasets provide\nnearly decisive evidence favoring the PAge model over the standard $\\Lambda\n\\mathrm{CDM}$ model. These findings highlight the need for further\ninvestigation into the expansion history to better understand the deviations\nfrom the $\\Lambda \\mathrm{CDM}$ model."
                },
                "authors": [
                    {
                        "name": "Jia-Le Ling"
                    },
                    {
                        "name": "Guo-Hong Du"
                    },
                    {
                        "name": "Tian-Nuo Li"
                    },
                    {
                        "name": "Jing-Fei Zhang"
                    },
                    {
                        "name": "Shao-Jiang Wang"
                    },
                    {
                        "name": "Xin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Zhang"
                },
                "author": "Xin Zhang",
                "arxiv_comment": "13 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22369v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16169v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16169v2",
                "updated": "2025-05-28T13:56:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    56,
                    42,
                    2,
                    148,
                    0
                ],
                "published": "2025-02-22T10:03:19Z",
                "published_parsed": [
                    2025,
                    2,
                    22,
                    10,
                    3,
                    19,
                    5,
                    53,
                    0
                ],
                "title": "Patterns Over Principles: The Fragility of Inductive Reasoning in LLMs\n  under Noisy Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patterns Over Principles: The Fragility of Inductive Reasoning in LLMs\n  under Noisy Observations"
                },
                "summary": "Inductive reasoning, a cornerstone of human cognition, enables generalization\nfrom limited data but hasn't yet been fully achieved by large language models\n(LLMs). While modern LLMs excel at reasoning tasks, their ability to maintain\nstable and consistent rule abstraction under imperfect observations remains\nunderexplored. To fill this gap, in this work, we introduce Robust Rule\nInduction, a task that evaluates LLMs' capability in inferring rules from data\nthat are fused with noisy examples. To address this task, we further propose\nSample-steered Rule Refinement (SRR), a method enhancing reasoning stability\nvia observation diversification and execution-guided feedback. Experiments\nacross arithmetic, cryptography, and list functions reveal: (1) SRR outperforms\nother methods with minimal performance degradation under noise; (2) Despite\nslight accuracy variation, LLMs exhibit instability under noise (e.g., 0%\naccuracy change with only 70% consistent score); (3) Counterfactual task gaps\nhighlight LLMs' reliance on memorized patterns over genuine abstraction. Our\nfindings challenge LLMs' reasoning robustness, revealing susceptibility to\nhypothesis drift and pattern overfitting, while providing empirical evidence\ncritical for developing human-like inductive systems. Code and data are\navailable at https://github.com/HKUST-KnowComp/Robust-Rule-Induction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inductive reasoning, a cornerstone of human cognition, enables generalization\nfrom limited data but hasn't yet been fully achieved by large language models\n(LLMs). While modern LLMs excel at reasoning tasks, their ability to maintain\nstable and consistent rule abstraction under imperfect observations remains\nunderexplored. To fill this gap, in this work, we introduce Robust Rule\nInduction, a task that evaluates LLMs' capability in inferring rules from data\nthat are fused with noisy examples. To address this task, we further propose\nSample-steered Rule Refinement (SRR), a method enhancing reasoning stability\nvia observation diversification and execution-guided feedback. Experiments\nacross arithmetic, cryptography, and list functions reveal: (1) SRR outperforms\nother methods with minimal performance degradation under noise; (2) Despite\nslight accuracy variation, LLMs exhibit instability under noise (e.g., 0%\naccuracy change with only 70% consistent score); (3) Counterfactual task gaps\nhighlight LLMs' reliance on memorized patterns over genuine abstraction. Our\nfindings challenge LLMs' reasoning robustness, revealing susceptibility to\nhypothesis drift and pattern overfitting, while providing empirical evidence\ncritical for developing human-like inductive systems. Code and data are\navailable at https://github.com/HKUST-KnowComp/Robust-Rule-Induction."
                },
                "authors": [
                    {
                        "name": "Chunyang Li"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16169v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16169v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22368v1",
                "updated": "2025-05-28T13:56:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    56,
                    22,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T13:56:22Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    56,
                    22,
                    2,
                    148,
                    0
                ],
                "title": "AgentDNS: A Root Domain Naming System for LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentDNS: A Root Domain Naming System for LLM Agents"
                },
                "summary": "The rapid evolution of Large Language Model (LLM) agents has highlighted\ncritical challenges in cross-vendor service discovery, interoperability, and\ncommunication. Existing protocols like model context protocol and\nagent-to-agent protocol have made significant strides in standardizing\ninteroperability between agents and tools, as well as communication among\nmulti-agents. However, there remains a lack of standardized protocols and\nsolutions for service discovery across different agent and tool vendors. In\nthis paper, we propose AgentDNS, a root domain naming and service discovery\nsystem designed to enable LLM agents to autonomously discover, resolve, and\nsecurely invoke third-party agent and tool services across organizational and\ntechnological boundaries. Inspired by the principles of the traditional DNS,\nAgentDNS introduces a structured mechanism for service registration, semantic\nservice discovery, secure invocation, and unified billing. We detail the\narchitecture, core functionalities, and use cases of AgentDNS, demonstrating\nits potential to streamline multi-agent collaboration in real-world scenarios.\nThe source code will be published on https://github.com/agentdns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Model (LLM) agents has highlighted\ncritical challenges in cross-vendor service discovery, interoperability, and\ncommunication. Existing protocols like model context protocol and\nagent-to-agent protocol have made significant strides in standardizing\ninteroperability between agents and tools, as well as communication among\nmulti-agents. However, there remains a lack of standardized protocols and\nsolutions for service discovery across different agent and tool vendors. In\nthis paper, we propose AgentDNS, a root domain naming and service discovery\nsystem designed to enable LLM agents to autonomously discover, resolve, and\nsecurely invoke third-party agent and tool services across organizational and\ntechnological boundaries. Inspired by the principles of the traditional DNS,\nAgentDNS introduces a structured mechanism for service registration, semantic\nservice discovery, secure invocation, and unified billing. We detail the\narchitecture, core functionalities, and use cases of AgentDNS, demonstrating\nits potential to streamline multi-agent collaboration in real-world scenarios.\nThe source code will be published on https://github.com/agentdns."
                },
                "authors": [
                    {
                        "name": "Enfang Cui"
                    },
                    {
                        "name": "Yujun Cheng"
                    },
                    {
                        "name": "Rui She"
                    },
                    {
                        "name": "Dan Liu"
                    },
                    {
                        "name": "Zhiyuan Liang"
                    },
                    {
                        "name": "Minxin Guo"
                    },
                    {
                        "name": "Tianzheng Li"
                    },
                    {
                        "name": "Qian Wei"
                    },
                    {
                        "name": "Wenjuan Xing"
                    },
                    {
                        "name": "Zhijie Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Zhong"
                },
                "author": "Zhijie Zhong",
                "arxiv_comment": "7 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01562v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01562v2",
                "updated": "2025-05-28T13:55:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    55,
                    35,
                    2,
                    148,
                    0
                ],
                "published": "2025-02-03T17:45:46Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    17,
                    45,
                    46,
                    0,
                    34,
                    0
                ],
                "title": "Memento No More: Coaching AI Agents to Master Multiple Tasks via Hints\n  Internalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memento No More: Coaching AI Agents to Master Multiple Tasks via Hints\n  Internalization"
                },
                "summary": "As the general capabilities of artificial intelligence (AI) agents continue\nto evolve, their ability to learn to master multiple complex tasks through\nexperience remains a key challenge. Current LLM agents, particularly those\nbased on proprietary language models, typically rely on prompts to incorporate\nknowledge about the target tasks. This approach does not allow the agent to\ninternalize this information and instead relies on ever-expanding prompts to\nsustain its functionality in diverse scenarios. This resembles a system of\nnotes used by a person affected by anterograde amnesia, the inability to form\nnew memories. In this paper, we propose a novel method to train AI agents to\nincorporate knowledge and skills for multiple tasks without the need for either\ncumbersome note systems or prior high-quality demonstration data. Our approach\nemploys an iterative process where the agent collects new experiences, receives\ncorrective feedback from humans in the form of hints, and integrates this\nfeedback into its weights via a context distillation training procedure. We\ndemonstrate the efficacy of our approach by implementing it in a Llama-3-based\nagent that, after only a few rounds of feedback, outperforms advanced models\nGPT-4o and DeepSeek-V3 in tasksets requiring correct sequencing of information\nretrieval, tool use, and question answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the general capabilities of artificial intelligence (AI) agents continue\nto evolve, their ability to learn to master multiple complex tasks through\nexperience remains a key challenge. Current LLM agents, particularly those\nbased on proprietary language models, typically rely on prompts to incorporate\nknowledge about the target tasks. This approach does not allow the agent to\ninternalize this information and instead relies on ever-expanding prompts to\nsustain its functionality in diverse scenarios. This resembles a system of\nnotes used by a person affected by anterograde amnesia, the inability to form\nnew memories. In this paper, we propose a novel method to train AI agents to\nincorporate knowledge and skills for multiple tasks without the need for either\ncumbersome note systems or prior high-quality demonstration data. Our approach\nemploys an iterative process where the agent collects new experiences, receives\ncorrective feedback from humans in the form of hints, and integrates this\nfeedback into its weights via a context distillation training procedure. We\ndemonstrate the efficacy of our approach by implementing it in a Llama-3-based\nagent that, after only a few rounds of feedback, outperforms advanced models\nGPT-4o and DeepSeek-V3 in tasksets requiring correct sequencing of information\nretrieval, tool use, and question answering."
                },
                "authors": [
                    {
                        "name": "Minttu Alakuijala"
                    },
                    {
                        "name": "Ya Gao"
                    },
                    {
                        "name": "Georgy Ananov"
                    },
                    {
                        "name": "Samuel Kaski"
                    },
                    {
                        "name": "Pekka Marttinen"
                    },
                    {
                        "name": "Alexander Ilin"
                    },
                    {
                        "name": "Harri Valpola"
                    }
                ],
                "author_detail": {
                    "name": "Harri Valpola"
                },
                "author": "Harri Valpola",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01562v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01562v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22358v1",
                "updated": "2025-05-28T13:38:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    38,
                    21,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T13:38:21Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    38,
                    21,
                    2,
                    148,
                    0
                ],
                "title": "Budget-Adaptive Adapter Tuning in Orthogonal Subspaces for Continual\n  Learning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Budget-Adaptive Adapter Tuning in Orthogonal Subspaces for Continual\n  Learning in LLMs"
                },
                "summary": "Large language models (LLMs) often suffer from catastrophic forgetting in\ncontinual learning (CL) scenarios, where performance on previously learned\ntasks degrades severely while training on sequentially arriving tasks. Although\npioneering CL approaches using orthogonal subspaces can mitigate task\ninterference, they typically employ fixed budget allocation, neglecting the\nvarying complexity across tasks and layers. Besides, recent budget-adaptive\ntuning methods for LLMs often adopt multi-stage paradigms that decouple\noptimization and budget allocation. Such decoupling results in potential\nmisalignment, which hinders those approaches' practical application in CL\nscenarios. To address these limitations, we propose OA-Adapter, a novel\nparameter-efficient approach for continual learning in LLMs that unifies\ndynamic budget adaptation with orthogonal subspace learning in a single\nend-to-end training stage. Specifically, OA-Adapter introduces a dynamic\nbottleneck dimension adaptation mechanism that simultaneously allocates an\nefficient parameter budget and optimizes task objectives without misalignment.\nTo effectively preserve previously acquired knowledge while coordinating with\nthe dynamic budget allocation, orthogonal constraints are applied specifically\nbetween the parameter subspace of the current task and the dynamically\nallocated parameter subspaces of historical tasks. Experimental results on\ncontinual learning benchmarks demonstrate that OA-Adapter outperforms\nstate-of-the-art methods in both accuracy and parameter efficiency, achieving\nhigher average accuracy while using 58.5% fewer parameters on the standard CL\nbenchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often suffer from catastrophic forgetting in\ncontinual learning (CL) scenarios, where performance on previously learned\ntasks degrades severely while training on sequentially arriving tasks. Although\npioneering CL approaches using orthogonal subspaces can mitigate task\ninterference, they typically employ fixed budget allocation, neglecting the\nvarying complexity across tasks and layers. Besides, recent budget-adaptive\ntuning methods for LLMs often adopt multi-stage paradigms that decouple\noptimization and budget allocation. Such decoupling results in potential\nmisalignment, which hinders those approaches' practical application in CL\nscenarios. To address these limitations, we propose OA-Adapter, a novel\nparameter-efficient approach for continual learning in LLMs that unifies\ndynamic budget adaptation with orthogonal subspace learning in a single\nend-to-end training stage. Specifically, OA-Adapter introduces a dynamic\nbottleneck dimension adaptation mechanism that simultaneously allocates an\nefficient parameter budget and optimizes task objectives without misalignment.\nTo effectively preserve previously acquired knowledge while coordinating with\nthe dynamic budget allocation, orthogonal constraints are applied specifically\nbetween the parameter subspace of the current task and the dynamically\nallocated parameter subspaces of historical tasks. Experimental results on\ncontinual learning benchmarks demonstrate that OA-Adapter outperforms\nstate-of-the-art methods in both accuracy and parameter efficiency, achieving\nhigher average accuracy while using 58.5% fewer parameters on the standard CL\nbenchmark."
                },
                "authors": [
                    {
                        "name": "Zhiyi Wan"
                    },
                    {
                        "name": "Wanrou Du"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Miao Pan"
                    },
                    {
                        "name": "Xiaoqi Qin"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqi Qin"
                },
                "author": "Xiaoqi Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.22664v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22664v1",
                "updated": "2025-05-28T17:59:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    59,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:59:59Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    59,
                    2,
                    148,
                    0
                ],
                "title": "Zero-Shot Vision Encoder Grafting via LLM Surrogates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Vision Encoder Grafting via LLM Surrogates"
                },
                "summary": "Vision language models (VLMs) typically pair a modestly sized vision encoder\nwith a large language model (LLM), e.g., Llama-70B, making the decoder the\nprimary computational burden during training. To reduce costs, a potential\npromising strategy is to first train the vision encoder using a small language\nmodel before transferring it to the large one. We construct small \"surrogate\nmodels\" that share the same embedding space and representation language as the\nlarge target LLM by directly inheriting its shallow layers. Vision encoders\ntrained on the surrogate can then be directly transferred to the larger model,\na process we call zero-shot grafting -- when plugged directly into the\nfull-size target LLM, the grafted pair surpasses the encoder-surrogate pair\nand, on some benchmarks, even performs on par with full decoder training with\nthe target LLM. Furthermore, our surrogate training approach reduces overall\nVLM training costs by ~45% when using Llama-70B as the decoder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision language models (VLMs) typically pair a modestly sized vision encoder\nwith a large language model (LLM), e.g., Llama-70B, making the decoder the\nprimary computational burden during training. To reduce costs, a potential\npromising strategy is to first train the vision encoder using a small language\nmodel before transferring it to the large one. We construct small \"surrogate\nmodels\" that share the same embedding space and representation language as the\nlarge target LLM by directly inheriting its shallow layers. Vision encoders\ntrained on the surrogate can then be directly transferred to the larger model,\na process we call zero-shot grafting -- when plugged directly into the\nfull-size target LLM, the grafted pair surpasses the encoder-surrogate pair\nand, on some benchmarks, even performs on par with full decoder training with\nthe target LLM. Furthermore, our surrogate training approach reduces overall\nVLM training costs by ~45% when using Llama-70B as the decoder."
                },
                "authors": [
                    {
                        "name": "Kaiyu Yue"
                    },
                    {
                        "name": "Vasu Singla"
                    },
                    {
                        "name": "Menglin Jia"
                    },
                    {
                        "name": "John Kirchenbauer"
                    },
                    {
                        "name": "Rifaa Qadri"
                    },
                    {
                        "name": "Zikui Cai"
                    },
                    {
                        "name": "Abhinav Bhatele"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Tom Goldstein"
                    }
                ],
                "author_detail": {
                    "name": "Tom Goldstein"
                },
                "author": "Tom Goldstein",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22664v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22664v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22662v1",
                "updated": "2025-05-28T17:59:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    53,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:59:53Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    53,
                    2,
                    148,
                    0
                ],
                "title": "AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models"
                },
                "summary": "The reasoning-capable large language models (LLMs) demonstrate strong\nperformance on complex reasoning tasks but often suffer from overthinking,\ngenerating unnecessarily long chain-of-thought (CoT) reasoning paths for easy\nreasoning questions, thereby increasing inference cost and latency. Recent\napproaches attempt to address this challenge by manually deciding when to apply\nlong or short reasoning. However, they lack the flexibility to adapt CoT length\ndynamically based on question complexity. In this paper, we propose Auto\nLong-Short Reasoning (AutoL2S), a dynamic and model-agnostic framework that\nenables LLMs to dynamically compress their generated reasoning path based on\nthe complexity of the reasoning question. AutoL2S enables a learned paradigm,\nin which LLMs themselves can decide when longer reasoning is necessary and when\nshorter reasoning suffices, by training on data annotated with our proposed\nmethod, which includes both long and short CoT paths and a special <EASY>\ntoken. We then use <EASY> token to indicate when the model can skip generating\nlengthy CoT reasoning. This proposed annotation strategy can enhance the LLMs'\nability to generate shorter CoT reasoning paths with improved quality after\ntraining. Extensive evaluation results show that AutoL2S reduces the length of\nreasoning generation by up to 57% without compromising performance,\ndemonstrating the effectiveness of AutoL2S for scalable and efficient LLM\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning-capable large language models (LLMs) demonstrate strong\nperformance on complex reasoning tasks but often suffer from overthinking,\ngenerating unnecessarily long chain-of-thought (CoT) reasoning paths for easy\nreasoning questions, thereby increasing inference cost and latency. Recent\napproaches attempt to address this challenge by manually deciding when to apply\nlong or short reasoning. However, they lack the flexibility to adapt CoT length\ndynamically based on question complexity. In this paper, we propose Auto\nLong-Short Reasoning (AutoL2S), a dynamic and model-agnostic framework that\nenables LLMs to dynamically compress their generated reasoning path based on\nthe complexity of the reasoning question. AutoL2S enables a learned paradigm,\nin which LLMs themselves can decide when longer reasoning is necessary and when\nshorter reasoning suffices, by training on data annotated with our proposed\nmethod, which includes both long and short CoT paths and a special <EASY>\ntoken. We then use <EASY> token to indicate when the model can skip generating\nlengthy CoT reasoning. This proposed annotation strategy can enhance the LLMs'\nability to generate shorter CoT reasoning paths with improved quality after\ntraining. Extensive evaluation results show that AutoL2S reduces the length of\nreasoning generation by up to 57% without compromising performance,\ndemonstrating the effectiveness of AutoL2S for scalable and efficient LLM\nreasoning."
                },
                "authors": [
                    {
                        "name": "Feng Luo"
                    },
                    {
                        "name": "Yu-Neng Chuang"
                    },
                    {
                        "name": "Guanchu Wang"
                    },
                    {
                        "name": "Hoang Anh Duy Le"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Hongyi Liu"
                    },
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Vladimir Braverman"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22661v1",
                "updated": "2025-05-28T17:59:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    43,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:59:43Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    43,
                    2,
                    148,
                    0
                ],
                "title": "GuessArena: Guess Who I Am? A Self-Adaptive Framework for Evaluating\n  LLMs in Domain-Specific Knowledge and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GuessArena: Guess Who I Am? A Self-Adaptive Framework for Evaluating\n  LLMs in Domain-Specific Knowledge and Reasoning"
                },
                "summary": "The evaluation of large language models (LLMs) has traditionally relied on\nstatic benchmarks, a paradigm that poses two major limitations: (1) predefined\ntest sets lack adaptability to diverse application domains, and (2)\nstandardized evaluation protocols often fail to capture fine-grained\nassessments of domain-specific knowledge and contextual reasoning abilities. To\novercome these challenges, we propose GuessArena, an adaptive evaluation\nframework grounded in adversarial game-based interactions. Inspired by the\ninteractive structure of the Guess Who I Am? game, our framework seamlessly\nintegrates dynamic domain knowledge modeling with progressive reasoning\nassessment to improve evaluation fidelity. Empirical studies across five\nvertical domains-finance, healthcare, manufacturing, information technology,\nand education-demonstrate that GuessArena effectively distinguishes LLMs in\nterms of domain knowledge coverage and reasoning chain completeness. Compared\nto conventional benchmarks, our method provides substantial advantages in\ninterpretability, scalability, and scenario adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of large language models (LLMs) has traditionally relied on\nstatic benchmarks, a paradigm that poses two major limitations: (1) predefined\ntest sets lack adaptability to diverse application domains, and (2)\nstandardized evaluation protocols often fail to capture fine-grained\nassessments of domain-specific knowledge and contextual reasoning abilities. To\novercome these challenges, we propose GuessArena, an adaptive evaluation\nframework grounded in adversarial game-based interactions. Inspired by the\ninteractive structure of the Guess Who I Am? game, our framework seamlessly\nintegrates dynamic domain knowledge modeling with progressive reasoning\nassessment to improve evaluation fidelity. Empirical studies across five\nvertical domains-finance, healthcare, manufacturing, information technology,\nand education-demonstrate that GuessArena effectively distinguishes LLMs in\nterms of domain knowledge coverage and reasoning chain completeness. Compared\nto conventional benchmarks, our method provides substantial advantages in\ninterpretability, scalability, and scenario adaptability."
                },
                "authors": [
                    {
                        "name": "Qingchen Yu"
                    },
                    {
                        "name": "Zifan Zheng"
                    },
                    {
                        "name": "Ding Chen"
                    },
                    {
                        "name": "Simin Niu"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "arxiv_comment": "Accepted by ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22657v1",
                "updated": "2025-05-28T17:59:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    13,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:59:13Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    13,
                    2,
                    148,
                    0
                ],
                "title": "3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large\n  Language Model"
                },
                "summary": "Humans excel at performing complex tasks by leveraging long-term memory\nacross temporal and spatial experiences. In contrast, current Large Language\nModels (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D\nenvironments. We posit that part of this limitation is due to the lack of\nproper 3D spatial-temporal memory modeling in LLMs. To address this, we first\nintroduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000\ntrajectories and 2,892 embodied tasks, question-answering and captioning,\ndesigned to evaluate an agent's ability to reason over long-term memory in 3D\nenvironments. Second, we propose 3DLLM-Mem, a novel dynamic memory management\nand fusion model for embodied spatial-temporal reasoning and actions in LLMs.\nOur model uses working memory tokens, which represents current observations, as\nqueries to selectively attend to and fuse the most useful spatial and temporal\nfeatures from episodic memory, which stores past observations and interactions.\nOur approach allows the agent to focus on task-relevant information while\nmaintaining memory efficiency in complex, long-horizon environments.\nExperimental results demonstrate that 3DLLM-Mem achieves state-of-the-art\nperformance across various tasks, outperforming the strongest baselines by\n16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans excel at performing complex tasks by leveraging long-term memory\nacross temporal and spatial experiences. In contrast, current Large Language\nModels (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D\nenvironments. We posit that part of this limitation is due to the lack of\nproper 3D spatial-temporal memory modeling in LLMs. To address this, we first\nintroduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000\ntrajectories and 2,892 embodied tasks, question-answering and captioning,\ndesigned to evaluate an agent's ability to reason over long-term memory in 3D\nenvironments. Second, we propose 3DLLM-Mem, a novel dynamic memory management\nand fusion model for embodied spatial-temporal reasoning and actions in LLMs.\nOur model uses working memory tokens, which represents current observations, as\nqueries to selectively attend to and fuse the most useful spatial and temporal\nfeatures from episodic memory, which stores past observations and interactions.\nOur approach allows the agent to focus on task-relevant information while\nmaintaining memory efficiency in complex, long-horizon environments.\nExperimental results demonstrate that 3DLLM-Mem achieves state-of-the-art\nperformance across various tasks, outperforming the strongest baselines by\n16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied\ntasks."
                },
                "authors": [
                    {
                        "name": "Wenbo Hu"
                    },
                    {
                        "name": "Yining Hong"
                    },
                    {
                        "name": "Yanjun Wang"
                    },
                    {
                        "name": "Leison Gao"
                    },
                    {
                        "name": "Zibu Wei"
                    },
                    {
                        "name": "Xingcheng Yao"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Yonatan Bitton"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Wei Chang"
                },
                "author": "Kai-Wei Chang",
                "arxiv_comment": "demos at: https://3dllm-mem.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22654v1",
                "updated": "2025-05-28T17:59:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    8,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:59:08Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    8,
                    2,
                    148,
                    0
                ],
                "title": "VScan: Rethinking Visual Token Reduction for Efficient Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VScan: Rethinking Visual Token Reduction for Efficient Large\n  Vision-Language Models"
                },
                "summary": "Recent Large Vision-Language Models (LVLMs) have advanced multi-modal\nunderstanding by incorporating finer-grained visual perception and encoding.\nHowever, such methods incur significant computational costs due to longer\nvisual token sequences, posing challenges for real-time deployment. To mitigate\nthis, prior studies have explored pruning unimportant visual tokens either at\nthe output layer of the visual encoder or at the early layers of the language\nmodel. In this work, we revisit these design choices and reassess their\neffectiveness through comprehensive empirical studies of how visual tokens are\nprocessed throughout the visual encoding and language decoding stages. Guided\nby these insights, we propose VScan, a two-stage visual token reduction\nframework that addresses token redundancy by: (1) integrating complementary\nglobal and local scans with token merging during visual encoding, and (2)\nintroducing pruning at intermediate layers of the language model. Extensive\nexperimental results across four LVLMs validate the effectiveness of VScan in\naccelerating inference and demonstrate its superior performance over current\nstate-of-the-arts on sixteen benchmarks. Notably, when applied to\nLLaVA-NeXT-7B, VScan achieves a 2.91$\\times$ speedup in prefilling and a\n10$\\times$ reduction in FLOPs, while retaining 95.4% of the original\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Large Vision-Language Models (LVLMs) have advanced multi-modal\nunderstanding by incorporating finer-grained visual perception and encoding.\nHowever, such methods incur significant computational costs due to longer\nvisual token sequences, posing challenges for real-time deployment. To mitigate\nthis, prior studies have explored pruning unimportant visual tokens either at\nthe output layer of the visual encoder or at the early layers of the language\nmodel. In this work, we revisit these design choices and reassess their\neffectiveness through comprehensive empirical studies of how visual tokens are\nprocessed throughout the visual encoding and language decoding stages. Guided\nby these insights, we propose VScan, a two-stage visual token reduction\nframework that addresses token redundancy by: (1) integrating complementary\nglobal and local scans with token merging during visual encoding, and (2)\nintroducing pruning at intermediate layers of the language model. Extensive\nexperimental results across four LVLMs validate the effectiveness of VScan in\naccelerating inference and demonstrate its superior performance over current\nstate-of-the-arts on sixteen benchmarks. Notably, when applied to\nLLaVA-NeXT-7B, VScan achieves a 2.91$\\times$ speedup in prefilling and a\n10$\\times$ reduction in FLOPs, while retaining 95.4% of the original\nperformance."
                },
                "authors": [
                    {
                        "name": "Ce Zhang"
                    },
                    {
                        "name": "Kaixin Ma"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Yaqi Xie"
                    },
                    {
                        "name": "Katia Sycara"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22655v1",
                "updated": "2025-05-28T17:59:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    8,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:59:08Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    8,
                    2,
                    148,
                    0
                ],
                "title": "Position: Uncertainty Quantification Needs Reassessment for\n  Large-language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: Uncertainty Quantification Needs Reassessment for\n  Large-language Model Agents"
                },
                "summary": "Large-language models (LLMs) and chatbot agents are known to provide wrong\noutputs at times, and it was recently found that this can never be fully\nprevented. Hence, uncertainty quantification plays a crucial role, aiming to\nquantify the level of ambiguity in either one overall number or two numbers for\naleatoric and epistemic uncertainty. This position paper argues that this\ntraditional dichotomy of uncertainties is too limited for the open and\ninteractive setup that LLM agents operate in when communicating with a user,\nand that we need to research avenues that enrich uncertainties in this novel\nscenario. We review the literature and find that popular definitions of\naleatoric and epistemic uncertainties directly contradict each other and lose\ntheir meaning in interactive LLM agent settings. Hence, we propose three novel\nresearch directions that focus on uncertainties in such human-computer\ninteractions: Underspecification uncertainties, for when users do not provide\nall information or define the exact task at the first go, interactive learning,\nto ask follow-up questions and reduce the uncertainty about the current\ncontext, and output uncertainties, to utilize the rich language and speech\nspace to express uncertainties as more than mere numbers. We expect that these\nnew ways of dealing with and communicating uncertainties will lead to LLM agent\ninteractions that are more transparent, trustworthy, and intuitive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-language models (LLMs) and chatbot agents are known to provide wrong\noutputs at times, and it was recently found that this can never be fully\nprevented. Hence, uncertainty quantification plays a crucial role, aiming to\nquantify the level of ambiguity in either one overall number or two numbers for\naleatoric and epistemic uncertainty. This position paper argues that this\ntraditional dichotomy of uncertainties is too limited for the open and\ninteractive setup that LLM agents operate in when communicating with a user,\nand that we need to research avenues that enrich uncertainties in this novel\nscenario. We review the literature and find that popular definitions of\naleatoric and epistemic uncertainties directly contradict each other and lose\ntheir meaning in interactive LLM agent settings. Hence, we propose three novel\nresearch directions that focus on uncertainties in such human-computer\ninteractions: Underspecification uncertainties, for when users do not provide\nall information or define the exact task at the first go, interactive learning,\nto ask follow-up questions and reduce the uncertainty about the current\ncontext, and output uncertainties, to utilize the rich language and speech\nspace to express uncertainties as more than mere numbers. We expect that these\nnew ways of dealing with and communicating uncertainties will lead to LLM agent\ninteractions that are more transparent, trustworthy, and intuitive."
                },
                "authors": [
                    {
                        "name": "Michael Kirchhof"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Enkelejda Kasneci"
                },
                "author": "Enkelejda Kasneci",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22653v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22653v1",
                "updated": "2025-05-28T17:59:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    3,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:59:03Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    3,
                    2,
                    148,
                    0
                ],
                "title": "The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in\n  Learning to Reason",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in\n  Learning to Reason"
                },
                "summary": "Recent studies on post-training large language models (LLMs) for reasoning\nthrough reinforcement learning (RL) typically focus on tasks that can be\naccurately verified and rewarded, such as solving math problems. In contrast,\nour research investigates the impact of reward noise, a more practical\nconsideration for real-world scenarios involving the post-training of LLMs\nusing reward models. We found that LLMs demonstrate strong robustness to\nsubstantial reward noise. For example, manually flipping 40% of the reward\nfunction's outputs in math tasks still allows a Qwen-2.5-7B model to achieve\nrapid convergence, improving its performance on math tasks from 5% to 72%,\ncompared to the 75% accuracy achieved by a model trained with noiseless\nrewards. Surprisingly, by only rewarding the appearance of key reasoning\nphrases (namely reasoning pattern reward, RPR), such as ``first, I need\nto''-without verifying the correctness of answers, the model achieved peak\ndownstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models\ntrained with strict correctness verification and accurate rewards. Recognizing\nthe importance of the reasoning process over the final results, we combined RPR\nwith noisy reward models. RPR helped calibrate the noisy reward models,\nmitigating potential false negatives and enhancing the LLM's performance on\nopen-ended tasks. These findings suggest the importance of improving models'\nfoundational abilities during the pre-training phase while providing insights\nfor advancing post-training techniques. Our code and scripts are available at\nhttps://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies on post-training large language models (LLMs) for reasoning\nthrough reinforcement learning (RL) typically focus on tasks that can be\naccurately verified and rewarded, such as solving math problems. In contrast,\nour research investigates the impact of reward noise, a more practical\nconsideration for real-world scenarios involving the post-training of LLMs\nusing reward models. We found that LLMs demonstrate strong robustness to\nsubstantial reward noise. For example, manually flipping 40% of the reward\nfunction's outputs in math tasks still allows a Qwen-2.5-7B model to achieve\nrapid convergence, improving its performance on math tasks from 5% to 72%,\ncompared to the 75% accuracy achieved by a model trained with noiseless\nrewards. Surprisingly, by only rewarding the appearance of key reasoning\nphrases (namely reasoning pattern reward, RPR), such as ``first, I need\nto''-without verifying the correctness of answers, the model achieved peak\ndownstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models\ntrained with strict correctness verification and accurate rewards. Recognizing\nthe importance of the reasoning process over the final results, we combined RPR\nwith noisy reward models. RPR helped calibrate the noisy reward models,\nmitigating potential false negatives and enhancing the LLM's performance on\nopen-ended tasks. These findings suggest the importance of improving models'\nfoundational abilities during the pre-training phase while providing insights\nfor advancing post-training techniques. Our code and scripts are available at\nhttps://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason."
                },
                "authors": [
                    {
                        "name": "Ang Lv"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22653v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22653v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22650v1",
                "updated": "2025-05-28T17:57:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    57,
                    29,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:57:29Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    57,
                    29,
                    2,
                    148,
                    0
                ],
                "title": "On Learning Verifiers for Chain-of-Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Learning Verifiers for Chain-of-Thought Reasoning"
                },
                "summary": "Chain-of-Thought reasoning has emerged as a powerful approach for solving\ncomplex mathematical and logical problems. However, it can often veer off track\nthrough incorrect or unsubstantiated inferences. Formal mathematical reasoning,\nwhich can be checked with a formal verifier, is one approach to addressing this\nissue. However, currently LLMs are simply not good enough to solve complex\nproblems in a formal way, and even just formalizing an informal problem\nstatement can be challenging. Motivated by this fact, in this work we consider\nthe problem of learning reliable verifiers for natural language\nChain-of-Thought reasoning. That is, given a problem statement and step-by-step\nsolution in natural language, the aim of the verifier is to output [Yes] if the\nreasoning steps in the solution are all valid, and [No] otherwise. In this work\nwe give a formal PAC-learning framework for studying this problem. We propose\nand analyze several natural verification goals, at different levels of\nstrength, in this framework. We provide sample complexity upper-bounds for\nlearning verifiers satisfying these goals, as well as lower-bound and\nimpossibility results for learning other natural verification objectives\nwithout additional assumptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought reasoning has emerged as a powerful approach for solving\ncomplex mathematical and logical problems. However, it can often veer off track\nthrough incorrect or unsubstantiated inferences. Formal mathematical reasoning,\nwhich can be checked with a formal verifier, is one approach to addressing this\nissue. However, currently LLMs are simply not good enough to solve complex\nproblems in a formal way, and even just formalizing an informal problem\nstatement can be challenging. Motivated by this fact, in this work we consider\nthe problem of learning reliable verifiers for natural language\nChain-of-Thought reasoning. That is, given a problem statement and step-by-step\nsolution in natural language, the aim of the verifier is to output [Yes] if the\nreasoning steps in the solution are all valid, and [No] otherwise. In this work\nwe give a formal PAC-learning framework for studying this problem. We propose\nand analyze several natural verification goals, at different levels of\nstrength, in this framework. We provide sample complexity upper-bounds for\nlearning verifiers satisfying these goals, as well as lower-bound and\nimpossibility results for learning other natural verification objectives\nwithout additional assumptions."
                },
                "authors": [
                    {
                        "name": "Maria-Florina Balcan"
                    },
                    {
                        "name": "Avrim Blum"
                    },
                    {
                        "name": "Zhiyuan Li"
                    },
                    {
                        "name": "Dravyansh Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Dravyansh Sharma"
                },
                "author": "Dravyansh Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22645v1",
                "updated": "2025-05-28T17:56:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    56,
                    49,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:56:49Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    56,
                    49,
                    2,
                    148,
                    0
                ],
                "title": "Characterizing Bias: Benchmarking Large Language Models in Simplified\n  versus Traditional Chinese",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing Bias: Benchmarking Large Language Models in Simplified\n  versus Traditional Chinese"
                },
                "summary": "While the capabilities of Large Language Models (LLMs) have been studied in\nboth Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit\ndifferential performance when prompted in these two variants of written\nChinese. This understanding is critical, as disparities in the quality of LLM\nresponses can perpetuate representational harms by ignoring the different\ncultural contexts underlying Simplified versus Traditional Chinese, and can\nexacerbate downstream harms in LLM-facilitated decision-making in domains such\nas education or hiring. To investigate potential LLM performance disparities,\nwe design two benchmark tasks that reflect real-world scenarios: regional term\nchoice (prompting the LLM to name a described item which is referred to\ndifferently in Mainland China and Taiwan), and regional name choice (prompting\nthe LLM to choose who to hire from a list of names in both Simplified and\nTraditional Chinese). For both tasks, we audit the performance of 11 leading\ncommercial LLM services and open-sourced models -- spanning those primarily\ntrained on English, Simplified Chinese, or Traditional Chinese. Our analyses\nindicate that biases in LLM responses are dependent on both the task and\nprompting language: while most LLMs disproportionately favored Simplified\nChinese responses in the regional term choice task, they surprisingly favored\nTraditional Chinese names in the regional name choice task. We find that these\ndisparities may arise from differences in training data representation, written\ncharacter preferences, and tokenization of Simplified and Traditional Chinese.\nThese findings highlight the need for further analysis of LLM biases; as such,\nwe provide an open-sourced benchmark dataset to foster reproducible evaluations\nof future LLM behavior across Chinese language variants\n(https://github.com/brucelyu17/SC-TC-Bench).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the capabilities of Large Language Models (LLMs) have been studied in\nboth Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit\ndifferential performance when prompted in these two variants of written\nChinese. This understanding is critical, as disparities in the quality of LLM\nresponses can perpetuate representational harms by ignoring the different\ncultural contexts underlying Simplified versus Traditional Chinese, and can\nexacerbate downstream harms in LLM-facilitated decision-making in domains such\nas education or hiring. To investigate potential LLM performance disparities,\nwe design two benchmark tasks that reflect real-world scenarios: regional term\nchoice (prompting the LLM to name a described item which is referred to\ndifferently in Mainland China and Taiwan), and regional name choice (prompting\nthe LLM to choose who to hire from a list of names in both Simplified and\nTraditional Chinese). For both tasks, we audit the performance of 11 leading\ncommercial LLM services and open-sourced models -- spanning those primarily\ntrained on English, Simplified Chinese, or Traditional Chinese. Our analyses\nindicate that biases in LLM responses are dependent on both the task and\nprompting language: while most LLMs disproportionately favored Simplified\nChinese responses in the regional term choice task, they surprisingly favored\nTraditional Chinese names in the regional name choice task. We find that these\ndisparities may arise from differences in training data representation, written\ncharacter preferences, and tokenization of Simplified and Traditional Chinese.\nThese findings highlight the need for further analysis of LLM biases; as such,\nwe provide an open-sourced benchmark dataset to foster reproducible evaluations\nof future LLM behavior across Chinese language variants\n(https://github.com/brucelyu17/SC-TC-Bench)."
                },
                "authors": [
                    {
                        "name": "Hanjia Lyu"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Jian Kang"
                    },
                    {
                        "name": "Allison Koenecke"
                    }
                ],
                "author_detail": {
                    "name": "Allison Koenecke"
                },
                "author": "Allison Koenecke",
                "arxiv_comment": "To appear in the 2025 ACM Conference on Fairness, Accountability, and\n  Transparency (FAccT '25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18680v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18680v2",
                "updated": "2025-05-28T17:54:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    54,
                    54,
                    2,
                    148,
                    0
                ],
                "published": "2025-04-25T20:03:32Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    20,
                    3,
                    32,
                    4,
                    115,
                    0
                ],
                "title": "GReX: An Instrument Overview and New Upper Limits on the Galactic FRB\n  Population",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GReX: An Instrument Overview and New Upper Limits on the Galactic FRB\n  Population"
                },
                "summary": "We present the instrument design and initial results for the Galactic Radio\nExplorer (GReX), an all-sky monitor for exceptionally bright transients in the\nradio sky. This instrument builds on the success of STARE2 to search for fast\nradio bursts (FRBs) from the Milky Way and its satellites. This instrument has\ndeployments across the globe, with wide sky coverage and searching down to\n$32\\,\\mu\\text{s}$ time resolution, enabling the discovery of new super giant\npulses. Presented here are the details of the hardware and software design of\nthe instrument, performance in sensitivity and other key metrics, and\nexperience in building a global-scale, low-cost experiment. We follow this\ndiscussion with experimental results on validation of the sensitivity via\nhydrogen-line measurements. We then update the rate of Galactic FRBs based on\nnon-detection in the time since FRB 200428. Our results suggest FRB-like events\nare even rarer than initially implied by the detection of a MJy burst from SGR\nJ1935+2154 in April 2020.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the instrument design and initial results for the Galactic Radio\nExplorer (GReX), an all-sky monitor for exceptionally bright transients in the\nradio sky. This instrument builds on the success of STARE2 to search for fast\nradio bursts (FRBs) from the Milky Way and its satellites. This instrument has\ndeployments across the globe, with wide sky coverage and searching down to\n$32\\,\\mu\\text{s}$ time resolution, enabling the discovery of new super giant\npulses. Presented here are the details of the hardware and software design of\nthe instrument, performance in sensitivity and other key metrics, and\nexperience in building a global-scale, low-cost experiment. We follow this\ndiscussion with experimental results on validation of the sensitivity via\nhydrogen-line measurements. We then update the rate of Galactic FRBs based on\nnon-detection in the time since FRB 200428. Our results suggest FRB-like events\nare even rarer than initially implied by the detection of a MJy burst from SGR\nJ1935+2154 in April 2020."
                },
                "authors": [
                    {
                        "name": "K. A. Shila"
                    },
                    {
                        "name": "S. Niedbalski"
                    },
                    {
                        "name": "L. Connor"
                    },
                    {
                        "name": "S. R. Kulkarni"
                    },
                    {
                        "name": "L. Segev"
                    },
                    {
                        "name": "P. Shukla"
                    },
                    {
                        "name": "E. F. Keane"
                    },
                    {
                        "name": "J. McCauley"
                    },
                    {
                        "name": "O. A. Johnson"
                    },
                    {
                        "name": "B. Watters"
                    },
                    {
                        "name": "W. Farah"
                    },
                    {
                        "name": "A. W. Pollak"
                    },
                    {
                        "name": "K. Belov"
                    },
                    {
                        "name": "H. Tang"
                    },
                    {
                        "name": "Z. Huai"
                    },
                    {
                        "name": "S. Chatterjee"
                    },
                    {
                        "name": "J. M. Cordes"
                    }
                ],
                "author_detail": {
                    "name": "J. M. Cordes"
                },
                "author": "J. M. Cordes",
                "arxiv_comment": "22 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18680v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18680v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22641v1",
                "updated": "2025-05-28T17:54:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    54,
                    39,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:54:39Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    54,
                    39,
                    2,
                    148,
                    0
                ],
                "title": "Spectral Survival Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectral Survival Analysis"
                },
                "summary": "Survival analysis is widely deployed in a diverse set of fields, including\nhealthcare, business, ecology, etc. The Cox Proportional Hazard (CoxPH) model\nis a semi-parametric model often encountered in the literature. Despite its\npopularity, wide deployment, and numerous variants, scaling CoxPH to large\ndatasets and deep architectures poses a challenge, especially in the\nhigh-dimensional regime. We identify a fundamental connection between rank\nregression and the CoxPH model: this allows us to adapt and extend the\nso-called spectral method for rank regression to survival analysis. Our\napproach is versatile, naturally generalizing to several CoxPH variants,\nincluding deep models. We empirically verify our method's scalability on\nmultiple real-world high-dimensional datasets; our method outperforms legacy\nmethods w.r.t. predictive performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survival analysis is widely deployed in a diverse set of fields, including\nhealthcare, business, ecology, etc. The Cox Proportional Hazard (CoxPH) model\nis a semi-parametric model often encountered in the literature. Despite its\npopularity, wide deployment, and numerous variants, scaling CoxPH to large\ndatasets and deep architectures poses a challenge, especially in the\nhigh-dimensional regime. We identify a fundamental connection between rank\nregression and the CoxPH model: this allows us to adapt and extend the\nso-called spectral method for rank regression to survival analysis. Our\napproach is versatile, naturally generalizing to several CoxPH variants,\nincluding deep models. We empirically verify our method's scalability on\nmultiple real-world high-dimensional datasets; our method outperforms legacy\nmethods w.r.t. predictive performance and efficiency."
                },
                "authors": [
                    {
                        "name": "Chengzhi Shi"
                    },
                    {
                        "name": "Stratis Ioannidis"
                    }
                ],
                "author_detail": {
                    "name": "Stratis Ioannidis"
                },
                "author": "Stratis Ioannidis",
                "arxiv_doi": "10.1145/3711896.3737134",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711896.3737134",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.22641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Extended version of conference paper appearing in KDD 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22635v1",
                "updated": "2025-05-28T17:51:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    51,
                    10,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:51:10Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    51,
                    10,
                    2,
                    148,
                    0
                ],
                "title": "Learning Composable Chains-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Composable Chains-of-Thought"
                },
                "summary": "A common approach for teaching large language models (LLMs) to reason is to\ntrain on chain-of-thought (CoT) traces of in-distribution reasoning problems,\nbut such annotated data is costly to obtain for every problem of interest. We\nwant reasoning models to generalize beyond their training distribution, and\nideally to generalize compositionally: combine atomic reasoning skills to solve\nharder, unseen reasoning tasks. We take a step towards compositional\ngeneralization of reasoning skills when addressing a target compositional task\nthat has no labeled CoT data. We find that simply training models on CoT data\nof atomic tasks leads to limited generalization, but minimally modifying CoT\nformats of constituent atomic tasks to be composable can lead to improvements.\nWe can train \"atomic CoT\" models on the atomic tasks with Composable CoT data\nand combine them with multitask learning or model merging for better zero-shot\nperformance on the target compositional task. Such a combined model can be\nfurther bootstrapped on a small amount of compositional data using rejection\nsampling fine-tuning (RFT). Results on string operations and natural language\nskill compositions show that training LLMs on Composable CoT outperforms\nmultitask learning and continued fine-tuning baselines within a given training\ndata budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common approach for teaching large language models (LLMs) to reason is to\ntrain on chain-of-thought (CoT) traces of in-distribution reasoning problems,\nbut such annotated data is costly to obtain for every problem of interest. We\nwant reasoning models to generalize beyond their training distribution, and\nideally to generalize compositionally: combine atomic reasoning skills to solve\nharder, unseen reasoning tasks. We take a step towards compositional\ngeneralization of reasoning skills when addressing a target compositional task\nthat has no labeled CoT data. We find that simply training models on CoT data\nof atomic tasks leads to limited generalization, but minimally modifying CoT\nformats of constituent atomic tasks to be composable can lead to improvements.\nWe can train \"atomic CoT\" models on the atomic tasks with Composable CoT data\nand combine them with multitask learning or model merging for better zero-shot\nperformance on the target compositional task. Such a combined model can be\nfurther bootstrapped on a small amount of compositional data using rejection\nsampling fine-tuning (RFT). Results on string operations and natural language\nskill compositions show that training LLMs on Composable CoT outperforms\nmultitask learning and continued fine-tuning baselines within a given training\ndata budget."
                },
                "authors": [
                    {
                        "name": "Fangcong Yin"
                    },
                    {
                        "name": "Zeyu Leo Liu"
                    },
                    {
                        "name": "Liu Leqi"
                    },
                    {
                        "name": "Xi Ye"
                    },
                    {
                        "name": "Greg Durrett"
                    }
                ],
                "author_detail": {
                    "name": "Greg Durrett"
                },
                "author": "Greg Durrett",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22630v1",
                "updated": "2025-05-28T17:47:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    47,
                    52,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:47:52Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    47,
                    52,
                    2,
                    148,
                    0
                ],
                "title": "Stochastic Chameleons: Irrelevant Context Hallucinations Reveal\n  Class-Based (Mis)Generalization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Chameleons: Irrelevant Context Hallucinations Reveal\n  Class-Based (Mis)Generalization in LLMs"
                },
                "summary": "The widespread success of large language models (LLMs) on NLP benchmarks has\nbeen accompanied by concerns that LLMs function primarily as stochastic parrots\nthat reproduce texts similar to what they saw during pre-training, often\nerroneously. But what is the nature of their errors, and do these errors\nexhibit any regularities? In this work, we examine irrelevant context\nhallucinations, in which models integrate misleading contextual cues into their\npredictions. Through behavioral analysis, we show that these errors result from\na structured yet flawed mechanism that we term class-based (mis)generalization,\nin which models combine abstract class cues with features extracted from the\nquery or context to derive answers. Furthermore, mechanistic interpretability\nexperiments on Llama-3, Mistral, and Pythia across 39 factual recall relation\ntypes reveal that this behavior is reflected in the model's internal\ncomputations: (i) abstract class representations are constructed in lower\nlayers before being refined into specific answers in higher layers, (ii)\nfeature selection is governed by two competing circuits -- one prioritizing\ndirect query-based reasoning, the other incorporating contextual cues -- whose\nrelative influences determine the final output. Our findings provide a more\nnuanced perspective on the stochastic parrot argument: through form-based\ntraining, LLMs can exhibit generalization leveraging abstractions, albeit in\nunreliable ways based on contextual cues -- what we term stochastic chameleons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread success of large language models (LLMs) on NLP benchmarks has\nbeen accompanied by concerns that LLMs function primarily as stochastic parrots\nthat reproduce texts similar to what they saw during pre-training, often\nerroneously. But what is the nature of their errors, and do these errors\nexhibit any regularities? In this work, we examine irrelevant context\nhallucinations, in which models integrate misleading contextual cues into their\npredictions. Through behavioral analysis, we show that these errors result from\na structured yet flawed mechanism that we term class-based (mis)generalization,\nin which models combine abstract class cues with features extracted from the\nquery or context to derive answers. Furthermore, mechanistic interpretability\nexperiments on Llama-3, Mistral, and Pythia across 39 factual recall relation\ntypes reveal that this behavior is reflected in the model's internal\ncomputations: (i) abstract class representations are constructed in lower\nlayers before being refined into specific answers in higher layers, (ii)\nfeature selection is governed by two competing circuits -- one prioritizing\ndirect query-based reasoning, the other incorporating contextual cues -- whose\nrelative influences determine the final output. Our findings provide a more\nnuanced perspective on the stochastic parrot argument: through form-based\ntraining, LLMs can exhibit generalization leveraging abstractions, albeit in\nunreliable ways based on contextual cues -- what we term stochastic chameleons."
                },
                "authors": [
                    {
                        "name": "Ziling Cheng"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Marc-Antoine Rondeau"
                    },
                    {
                        "name": "Jackie Chi Kit Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Jackie Chi Kit Cheung"
                },
                "author": "Jackie Chi Kit Cheung",
                "arxiv_comment": "Accepted to ACL 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12764v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12764v3",
                "updated": "2025-05-28T17:47:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    47,
                    46,
                    2,
                    148,
                    0
                ],
                "published": "2025-04-17T09:01:16Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    9,
                    1,
                    16,
                    3,
                    107,
                    0
                ],
                "title": "GraphOmni: A Comprehensive and Extendable Benchmark Framework for Large\n  Language Models on Graph-theoretic Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphOmni: A Comprehensive and Extendable Benchmark Framework for Large\n  Language Models on Graph-theoretic Tasks"
                },
                "summary": "This paper introduces GraphOmni, a comprehensive benchmark designed to\nevaluate the reasoning capabilities of LLMs on graph-theoretic tasks\narticulated in natural language. GraphOmni encompasses diverse graph types,\nserialization formats, and prompting schemes, significantly exceeding prior\nefforts in both scope and depth. Through extensive systematic evaluation, we\nidentify critical interactions among these dimensions, demonstrating their\nsubstantial impact on model performance. Our experiments reveal that\nstate-of-the-art models like Claude-3.5 and o4-mini consistently outperform\nother models, yet even these leading models exhibit substantial room for\nimprovement. Performance variability is evident depending on the specific\ncombinations of factors we considered, underscoring the necessity of\ncomprehensive evaluations across these interconnected dimensions. Additionally,\nwe observe distinct impacts of serialization and prompting strategies between\nopen-source and closed-source models, encouraging the development of tailored\napproaches. Motivated by the findings, we also propose a reinforcement\nlearning-inspired framework that adaptively selects the optimal factors\ninfluencing LLM reasoning capabilities. This flexible and extendable benchmark\nnot only deepens our understanding of LLM performance on structured tasks but\nalso provides a robust foundation for advancing research in LLM-based graph\nreasoning. The code and datasets are available at\nhttps://github.com/GAI-Community/GraphOmni.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces GraphOmni, a comprehensive benchmark designed to\nevaluate the reasoning capabilities of LLMs on graph-theoretic tasks\narticulated in natural language. GraphOmni encompasses diverse graph types,\nserialization formats, and prompting schemes, significantly exceeding prior\nefforts in both scope and depth. Through extensive systematic evaluation, we\nidentify critical interactions among these dimensions, demonstrating their\nsubstantial impact on model performance. Our experiments reveal that\nstate-of-the-art models like Claude-3.5 and o4-mini consistently outperform\nother models, yet even these leading models exhibit substantial room for\nimprovement. Performance variability is evident depending on the specific\ncombinations of factors we considered, underscoring the necessity of\ncomprehensive evaluations across these interconnected dimensions. Additionally,\nwe observe distinct impacts of serialization and prompting strategies between\nopen-source and closed-source models, encouraging the development of tailored\napproaches. Motivated by the findings, we also propose a reinforcement\nlearning-inspired framework that adaptively selects the optimal factors\ninfluencing LLM reasoning capabilities. This flexible and extendable benchmark\nnot only deepens our understanding of LLM performance on structured tasks but\nalso provides a robust foundation for advancing research in LLM-based graph\nreasoning. The code and datasets are available at\nhttps://github.com/GAI-Community/GraphOmni."
                },
                "authors": [
                    {
                        "name": "Hao Xu"
                    },
                    {
                        "name": "Xiangru Jian"
                    },
                    {
                        "name": "Xinjian Zhao"
                    },
                    {
                        "name": "Wei Pang"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Suyuchen Wang"
                    },
                    {
                        "name": "Qixin Zhang"
                    },
                    {
                        "name": "Zhengyuan Dong"
                    },
                    {
                        "name": "Joao Monteiro"
                    },
                    {
                        "name": "Bang Liu"
                    },
                    {
                        "name": "Qiuzhuang Sun"
                    },
                    {
                        "name": "Tianshu Yu"
                    }
                ],
                "author_detail": {
                    "name": "Tianshu Yu"
                },
                "author": "Tianshu Yu",
                "arxiv_comment": "Project Page: https://gai-community.github.io/Graph-Omni/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12764v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12764v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22618v1",
                "updated": "2025-05-28T17:39:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:39:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding"
                },
                "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22617v1",
                "updated": "2025-05-28T17:38:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    38,
                    45,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:38:45Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    38,
                    45,
                    2,
                    148,
                    0
                ],
                "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language\n  Models"
                },
                "summary": "This paper aims to overcome a major obstacle in scaling RL for reasoning with\nLLMs, namely the collapse of policy entropy. Such phenomenon is consistently\nobserved across vast RL runs without entropy intervention, where the policy\nentropy dropped sharply at the early training stage, this diminished\nexploratory ability is always accompanied with the saturation of policy\nperformance. In practice, we establish a transformation equation R=-a*e^H+b\nbetween entropy H and downstream performance R. This empirical law strongly\nindicates that, the policy performance is traded from policy entropy, thus\nbottlenecked by its exhaustion, and the ceiling is fully predictable H=0,\nR=-a+b. Our finding necessitates entropy management for continuous exploration\ntoward scaling compute for RL. To this end, we investigate entropy dynamics\nboth theoretically and empirically. Our derivation highlights that, the change\nin policy entropy is driven by the covariance between action probability and\nthe change in logits, which is proportional to its advantage when using Policy\nGradient-like algorithms. Empirical study shows that, the values of covariance\nterm and entropy differences matched exactly, supporting the theoretical\nconclusion. Moreover, the covariance term stays mostly positive throughout\ntraining, further explaining why policy entropy would decrease monotonically.\nThrough understanding the mechanism behind entropy dynamics, we motivate to\ncontrol entropy by restricting the update of high-covariance tokens.\nSpecifically, we propose two simple yet effective techniques, namely Clip-Cov\nand KL-Cov, which clip and apply KL penalty to tokens with high covariances\nrespectively. Experiments show that these methods encourage exploration, thus\nhelping policy escape entropy collapse and achieve better downstream\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper aims to overcome a major obstacle in scaling RL for reasoning with\nLLMs, namely the collapse of policy entropy. Such phenomenon is consistently\nobserved across vast RL runs without entropy intervention, where the policy\nentropy dropped sharply at the early training stage, this diminished\nexploratory ability is always accompanied with the saturation of policy\nperformance. In practice, we establish a transformation equation R=-a*e^H+b\nbetween entropy H and downstream performance R. This empirical law strongly\nindicates that, the policy performance is traded from policy entropy, thus\nbottlenecked by its exhaustion, and the ceiling is fully predictable H=0,\nR=-a+b. Our finding necessitates entropy management for continuous exploration\ntoward scaling compute for RL. To this end, we investigate entropy dynamics\nboth theoretically and empirically. Our derivation highlights that, the change\nin policy entropy is driven by the covariance between action probability and\nthe change in logits, which is proportional to its advantage when using Policy\nGradient-like algorithms. Empirical study shows that, the values of covariance\nterm and entropy differences matched exactly, supporting the theoretical\nconclusion. Moreover, the covariance term stays mostly positive throughout\ntraining, further explaining why policy entropy would decrease monotonically.\nThrough understanding the mechanism behind entropy dynamics, we motivate to\ncontrol entropy by restricting the update of high-covariance tokens.\nSpecifically, we propose two simple yet effective techniques, namely Clip-Cov\nand KL-Cov, which clip and apply KL penalty to tokens with high covariances\nrespectively. Experiments show that these methods encourage exploration, thus\nhelping policy escape entropy collapse and achieve better downstream\nperformance."
                },
                "authors": [
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Jiacheng Chen"
                    },
                    {
                        "name": "Lifan Yuan"
                    },
                    {
                        "name": "Zhi Wang"
                    },
                    {
                        "name": "Yuxin Zuo"
                    },
                    {
                        "name": "Haozhan Li"
                    },
                    {
                        "name": "Yuchen Fan"
                    },
                    {
                        "name": "Huayu Chen"
                    },
                    {
                        "name": "Weize Chen"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Bowen Zhou"
                    },
                    {
                        "name": "Ning Ding"
                    }
                ],
                "author_detail": {
                    "name": "Ning Ding"
                },
                "author": "Ning Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11326v2",
                "updated": "2025-05-28T17:37:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    37,
                    39,
                    2,
                    148,
                    0
                ],
                "published": "2024-08-21T04:19:52Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    19,
                    52,
                    2,
                    234,
                    0
                ],
                "title": "Automating Thought of Search: A Journey Towards Soundness and\n  Completeness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Thought of Search: A Journey Towards Soundness and\n  Completeness"
                },
                "summary": "Large language models (LLMs) are being used to solve planning problems that\nrequire search. Most of the literature uses LLMs as world models to define the\nsearch space, forgoing soundness for the sake of flexibility. A recent work,\nThought of Search (ToS), proposed defining the search space with code, having\nLLMs produce that code. ToS requires a human in the loop, collaboratively\nproducing a sound successor function and goal test. The result, however, is\nworth the effort: all the tested datasets were solved with 100% accuracy.\nConsequently, there is great potential to automate the ToS process. We take a\nfirst major step towards automating ToS (AutoToS), taking the human out of the\nloop of interactions with the language model. AutoToS guides the language model\nstep by step towards the generation of sound and complete search components,\nthrough feedback from both generic and domain specific unit tests. We show that\nAutoToS is able to achieve 100% accuracy on all the evaluated domains with a\nsmall number of LLM calls.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are being used to solve planning problems that\nrequire search. Most of the literature uses LLMs as world models to define the\nsearch space, forgoing soundness for the sake of flexibility. A recent work,\nThought of Search (ToS), proposed defining the search space with code, having\nLLMs produce that code. ToS requires a human in the loop, collaboratively\nproducing a sound successor function and goal test. The result, however, is\nworth the effort: all the tested datasets were solved with 100% accuracy.\nConsequently, there is great potential to automate the ToS process. We take a\nfirst major step towards automating ToS (AutoToS), taking the human out of the\nloop of interactions with the language model. AutoToS guides the language model\nstep by step towards the generation of sound and complete search components,\nthrough feedback from both generic and domain specific unit tests. We show that\nAutoToS is able to achieve 100% accuracy on all the evaluated domains with a\nsmall number of LLM calls."
                },
                "authors": [
                    {
                        "name": "Daniel Cao"
                    },
                    {
                        "name": "Michael Katz"
                    },
                    {
                        "name": "Harsha Kokel"
                    },
                    {
                        "name": "Kavitha Srinivas"
                    },
                    {
                        "name": "Shirin Sohrabi"
                    }
                ],
                "author_detail": {
                    "name": "Shirin Sohrabi"
                },
                "author": "Shirin Sohrabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18116v2",
                "updated": "2025-05-28T17:31:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    31,
                    37,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-23T17:17:40Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    17,
                    17,
                    40,
                    4,
                    143,
                    0
                ],
                "title": "Bridging Supervised Learning and Reinforcement Learning in Math\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Supervised Learning and Reinforcement Learning in Math\n  Reasoning"
                },
                "summary": "Reinforcement Learning (RL) has played a central role in the recent surge of\nLLMs' math abilities by enabling self-improvement through binary verifier\nsignals. In contrast, Supervised Learning (SL) is rarely considered for such\nverification-driven training, largely due to its heavy reliance on reference\nanswers and inability to reflect on mistakes. In this work, we challenge the\nprevailing notion that self-improvement is exclusive to RL and propose\nNegative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to\nreflect on their failures and improve autonomously with no external teachers.\nIn online training, instead of throwing away self-generated negative answers,\nNFT constructs an implicit negative policy to model them. This implicit policy\nis parameterized with the same positive LLM we target to optimize on positive\ndata, enabling direct policy optimization on all LLMs' generations. We conduct\nexperiments on 7B and 32B models in math reasoning tasks. Results consistently\nshow that through the additional leverage of negative feedback, NFT\nsignificantly improves over SL baselines like Rejection sampling Fine-Tuning,\nmatching or even surpassing leading RL algorithms like GRPO and DAPO.\nFurthermore, we demonstrate that NFT and GRPO are actually equivalent in\nstrict-on-policy training, even though they originate from entirely different\ntheoretical foundations. Our experiments and theoretical findings bridge the\ngap between SL and RL methods in binary-feedback learning systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has played a central role in the recent surge of\nLLMs' math abilities by enabling self-improvement through binary verifier\nsignals. In contrast, Supervised Learning (SL) is rarely considered for such\nverification-driven training, largely due to its heavy reliance on reference\nanswers and inability to reflect on mistakes. In this work, we challenge the\nprevailing notion that self-improvement is exclusive to RL and propose\nNegative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to\nreflect on their failures and improve autonomously with no external teachers.\nIn online training, instead of throwing away self-generated negative answers,\nNFT constructs an implicit negative policy to model them. This implicit policy\nis parameterized with the same positive LLM we target to optimize on positive\ndata, enabling direct policy optimization on all LLMs' generations. We conduct\nexperiments on 7B and 32B models in math reasoning tasks. Results consistently\nshow that through the additional leverage of negative feedback, NFT\nsignificantly improves over SL baselines like Rejection sampling Fine-Tuning,\nmatching or even surpassing leading RL algorithms like GRPO and DAPO.\nFurthermore, we demonstrate that NFT and GRPO are actually equivalent in\nstrict-on-policy training, even though they originate from entirely different\ntheoretical foundations. Our experiments and theoretical findings bridge the\ngap between SL and RL methods in binary-feedback learning systems."
                },
                "authors": [
                    {
                        "name": "Huayu Chen"
                    },
                    {
                        "name": "Kaiwen Zheng"
                    },
                    {
                        "name": "Qinsheng Zhang"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Yin Cui"
                    },
                    {
                        "name": "Haotian Ye"
                    },
                    {
                        "name": "Tsung-Yi Lin"
                    },
                    {
                        "name": "Ming-Yu Liu"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Haoxiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoxiang Wang"
                },
                "author": "Haoxiang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00134v2",
                "updated": "2025-05-28T17:18:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    18,
                    49,
                    2,
                    148,
                    0
                ],
                "published": "2025-02-28T19:25:04Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    19,
                    25,
                    4,
                    4,
                    59,
                    0
                ],
                "title": "Personalized Causal Graph Reasoning for LLMs: A Case Study on Dietary\n  Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Causal Graph Reasoning for LLMs: A Case Study on Dietary\n  Recommendations"
                },
                "summary": "Large Language Models (LLMs) effectively leverage common-sense knowledge for\ngeneral reasoning, yet they struggle with personalized reasoning when tasked\nwith interpreting multifactor personal data. This limitation restricts their\napplicability in domains that require context-aware decision-making tailored to\nindividuals. This paper introduces Personalized Causal Graph Reasoning as an\nagentic framework that enhances LLM reasoning by incorporating personal causal\ngraphs derived from data of individuals. These graphs provide a foundation that\nguides the LLM's reasoning process. We evaluate it on a case study on\nnutrient-oriented dietary recommendations, which requires personal reasoning\ndue to the implicit unique dietary effects. We propose a counterfactual\nevaluation to estimate the efficiency of LLM-recommended foods for glucose\nmanagement. Results demonstrate that the proposed method efficiently provides\npersonalized dietary recommendations to reduce average glucose iAUC across\nthree time windows, which outperforms the previous approach. LLM-as-a-judge\nevaluation results indicate that our proposed method enhances personalization\nin the reasoning process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) effectively leverage common-sense knowledge for\ngeneral reasoning, yet they struggle with personalized reasoning when tasked\nwith interpreting multifactor personal data. This limitation restricts their\napplicability in domains that require context-aware decision-making tailored to\nindividuals. This paper introduces Personalized Causal Graph Reasoning as an\nagentic framework that enhances LLM reasoning by incorporating personal causal\ngraphs derived from data of individuals. These graphs provide a foundation that\nguides the LLM's reasoning process. We evaluate it on a case study on\nnutrient-oriented dietary recommendations, which requires personal reasoning\ndue to the implicit unique dietary effects. We propose a counterfactual\nevaluation to estimate the efficiency of LLM-recommended foods for glucose\nmanagement. Results demonstrate that the proposed method efficiently provides\npersonalized dietary recommendations to reduce average glucose iAUC across\nthree time windows, which outperforms the previous approach. LLM-as-a-judge\nevaluation results indicate that our proposed method enhances personalization\nin the reasoning process."
                },
                "authors": [
                    {
                        "name": "Zhongqi Yang"
                    },
                    {
                        "name": "Amir Rahmani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Rahmani"
                },
                "author": "Amir Rahmani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17284v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17284v5",
                "updated": "2025-05-28T17:16:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    16,
                    40,
                    2,
                    148,
                    0
                ],
                "published": "2024-11-26T10:13:39Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    13,
                    39,
                    1,
                    331,
                    0
                ],
                "title": "AutoElicit: Using Large Language Models for Expert Prior Elicitation in\n  Predictive Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoElicit: Using Large Language Models for Expert Prior Elicitation in\n  Predictive Modelling"
                },
                "summary": "Large language models (LLMs) acquire a breadth of information across various\ndomains. However, their computational complexity, cost, and lack of\ntransparency often hinder their direct application for predictive tasks where\nprivacy and interpretability are paramount. In fields such as healthcare,\nbiology, and finance, specialised and interpretable linear models still hold\nconsiderable value. In such domains, labelled data may be scarce or expensive\nto obtain. Well-specified prior distributions over model parameters can reduce\nthe sample complexity of learning through Bayesian inference; however,\neliciting expert priors can be time-consuming. We therefore introduce\nAutoElicit to extract knowledge from LLMs and construct priors for predictive\nmodels. We show these priors are informative and can be refined using natural\nlanguage. We perform a careful study contrasting AutoElicit with in-context\nlearning and demonstrate how to perform model selection between the two\nmethods. We find that AutoElicit yields priors that can substantially reduce\nerror over uninformative priors, using fewer labels, and consistently\noutperform in-context learning. We show that AutoElicit saves over 6 months of\nlabelling effort when building a new predictive model for urinary tract\ninfections from sensor recordings of people living with dementia.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) acquire a breadth of information across various\ndomains. However, their computational complexity, cost, and lack of\ntransparency often hinder their direct application for predictive tasks where\nprivacy and interpretability are paramount. In fields such as healthcare,\nbiology, and finance, specialised and interpretable linear models still hold\nconsiderable value. In such domains, labelled data may be scarce or expensive\nto obtain. Well-specified prior distributions over model parameters can reduce\nthe sample complexity of learning through Bayesian inference; however,\neliciting expert priors can be time-consuming. We therefore introduce\nAutoElicit to extract knowledge from LLMs and construct priors for predictive\nmodels. We show these priors are informative and can be refined using natural\nlanguage. We perform a careful study contrasting AutoElicit with in-context\nlearning and demonstrate how to perform model selection between the two\nmethods. We find that AutoElicit yields priors that can substantially reduce\nerror over uninformative priors, using fewer labels, and consistently\noutperform in-context learning. We show that AutoElicit saves over 6 months of\nlabelling effort when building a new predictive model for urinary tract\ninfections from sensor recordings of people living with dementia."
                },
                "authors": [
                    {
                        "name": "Alexander Capstick"
                    },
                    {
                        "name": "Rahul G. Krishnan"
                    },
                    {
                        "name": "Payam Barnaghi"
                    }
                ],
                "author_detail": {
                    "name": "Payam Barnaghi"
                },
                "author": "Payam Barnaghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17284v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17284v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03561v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03561v2",
                "updated": "2025-05-28T17:03:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    3,
                    17,
                    2,
                    148,
                    0
                ],
                "published": "2025-04-04T16:10:57Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    10,
                    57,
                    4,
                    94,
                    0
                ],
                "title": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge\n  Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge\n  Refinement"
                },
                "summary": "In the interaction between agents and their environments, agents expand their\ncapabilities by planning and executing actions. However, LLM-based agents face\nsubstantial challenges when deployed in novel environments or required to\nnavigate unconventional action spaces. To empower agents to autonomously\nexplore environments, optimize workflows, and enhance their understanding of\nactions, we propose SynWorld, a framework that allows agents to synthesize\npossible scenarios with multi-step action invocation within the action space\nand perform Monte Carlo Tree Search (MCTS) exploration to effectively refine\ntheir action knowledge in the current environment. Our experiments demonstrate\nthat SynWorld is an effective and general approach to learning action knowledge\nin new environments. Code is available at https://github.com/zjunlp/SynWorld.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the interaction between agents and their environments, agents expand their\ncapabilities by planning and executing actions. However, LLM-based agents face\nsubstantial challenges when deployed in novel environments or required to\nnavigate unconventional action spaces. To empower agents to autonomously\nexplore environments, optimize workflows, and enhance their understanding of\nactions, we propose SynWorld, a framework that allows agents to synthesize\npossible scenarios with multi-step action invocation within the action space\nand perform Monte Carlo Tree Search (MCTS) exploration to effectively refine\ntheir action knowledge in the current environment. Our experiments demonstrate\nthat SynWorld is an effective and general approach to learning action knowledge\nin new environments. Code is available at https://github.com/zjunlp/SynWorld."
                },
                "authors": [
                    {
                        "name": "Runnan Fang"
                    },
                    {
                        "name": "Xiaobin Wang"
                    },
                    {
                        "name": "Yuan Liang"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zekun Xi"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03561v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03561v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22591v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22591v1",
                "updated": "2025-05-28T17:02:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    2,
                    47,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:02:47Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    2,
                    47,
                    2,
                    148,
                    0
                ],
                "title": "Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical\n  Reasoning"
                },
                "summary": "Although large language models demonstrate strong performance across various\ndomains, they still struggle with numerous bad cases in mathematical reasoning.\nPrevious approaches to learning from errors synthesize training data by solely\nextrapolating from isolated bad cases, thereby failing to generalize the\nextensive patterns inherent within these cases. This paper presents\nSelf-Error-Instruct (SEI), a framework that addresses these model weaknesses\nand synthesizes more generalized targeted training data. Specifically, we\nexplore a target model on two mathematical datasets, GSM8K and MATH, to\npinpoint bad cases. Then, we generate error keyphrases for these cases based on\nthe instructor model's (GPT-4o) analysis and identify error types by clustering\nthese keyphrases. Next, we sample a few bad cases during each generation for\neach identified error type and input them into the instructor model, which\nsynthesizes additional training data using a self-instruct approach. This new\ndata is refined through a one-shot learning process to ensure that only the\nmost effective examples are kept. Finally, we use these curated data to\nfine-tune the target model, iteratively repeating the process to enhance\nperformance. We apply our framework to various models and observe improvements\nin their reasoning abilities across both in-domain and out-of-domain\nmathematics datasets. These results demonstrate the effectiveness of self-error\ninstruction in improving LLMs' mathematical reasoning through error\ngeneralization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models demonstrate strong performance across various\ndomains, they still struggle with numerous bad cases in mathematical reasoning.\nPrevious approaches to learning from errors synthesize training data by solely\nextrapolating from isolated bad cases, thereby failing to generalize the\nextensive patterns inherent within these cases. This paper presents\nSelf-Error-Instruct (SEI), a framework that addresses these model weaknesses\nand synthesizes more generalized targeted training data. Specifically, we\nexplore a target model on two mathematical datasets, GSM8K and MATH, to\npinpoint bad cases. Then, we generate error keyphrases for these cases based on\nthe instructor model's (GPT-4o) analysis and identify error types by clustering\nthese keyphrases. Next, we sample a few bad cases during each generation for\neach identified error type and input them into the instructor model, which\nsynthesizes additional training data using a self-instruct approach. This new\ndata is refined through a one-shot learning process to ensure that only the\nmost effective examples are kept. Finally, we use these curated data to\nfine-tune the target model, iteratively repeating the process to enhance\nperformance. We apply our framework to various models and observe improvements\nin their reasoning abilities across both in-domain and out-of-domain\nmathematics datasets. These results demonstrate the effectiveness of self-error\ninstruction in improving LLMs' mathematical reasoning through error\ngeneralization."
                },
                "authors": [
                    {
                        "name": "Erxin Yu"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Ming Liao"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Boyang Xue"
                    },
                    {
                        "name": "Minghui Xu"
                    },
                    {
                        "name": "Baojun Wang"
                    },
                    {
                        "name": "Lanqing Hong"
                    },
                    {
                        "name": "Fei Mi"
                    },
                    {
                        "name": "Lifeng Shang"
                    }
                ],
                "author_detail": {
                    "name": "Lifeng Shang"
                },
                "author": "Lifeng Shang",
                "arxiv_comment": "16 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22591v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22591v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19318v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19318v3",
                "updated": "2025-05-29T02:06:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    2,
                    6,
                    32,
                    3,
                    149,
                    0
                ],
                "published": "2025-01-31T17:15:33Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    15,
                    33,
                    4,
                    31,
                    0
                ],
                "title": "MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented\n  Reinforcement in Embodied Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented\n  Reinforcement in Embodied Systems"
                },
                "summary": "While large language models (LLMs) have shown promising capabilities as\nzero-shot planners for embodied agents, their inability to learn from\nexperience and build persistent mental models limits their robustness in\ncomplex open-world environments like Minecraft. We introduce MINDSTORES, an\nexperience-augmented planning framework that enables embodied agents to build\nand leverage mental models through natural interaction with their environment.\nDrawing inspiration from how humans construct and refine cognitive mental\nmodels, our approach extends existing zero-shot LLM planning by maintaining a\ndatabase of past experiences that informs future planning iterations. The key\ninnovation is representing accumulated experiences as natural language\nembeddings of (state, task, plan, outcome) tuples, which can then be\nefficiently retrieved and reasoned over by an LLM planner to generate insights\nand guide plan refinement for novel states and tasks. Through extensive\nexperiments in the MineDojo environment, a simulation environment for agents in\nMinecraft that provides low-level controls for Minecraft, we find that\nMINDSTORES learns and applies its knowledge significantly better than existing\nmemory-based LLM planners while maintaining the flexibility and generalization\nbenefits of zero-shot approaches, representing an important step toward more\ncapable embodied AI systems that can learn continuously through natural\nexperience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have shown promising capabilities as\nzero-shot planners for embodied agents, their inability to learn from\nexperience and build persistent mental models limits their robustness in\ncomplex open-world environments like Minecraft. We introduce MINDSTORES, an\nexperience-augmented planning framework that enables embodied agents to build\nand leverage mental models through natural interaction with their environment.\nDrawing inspiration from how humans construct and refine cognitive mental\nmodels, our approach extends existing zero-shot LLM planning by maintaining a\ndatabase of past experiences that informs future planning iterations. The key\ninnovation is representing accumulated experiences as natural language\nembeddings of (state, task, plan, outcome) tuples, which can then be\nefficiently retrieved and reasoned over by an LLM planner to generate insights\nand guide plan refinement for novel states and tasks. Through extensive\nexperiments in the MineDojo environment, a simulation environment for agents in\nMinecraft that provides low-level controls for Minecraft, we find that\nMINDSTORES learns and applies its knowledge significantly better than existing\nmemory-based LLM planners while maintaining the flexibility and generalization\nbenefits of zero-shot approaches, representing an important step toward more\ncapable embodied AI systems that can learn continuously through natural\nexperience."
                },
                "authors": [
                    {
                        "name": "Anirudh Chari"
                    },
                    {
                        "name": "Suraj Reddy"
                    },
                    {
                        "name": "Aditya Tiwari"
                    },
                    {
                        "name": "Richard Lian"
                    },
                    {
                        "name": "Brian Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Brian Zhou"
                },
                "author": "Brian Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19318v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19318v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22586v1",
                "updated": "2025-05-28T16:58:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    58,
                    23,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T16:58:23Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    58,
                    23,
                    2,
                    148,
                    0
                ],
                "title": "Precise In-Parameter Concept Erasure in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precise In-Parameter Concept Erasure in Large Language Models"
                },
                "summary": "Large language models (LLMs) often acquire knowledge during pretraining that\nis undesirable in downstream deployments, e.g., sensitive information or\ncopyrighted content. Existing approaches for removing such knowledge rely on\nfine-tuning, training low-rank adapters or fact-level editing, but these are\neither too coarse, too shallow, or ineffective. In this work, we propose PISCES\n(Precise In-parameter Suppression for Concept EraSure), a novel framework for\nprecisely erasing entire concepts from model parameters by directly editing\ndirections that encode them in parameter space. PISCES uses a disentangler\nmodel to decompose MLP vectors into interpretable features, identifies those\nassociated with a target concept using automated interpretability techniques,\nand removes them from model parameters. Experiments on Gemma 2 and Llama 3.1\nover various concepts show that PISCES achieves modest gains in efficacy over\nleading erasure methods, reducing accuracy on the target concept to as low as\n7.7%, while dramatically improving erasure specificity (by up to 31%) and\nrobustness (by up to 38%). Overall, these results demonstrate that\nfeature-based in-parameter editing enables a more precise and reliable approach\nfor removing conceptual knowledge in language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often acquire knowledge during pretraining that\nis undesirable in downstream deployments, e.g., sensitive information or\ncopyrighted content. Existing approaches for removing such knowledge rely on\nfine-tuning, training low-rank adapters or fact-level editing, but these are\neither too coarse, too shallow, or ineffective. In this work, we propose PISCES\n(Precise In-parameter Suppression for Concept EraSure), a novel framework for\nprecisely erasing entire concepts from model parameters by directly editing\ndirections that encode them in parameter space. PISCES uses a disentangler\nmodel to decompose MLP vectors into interpretable features, identifies those\nassociated with a target concept using automated interpretability techniques,\nand removes them from model parameters. Experiments on Gemma 2 and Llama 3.1\nover various concepts show that PISCES achieves modest gains in efficacy over\nleading erasure methods, reducing accuracy on the target concept to as low as\n7.7%, while dramatically improving erasure specificity (by up to 31%) and\nrobustness (by up to 38%). Overall, these results demonstrate that\nfeature-based in-parameter editing enables a more precise and reliable approach\nfor removing conceptual knowledge in language models."
                },
                "authors": [
                    {
                        "name": "Yoav Gur-Arieh"
                    },
                    {
                        "name": "Clara Suslik"
                    },
                    {
                        "name": "Yihuai Hong"
                    },
                    {
                        "name": "Fazl Barez"
                    },
                    {
                        "name": "Mor Geva"
                    }
                ],
                "author_detail": {
                    "name": "Mor Geva"
                },
                "author": "Mor Geva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19255v2",
                "updated": "2025-05-28T16:58:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    58,
                    13,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-25T18:23:39Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    18,
                    23,
                    39,
                    6,
                    145,
                    0
                ],
                "title": "VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on\n  Multimodal Tool Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on\n  Multimodal Tool Use"
                },
                "summary": "Reinforcement Learning Finetuning (RFT) has significantly advanced the\nreasoning capabilities of large language models (LLMs) by enabling long chains\nof thought, self-correction, and effective tool use. While recent works attempt\nto extend RFT to vision-language models (VLMs), these efforts largely produce\ntext-only reasoning conditioned on static image inputs, falling short of true\nmultimodal reasoning in the response. In contrast, test-time methods like\nVisual Sketchpad incorporate visual steps but lack training mechanisms.\n  We introduce VTool-R1, the first framework that trains VLMs to generate\nmultimodal chains of thought by interleaving text and intermediate visual\nreasoning steps. VTool-R1 integrates Python-based visual editing tools into the\nRFT process, enabling VLMs to learn when and how to generate visual reasoning\nsteps that benefit final reasoning. Trained with outcome-based rewards tied to\ntask accuracy, our approach elicits strategic visual tool use for reasoning\nwithout relying on process-based supervision. Experiments on structured visual\nquestion answering over charts and tables show that VTool-R1 enhances reasoning\nperformance by teaching VLMs to \"think with images\" and generate multimodal\nchain of thoughts with tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning Finetuning (RFT) has significantly advanced the\nreasoning capabilities of large language models (LLMs) by enabling long chains\nof thought, self-correction, and effective tool use. While recent works attempt\nto extend RFT to vision-language models (VLMs), these efforts largely produce\ntext-only reasoning conditioned on static image inputs, falling short of true\nmultimodal reasoning in the response. In contrast, test-time methods like\nVisual Sketchpad incorporate visual steps but lack training mechanisms.\n  We introduce VTool-R1, the first framework that trains VLMs to generate\nmultimodal chains of thought by interleaving text and intermediate visual\nreasoning steps. VTool-R1 integrates Python-based visual editing tools into the\nRFT process, enabling VLMs to learn when and how to generate visual reasoning\nsteps that benefit final reasoning. Trained with outcome-based rewards tied to\ntask accuracy, our approach elicits strategic visual tool use for reasoning\nwithout relying on process-based supervision. Experiments on structured visual\nquestion answering over charts and tables show that VTool-R1 enhances reasoning\nperformance by teaching VLMs to \"think with images\" and generate multimodal\nchain of thoughts with tools."
                },
                "authors": [
                    {
                        "name": "Mingyuan Wu"
                    },
                    {
                        "name": "Jingcheng Yang"
                    },
                    {
                        "name": "Jize Jiang"
                    },
                    {
                        "name": "Meitang Li"
                    },
                    {
                        "name": "Kaizhuo Yan"
                    },
                    {
                        "name": "Hanchao Yu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Chengxiang Zhai"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    }
                ],
                "author_detail": {
                    "name": "Klara Nahrstedt"
                },
                "author": "Klara Nahrstedt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22584v1",
                "updated": "2025-05-28T16:56:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    56,
                    41,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T16:56:41Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    56,
                    41,
                    2,
                    148,
                    0
                ],
                "title": "DocReRank: Single-Page Hard Negative Query Generation for Training\n  Multi-Modal RAG Rerankers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DocReRank: Single-Page Hard Negative Query Generation for Training\n  Multi-Modal RAG Rerankers"
                },
                "summary": "Rerankers play a critical role in multimodal Retrieval-Augmented Generation\n(RAG) by refining ranking of an initial set of retrieved documents. Rerankers\nare typically trained using hard negative mining, whose goal is to select pages\nfor each query which rank high, but are actually irrelevant. However, this\nselection process is typically passive and restricted to what the retriever can\nfind in the available corpus, leading to several inherent limitations. These\ninclude: limited diversity, negative examples which are often not hard enough,\nlow controllability, and frequent false negatives which harm training. Our\npaper proposes an alternative approach: Single-Page Hard Negative Query\nGeneration, which goes the other way around. Instead of retrieving negative\npages per query, we generate hard negative queries per page. Using an automated\nLLM-VLM pipeline, and given a page and its positive query, we create hard\nnegatives by rephrasing the query to be as similar as possible in form and\ncontext, yet not answerable from the page. This paradigm enables fine-grained\ncontrol over the generated queries, resulting in diverse, hard, and targeted\nnegatives. It also supports efficient false negative verification. Our\nexperiments show that rerankers trained with data generated using our approach\noutperform existing models and significantly improve retrieval performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rerankers play a critical role in multimodal Retrieval-Augmented Generation\n(RAG) by refining ranking of an initial set of retrieved documents. Rerankers\nare typically trained using hard negative mining, whose goal is to select pages\nfor each query which rank high, but are actually irrelevant. However, this\nselection process is typically passive and restricted to what the retriever can\nfind in the available corpus, leading to several inherent limitations. These\ninclude: limited diversity, negative examples which are often not hard enough,\nlow controllability, and frequent false negatives which harm training. Our\npaper proposes an alternative approach: Single-Page Hard Negative Query\nGeneration, which goes the other way around. Instead of retrieving negative\npages per query, we generate hard negative queries per page. Using an automated\nLLM-VLM pipeline, and given a page and its positive query, we create hard\nnegatives by rephrasing the query to be as similar as possible in form and\ncontext, yet not answerable from the page. This paradigm enables fine-grained\ncontrol over the generated queries, resulting in diverse, hard, and targeted\nnegatives. It also supports efficient false negative verification. Our\nexperiments show that rerankers trained with data generated using our approach\noutperform existing models and significantly improve retrieval performance."
                },
                "authors": [
                    {
                        "name": "Navve Wasserman"
                    },
                    {
                        "name": "Oliver Heinimann"
                    },
                    {
                        "name": "Yuval Golbari"
                    },
                    {
                        "name": "Tal Zimbalist"
                    },
                    {
                        "name": "Eli Schwartz"
                    },
                    {
                        "name": "Michal Irani"
                    }
                ],
                "author_detail": {
                    "name": "Michal Irani"
                },
                "author": "Michal Irani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22582v1",
                "updated": "2025-05-28T16:54:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    54,
                    53,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T16:54:53Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    54,
                    53,
                    2,
                    148,
                    0
                ],
                "title": "Less, but Better: Efficient Multilingual Expansion for LLMs via\n  Layer-wise Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less, but Better: Efficient Multilingual Expansion for LLMs via\n  Layer-wise Mixture-of-Experts"
                },
                "summary": "Continually expanding new languages for existing large language models (LLMs)\nis a promising yet challenging approach to building powerful multilingual LLMs.\nThe biggest challenge is to make the model continuously learn new languages\nwhile preserving the proficient ability of old languages. To achieve this,\nrecent work utilizes the Mixture-of-Experts (MoE) architecture to expand new\nlanguages by adding new experts and avoid catastrophic forgetting of old\nlanguages by routing corresponding tokens to the original model backbone (old\nexperts). Although intuitive, this kind of method is parameter-costly when\nexpanding new languages and still inevitably impacts the performance of old\nlanguages. To address these limitations, we analyze the language\ncharacteristics of different layers in LLMs and propose a layer-wise expert\nallocation algorithm (LayerMoE) to determine the appropriate number of new\nexperts for each layer. Specifically, we find different layers in LLMs exhibit\ndifferent representation similarities between languages and then utilize the\nsimilarity as the indicator to allocate experts for each layer, i.e., the\nhigher similarity, the fewer experts. Additionally, to further mitigate the\nforgetting of old languages, we add a classifier in front of the router network\non the layers with higher similarity to guide the routing of old language\ntokens. Experimental results show that our method outperforms the previous\nstate-of-the-art baseline with 60% fewer experts in the single-expansion\nsetting and with 33.3% fewer experts in the lifelong-expansion setting,\ndemonstrating the effectiveness of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continually expanding new languages for existing large language models (LLMs)\nis a promising yet challenging approach to building powerful multilingual LLMs.\nThe biggest challenge is to make the model continuously learn new languages\nwhile preserving the proficient ability of old languages. To achieve this,\nrecent work utilizes the Mixture-of-Experts (MoE) architecture to expand new\nlanguages by adding new experts and avoid catastrophic forgetting of old\nlanguages by routing corresponding tokens to the original model backbone (old\nexperts). Although intuitive, this kind of method is parameter-costly when\nexpanding new languages and still inevitably impacts the performance of old\nlanguages. To address these limitations, we analyze the language\ncharacteristics of different layers in LLMs and propose a layer-wise expert\nallocation algorithm (LayerMoE) to determine the appropriate number of new\nexperts for each layer. Specifically, we find different layers in LLMs exhibit\ndifferent representation similarities between languages and then utilize the\nsimilarity as the indicator to allocate experts for each layer, i.e., the\nhigher similarity, the fewer experts. Additionally, to further mitigate the\nforgetting of old languages, we add a classifier in front of the router network\non the layers with higher similarity to guide the routing of old language\ntokens. Experimental results show that our method outperforms the previous\nstate-of-the-art baseline with 60% fewer experts in the single-expansion\nsetting and with 33.3% fewer experts in the lifelong-expansion setting,\ndemonstrating the effectiveness of our method."
                },
                "authors": [
                    {
                        "name": "Xue Zhang"
                    },
                    {
                        "name": "Yunlong Liang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Songming Zhang"
                    },
                    {
                        "name": "Yufeng Chen"
                    },
                    {
                        "name": "Jinan Xu"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "arxiv_comment": "ACL 2025 (Main), 16 pages, 5 figures, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22572v1",
                "updated": "2025-05-28T16:46:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    46,
                    55,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T16:46:55Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    46,
                    55,
                    2,
                    148,
                    0
                ],
                "title": "Fusion Steering: Prompt-Specific Activation Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusion Steering: Prompt-Specific Activation Control"
                },
                "summary": "We present Fusion Steering, an activation steering methodology that improves\nfactual accuracy in large language models (LLMs) for question-answering (QA)\ntasks. This approach introduces flexible steering configurations, including\nfull-layer steering and segmented steering. Unlike traditional methods\nconstrained to single-layer or fixed-layer operations, Fusion Steering employs\ndynamic injection of prompt-specific activation deltas across all transformer\nlayers. These activation deltas are derived from reference completions that\ncombine the ground-truth answer with a model-generated explanation to\nfacilitate semantically enriched, example-specific steering. The injection\nweights are optimized per prompt using Optuna, targeting a joint objective that\nbalances token overlap (factual alignment) and perplexity (fluency proxy).\nEvaluation employs a composite score integrating token overlap and LLM-graded\nquality, encompassing factual accuracy, coherence, and relevance. Empirical\nresults on 260 SimpleQA prompts (selected from 500 where the baseline failed)\nshowcase the efficacy of segmented steering. Using Gemma-2-2B-IT with 8-bit\nquantization, segmented steering achieves an accuracy of 25.4% (outputs scoring\n$\\geq 0.6$), outperforming the baseline at 3.5% and full-layer steering at\n16.2%. Under the stricter SimpleQA rubric, segmented steering boosts fully\ncorrect responses from 0.0% to 13.1%. These findings highlight the strengths of\nsegmented, dynamic intervention strategies and the promise of per-prompt,\nfull-network activation control. Fusion Steering is also amenable to sparse\nrepresentations, such as Neuronpedia or sparse crosscoders, suggesting a\npromising direction for interpretable and scalable activation-level control in\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Fusion Steering, an activation steering methodology that improves\nfactual accuracy in large language models (LLMs) for question-answering (QA)\ntasks. This approach introduces flexible steering configurations, including\nfull-layer steering and segmented steering. Unlike traditional methods\nconstrained to single-layer or fixed-layer operations, Fusion Steering employs\ndynamic injection of prompt-specific activation deltas across all transformer\nlayers. These activation deltas are derived from reference completions that\ncombine the ground-truth answer with a model-generated explanation to\nfacilitate semantically enriched, example-specific steering. The injection\nweights are optimized per prompt using Optuna, targeting a joint objective that\nbalances token overlap (factual alignment) and perplexity (fluency proxy).\nEvaluation employs a composite score integrating token overlap and LLM-graded\nquality, encompassing factual accuracy, coherence, and relevance. Empirical\nresults on 260 SimpleQA prompts (selected from 500 where the baseline failed)\nshowcase the efficacy of segmented steering. Using Gemma-2-2B-IT with 8-bit\nquantization, segmented steering achieves an accuracy of 25.4% (outputs scoring\n$\\geq 0.6$), outperforming the baseline at 3.5% and full-layer steering at\n16.2%. Under the stricter SimpleQA rubric, segmented steering boosts fully\ncorrect responses from 0.0% to 13.1%. These findings highlight the strengths of\nsegmented, dynamic intervention strategies and the promise of per-prompt,\nfull-network activation control. Fusion Steering is also amenable to sparse\nrepresentations, such as Neuronpedia or sparse crosscoders, suggesting a\npromising direction for interpretable and scalable activation-level control in\nLLMs."
                },
                "authors": [
                    {
                        "name": "Waldemar Chang"
                    },
                    {
                        "name": "Alhassan Yasin"
                    }
                ],
                "author_detail": {
                    "name": "Alhassan Yasin"
                },
                "author": "Alhassan Yasin",
                "arxiv_comment": "14 pages, 4 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22571v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22571v2",
                "updated": "2025-05-29T01:52:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    1,
                    52,
                    23,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-28T16:46:31Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    46,
                    31,
                    2,
                    148,
                    0
                ],
                "title": "Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified\n  Retrieval-Augmented Generation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified\n  Retrieval-Augmented Generation Systems"
                },
                "summary": "This paper presents a novel approach for unified retrieval-augmented\ngeneration (RAG) systems using the recent emerging large language model (LLM)\nagent concept. Specifically, Agent LLM, which utilizes LLM as fundamental\ncontrollers, has become a promising approach to enable the interpretability of\nRAG tasks, especially for complex reasoning question-answering systems (e.g.,\nmulti-hop queries). Nonetheless, previous works mainly focus on solving RAG\nsystems with either single-hop or multi-hop approaches separately, which limits\nthe application of those approaches to real-world applications. In this study,\nwe propose a trainable agent framework called Agent-UniRAG for unified\nretrieval-augmented LLM systems, which enhances the effectiveness and\ninterpretability of RAG systems. The main idea is to design an LLM agent\nframework to solve RAG tasks step-by-step based on the complexity of the\ninputs, simultaneously including single-hop and multi-hop queries in an\nend-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset\nto enable the proposed agent framework for small open-source LLMs (e.g.,\nLlama-3-8B). The results show comparable performances with closed-source and\nlarger open-source LLMs across various RAG benchmarks. Our source code and\ndataset are publicly available for further exploitation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach for unified retrieval-augmented\ngeneration (RAG) systems using the recent emerging large language model (LLM)\nagent concept. Specifically, Agent LLM, which utilizes LLM as fundamental\ncontrollers, has become a promising approach to enable the interpretability of\nRAG tasks, especially for complex reasoning question-answering systems (e.g.,\nmulti-hop queries). Nonetheless, previous works mainly focus on solving RAG\nsystems with either single-hop or multi-hop approaches separately, which limits\nthe application of those approaches to real-world applications. In this study,\nwe propose a trainable agent framework called Agent-UniRAG for unified\nretrieval-augmented LLM systems, which enhances the effectiveness and\ninterpretability of RAG systems. The main idea is to design an LLM agent\nframework to solve RAG tasks step-by-step based on the complexity of the\ninputs, simultaneously including single-hop and multi-hop queries in an\nend-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset\nto enable the proposed agent framework for small open-source LLMs (e.g.,\nLlama-3-8B). The results show comparable performances with closed-source and\nlarger open-source LLMs across various RAG benchmarks. Our source code and\ndataset are publicly available for further exploitation."
                },
                "authors": [
                    {
                        "name": "Hoang Pham"
                    },
                    {
                        "name": "Thuy-Duong Nguyen"
                    },
                    {
                        "name": "Khac-Hoai Nam Bui"
                    }
                ],
                "author_detail": {
                    "name": "Khac-Hoai Nam Bui"
                },
                "author": "Khac-Hoai Nam Bui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22571v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22571v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22566v1",
                "updated": "2025-05-28T16:43:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    43,
                    1,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T16:43:01Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    43,
                    1,
                    2,
                    148,
                    0
                ],
                "title": "Universal Visuo-Tactile Video Understanding for Embodied Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Visuo-Tactile Video Understanding for Embodied Interaction"
                },
                "summary": "Tactile perception is essential for embodied agents to understand physical\nattributes of objects that cannot be determined through visual inspection\nalone. While existing approaches have made progress in visual and language\nmodalities for physical understanding, they fail to effectively incorporate\ntactile information that provides crucial haptic feedback for real-world\ninteraction. In this paper, we present VTV-LLM, the first multi-modal large\nlanguage model for universal Visuo-Tactile Video (VTV) understanding that\nbridges the gap between tactile perception and natural language. To address the\nchallenges of cross-sensor and cross-modal integration, we contribute VTV150K,\na comprehensive dataset comprising 150,000 video frames from 100 diverse\nobjects captured across three different tactile sensors (GelSight Mini, DIGIT,\nand Tac3D), annotated with four fundamental tactile attributes (hardness,\nprotrusion, elasticity, and friction). We develop a novel three-stage training\nparadigm that includes VTV enhancement for robust visuo-tactile representation,\nVTV-text alignment for cross-modal correspondence, and text prompt finetuning\nfor natural language generation. Our framework enables sophisticated tactile\nreasoning capabilities including feature assessment, comparative analysis,\nscenario-based decision making and so on. Experimental evaluations demonstrate\nthat VTV-LLM achieves superior performance in tactile video understanding\ntasks, establishing a foundation for more intuitive human-machine interaction\nin tactile domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tactile perception is essential for embodied agents to understand physical\nattributes of objects that cannot be determined through visual inspection\nalone. While existing approaches have made progress in visual and language\nmodalities for physical understanding, they fail to effectively incorporate\ntactile information that provides crucial haptic feedback for real-world\ninteraction. In this paper, we present VTV-LLM, the first multi-modal large\nlanguage model for universal Visuo-Tactile Video (VTV) understanding that\nbridges the gap between tactile perception and natural language. To address the\nchallenges of cross-sensor and cross-modal integration, we contribute VTV150K,\na comprehensive dataset comprising 150,000 video frames from 100 diverse\nobjects captured across three different tactile sensors (GelSight Mini, DIGIT,\nand Tac3D), annotated with four fundamental tactile attributes (hardness,\nprotrusion, elasticity, and friction). We develop a novel three-stage training\nparadigm that includes VTV enhancement for robust visuo-tactile representation,\nVTV-text alignment for cross-modal correspondence, and text prompt finetuning\nfor natural language generation. Our framework enables sophisticated tactile\nreasoning capabilities including feature assessment, comparative analysis,\nscenario-based decision making and so on. Experimental evaluations demonstrate\nthat VTV-LLM achieves superior performance in tactile video understanding\ntasks, establishing a foundation for more intuitive human-machine interaction\nin tactile domains."
                },
                "authors": [
                    {
                        "name": "Yifan Xie"
                    },
                    {
                        "name": "Mingyang Li"
                    },
                    {
                        "name": "Shoujie Li"
                    },
                    {
                        "name": "Xingting Li"
                    },
                    {
                        "name": "Guangyu Chen"
                    },
                    {
                        "name": "Fei Ma"
                    },
                    {
                        "name": "Fei Richard Yu"
                    },
                    {
                        "name": "Wenbo Ding"
                    }
                ],
                "author_detail": {
                    "name": "Wenbo Ding"
                },
                "author": "Wenbo Ding",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12051v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12051v3",
                "updated": "2025-05-28T16:40:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    40,
                    24,
                    2,
                    148,
                    0
                ],
                "published": "2025-03-15T08:54:25Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    8,
                    54,
                    25,
                    5,
                    74,
                    0
                ],
                "title": "TLUE: A Tibetan Language Understanding Evaluation Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TLUE: A Tibetan Language Understanding Evaluation Benchmark"
                },
                "summary": "Large language models (LLMs) have made tremendous progress in recent years,\nbut low-resource languages, such as Tibetan, remain significantly\nunderrepresented in their evaluation. Despite Tibetan being spoken by over\nseven million people, it has largely been neglected in the development and\nassessment of LLMs. To address this gap, we present TLUE (A Tibetan Language\nUnderstanding Evaluation Benchmark), the first large-scale benchmark for\nassessing LLMs' capabilities in Tibetan. TLUE comprises two major components:\n(1) a comprehensive multi-task understanding benchmark spanning 5 domains and\n67 subdomains, and (2) a safety benchmark covering 7 subdomains. We evaluate a\ndiverse set of state-of-the-art LLMs. Experimental results demonstrate that\nmost LLMs perform below the random baseline, highlighting the considerable\nchallenges LLMs face in processing Tibetan, a low-resource language. TLUE\nprovides an essential foundation for driving future research and progress in\nTibetan language understanding and underscores the need for greater inclusivity\nin LLM development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made tremendous progress in recent years,\nbut low-resource languages, such as Tibetan, remain significantly\nunderrepresented in their evaluation. Despite Tibetan being spoken by over\nseven million people, it has largely been neglected in the development and\nassessment of LLMs. To address this gap, we present TLUE (A Tibetan Language\nUnderstanding Evaluation Benchmark), the first large-scale benchmark for\nassessing LLMs' capabilities in Tibetan. TLUE comprises two major components:\n(1) a comprehensive multi-task understanding benchmark spanning 5 domains and\n67 subdomains, and (2) a safety benchmark covering 7 subdomains. We evaluate a\ndiverse set of state-of-the-art LLMs. Experimental results demonstrate that\nmost LLMs perform below the random baseline, highlighting the considerable\nchallenges LLMs face in processing Tibetan, a low-resource language. TLUE\nprovides an essential foundation for driving future research and progress in\nTibetan language understanding and underscores the need for greater inclusivity\nin LLM development."
                },
                "authors": [
                    {
                        "name": "Fan Gao"
                    },
                    {
                        "name": "Cheng Huang"
                    },
                    {
                        "name": "Nyima Tashi"
                    },
                    {
                        "name": "Xiangxiang Wang"
                    },
                    {
                        "name": "Thupten Tsering"
                    },
                    {
                        "name": "Ban Ma-bao"
                    },
                    {
                        "name": "Renzeg Duojie"
                    },
                    {
                        "name": "Gadeng Luosang"
                    },
                    {
                        "name": "Rinchen Dongrub"
                    },
                    {
                        "name": "Dorje Tashi"
                    },
                    {
                        "name": "Hao Wang Xiao Feng"
                    },
                    {
                        "name": "Yongbin Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Yu"
                },
                "author": "Yongbin Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12051v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22563v1",
                "updated": "2025-05-28T16:40:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    40,
                    6,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T16:40:06Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    40,
                    6,
                    2,
                    148,
                    0
                ],
                "title": "Do Large Language Models Think Like the Brain? Sentence-Level Evidence\n  from fMRI and Hierarchical Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Think Like the Brain? Sentence-Level Evidence\n  from fMRI and Hierarchical Embeddings"
                },
                "summary": "Understanding whether large language models (LLMs) and the human brain\nconverge on similar computational principles remains a fundamental and\nimportant question in cognitive neuroscience and AI. Do the brain-like patterns\nobserved in LLMs emerge simply from scaling, or do they reflect deeper\nalignment with the architecture of human language processing? This study\nfocuses on the sentence-level neural mechanisms of language models,\nsystematically investigating how hierarchical representations in LLMs align\nwith the dynamic neural responses during human sentence comprehension. By\ncomparing hierarchical embeddings from 14 publicly available LLMs with fMRI\ndata collected from participants, who were exposed to a naturalistic narrative\nstory, we constructed sentence-level neural prediction models to precisely\nidentify the model layers most significantly correlated with brain region\nactivations. Results show that improvements in model performance drive the\nevolution of representational architectures toward brain-like hierarchies,\nparticularly achieving stronger functional and anatomical correspondence at\nhigher semantic abstraction levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding whether large language models (LLMs) and the human brain\nconverge on similar computational principles remains a fundamental and\nimportant question in cognitive neuroscience and AI. Do the brain-like patterns\nobserved in LLMs emerge simply from scaling, or do they reflect deeper\nalignment with the architecture of human language processing? This study\nfocuses on the sentence-level neural mechanisms of language models,\nsystematically investigating how hierarchical representations in LLMs align\nwith the dynamic neural responses during human sentence comprehension. By\ncomparing hierarchical embeddings from 14 publicly available LLMs with fMRI\ndata collected from participants, who were exposed to a naturalistic narrative\nstory, we constructed sentence-level neural prediction models to precisely\nidentify the model layers most significantly correlated with brain region\nactivations. Results show that improvements in model performance drive the\nevolution of representational architectures toward brain-like hierarchies,\nparticularly achieving stronger functional and anatomical correspondence at\nhigher semantic abstraction levels."
                },
                "authors": [
                    {
                        "name": "Yu Lei"
                    },
                    {
                        "name": "Xingyang Ge"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Yiming Yang"
                    },
                    {
                        "name": "Bolei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Bolei Ma"
                },
                "author": "Bolei Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22552v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22552v1",
                "updated": "2025-05-28T16:34:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    34,
                    14,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T16:34:14Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    34,
                    14,
                    2,
                    148,
                    0
                ],
                "title": "ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation\n  with Lightweight Specialized LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation\n  with Lightweight Specialized LLM"
                },
                "summary": "Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of\nlarge language models (LLMs) is an emerging research challenge in claim\nverification. While KGs provide structured, semantically rich representations\nwell-suited for reasoning, most existing verification methods rely on\nunstructured text corpora, limiting their ability to effectively leverage KGs.\nAdditionally, despite possessing strong reasoning abilities, modern LLMs\nstruggle with multi-step modular pipelines and reasoning over KGs without\nadaptation. To address these challenges, we propose ClaimPKG, an end-to-end\nframework that seamlessly integrates LLM reasoning with structured knowledge\nfrom KGs. Specifically, the main idea of ClaimPKG is to employ a lightweight,\nspecialized LLM to represent the input claim as pseudo-subgraphs, guiding a\ndedicated subgraph retrieval module to identify relevant KG subgraphs. These\nretrieved subgraphs are then processed by a general-purpose LLM to produce the\nfinal verdict and justification. Extensive experiments on the FactKG dataset\ndemonstrate that ClaimPKG achieves state-of-the-art performance, outperforming\nstrong baselines in this research field by 9%-12% accuracy points across\nmultiple categories. Furthermore, ClaimPKG exhibits zero-shot generalizability\nto unstructured datasets such as HoVer and FEVEROUS, effectively combining\nstructured knowledge from KGs with LLM reasoning across various LLM backbones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of\nlarge language models (LLMs) is an emerging research challenge in claim\nverification. While KGs provide structured, semantically rich representations\nwell-suited for reasoning, most existing verification methods rely on\nunstructured text corpora, limiting their ability to effectively leverage KGs.\nAdditionally, despite possessing strong reasoning abilities, modern LLMs\nstruggle with multi-step modular pipelines and reasoning over KGs without\nadaptation. To address these challenges, we propose ClaimPKG, an end-to-end\nframework that seamlessly integrates LLM reasoning with structured knowledge\nfrom KGs. Specifically, the main idea of ClaimPKG is to employ a lightweight,\nspecialized LLM to represent the input claim as pseudo-subgraphs, guiding a\ndedicated subgraph retrieval module to identify relevant KG subgraphs. These\nretrieved subgraphs are then processed by a general-purpose LLM to produce the\nfinal verdict and justification. Extensive experiments on the FactKG dataset\ndemonstrate that ClaimPKG achieves state-of-the-art performance, outperforming\nstrong baselines in this research field by 9%-12% accuracy points across\nmultiple categories. Furthermore, ClaimPKG exhibits zero-shot generalizability\nto unstructured datasets such as HoVer and FEVEROUS, effectively combining\nstructured knowledge from KGs with LLM reasoning across various LLM backbones."
                },
                "authors": [
                    {
                        "name": "Hoang Pham"
                    },
                    {
                        "name": "Thanh-Do Nguyen"
                    },
                    {
                        "name": "Khac-Hoai Nam Bui"
                    }
                ],
                "author_detail": {
                    "name": "Khac-Hoai Nam Bui"
                },
                "author": "Khac-Hoai Nam Bui",
                "arxiv_comment": "Accepted by ACL 2025 findings",
                "arxiv_journal_ref": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22552v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22552v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22548v1",
                "updated": "2025-05-28T16:32:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    32,
                    16,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T16:32:16Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    32,
                    16,
                    2,
                    148,
                    0
                ],
                "title": "Emotion-o1: Adaptive Long Reasoning for Emotion Understanding in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotion-o1: Adaptive Long Reasoning for Emotion Understanding in LLMs"
                },
                "summary": "Emotion understanding includes basic tasks (e.g., sentiment/emotion\nclassification) and advanced tasks (e.g., sarcasm/humor detection). Current\nmethods rely on fixed-length CoT reasoning, failing to adapt to the varying\ncomplexity of emotions. We propose a task-adaptive reasoning framework that\nemploys DeepSeek-R1 to generate variable-length reasoning chains for different\nemotion tasks. By combining fine-tuning with reinforcement learning, we design\na composite reward function that balances four objectives: prediction accuracy,\nadaptive reasoning depth control, structural diversity in reasoning paths, and\nsuppression of repetitive logic. This approach achieves dynamic\ncontext-sensitive inference while enabling LLMs to autonomously develop deep\nreasoning capabilities. Experimental results demonstrate consistent\nimprovements in both Acc and F1 scores across four tasks: emotion, sentiment,\nhumor, and sarcasm. Notably, peak enhancements reached 3.56% F1 (2.76% Acc) for\nbasic tasks and 37.95% F1 (23.14% Acc) for advanced tasks. Our work bridges\nrigid CoT reasoning and emotional complexity through adaptive-depth analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotion understanding includes basic tasks (e.g., sentiment/emotion\nclassification) and advanced tasks (e.g., sarcasm/humor detection). Current\nmethods rely on fixed-length CoT reasoning, failing to adapt to the varying\ncomplexity of emotions. We propose a task-adaptive reasoning framework that\nemploys DeepSeek-R1 to generate variable-length reasoning chains for different\nemotion tasks. By combining fine-tuning with reinforcement learning, we design\na composite reward function that balances four objectives: prediction accuracy,\nadaptive reasoning depth control, structural diversity in reasoning paths, and\nsuppression of repetitive logic. This approach achieves dynamic\ncontext-sensitive inference while enabling LLMs to autonomously develop deep\nreasoning capabilities. Experimental results demonstrate consistent\nimprovements in both Acc and F1 scores across four tasks: emotion, sentiment,\nhumor, and sarcasm. Notably, peak enhancements reached 3.56% F1 (2.76% Acc) for\nbasic tasks and 37.95% F1 (23.14% Acc) for advanced tasks. Our work bridges\nrigid CoT reasoning and emotional complexity through adaptive-depth analysis."
                },
                "authors": [
                    {
                        "name": "Changhao Song"
                    },
                    {
                        "name": "Yazhou Zhang"
                    },
                    {
                        "name": "Peng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Zhang"
                },
                "author": "Peng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22541v1",
                "updated": "2025-05-28T16:23:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    23,
                    48,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T16:23:48Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    23,
                    48,
                    2,
                    148,
                    0
                ],
                "title": "A Human-Centric Approach to Explainable AI for Personalized Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Human-Centric Approach to Explainable AI for Personalized Education"
                },
                "summary": "Deep neural networks form the backbone of artificial intelligence research,\nwith potential to transform the human experience in areas ranging from\nautonomous driving to personal assistants, healthcare to education. However,\ntheir integration into the daily routines of real-world classrooms remains\nlimited. It is not yet common for a teacher to assign students individualized\nhomework targeting their specific weaknesses, provide students with instant\nfeedback, or simulate student responses to a new exam question. While these\nmodels excel in predictive performance, this lack of adoption can be attributed\nto a significant weakness: the lack of explainability of model decisions,\nleading to a lack of trust from students, parents, and teachers. This thesis\naims to bring human needs to the forefront of eXplainable AI (XAI) research,\ngrounded in the concrete use case of personalized learning and teaching. We\nframe the contributions along two verticals: technical advances in XAI and\ntheir aligned human studies. We investigate explainability in AI for education,\nrevealing systematic disagreements between post-hoc explainers and identifying\na need for inherently interpretable model architectures. We propose four novel\ntechnical contributions in interpretability with a multimodal modular\narchitecture (MultiModN), an interpretable mixture-of-experts model\n(InterpretCC), adversarial training for explainer stability, and a\ntheory-driven LLM-XAI framework to present explanations to students\n(iLLuMinaTE), which we evaluate in diverse settings with professors, teachers,\nlearning scientists, and university students. By combining empirical\nevaluations of existing explainers with novel architectural designs and human\nstudies, our work lays a foundation for human-centric AI systems that balance\nstate-of-the-art performance with built-in transparency and trust.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks form the backbone of artificial intelligence research,\nwith potential to transform the human experience in areas ranging from\nautonomous driving to personal assistants, healthcare to education. However,\ntheir integration into the daily routines of real-world classrooms remains\nlimited. It is not yet common for a teacher to assign students individualized\nhomework targeting their specific weaknesses, provide students with instant\nfeedback, or simulate student responses to a new exam question. While these\nmodels excel in predictive performance, this lack of adoption can be attributed\nto a significant weakness: the lack of explainability of model decisions,\nleading to a lack of trust from students, parents, and teachers. This thesis\naims to bring human needs to the forefront of eXplainable AI (XAI) research,\ngrounded in the concrete use case of personalized learning and teaching. We\nframe the contributions along two verticals: technical advances in XAI and\ntheir aligned human studies. We investigate explainability in AI for education,\nrevealing systematic disagreements between post-hoc explainers and identifying\na need for inherently interpretable model architectures. We propose four novel\ntechnical contributions in interpretability with a multimodal modular\narchitecture (MultiModN), an interpretable mixture-of-experts model\n(InterpretCC), adversarial training for explainer stability, and a\ntheory-driven LLM-XAI framework to present explanations to students\n(iLLuMinaTE), which we evaluate in diverse settings with professors, teachers,\nlearning scientists, and university students. By combining empirical\nevaluations of existing explainers with novel architectural designs and human\nstudies, our work lays a foundation for human-centric AI systems that balance\nstate-of-the-art performance with built-in transparency and trust."
                },
                "authors": [
                    {
                        "name": "Vinitra Swamy"
                    }
                ],
                "author_detail": {
                    "name": "Vinitra Swamy"
                },
                "author": "Vinitra Swamy",
                "arxiv_comment": "PhD Thesis, EPFL (Computer Science)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22523v1",
                "updated": "2025-05-28T16:09:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    9,
                    33,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T16:09:33Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    9,
                    33,
                    2,
                    148,
                    0
                ],
                "title": "PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image\n  Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image\n  Generative Models"
                },
                "summary": "Generating high-quality, multi-layer transparent images from text prompts can\nunlock a new level of creative control, allowing users to edit each layer as\neffortlessly as editing text outputs from LLMs. However, the development of\nmulti-layer generative models lags behind that of conventional text-to-image\nmodels due to the absence of a large, high-quality corpus of multi-layer\ntransparent data. In this paper, we address this fundamental challenge by: (i)\nreleasing the first open, ultra-high-fidelity PrismLayers (PrismLayersPro)\ndataset of 200K (20K) multilayer transparent images with accurate alpha mattes,\n(ii) introducing a trainingfree synthesis pipeline that generates such data on\ndemand using off-the-shelf diffusion models, and (iii) delivering a strong,\nopen-source multi-layer generation model, ART+, which matches the aesthetics of\nmodern text-to-image generation models. The key technical contributions\ninclude: LayerFLUX, which excels at generating high-quality single transparent\nlayers with accurate alpha mattes, and MultiLayerFLUX, which composes multiple\nLayerFLUX outputs into complete images, guided by human-annotated semantic\nlayout. To ensure higher quality, we apply a rigorous filtering stage to remove\nartifacts and semantic mismatches, followed by human selection. Fine-tuning the\nstate-of-the-art ART model on our synthetic PrismLayersPro yields ART+, which\noutperforms the original ART in 60% of head-to-head user study comparisons and\neven matches the visual quality of images generated by the FLUX.1-[dev] model.\nWe anticipate that our work will establish a solid dataset foundation for the\nmulti-layer transparent image generation task, enabling research and\napplications that require precise, editable, and visually compelling layered\nimagery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating high-quality, multi-layer transparent images from text prompts can\nunlock a new level of creative control, allowing users to edit each layer as\neffortlessly as editing text outputs from LLMs. However, the development of\nmulti-layer generative models lags behind that of conventional text-to-image\nmodels due to the absence of a large, high-quality corpus of multi-layer\ntransparent data. In this paper, we address this fundamental challenge by: (i)\nreleasing the first open, ultra-high-fidelity PrismLayers (PrismLayersPro)\ndataset of 200K (20K) multilayer transparent images with accurate alpha mattes,\n(ii) introducing a trainingfree synthesis pipeline that generates such data on\ndemand using off-the-shelf diffusion models, and (iii) delivering a strong,\nopen-source multi-layer generation model, ART+, which matches the aesthetics of\nmodern text-to-image generation models. The key technical contributions\ninclude: LayerFLUX, which excels at generating high-quality single transparent\nlayers with accurate alpha mattes, and MultiLayerFLUX, which composes multiple\nLayerFLUX outputs into complete images, guided by human-annotated semantic\nlayout. To ensure higher quality, we apply a rigorous filtering stage to remove\nartifacts and semantic mismatches, followed by human selection. Fine-tuning the\nstate-of-the-art ART model on our synthetic PrismLayersPro yields ART+, which\noutperforms the original ART in 60% of head-to-head user study comparisons and\neven matches the visual quality of images generated by the FLUX.1-[dev] model.\nWe anticipate that our work will establish a solid dataset foundation for the\nmulti-layer transparent image generation task, enabling research and\napplications that require precise, editable, and visually compelling layered\nimagery."
                },
                "authors": [
                    {
                        "name": "Junwen Chen"
                    },
                    {
                        "name": "Heyang Jiang"
                    },
                    {
                        "name": "Yanbin Wang"
                    },
                    {
                        "name": "Keming Wu"
                    },
                    {
                        "name": "Ji Li"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Keiji Yanai"
                    },
                    {
                        "name": "Dong Chen"
                    },
                    {
                        "name": "Yuhui Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Yuhui Yuan"
                },
                "author": "Yuhui Yuan",
                "arxiv_comment": "Homepage: https://prism-layers.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22521v1",
                "updated": "2025-05-28T16:08:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    8,
                    4,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T16:08:04Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    8,
                    4,
                    2,
                    148,
                    0
                ],
                "title": "Evaluating Supervised Learning Models for Fraud Detection: A Comparative\n  Study of Classical and Deep Architectures on Imbalanced Transaction Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Supervised Learning Models for Fraud Detection: A Comparative\n  Study of Classical and Deep Architectures on Imbalanced Transaction Data"
                },
                "summary": "Fraud detection remains a critical task in high-stakes domains such as\nfinance and e-commerce, where undetected fraudulent transactions can lead to\nsignificant economic losses. In this study, we systematically compare the\nperformance of four supervised learning models - Logistic Regression, Random\nForest, Light Gradient Boosting Machine (LightGBM), and a Gated Recurrent Unit\n(GRU) network - on a large-scale, highly imbalanced online transaction dataset.\nWhile ensemble methods such as Random Forest and LightGBM demonstrated superior\nperformance in both overall and class-specific metrics, Logistic Regression\noffered a reliable and interpretable baseline. The GRU model showed strong\nrecall for the minority fraud class, though at the cost of precision,\nhighlighting a trade-off relevant for real-world deployment. Our evaluation\nemphasizes not only weighted averages but also per-class precision, recall, and\nF1-scores, providing a nuanced view of each model's effectiveness in detecting\nrare but consequential fraudulent activity. The findings underscore the\nimportance of choosing models based on the specific risk tolerance and\noperational needs of fraud detection systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fraud detection remains a critical task in high-stakes domains such as\nfinance and e-commerce, where undetected fraudulent transactions can lead to\nsignificant economic losses. In this study, we systematically compare the\nperformance of four supervised learning models - Logistic Regression, Random\nForest, Light Gradient Boosting Machine (LightGBM), and a Gated Recurrent Unit\n(GRU) network - on a large-scale, highly imbalanced online transaction dataset.\nWhile ensemble methods such as Random Forest and LightGBM demonstrated superior\nperformance in both overall and class-specific metrics, Logistic Regression\noffered a reliable and interpretable baseline. The GRU model showed strong\nrecall for the minority fraud class, though at the cost of precision,\nhighlighting a trade-off relevant for real-world deployment. Our evaluation\nemphasizes not only weighted averages but also per-class precision, recall, and\nF1-scores, providing a nuanced view of each model's effectiveness in detecting\nrare but consequential fraudulent activity. The findings underscore the\nimportance of choosing models based on the specific risk tolerance and\noperational needs of fraud detection systems."
                },
                "authors": [
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Chuanhao Nie"
                    },
                    {
                        "name": "Yunbo Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunbo Liu"
                },
                "author": "Yunbo Liu",
                "arxiv_comment": "5 pages. Chao Wang, Chuanhao Nie, and Yunbo Liu contributed equally\n  to this work. Corresponding author: Yunbo Liu (yunbo.liu954@duke.edu).\n  Submitted to the 3rd International Conference on Management Innovation and\n  Economy Development (MIED 2025), Chongqing, China",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19641v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19641v3",
                "updated": "2025-05-28T16:04:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    16,
                    4,
                    3,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-26T07:59:36Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    59,
                    36,
                    0,
                    146,
                    0
                ],
                "title": "SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning\n  Logical Reasoning and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning\n  Logical Reasoning and Beyond"
                },
                "summary": "Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the\npotential of Reinforcement Learning (RL) to enhance reasoning abilities in\nLarge Language Models (LLMs). While open-source replication efforts have\nprimarily focused on mathematical and coding domains, methods and resources for\ndeveloping general reasoning capabilities remain underexplored. This gap is\npartly due to the challenge of collecting diverse and verifiable reasoning data\nsuitable for RL. We hypothesize that logical reasoning is critical for\ndeveloping general reasoning capabilities, as logic forms a fundamental\nbuilding block of reasoning. In this work, we present SynLogic, a data\nsynthesis framework and dataset that generates diverse logical reasoning data\nat scale, encompassing 35 diverse logical reasoning tasks. The SynLogic\napproach enables controlled synthesis of data with adjustable difficulty and\nquantity. Importantly, all examples can be verified by simple rules, making\nthem ideally suited for RL with verifiable rewards. In our experiments, we\nvalidate the effectiveness of RL training on the SynLogic dataset based on 7B\nand 32B models. SynLogic leads to state-of-the-art logical reasoning\nperformance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B\nby 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and\ncoding tasks improves the training efficiency of these domains and\nsignificantly enhances reasoning generalization. Notably, our mixed training\nmodel outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These\nfindings position SynLogic as a valuable resource for advancing the broader\nreasoning capabilities of LLMs. We open-source both the data synthesis pipeline\nand the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the\npotential of Reinforcement Learning (RL) to enhance reasoning abilities in\nLarge Language Models (LLMs). While open-source replication efforts have\nprimarily focused on mathematical and coding domains, methods and resources for\ndeveloping general reasoning capabilities remain underexplored. This gap is\npartly due to the challenge of collecting diverse and verifiable reasoning data\nsuitable for RL. We hypothesize that logical reasoning is critical for\ndeveloping general reasoning capabilities, as logic forms a fundamental\nbuilding block of reasoning. In this work, we present SynLogic, a data\nsynthesis framework and dataset that generates diverse logical reasoning data\nat scale, encompassing 35 diverse logical reasoning tasks. The SynLogic\napproach enables controlled synthesis of data with adjustable difficulty and\nquantity. Importantly, all examples can be verified by simple rules, making\nthem ideally suited for RL with verifiable rewards. In our experiments, we\nvalidate the effectiveness of RL training on the SynLogic dataset based on 7B\nand 32B models. SynLogic leads to state-of-the-art logical reasoning\nperformance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B\nby 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and\ncoding tasks improves the training efficiency of these domains and\nsignificantly enhances reasoning generalization. Notably, our mixed training\nmodel outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These\nfindings position SynLogic as a valuable resource for advancing the broader\nreasoning capabilities of LLMs. We open-source both the data synthesis pipeline\nand the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic."
                },
                "authors": [
                    {
                        "name": "Junteng Liu"
                    },
                    {
                        "name": "Yuanxiang Fan"
                    },
                    {
                        "name": "Zhuo Jiang"
                    },
                    {
                        "name": "Han Ding"
                    },
                    {
                        "name": "Yongyi Hu"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Yiqi Shi"
                    },
                    {
                        "name": "Shitong Weng"
                    },
                    {
                        "name": "Aili Chen"
                    },
                    {
                        "name": "Shiqi Chen"
                    },
                    {
                        "name": "Yunan Huang"
                    },
                    {
                        "name": "Mozhi Zhang"
                    },
                    {
                        "name": "Pengyu Zhao"
                    },
                    {
                        "name": "Junjie Yan"
                    },
                    {
                        "name": "Junxian He"
                    }
                ],
                "author_detail": {
                    "name": "Junxian He"
                },
                "author": "Junxian He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19641v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19641v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20201v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20201v2",
                "updated": "2025-05-28T15:55:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    55,
                    49,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-26T16:42:02Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    16,
                    42,
                    2,
                    0,
                    146,
                    0
                ],
                "title": "Reasoning Is Not All You Need: Examining LLMs for Multi-Turn Mental\n  Health Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Is Not All You Need: Examining LLMs for Multi-Turn Mental\n  Health Conversations"
                },
                "summary": "Limited access to mental healthcare, extended wait times, and increasing\ncapabilities of Large Language Models (LLMs) has led individuals to turn to\nLLMs for fulfilling their mental health needs. However, examining the\nmulti-turn mental health conversation capabilities of LLMs remains\nunder-explored. Existing evaluation frameworks typically focus on diagnostic\naccuracy and win-rates and often overlook alignment with patient-specific\ngoals, values, and personalities required for meaningful conversations. To\naddress this, we introduce MedAgent, a novel framework for synthetically\ngenerating realistic, multi-turn mental health sensemaking conversations and\nuse it to create the Mental Health Sensemaking Dialogue (MHSD) dataset,\ncomprising over 2,200 patient-LLM conversations. Additionally, we present\nMultiSenseEval, a holistic framework to evaluate the multi-turn conversation\nabilities of LLMs in healthcare settings using human-centric criteria. Our\nfindings reveal that frontier reasoning models yield below-par performance for\npatient-centric communication and struggle at advanced diagnostic capabilities\nwith average score of 31%. Additionally, we observed variation in model\nperformance based on patient's persona and performance drop with increasing\nturns in the conversation. Our work provides a comprehensive synthetic data\ngeneration framework, a dataset and evaluation framework for assessing LLMs in\nmulti-turn mental health conversations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limited access to mental healthcare, extended wait times, and increasing\ncapabilities of Large Language Models (LLMs) has led individuals to turn to\nLLMs for fulfilling their mental health needs. However, examining the\nmulti-turn mental health conversation capabilities of LLMs remains\nunder-explored. Existing evaluation frameworks typically focus on diagnostic\naccuracy and win-rates and often overlook alignment with patient-specific\ngoals, values, and personalities required for meaningful conversations. To\naddress this, we introduce MedAgent, a novel framework for synthetically\ngenerating realistic, multi-turn mental health sensemaking conversations and\nuse it to create the Mental Health Sensemaking Dialogue (MHSD) dataset,\ncomprising over 2,200 patient-LLM conversations. Additionally, we present\nMultiSenseEval, a holistic framework to evaluate the multi-turn conversation\nabilities of LLMs in healthcare settings using human-centric criteria. Our\nfindings reveal that frontier reasoning models yield below-par performance for\npatient-centric communication and struggle at advanced diagnostic capabilities\nwith average score of 31%. Additionally, we observed variation in model\nperformance based on patient's persona and performance drop with increasing\nturns in the conversation. Our work provides a comprehensive synthetic data\ngeneration framework, a dataset and evaluation framework for assessing LLMs in\nmulti-turn mental health conversations."
                },
                "authors": [
                    {
                        "name": "Mohit Chandra"
                    },
                    {
                        "name": "Siddharth Sriraman"
                    },
                    {
                        "name": "Harneet Singh Khanuja"
                    },
                    {
                        "name": "Yiqiao Jin"
                    },
                    {
                        "name": "Munmun De Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Munmun De Choudhury"
                },
                "author": "Munmun De Choudhury",
                "arxiv_comment": "34 pages, 5 figures, 30 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20201v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20201v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22503v1",
                "updated": "2025-05-28T15:51:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    51,
                    13,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T15:51:13Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    51,
                    13,
                    2,
                    148,
                    0
                ],
                "title": "From Strangers to Assistants: Fast Desire Alignment for Embodied\n  Agent-User Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Strangers to Assistants: Fast Desire Alignment for Embodied\n  Agent-User Adaptation"
                },
                "summary": "While embodied agents have made significant progress in performing complex\nphysical tasks, real-world applications demand more than pure task execution.\nThe agents must collaborate with unfamiliar agents and human users, whose goals\nare often vague and implicit. In such settings, interpreting ambiguous\ninstructions and uncovering underlying desires is essential for effective\nassistance. Therefore, fast and accurate desire alignment becomes a critical\ncapability for embodied agents. In this work, we first develop a home\nassistance simulation environment HA-Desire that integrates an LLM-driven human\nuser agent exhibiting realistic value-driven goal selection and communication.\nThe ego agent must interact with this proxy user to infer and adapt to the\nuser's latent desires. To achieve this, we present a novel framework FAMER for\nfast desire alignment, which introduces a desire-based mental reasoning\nmechanism to identify user intent and filter desire-irrelevant actions. We\nfurther design a reflection-based communication module that reduces redundant\ninquiries, and incorporate goal-relevant information extraction with memory\npersistence to improve information reuse and reduce unnecessary exploration.\nExtensive experiments demonstrate that our framework significantly enhances\nboth task execution and communication efficiency, enabling embodied agents to\nquickly adapt to user-specific desires in complex embodied environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While embodied agents have made significant progress in performing complex\nphysical tasks, real-world applications demand more than pure task execution.\nThe agents must collaborate with unfamiliar agents and human users, whose goals\nare often vague and implicit. In such settings, interpreting ambiguous\ninstructions and uncovering underlying desires is essential for effective\nassistance. Therefore, fast and accurate desire alignment becomes a critical\ncapability for embodied agents. In this work, we first develop a home\nassistance simulation environment HA-Desire that integrates an LLM-driven human\nuser agent exhibiting realistic value-driven goal selection and communication.\nThe ego agent must interact with this proxy user to infer and adapt to the\nuser's latent desires. To achieve this, we present a novel framework FAMER for\nfast desire alignment, which introduces a desire-based mental reasoning\nmechanism to identify user intent and filter desire-irrelevant actions. We\nfurther design a reflection-based communication module that reduces redundant\ninquiries, and incorporate goal-relevant information extraction with memory\npersistence to improve information reuse and reduce unnecessary exploration.\nExtensive experiments demonstrate that our framework significantly enhances\nboth task execution and communication efficiency, enabling embodied agents to\nquickly adapt to user-specific desires in complex embodied environments."
                },
                "authors": [
                    {
                        "name": "Yuanfei Wang"
                    },
                    {
                        "name": "Xinju Huang"
                    },
                    {
                        "name": "Fangwei Zhong"
                    },
                    {
                        "name": "Yaodong Yang"
                    },
                    {
                        "name": "Yizhou Wang"
                    },
                    {
                        "name": "Yuanpei Chen"
                    },
                    {
                        "name": "Hao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Hao Dong"
                },
                "author": "Hao Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22501v1",
                "updated": "2025-05-28T15:50:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    50,
                    48,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T15:50:48Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    50,
                    48,
                    2,
                    148,
                    0
                ],
                "title": "EvolveSearch: An Iterative Self-Evolving Search Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvolveSearch: An Iterative Self-Evolving Search Agent"
                },
                "summary": "The rapid advancement of large language models (LLMs) has transformed the\nlandscape of agentic information seeking capabilities through the integration\nof tools such as search engines and web browsers. However, current mainstream\napproaches for enabling LLM web search proficiency face significant challenges:\nsupervised fine-tuning struggles with data production in open-search domains,\nwhile RL converges quickly, limiting their data utilization efficiency. To\naddress these issues, we propose EvolveSearch, a novel iterative self-evolution\nframework that combines SFT and RL to enhance agentic web search capabilities\nwithout any external human-annotated reasoning data. Extensive experiments on\nseven multi-hop question-answering (MHQA) benchmarks demonstrate that\nEvolveSearch consistently improves performance across iterations, ultimately\nachieving an average improvement of 4.7\\% over the current state-of-the-art\nacross seven benchmarks, opening the door to self-evolution agentic\ncapabilities in open web search domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has transformed the\nlandscape of agentic information seeking capabilities through the integration\nof tools such as search engines and web browsers. However, current mainstream\napproaches for enabling LLM web search proficiency face significant challenges:\nsupervised fine-tuning struggles with data production in open-search domains,\nwhile RL converges quickly, limiting their data utilization efficiency. To\naddress these issues, we propose EvolveSearch, a novel iterative self-evolution\nframework that combines SFT and RL to enhance agentic web search capabilities\nwithout any external human-annotated reasoning data. Extensive experiments on\nseven multi-hop question-answering (MHQA) benchmarks demonstrate that\nEvolveSearch consistently improves performance across iterations, ultimately\nachieving an average improvement of 4.7\\% over the current state-of-the-art\nacross seven benchmarks, opening the door to self-evolution agentic\ncapabilities in open web search domains."
                },
                "authors": [
                    {
                        "name": "Dingchu Zhang"
                    },
                    {
                        "name": "Yida Zhao"
                    },
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Baixuan Li"
                    },
                    {
                        "name": "Wenbiao Yin"
                    },
                    {
                        "name": "Liwen Zhang"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Yufeng Li"
                    },
                    {
                        "name": "Kewei Tu"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Huang"
                },
                "author": "Fei Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22499v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22499v2",
                "updated": "2025-05-29T07:38:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    7,
                    38,
                    20,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-28T15:49:54Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    49,
                    54,
                    2,
                    148,
                    0
                ],
                "title": "The Meeseeks Mesh: Spatially Consistent 3D Adversarial Objects for BEV\n  Detector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Meeseeks Mesh: Spatially Consistent 3D Adversarial Objects for BEV\n  Detector"
                },
                "summary": "3D object detection is a critical component in autonomous driving systems. It\nallows real-time recognition and detection of vehicles, pedestrians and\nobstacles under varying environmental conditions. Among existing methods, 3D\nobject detection in the Bird's Eye View (BEV) has emerged as the mainstream\nframework. To guarantee a safe, robust and trustworthy 3D object detection, 3D\nadversarial attacks are investigated, where attacks are placed in 3D\nenvironments to evaluate the model performance, e.g. putting a film on a car,\nclothing a pedestrian. The vulnerability of 3D object detection models to 3D\nadversarial attacks serves as an important indicator to evaluate the robustness\nof the model against perturbations. To investigate this vulnerability, we\ngenerate non-invasive 3D adversarial objects tailored for real-world attack\nscenarios. Our method verifies the existence of universal adversarial objects\nthat are spatially consistent across time and camera views. Specifically, we\nemploy differentiable rendering techniques to accurately model the spatial\nrelationship between adversarial objects and the target vehicle. Furthermore,\nwe introduce an occlusion-aware module to enhance visual consistency and\nrealism under different viewpoints. To maintain attack effectiveness across\nmultiple frames, we design a BEV spatial feature-guided optimization strategy.\nExperimental results demonstrate that our approach can reliably suppress\nvehicle predictions from state-of-the-art 3D object detectors, serving as an\nimportant tool to test robustness of 3D object detection models before\ndeployment. Moreover, the generated adversarial objects exhibit strong\ngeneralization capabilities, retaining its effectiveness at various positions\nand distances in the scene.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D object detection is a critical component in autonomous driving systems. It\nallows real-time recognition and detection of vehicles, pedestrians and\nobstacles under varying environmental conditions. Among existing methods, 3D\nobject detection in the Bird's Eye View (BEV) has emerged as the mainstream\nframework. To guarantee a safe, robust and trustworthy 3D object detection, 3D\nadversarial attacks are investigated, where attacks are placed in 3D\nenvironments to evaluate the model performance, e.g. putting a film on a car,\nclothing a pedestrian. The vulnerability of 3D object detection models to 3D\nadversarial attacks serves as an important indicator to evaluate the robustness\nof the model against perturbations. To investigate this vulnerability, we\ngenerate non-invasive 3D adversarial objects tailored for real-world attack\nscenarios. Our method verifies the existence of universal adversarial objects\nthat are spatially consistent across time and camera views. Specifically, we\nemploy differentiable rendering techniques to accurately model the spatial\nrelationship between adversarial objects and the target vehicle. Furthermore,\nwe introduce an occlusion-aware module to enhance visual consistency and\nrealism under different viewpoints. To maintain attack effectiveness across\nmultiple frames, we design a BEV spatial feature-guided optimization strategy.\nExperimental results demonstrate that our approach can reliably suppress\nvehicle predictions from state-of-the-art 3D object detectors, serving as an\nimportant tool to test robustness of 3D object detection models before\ndeployment. Moreover, the generated adversarial objects exhibit strong\ngeneralization capabilities, retaining its effectiveness at various positions\nand distances in the scene."
                },
                "authors": [
                    {
                        "name": "Aixuan Li"
                    },
                    {
                        "name": "Mochu Xiang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Yuchao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Yuchao Dai"
                },
                "author": "Yuchao Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22499v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22499v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22496v1",
                "updated": "2025-05-28T15:47:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    47,
                    10,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T15:47:10Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    47,
                    10,
                    2,
                    148,
                    0
                ],
                "title": "Risk-Sensitive Conformal Prediction for Catheter Placement Detection in\n  Chest X-rays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Risk-Sensitive Conformal Prediction for Catheter Placement Detection in\n  Chest X-rays"
                },
                "summary": "This paper presents a novel approach to catheter and line position detection\nin chest X-rays, combining multi-task learning with risk-sensitive conformal\nprediction to address critical clinical requirements. Our model simultaneously\nperforms classification, segmentation, and landmark detection, leveraging the\nsynergistic relationship between these tasks to improve overall performance. We\nfurther enhance clinical reliability through risk-sensitive conformal\nprediction, which provides statistically guaranteed prediction sets with higher\nreliability for clinically critical findings. Experimental results demonstrate\nexcellent performance with 90.68\\% overall empirical coverage and 99.29\\%\ncoverage for critical conditions, while maintaining remarkable precision in\nprediction sets. Most importantly, our risk-sensitive approach achieves zero\nhigh-risk mispredictions (cases where the system dangerously declares\nproblematic tubes as confidently normal), making the system particularly\nsuitable for clinical deployment. This work offers both accurate predictions\nand reliably quantified uncertainty -- essential features for life-critical\nmedical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach to catheter and line position detection\nin chest X-rays, combining multi-task learning with risk-sensitive conformal\nprediction to address critical clinical requirements. Our model simultaneously\nperforms classification, segmentation, and landmark detection, leveraging the\nsynergistic relationship between these tasks to improve overall performance. We\nfurther enhance clinical reliability through risk-sensitive conformal\nprediction, which provides statistically guaranteed prediction sets with higher\nreliability for clinically critical findings. Experimental results demonstrate\nexcellent performance with 90.68\\% overall empirical coverage and 99.29\\%\ncoverage for critical conditions, while maintaining remarkable precision in\nprediction sets. Most importantly, our risk-sensitive approach achieves zero\nhigh-risk mispredictions (cases where the system dangerously declares\nproblematic tubes as confidently normal), making the system particularly\nsuitable for clinical deployment. This work offers both accurate predictions\nand reliably quantified uncertainty -- essential features for life-critical\nmedical applications."
                },
                "authors": [
                    {
                        "name": "Long Hui"
                    }
                ],
                "author_detail": {
                    "name": "Long Hui"
                },
                "author": "Long Hui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13171v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13171v2",
                "updated": "2025-05-28T15:39:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    39,
                    49,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-19T14:28:35Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    28,
                    35,
                    0,
                    139,
                    0
                ],
                "title": "Positional Fragility in LLMs: How Offset Effects Reshape Our\n  Understanding of Memorization Risks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Positional Fragility in LLMs: How Offset Effects Reshape Our\n  Understanding of Memorization Risks"
                },
                "summary": "Large language models are known to memorize parts of their training data,\nposing risk of copyright violations. To systematically examine this risk, we\npretrain language models (1B/3B/8B) from scratch on 83B tokens, mixing\nweb-scale data with public domain books used to simulate copyrighted content at\ncontrolled frequencies at lengths at least ten times longer than prior work. We\nthereby identified the offset effect, a phenomenon characterized by two key\nfindings: (1) verbatim memorization is most strongly triggered by short\nprefixes drawn from the beginning of the context window, with memorization\ndecreasing counterintuitively as prefix length increases; and (2) a sharp\ndecline in verbatim recall when prefix begins offset from the initial tokens of\nthe context window. We attribute this to positional fragility: models rely\ndisproportionately on the earliest tokens in their context window as retrieval\nanchors, making them sensitive to even slight shifts. We further observe that\nwhen the model fails to retrieve memorized content, it often produces\ndegenerated text. Leveraging these findings, we show that shifting sensitive\ndata deeper into the context window suppresses both extractable memorization\nand degeneration. Our results suggest that positional offset is a critical and\npreviously overlooked axis for evaluating memorization risks, since prior work\nimplicitly assumed uniformity by probing only from the beginning of training\nsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are known to memorize parts of their training data,\nposing risk of copyright violations. To systematically examine this risk, we\npretrain language models (1B/3B/8B) from scratch on 83B tokens, mixing\nweb-scale data with public domain books used to simulate copyrighted content at\ncontrolled frequencies at lengths at least ten times longer than prior work. We\nthereby identified the offset effect, a phenomenon characterized by two key\nfindings: (1) verbatim memorization is most strongly triggered by short\nprefixes drawn from the beginning of the context window, with memorization\ndecreasing counterintuitively as prefix length increases; and (2) a sharp\ndecline in verbatim recall when prefix begins offset from the initial tokens of\nthe context window. We attribute this to positional fragility: models rely\ndisproportionately on the earliest tokens in their context window as retrieval\nanchors, making them sensitive to even slight shifts. We further observe that\nwhen the model fails to retrieve memorized content, it often produces\ndegenerated text. Leveraging these findings, we show that shifting sensitive\ndata deeper into the context window suppresses both extractable memorization\nand degeneration. Our results suggest that positional offset is a critical and\npreviously overlooked axis for evaluating memorization risks, since prior work\nimplicitly assumed uniformity by probing only from the beginning of training\nsequences."
                },
                "authors": [
                    {
                        "name": "Yixuan Xu"
                    },
                    {
                        "name": "Antoni-Joan Solergibert i Llaquet"
                    },
                    {
                        "name": "Antoine Bosselut"
                    },
                    {
                        "name": "Imanol Schlag"
                    }
                ],
                "author_detail": {
                    "name": "Imanol Schlag"
                },
                "author": "Imanol Schlag",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13171v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13171v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01747v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01747v3",
                "updated": "2025-05-28T15:29:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    29,
                    3,
                    2,
                    148,
                    0
                ],
                "published": "2025-03-03T17:15:17Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    17,
                    15,
                    17,
                    0,
                    62,
                    0
                ],
                "title": "Position: Don't Use the CLT in LLM Evals With Fewer Than a Few Hundred\n  Datapoints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: Don't Use the CLT in LLM Evals With Fewer Than a Few Hundred\n  Datapoints"
                },
                "summary": "Rigorous statistical evaluations of large language models (LLMs), including\nvalid error bars and significance testing, are essential for meaningful and\nreliable performance assessment. Currently, when such statistical measures are\nreported, they typically rely on the Central Limit Theorem (CLT). In this\nposition paper, we argue that while CLT-based methods for uncertainty\nquantification are appropriate when benchmarks consist of thousands of\nexamples, they fail to provide adequate uncertainty estimates for LLM\nevaluations that rely on smaller, highly specialized benchmarks. In these\nsmall-data settings, we demonstrate that CLT-based methods perform very poorly,\nusually dramatically underestimating uncertainty (i.e. producing error bars\nthat are too small). We give recommendations for alternative frequentist and\nBayesian methods that are both easy to implement and more appropriate in these\nincreasingly common scenarios. We provide a simple Python library for these\nBayesian methods at https://github.com/sambowyer/bayes_evals .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rigorous statistical evaluations of large language models (LLMs), including\nvalid error bars and significance testing, are essential for meaningful and\nreliable performance assessment. Currently, when such statistical measures are\nreported, they typically rely on the Central Limit Theorem (CLT). In this\nposition paper, we argue that while CLT-based methods for uncertainty\nquantification are appropriate when benchmarks consist of thousands of\nexamples, they fail to provide adequate uncertainty estimates for LLM\nevaluations that rely on smaller, highly specialized benchmarks. In these\nsmall-data settings, we demonstrate that CLT-based methods perform very poorly,\nusually dramatically underestimating uncertainty (i.e. producing error bars\nthat are too small). We give recommendations for alternative frequentist and\nBayesian methods that are both easy to implement and more appropriate in these\nincreasingly common scenarios. We provide a simple Python library for these\nBayesian methods at https://github.com/sambowyer/bayes_evals ."
                },
                "authors": [
                    {
                        "name": "Sam Bowyer"
                    },
                    {
                        "name": "Laurence Aitchison"
                    },
                    {
                        "name": "Desi R. Ivanova"
                    }
                ],
                "author_detail": {
                    "name": "Desi R. Ivanova"
                },
                "author": "Desi R. Ivanova",
                "arxiv_comment": "42 pages, 39 figures. ICML 2025 Spotlight Position Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01747v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01747v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13913v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13913v2",
                "updated": "2025-05-28T15:25:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    25,
                    29,
                    2,
                    148,
                    0
                ],
                "published": "2025-02-19T17:46:30Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    46,
                    30,
                    2,
                    50,
                    0
                ],
                "title": "How Do LLMs Perform Two-Hop Reasoning in Context?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Do LLMs Perform Two-Hop Reasoning in Context?"
                },
                "summary": "``Socrates is human. All humans are mortal. Therefore, Socrates is mortal.''\nThis form of argument illustrates a typical pattern of two-hop reasoning.\nFormally, two-hop reasoning refers to the process of inferring a conclusion by\nmaking two logical steps, each connecting adjacent concepts, such that the\nfinal conclusion depends on the integration of both steps. It is one of the\nmost fundamental components of human reasoning and plays a crucial role in both\nformal logic and everyday decision-making. Despite recent progress in large\nlanguage models (LLMs), we surprisingly find that they can fail at solving\nsimple two-hop reasoning problems when distractors are present. We observe on a\nsynthetic dataset that pre-trained LLMs often resort to random guessing among\nall plausible conclusions. However, after few steps of fine-tuning, models\nachieve near-perfect accuracy and exhibit strong length generalization. To\nunderstand the underlying mechanisms, we train a 3-layer Transformer from\nscratch on a synthetic two-hop reasoning task and reverse-engineer its internal\ninformation flow. We observe a clear progression in the attention logits\nthroughout training. This pictures a sharp phase transition from an initial\nstage of random guessing to the emergence of a structured sequential query\nmechanism, where the model first retrieves the preceding and the bridge\nconcepts in the early layers and then uses them to infer the final answer.\nFinally, we show that these dynamics can be captured by a minimal\nthree-parameter attention-only network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "``Socrates is human. All humans are mortal. Therefore, Socrates is mortal.''\nThis form of argument illustrates a typical pattern of two-hop reasoning.\nFormally, two-hop reasoning refers to the process of inferring a conclusion by\nmaking two logical steps, each connecting adjacent concepts, such that the\nfinal conclusion depends on the integration of both steps. It is one of the\nmost fundamental components of human reasoning and plays a crucial role in both\nformal logic and everyday decision-making. Despite recent progress in large\nlanguage models (LLMs), we surprisingly find that they can fail at solving\nsimple two-hop reasoning problems when distractors are present. We observe on a\nsynthetic dataset that pre-trained LLMs often resort to random guessing among\nall plausible conclusions. However, after few steps of fine-tuning, models\nachieve near-perfect accuracy and exhibit strong length generalization. To\nunderstand the underlying mechanisms, we train a 3-layer Transformer from\nscratch on a synthetic two-hop reasoning task and reverse-engineer its internal\ninformation flow. We observe a clear progression in the attention logits\nthroughout training. This pictures a sharp phase transition from an initial\nstage of random guessing to the emergence of a structured sequential query\nmechanism, where the model first retrieves the preceding and the bridge\nconcepts in the early layers and then uses them to infer the final answer.\nFinally, we show that these dynamics can be captured by a minimal\nthree-parameter attention-only network."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Hanlin Zhu"
                    },
                    {
                        "name": "Ruiqi Zhang"
                    },
                    {
                        "name": "Jiantao Jiao"
                    },
                    {
                        "name": "Song Mei"
                    },
                    {
                        "name": "Michael I. Jordan"
                    },
                    {
                        "name": "Stuart Russell"
                    }
                ],
                "author_detail": {
                    "name": "Stuart Russell"
                },
                "author": "Stuart Russell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13913v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13913v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11926v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11926v3",
                "updated": "2025-05-28T15:24:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    24,
                    43,
                    2,
                    148,
                    0
                ],
                "published": "2025-02-17T15:39:50Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    15,
                    39,
                    50,
                    0,
                    48,
                    0
                ],
                "title": "BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion\n  Recognition Datasets for 28 Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion\n  Recognition Datasets for 28 Languages"
                },
                "summary": "People worldwide use language in subtle and complex ways to express emotions.\nAlthough emotion recognition--an umbrella term for several NLP tasks--impacts\nvarious applications within NLP and beyond, most work in this area has focused\non high-resource languages. This has led to significant disparities in research\nefforts and proposed solutions, particularly for under-resourced languages,\nwhich often lack high-quality annotated datasets. In this paper, we present\nBRIGHTER--a collection of multilabeled, emotion-annotated datasets in 28\ndifferent languages and across several domains. BRIGHTER primarily covers\nlow-resource languages from Africa, Asia, Eastern Europe, and Latin America,\nwith instances labeled by fluent speakers. We highlight the challenges related\nto the data collection and annotation processes, and then report experimental\nresults for monolingual and crosslingual multi-label emotion identification, as\nwell as emotion intensity recognition. We analyse the variability in\nperformance across languages and text domains, both with and without the use of\nLLMs, and show that the BRIGHTER datasets represent a meaningful step towards\naddressing the gap in text-based emotion recognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "People worldwide use language in subtle and complex ways to express emotions.\nAlthough emotion recognition--an umbrella term for several NLP tasks--impacts\nvarious applications within NLP and beyond, most work in this area has focused\non high-resource languages. This has led to significant disparities in research\nefforts and proposed solutions, particularly for under-resourced languages,\nwhich often lack high-quality annotated datasets. In this paper, we present\nBRIGHTER--a collection of multilabeled, emotion-annotated datasets in 28\ndifferent languages and across several domains. BRIGHTER primarily covers\nlow-resource languages from Africa, Asia, Eastern Europe, and Latin America,\nwith instances labeled by fluent speakers. We highlight the challenges related\nto the data collection and annotation processes, and then report experimental\nresults for monolingual and crosslingual multi-label emotion identification, as\nwell as emotion intensity recognition. We analyse the variability in\nperformance across languages and text domains, both with and without the use of\nLLMs, and show that the BRIGHTER datasets represent a meaningful step towards\naddressing the gap in text-based emotion recognition."
                },
                "authors": [
                    {
                        "name": "Shamsuddeen Hassan Muhammad"
                    },
                    {
                        "name": "Nedjma Ousidhoum"
                    },
                    {
                        "name": "Idris Abdulmumin"
                    },
                    {
                        "name": "Jan Philip Wahle"
                    },
                    {
                        "name": "Terry Ruas"
                    },
                    {
                        "name": "Meriem Beloucif"
                    },
                    {
                        "name": "Christine de Kock"
                    },
                    {
                        "name": "Nirmal Surange"
                    },
                    {
                        "name": "Daniela Teodorescu"
                    },
                    {
                        "name": "Ibrahim Said Ahmad"
                    },
                    {
                        "name": "David Ifeoluwa Adelani"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    },
                    {
                        "name": "Felermino D. M. A. Ali"
                    },
                    {
                        "name": "Ilseyar Alimova"
                    },
                    {
                        "name": "Vladimir Araujo"
                    },
                    {
                        "name": "Nikolay Babakov"
                    },
                    {
                        "name": "Naomi Baes"
                    },
                    {
                        "name": "Ana-Maria Bucur"
                    },
                    {
                        "name": "Andiswa Bukula"
                    },
                    {
                        "name": "Guanqun Cao"
                    },
                    {
                        "name": "Rodrigo Tufino Cardenas"
                    },
                    {
                        "name": "Rendi Chevi"
                    },
                    {
                        "name": "Chiamaka Ijeoma Chukwuneke"
                    },
                    {
                        "name": "Alexandra Ciobotaru"
                    },
                    {
                        "name": "Daryna Dementieva"
                    },
                    {
                        "name": "Murja Sani Gadanya"
                    },
                    {
                        "name": "Robert Geislinger"
                    },
                    {
                        "name": "Bela Gipp"
                    },
                    {
                        "name": "Oumaima Hourrane"
                    },
                    {
                        "name": "Oana Ignat"
                    },
                    {
                        "name": "Falalu Ibrahim Lawan"
                    },
                    {
                        "name": "Rooweither Mabuya"
                    },
                    {
                        "name": "Rahmad Mahendra"
                    },
                    {
                        "name": "Vukosi Marivate"
                    },
                    {
                        "name": "Alexander Panchenko"
                    },
                    {
                        "name": "Andrew Piper"
                    },
                    {
                        "name": "Charles Henrique Porto Ferreira"
                    },
                    {
                        "name": "Vitaly Protasov"
                    },
                    {
                        "name": "Samuel Rutunda"
                    },
                    {
                        "name": "Manish Shrivastava"
                    },
                    {
                        "name": "Aura Cristina Udrea"
                    },
                    {
                        "name": "Lilian Diana Awuor Wanzare"
                    },
                    {
                        "name": "Sophie Wu"
                    },
                    {
                        "name": "Florian Valentin Wunderlich"
                    },
                    {
                        "name": "Hanif Muhammad Zhafran"
                    },
                    {
                        "name": "Tianhui Zhang"
                    },
                    {
                        "name": "Yi Zhou"
                    },
                    {
                        "name": "Saif M. Mohammad"
                    }
                ],
                "author_detail": {
                    "name": "Saif M. Mohammad"
                },
                "author": "Saif M. Mohammad",
                "arxiv_comment": "Accepted at ACL2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11926v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11926v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22467v2",
                "updated": "2025-05-29T04:17:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    4,
                    17,
                    13,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-28T15:20:09Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    20,
                    9,
                    2,
                    148,
                    0
                ],
                "title": "Topological Structure Learning Should Be A Research Priority for\n  LLM-Based Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topological Structure Learning Should Be A Research Priority for\n  LLM-Based Multi-Agent Systems"
                },
                "summary": "Large Language Model-based Multi-Agent Systems (MASs) have emerged as a\npowerful paradigm for tackling complex tasks through collaborative\nintelligence. Nevertheless, the question of how agents should be structurally\norganized for optimal cooperation remains largely unexplored. In this position\npaper, we aim to gently redirect the focus of the MAS research community toward\nthis critical dimension: develop topology-aware MASs for specific tasks.\nSpecifically, the system consists of three core components - agents,\ncommunication links, and communication patterns - that collectively shape its\ncoordination performance and efficiency. To this end, we introduce a\nsystematic, three-stage framework: agent selection, structure profiling, and\ntopology synthesis. Each stage would trigger new research opportunities in\nareas such as language models, reinforcement learning, graph learning, and\ngenerative modeling; together, they could unleash the full potential of MASs in\ncomplicated real-world applications. Then, we discuss the potential challenges\nand opportunities in the evaluation of multiple systems. We hope our\nperspective and framework can offer critical new insights in the era of agentic\nAI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-based Multi-Agent Systems (MASs) have emerged as a\npowerful paradigm for tackling complex tasks through collaborative\nintelligence. Nevertheless, the question of how agents should be structurally\norganized for optimal cooperation remains largely unexplored. In this position\npaper, we aim to gently redirect the focus of the MAS research community toward\nthis critical dimension: develop topology-aware MASs for specific tasks.\nSpecifically, the system consists of three core components - agents,\ncommunication links, and communication patterns - that collectively shape its\ncoordination performance and efficiency. To this end, we introduce a\nsystematic, three-stage framework: agent selection, structure profiling, and\ntopology synthesis. Each stage would trigger new research opportunities in\nareas such as language models, reinforcement learning, graph learning, and\ngenerative modeling; together, they could unleash the full potential of MASs in\ncomplicated real-world applications. Then, we discuss the potential challenges\nand opportunities in the evaluation of multiple systems. We hope our\nperspective and framework can offer critical new insights in the era of agentic\nAI."
                },
                "authors": [
                    {
                        "name": "Jiaxi Yang"
                    },
                    {
                        "name": "Mengqi Zhang"
                    },
                    {
                        "name": "Yiqiao Jin"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Lu Lin"
                    },
                    {
                        "name": "Yi He"
                    },
                    {
                        "name": "Weijie Xu"
                    },
                    {
                        "name": "James Evans"
                    },
                    {
                        "name": "Jindong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jindong Wang"
                },
                "author": "Jindong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00777v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00777v3",
                "updated": "2025-05-28T15:18:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    18,
                    7,
                    2,
                    148,
                    0
                ],
                "published": "2025-01-01T09:00:10Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    9,
                    0,
                    10,
                    2,
                    1,
                    0
                ],
                "title": "FitCF: A Framework for Automatic Feature Importance-guided\n  Counterfactual Example Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FitCF: A Framework for Automatic Feature Importance-guided\n  Counterfactual Example Generation"
                },
                "summary": "Counterfactual examples are widely used in natural language processing (NLP)\nas valuable data to improve models, and in explainable artificial intelligence\n(XAI) to understand model behavior. The automated generation of counterfactual\nexamples remains a challenging task even for large language models (LLMs),\ndespite their impressive performance on many tasks. In this paper, we first\nintroduce ZeroCF, a faithful approach for leveraging important words derived\nfrom feature attribution methods to generate counterfactual examples in a\nzero-shot setting. Second, we present a new framework, FitCF, which further\nverifies aforementioned counterfactuals by label flip verification and then\ninserts them as demonstrations for few-shot prompting, outperforming two\nstate-of-the-art baselines. Through ablation studies, we identify the\nimportance of each of FitCF's core components in improving the quality of\ncounterfactuals, as assessed through flip rate, perplexity, and similarity\nmeasures. Furthermore, we show the effectiveness of LIME and Integrated\nGradients as backbone attribution methods for FitCF and find that the number of\ndemonstrations has the largest effect on performance. Finally, we reveal a\nstrong correlation between the faithfulness of feature attribution scores and\nthe quality of generated counterfactuals, which we hope will serve as an\nimportant finding for future research in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual examples are widely used in natural language processing (NLP)\nas valuable data to improve models, and in explainable artificial intelligence\n(XAI) to understand model behavior. The automated generation of counterfactual\nexamples remains a challenging task even for large language models (LLMs),\ndespite their impressive performance on many tasks. In this paper, we first\nintroduce ZeroCF, a faithful approach for leveraging important words derived\nfrom feature attribution methods to generate counterfactual examples in a\nzero-shot setting. Second, we present a new framework, FitCF, which further\nverifies aforementioned counterfactuals by label flip verification and then\ninserts them as demonstrations for few-shot prompting, outperforming two\nstate-of-the-art baselines. Through ablation studies, we identify the\nimportance of each of FitCF's core components in improving the quality of\ncounterfactuals, as assessed through flip rate, perplexity, and similarity\nmeasures. Furthermore, we show the effectiveness of LIME and Integrated\nGradients as backbone attribution methods for FitCF and find that the number of\ndemonstrations has the largest effect on performance. Finally, we reveal a\nstrong correlation between the faithfulness of feature attribution scores and\nthe quality of generated counterfactuals, which we hope will serve as an\nimportant finding for future research in this direction."
                },
                "authors": [
                    {
                        "name": "Qianli Wang"
                    },
                    {
                        "name": "Nils Feldhus"
                    },
                    {
                        "name": "Simon Ostermann"
                    },
                    {
                        "name": "Luis Felipe Villa-Arenas"
                    },
                    {
                        "name": "Sebastian Möller"
                    },
                    {
                        "name": "Vera Schmitt"
                    }
                ],
                "author_detail": {
                    "name": "Vera Schmitt"
                },
                "author": "Vera Schmitt",
                "arxiv_comment": "ACL 2025 Findings; camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00777v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00777v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11418v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11418v2",
                "updated": "2025-05-28T15:14:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    14,
                    24,
                    2,
                    148,
                    0
                ],
                "published": "2024-12-16T03:34:40Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    3,
                    34,
                    40,
                    0,
                    351,
                    0
                ],
                "title": "ConKE: Conceptualization-Augmented Knowledge Editing in Large Language\n  Models for Commonsense Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConKE: Conceptualization-Augmented Knowledge Editing in Large Language\n  Models for Commonsense Reasoning"
                },
                "summary": "Knowledge Editing (KE) aims to adjust a Large Language Model's (LLM) internal\nrepresentations and parameters to correct inaccuracies and improve output\nconsistency without incurring the computational expense of re-training the\nentire model. However, editing commonsense knowledge still faces difficulties,\nincluding limited knowledge coverage in existing resources, the infeasibility\nof annotating labels for an overabundance of commonsense knowledge, and the\nstrict knowledge formats of current editing methods. In this paper, we address\nthese challenges by presenting ConceptEdit, a framework that integrates\nconceptualization and instantiation into the KE pipeline for LLMs to enhance\ntheir commonsense reasoning capabilities. ConceptEdit dynamically diagnoses\nimplausible commonsense knowledge within an LLM using another verifier LLM and\naugments the source knowledge to be edited with conceptualization for stronger\ngeneralizability. Experimental results demonstrate that LLMs enhanced with\nConceptEdit successfully generate commonsense knowledge with improved\nplausibility compared to other baselines and achieve stronger performance\nacross multiple question answering benchmarks. Our data, code, and models are\npublicly available at https://github.com/HKUST-KnowComp/ConKE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Editing (KE) aims to adjust a Large Language Model's (LLM) internal\nrepresentations and parameters to correct inaccuracies and improve output\nconsistency without incurring the computational expense of re-training the\nentire model. However, editing commonsense knowledge still faces difficulties,\nincluding limited knowledge coverage in existing resources, the infeasibility\nof annotating labels for an overabundance of commonsense knowledge, and the\nstrict knowledge formats of current editing methods. In this paper, we address\nthese challenges by presenting ConceptEdit, a framework that integrates\nconceptualization and instantiation into the KE pipeline for LLMs to enhance\ntheir commonsense reasoning capabilities. ConceptEdit dynamically diagnoses\nimplausible commonsense knowledge within an LLM using another verifier LLM and\naugments the source knowledge to be edited with conceptualization for stronger\ngeneralizability. Experimental results demonstrate that LLMs enhanced with\nConceptEdit successfully generate commonsense knowledge with improved\nplausibility compared to other baselines and achieve stronger performance\nacross multiple question answering benchmarks. Our data, code, and models are\npublicly available at https://github.com/HKUST-KnowComp/ConKE."
                },
                "authors": [
                    {
                        "name": "Liyu Zhang"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "Findings of ACL2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11418v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11418v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22457v1",
                "updated": "2025-05-28T15:13:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    13,
                    34,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T15:13:34Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    13,
                    34,
                    2,
                    148,
                    0
                ],
                "title": "Fostering Video Reasoning via Next-Event Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fostering Video Reasoning via Next-Event Prediction"
                },
                "summary": "Next-token prediction serves as the foundational learning task enabling\nreasoning in LLMs. But what should the learning task be when aiming to equip\nMLLMs with temporal reasoning capabilities over video inputs? Existing tasks\nsuch as video question answering often rely on annotations from humans or much\nstronger MLLMs, while video captioning tends to entangle temporal reasoning\nwith spatial information. To address this gap, we propose next-event prediction\n(NEP), a learning task that harnesses future video segments as a rich,\nself-supervised signal to foster temporal reasoning. We segment each video into\npast and future frames: the MLLM takes the past frames as input and predicts a\nsummary of events derived from the future frames, thereby encouraging the model\nto reason temporally in order to complete the task. To support this task, we\ncurate V1-33K, a dataset comprising 33,000 automatically extracted video\nsegments spanning diverse real-world scenarios. We further explore a range of\nvideo instruction-tuning strategies to study their effects on temporal\nreasoning. To evaluate progress, we introduce FutureBench to assess coherence\nin predicting unseen future events. Experiments validate that NEP offers a\nscalable and effective training paradigm for fostering temporal reasoning in\nMLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-token prediction serves as the foundational learning task enabling\nreasoning in LLMs. But what should the learning task be when aiming to equip\nMLLMs with temporal reasoning capabilities over video inputs? Existing tasks\nsuch as video question answering often rely on annotations from humans or much\nstronger MLLMs, while video captioning tends to entangle temporal reasoning\nwith spatial information. To address this gap, we propose next-event prediction\n(NEP), a learning task that harnesses future video segments as a rich,\nself-supervised signal to foster temporal reasoning. We segment each video into\npast and future frames: the MLLM takes the past frames as input and predicts a\nsummary of events derived from the future frames, thereby encouraging the model\nto reason temporally in order to complete the task. To support this task, we\ncurate V1-33K, a dataset comprising 33,000 automatically extracted video\nsegments spanning diverse real-world scenarios. We further explore a range of\nvideo instruction-tuning strategies to study their effects on temporal\nreasoning. To evaluate progress, we introduce FutureBench to assess coherence\nin predicting unseen future events. Experiments validate that NEP offers a\nscalable and effective training paradigm for fostering temporal reasoning in\nMLLMs."
                },
                "authors": [
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Hongfu Liu"
                    },
                    {
                        "name": "Xiangyan Liu"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Kenji Kawaguchi"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Tianyu Pang"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Pang"
                },
                "author": "Tianyu Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22453v1",
                "updated": "2025-05-28T15:11:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    11,
                    16,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T15:11:16Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    11,
                    16,
                    2,
                    148,
                    0
                ],
                "title": "Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO"
                },
                "summary": "Improving Multi-modal Large Language Models (MLLMs) in the post-training\nstage typically relies on supervised fine-tuning (SFT) or reinforcement\nlearning (RL). However, these supervised methods require expensive and manually\nannotated multi-modal data--an ultimately unsustainable resource. While recent\nefforts have explored unsupervised post-training, their methods are complex and\ndifficult to iterate. In this work, we are the first to investigate the use of\nGRPO, a stable and scalable online RL algorithm, for enabling continual\nself-improvement without any external supervision. We propose MM-UPT, a simple\nyet effective framework for unsupervised post-training of MLLMs. MM-UPT builds\nupon GRPO, replacing traditional reward signals with a self-rewarding mechanism\nbased on majority voting over multiple sampled responses. Our experiments\ndemonstrate that MM-UPT significantly improves the reasoning ability of\nQwen2.5-VL-7B (e.g., 66.3 %$\\rightarrow$72.9 % on MathVista, 62.9\n%$\\rightarrow$68.7 % on We-Math), using standard dataset without ground truth\nlabels. MM-UPT also outperforms prior unsupervised baselines and even\napproaches the results of supervised GRPO. Furthermore, we show that\nincorporating synthetic questions, generated solely by MLLM itself, can boost\nperformance as well, highlighting a promising approach for scalable\nself-improvement. Overall, MM-UPT offers a new paradigm for continual,\nautonomous enhancement of MLLMs in the absence of external supervision. Our\ncode is available at https://github.com/waltonfuture/MM-UPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Multi-modal Large Language Models (MLLMs) in the post-training\nstage typically relies on supervised fine-tuning (SFT) or reinforcement\nlearning (RL). However, these supervised methods require expensive and manually\nannotated multi-modal data--an ultimately unsustainable resource. While recent\nefforts have explored unsupervised post-training, their methods are complex and\ndifficult to iterate. In this work, we are the first to investigate the use of\nGRPO, a stable and scalable online RL algorithm, for enabling continual\nself-improvement without any external supervision. We propose MM-UPT, a simple\nyet effective framework for unsupervised post-training of MLLMs. MM-UPT builds\nupon GRPO, replacing traditional reward signals with a self-rewarding mechanism\nbased on majority voting over multiple sampled responses. Our experiments\ndemonstrate that MM-UPT significantly improves the reasoning ability of\nQwen2.5-VL-7B (e.g., 66.3 %$\\rightarrow$72.9 % on MathVista, 62.9\n%$\\rightarrow$68.7 % on We-Math), using standard dataset without ground truth\nlabels. MM-UPT also outperforms prior unsupervised baselines and even\napproaches the results of supervised GRPO. Furthermore, we show that\nincorporating synthetic questions, generated solely by MLLM itself, can boost\nperformance as well, highlighting a promising approach for scalable\nself-improvement. Overall, MM-UPT offers a new paradigm for continual,\nautonomous enhancement of MLLMs in the absence of external supervision. Our\ncode is available at https://github.com/waltonfuture/MM-UPT."
                },
                "authors": [
                    {
                        "name": "Lai Wei"
                    },
                    {
                        "name": "Yuting Li"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Weiran Huang"
                    },
                    {
                        "name": "Lichao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lichao Sun"
                },
                "author": "Lichao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22442v1",
                "updated": "2025-05-28T15:07:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    7,
                    24,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T15:07:24Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    7,
                    24,
                    2,
                    148,
                    0
                ],
                "title": "SOReL and TOReL: Two Methods for Fully Offline Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOReL and TOReL: Two Methods for Fully Offline Reinforcement Learning"
                },
                "summary": "Sample efficiency remains a major obstacle for real world adoption of\nreinforcement learning (RL): success has been limited to settings where\nsimulators provide access to essentially unlimited environment interactions,\nwhich in reality are typically costly or dangerous to obtain. Offline RL in\nprinciple offers a solution by exploiting offline data to learn a near-optimal\npolicy before deployment. In practice, however, current offline RL methods rely\non extensive online interactions for hyperparameter tuning, and have no\nreliable bound on their initial online performance. To address these two\nissues, we introduce two algorithms. Firstly, SOReL: an algorithm for safe\noffline reinforcement learning. Using only offline data, our Bayesian approach\ninfers a posterior over environment dynamics to obtain a reliable estimate of\nthe online performance via the posterior predictive uncertainty. Crucially, all\nhyperparameters are also tuned fully offline. Secondly, we introduce TOReL: a\ntuning for offline reinforcement learning algorithm that extends our\ninformation rate based offline hyperparameter tuning methods to general offline\nRL approaches. Our empirical evaluation confirms SOReL's ability to accurately\nestimate regret in the Bayesian setting whilst TOReL's offline hyperparameter\ntuning achieves competitive performance with the best online hyperparameter\ntuning methods using only offline data. Thus, SOReL and TOReL make a\nsignificant step towards safe and reliable offline RL, unlocking the potential\nfor RL in the real world. Our implementations are publicly available:\nhttps://github.com/CWibault/sorel\\_torel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sample efficiency remains a major obstacle for real world adoption of\nreinforcement learning (RL): success has been limited to settings where\nsimulators provide access to essentially unlimited environment interactions,\nwhich in reality are typically costly or dangerous to obtain. Offline RL in\nprinciple offers a solution by exploiting offline data to learn a near-optimal\npolicy before deployment. In practice, however, current offline RL methods rely\non extensive online interactions for hyperparameter tuning, and have no\nreliable bound on their initial online performance. To address these two\nissues, we introduce two algorithms. Firstly, SOReL: an algorithm for safe\noffline reinforcement learning. Using only offline data, our Bayesian approach\ninfers a posterior over environment dynamics to obtain a reliable estimate of\nthe online performance via the posterior predictive uncertainty. Crucially, all\nhyperparameters are also tuned fully offline. Secondly, we introduce TOReL: a\ntuning for offline reinforcement learning algorithm that extends our\ninformation rate based offline hyperparameter tuning methods to general offline\nRL approaches. Our empirical evaluation confirms SOReL's ability to accurately\nestimate regret in the Bayesian setting whilst TOReL's offline hyperparameter\ntuning achieves competitive performance with the best online hyperparameter\ntuning methods using only offline data. Thus, SOReL and TOReL make a\nsignificant step towards safe and reliable offline RL, unlocking the potential\nfor RL in the real world. Our implementations are publicly available:\nhttps://github.com/CWibault/sorel\\_torel."
                },
                "authors": [
                    {
                        "name": "Mattie Fellows"
                    },
                    {
                        "name": "Clarisse Wibault"
                    },
                    {
                        "name": "Uljad Berdica"
                    },
                    {
                        "name": "Johannes Forkel"
                    },
                    {
                        "name": "Jakob N. Foerster"
                    },
                    {
                        "name": "Michael A. Osborne"
                    }
                ],
                "author_detail": {
                    "name": "Michael A. Osborne"
                },
                "author": "Michael A. Osborne",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06365v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06365v2",
                "updated": "2025-05-28T15:06:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    6,
                    30,
                    2,
                    148,
                    0
                ],
                "published": "2025-01-10T22:07:56Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    22,
                    7,
                    56,
                    4,
                    10,
                    0
                ],
                "title": "Gender-Neutral Large Language Models for Medical Applications: Reducing\n  Bias in PubMed Abstracts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gender-Neutral Large Language Models for Medical Applications: Reducing\n  Bias in PubMed Abstracts"
                },
                "summary": "This paper presents a pipeline for mitigating gender bias in large language\nmodels (LLMs) used in medical literature by neutralizing gendered occupational\npronouns. A dataset of 379,000 PubMed abstracts from 1965-1980 was processed to\nidentify and modify pronouns tied to professions. We developed a BERT-based\nmodel, \"Modern Occupational Bias Elimination with Refined Training,\" or\n\"MOBERT,\" trained on these neutralized abstracts, and compared its performance\nwith \"1965BERT,\" trained on the original dataset. MOBERT achieved a 70%\ninclusive replacement rate, while 1965BERT reached only 4%. A further analysis\nof MOBERT revealed that pronoun replacement accuracy correlated with the\nfrequency of occupational terms in the training data. We propose expanding the\ndataset and refining the pipeline to improve performance and ensure more\nequitable language modeling in medical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a pipeline for mitigating gender bias in large language\nmodels (LLMs) used in medical literature by neutralizing gendered occupational\npronouns. A dataset of 379,000 PubMed abstracts from 1965-1980 was processed to\nidentify and modify pronouns tied to professions. We developed a BERT-based\nmodel, \"Modern Occupational Bias Elimination with Refined Training,\" or\n\"MOBERT,\" trained on these neutralized abstracts, and compared its performance\nwith \"1965BERT,\" trained on the original dataset. MOBERT achieved a 70%\ninclusive replacement rate, while 1965BERT reached only 4%. A further analysis\nof MOBERT revealed that pronoun replacement accuracy correlated with the\nfrequency of occupational terms in the training data. We propose expanding the\ndataset and refining the pipeline to improve performance and ensure more\nequitable language modeling in medical applications."
                },
                "authors": [
                    {
                        "name": "Elizabeth Schaefer"
                    },
                    {
                        "name": "Kirk Roberts"
                    }
                ],
                "author_detail": {
                    "name": "Kirk Roberts"
                },
                "author": "Kirk Roberts",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06365v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06365v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22435v1",
                "updated": "2025-05-28T14:58:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    58,
                    29,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T14:58:29Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    58,
                    29,
                    2,
                    148,
                    0
                ],
                "title": "Does Johnny Get the Message? Evaluating Cybersecurity Notifications for\n  Everyday Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Johnny Get the Message? Evaluating Cybersecurity Notifications for\n  Everyday Users"
                },
                "summary": "Due to the increasing presence of networked devices in everyday life, not\nonly cybersecurity specialists but also end users benefit from security\napplications such as firewalls, vulnerability scanners, and intrusion detection\nsystems. Recent approaches use large language models (LLMs) to rewrite brief,\ntechnical security alerts into intuitive language and suggest actionable\nmeasures, helping everyday users understand and respond appropriately to\nsecurity risks. However, it remains an open question how well such alerts are\nexplained to users. LLM outputs can also be hallucinated, inconsistent, or\nmisleading. In this work, we introduce the Human-Centered Security Alert\nEvaluation Framework (HCSAEF). HCSAEF assesses LLM-generated cybersecurity\nnotifications to support researchers who want to compare notifications\ngenerated for everyday users, improve them, or analyze the capabilities of\ndifferent LLMs in explaining cybersecurity issues. We demonstrate HCSAEF\nthrough three use cases, which allow us to quantify the impact of prompt\ndesign, model selection, and output consistency. Our findings indicate that\nHCSAEF effectively differentiates generated notifications along dimensions such\nas intuitiveness, urgency, and correctness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the increasing presence of networked devices in everyday life, not\nonly cybersecurity specialists but also end users benefit from security\napplications such as firewalls, vulnerability scanners, and intrusion detection\nsystems. Recent approaches use large language models (LLMs) to rewrite brief,\ntechnical security alerts into intuitive language and suggest actionable\nmeasures, helping everyday users understand and respond appropriately to\nsecurity risks. However, it remains an open question how well such alerts are\nexplained to users. LLM outputs can also be hallucinated, inconsistent, or\nmisleading. In this work, we introduce the Human-Centered Security Alert\nEvaluation Framework (HCSAEF). HCSAEF assesses LLM-generated cybersecurity\nnotifications to support researchers who want to compare notifications\ngenerated for everyday users, improve them, or analyze the capabilities of\ndifferent LLMs in explaining cybersecurity issues. We demonstrate HCSAEF\nthrough three use cases, which allow us to quantify the impact of prompt\ndesign, model selection, and output consistency. Our findings indicate that\nHCSAEF effectively differentiates generated notifications along dimensions such\nas intuitiveness, urgency, and correctness."
                },
                "authors": [
                    {
                        "name": "Victor Jüttner"
                    },
                    {
                        "name": "Erik Buchmann"
                    }
                ],
                "author_detail": {
                    "name": "Erik Buchmann"
                },
                "author": "Erik Buchmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17773v2",
                "updated": "2025-05-28T14:57:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    57,
                    51,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-23T11:44:02Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    11,
                    44,
                    2,
                    4,
                    143,
                    0
                ],
                "title": "C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in\n  Large Language Models"
                },
                "summary": "Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning\nlarge language models (LLMs), but it often produces overconfident predictions\nin data-scarce few-shot settings. To address this issue, several classical\nstatistical learning approaches have been repurposed for scalable\nuncertainty-aware LoRA fine-tuning. However, these approaches neglect how input\ncharacteristics affect the predictive uncertainty estimates. To address this\nlimitation, we propose Contextual Low-Rank Adaptation (\\textbf{C-LoRA}) as a\nnovel uncertainty-aware and parameter efficient fine-tuning approach, by\ndeveloping new lightweight LoRA modules contextualized to each input data\nsample to dynamically adapt uncertainty estimates. Incorporating data-driven\ncontexts into the parameter posteriors, C-LoRA mitigates overfitting, achieves\nwell-calibrated uncertainties, and yields robust predictions. Extensive\nexperiments demonstrate that C-LoRA consistently outperforms the\nstate-of-the-art uncertainty-aware LoRA methods in both uncertainty\nquantification and model generalization. Ablation studies further confirm the\ncritical role of our contextual modules in capturing sample-specific\nuncertainties. C-LoRA sets a new standard for robust, uncertainty-aware LLM\nfine-tuning in few-shot regimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning\nlarge language models (LLMs), but it often produces overconfident predictions\nin data-scarce few-shot settings. To address this issue, several classical\nstatistical learning approaches have been repurposed for scalable\nuncertainty-aware LoRA fine-tuning. However, these approaches neglect how input\ncharacteristics affect the predictive uncertainty estimates. To address this\nlimitation, we propose Contextual Low-Rank Adaptation (\\textbf{C-LoRA}) as a\nnovel uncertainty-aware and parameter efficient fine-tuning approach, by\ndeveloping new lightweight LoRA modules contextualized to each input data\nsample to dynamically adapt uncertainty estimates. Incorporating data-driven\ncontexts into the parameter posteriors, C-LoRA mitigates overfitting, achieves\nwell-calibrated uncertainties, and yields robust predictions. Extensive\nexperiments demonstrate that C-LoRA consistently outperforms the\nstate-of-the-art uncertainty-aware LoRA methods in both uncertainty\nquantification and model generalization. Ablation studies further confirm the\ncritical role of our contextual modules in capturing sample-specific\nuncertainties. C-LoRA sets a new standard for robust, uncertainty-aware LLM\nfine-tuning in few-shot regimes."
                },
                "authors": [
                    {
                        "name": "Amir Hossein Rahmati"
                    },
                    {
                        "name": "Sanket Jantre"
                    },
                    {
                        "name": "Weifeng Zhang"
                    },
                    {
                        "name": "Yucheng Wang"
                    },
                    {
                        "name": "Byung-Jun Yoon"
                    },
                    {
                        "name": "Nathan M. Urban"
                    },
                    {
                        "name": "Xiaoning Qian"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoning Qian"
                },
                "author": "Xiaoning Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22430v1",
                "updated": "2025-05-28T14:55:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    55,
                    33,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T14:55:33Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    55,
                    33,
                    2,
                    148,
                    0
                ],
                "title": "RAG-Zeval: Towards Robust and Interpretable Evaluation on RAG Responses\n  through End-to-End Rule-Guided Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-Zeval: Towards Robust and Interpretable Evaluation on RAG Responses\n  through End-to-End Rule-Guided Reasoning"
                },
                "summary": "Robust evaluation is critical for deploying trustworthy retrieval-augmented\ngeneration (RAG) systems. However, current LLM-based evaluation frameworks\npredominantly rely on directly prompting resource-intensive models with complex\nmulti-stage prompts, underutilizing models' reasoning capabilities and\nintroducing significant computational cost. In this paper, we present RAG-Zeval\n(RAG-Zero Evaluator), a novel end-to-end framework that formulates faithfulness\nand correctness evaluation as a rule-guided reasoning task. Our approach trains\nevaluators with reinforcement learning, facilitating compact models to generate\ncomprehensive and sound assessments with detailed explanation in one-pass. We\nintroduce a ranking-based outcome reward mechanism, using preference judgments\nrather than absolute scores, to address the challenge of obtaining precise\npointwise reward signals. To this end, we synthesize the ranking references by\ngenerating quality-controlled responses with zero human annotation. Experiments\ndemonstrate RAG-Zeval's superior performance, achieving the strongest\ncorrelation with human judgments and outperforming baselines that rely on LLMs\nwith 10-100 times more parameters. Our approach also exhibits superior\ninterpretability in response evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust evaluation is critical for deploying trustworthy retrieval-augmented\ngeneration (RAG) systems. However, current LLM-based evaluation frameworks\npredominantly rely on directly prompting resource-intensive models with complex\nmulti-stage prompts, underutilizing models' reasoning capabilities and\nintroducing significant computational cost. In this paper, we present RAG-Zeval\n(RAG-Zero Evaluator), a novel end-to-end framework that formulates faithfulness\nand correctness evaluation as a rule-guided reasoning task. Our approach trains\nevaluators with reinforcement learning, facilitating compact models to generate\ncomprehensive and sound assessments with detailed explanation in one-pass. We\nintroduce a ranking-based outcome reward mechanism, using preference judgments\nrather than absolute scores, to address the challenge of obtaining precise\npointwise reward signals. To this end, we synthesize the ranking references by\ngenerating quality-controlled responses with zero human annotation. Experiments\ndemonstrate RAG-Zeval's superior performance, achieving the strongest\ncorrelation with human judgments and outperforming baselines that rely on LLMs\nwith 10-100 times more parameters. Our approach also exhibits superior\ninterpretability in response evaluation."
                },
                "authors": [
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Yunxiang Li"
                    },
                    {
                        "name": "Tianhua Zhang"
                    },
                    {
                        "name": "Hongyin Luo"
                    },
                    {
                        "name": "Xixin Wu"
                    },
                    {
                        "name": "James Glass"
                    },
                    {
                        "name": "Helen Meng"
                    }
                ],
                "author_detail": {
                    "name": "Helen Meng"
                },
                "author": "Helen Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20538v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20538v2",
                "updated": "2025-05-28T14:54:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    54,
                    54,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-26T21:49:18Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    21,
                    49,
                    18,
                    0,
                    146,
                    0
                ],
                "title": "AstroVisBench: A Code Benchmark for Scientific Computing and\n  Visualization in Astronomy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AstroVisBench: A Code Benchmark for Scientific Computing and\n  Visualization in Astronomy"
                },
                "summary": "Large Language Models (LLMs) are being explored for applications in\nscientific research, including their capabilities to synthesize literature,\nanswer research questions, generate research ideas, and even conduct\ncomputational experiments. Ultimately, our goal is for these to help scientists\nderive novel scientific insights. In many areas of science, such insights often\narise from processing and visualizing data to understand its patterns. However,\nevaluating whether an LLM-mediated scientific workflow produces outputs\nconveying the correct scientific insights is challenging to evaluate and has\nnot been addressed in past work. We introduce AstroVisBench, the first\nbenchmark for both scientific computing and visualization in the astronomy\ndomain. AstroVisBench judges a language model's ability to both (1) create\nastronomy-specific workflows to process and analyze data and (2) visualize the\nresults of these workflows through complex plots. Our evaluation of\nvisualizations uses a novel LLM-as-a-judge workflow, which is validated against\nannotation by five professional astronomers. Using AstroVisBench we present an\nevaluation of state-of-the-art language models, showing a significant gap in\ntheir ability to engage in astronomy research as useful assistants. This\nevaluation provides a strong end-to-end evaluation for AI scientists that\noffers a path forward for the development of visualization-based workflows,\nwhich are central to a broad range of domains from physics to biology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are being explored for applications in\nscientific research, including their capabilities to synthesize literature,\nanswer research questions, generate research ideas, and even conduct\ncomputational experiments. Ultimately, our goal is for these to help scientists\nderive novel scientific insights. In many areas of science, such insights often\narise from processing and visualizing data to understand its patterns. However,\nevaluating whether an LLM-mediated scientific workflow produces outputs\nconveying the correct scientific insights is challenging to evaluate and has\nnot been addressed in past work. We introduce AstroVisBench, the first\nbenchmark for both scientific computing and visualization in the astronomy\ndomain. AstroVisBench judges a language model's ability to both (1) create\nastronomy-specific workflows to process and analyze data and (2) visualize the\nresults of these workflows through complex plots. Our evaluation of\nvisualizations uses a novel LLM-as-a-judge workflow, which is validated against\nannotation by five professional astronomers. Using AstroVisBench we present an\nevaluation of state-of-the-art language models, showing a significant gap in\ntheir ability to engage in astronomy research as useful assistants. This\nevaluation provides a strong end-to-end evaluation for AI scientists that\noffers a path forward for the development of visualization-based workflows,\nwhich are central to a broad range of domains from physics to biology."
                },
                "authors": [
                    {
                        "name": "Sebastian Antony Joseph"
                    },
                    {
                        "name": "Syed Murtaza Husain"
                    },
                    {
                        "name": "Stella S. R. Offner"
                    },
                    {
                        "name": "Stéphanie Juneau"
                    },
                    {
                        "name": "Paul Torrey"
                    },
                    {
                        "name": "Adam S. Bolton"
                    },
                    {
                        "name": "Juan P. Farias"
                    },
                    {
                        "name": "Niall Gaffney"
                    },
                    {
                        "name": "Greg Durrett"
                    },
                    {
                        "name": "Junyi Jessy Li"
                    }
                ],
                "author_detail": {
                    "name": "Junyi Jessy Li"
                },
                "author": "Junyi Jessy Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20538v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20538v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22429v1",
                "updated": "2025-05-28T14:53:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    53,
                    53,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T14:53:53Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    53,
                    53,
                    2,
                    148,
                    0
                ],
                "title": "Zero-Shot 3D Visual Grounding from Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot 3D Visual Grounding from Vision-Language Models"
                },
                "summary": "3D Visual Grounding (3DVG) seeks to locate target objects in 3D scenes using\nnatural language descriptions, enabling downstream applications such as\naugmented reality and robotics. Existing approaches typically rely on labeled\n3D data and predefined categories, limiting scalability to open-world settings.\nWe present SeeGround, a zero-shot 3DVG framework that leverages 2D\nVision-Language Models (VLMs) to bypass the need for 3D-specific training. To\nbridge the modality gap, we introduce a hybrid input format that pairs\nquery-aligned rendered views with spatially enriched textual descriptions. Our\nframework incorporates two core components: a Perspective Adaptation Module\nthat dynamically selects optimal viewpoints based on the query, and a Fusion\nAlignment Module that integrates visual and spatial signals to enhance\nlocalization precision. Extensive evaluations on ScanRefer and Nr3D confirm\nthat SeeGround achieves substantial improvements over existing zero-shot\nbaselines -- outperforming them by 7.7% and 7.1%, respectively -- and even\nrivals fully supervised alternatives, demonstrating strong generalization under\nchallenging conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Visual Grounding (3DVG) seeks to locate target objects in 3D scenes using\nnatural language descriptions, enabling downstream applications such as\naugmented reality and robotics. Existing approaches typically rely on labeled\n3D data and predefined categories, limiting scalability to open-world settings.\nWe present SeeGround, a zero-shot 3DVG framework that leverages 2D\nVision-Language Models (VLMs) to bypass the need for 3D-specific training. To\nbridge the modality gap, we introduce a hybrid input format that pairs\nquery-aligned rendered views with spatially enriched textual descriptions. Our\nframework incorporates two core components: a Perspective Adaptation Module\nthat dynamically selects optimal viewpoints based on the query, and a Fusion\nAlignment Module that integrates visual and spatial signals to enhance\nlocalization precision. Extensive evaluations on ScanRefer and Nr3D confirm\nthat SeeGround achieves substantial improvements over existing zero-shot\nbaselines -- outperforming them by 7.7% and 7.1%, respectively -- and even\nrivals fully supervised alternatives, demonstrating strong generalization under\nchallenging conditions."
                },
                "authors": [
                    {
                        "name": "Rong Li"
                    },
                    {
                        "name": "Shijie Li"
                    },
                    {
                        "name": "Lingdong Kong"
                    },
                    {
                        "name": "Xulei Yang"
                    },
                    {
                        "name": "Junwei Liang"
                    }
                ],
                "author_detail": {
                    "name": "Junwei Liang"
                },
                "author": "Junwei Liang",
                "arxiv_comment": "3D-LLM/VLA @ CVPR 2025; Project Page at https://seeground.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01002v2",
                "updated": "2025-05-28T14:52:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    52,
                    38,
                    2,
                    148,
                    0
                ],
                "published": "2025-04-01T17:40:12Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    40,
                    12,
                    1,
                    91,
                    0
                ],
                "title": "Token embeddings violate the manifold hypothesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token embeddings violate the manifold hypothesis"
                },
                "summary": "A full understanding of the behavior of a large language model (LLM) requires\nour understanding of its input token space. If this space differs from our\nassumptions, our understanding of and conclusions about the LLM will likely be\nflawed. We elucidate the structure of the token embeddings both empirically and\ntheoretically. We present a novel statistical test assuming that the\nneighborhood around each token has a relatively flat and smooth structure as\nthe null hypothesis. Failing to reject the null is uninformative, but rejecting\nit at a specific token $\\psi$ implies an irregularity in the token subspace in\na $\\psi$-neighborhood, $B(\\psi)$. The structure assumed in the null is a\ngeneralization of a manifold with boundary called a \\emph{smooth fiber bundle}\n(which can be split into two spatial regimes -- small and large radius), so we\ndenote our new hypothesis test as the ``fiber bundle hypothesis.'' Failure to\nreject the null hypothesis is uninformative, but rejecting it at $\\psi$\nindicates a statistically significant irregularity at $B(\\psi)$. By running our\ntest over several open-source LLMs, each with unique token embeddings, we find\nthat the null is frequently rejected, and so the evidence suggests that the\ntoken subspace is not a fiber bundle and hence also not a manifold. As a\nconsequence of our findings, when an LLM is presented with two semantically\nequivalent prompts, if one prompt contains a token implicated by our test, the\nresponse to that prompt will likely exhibit less stability than the other.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A full understanding of the behavior of a large language model (LLM) requires\nour understanding of its input token space. If this space differs from our\nassumptions, our understanding of and conclusions about the LLM will likely be\nflawed. We elucidate the structure of the token embeddings both empirically and\ntheoretically. We present a novel statistical test assuming that the\nneighborhood around each token has a relatively flat and smooth structure as\nthe null hypothesis. Failing to reject the null is uninformative, but rejecting\nit at a specific token $\\psi$ implies an irregularity in the token subspace in\na $\\psi$-neighborhood, $B(\\psi)$. The structure assumed in the null is a\ngeneralization of a manifold with boundary called a \\emph{smooth fiber bundle}\n(which can be split into two spatial regimes -- small and large radius), so we\ndenote our new hypothesis test as the ``fiber bundle hypothesis.'' Failure to\nreject the null hypothesis is uninformative, but rejecting it at $\\psi$\nindicates a statistically significant irregularity at $B(\\psi)$. By running our\ntest over several open-source LLMs, each with unique token embeddings, we find\nthat the null is frequently rejected, and so the evidence suggests that the\ntoken subspace is not a fiber bundle and hence also not a manifold. As a\nconsequence of our findings, when an LLM is presented with two semantically\nequivalent prompts, if one prompt contains a token implicated by our test, the\nresponse to that prompt will likely exhibit less stability than the other."
                },
                "authors": [
                    {
                        "name": "Michael Robinson"
                    },
                    {
                        "name": "Sourya Dey"
                    },
                    {
                        "name": "Tony Chiang"
                    }
                ],
                "author_detail": {
                    "name": "Tony Chiang"
                },
                "author": "Tony Chiang",
                "arxiv_comment": "27 pages, 6 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "53Z50, 62H15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22425v1",
                "updated": "2025-05-28T14:52:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    52,
                    15,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T14:52:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    52,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Scaling Reasoning without Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Reasoning without Attention"
                },
                "summary": "Large language models (LLMs) have made significant advances in complex\nreasoning tasks, yet they remain bottlenecked by two core challenges:\narchitectural inefficiency due to reliance on Transformers, and a lack of\nstructured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an\nattention-free language model that addresses both issues through architectural\nand data-centric innovations. Built on the state space dual (SSD) layers of\nMamba-2, our model eliminates the need for self-attention and key-value\ncaching, enabling fixed-memory, constant-time inference. To train it for\ncomplex reasoning, we propose a two-phase curriculum fine-tuning strategy based\non the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically\nstructured problems via abstract concept selection and rationale-guided\ngeneration. On benchmark evaluations, \\ourmodel-7B outperforms strong\nTransformer and hybrid models of comparable scale, and even surpasses the much\nlarger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on\nLivecodebench. These results highlight the potential of state space models as\nefficient and scalable alternatives to attention-based architectures for\nhigh-capacity reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant advances in complex\nreasoning tasks, yet they remain bottlenecked by two core challenges:\narchitectural inefficiency due to reliance on Transformers, and a lack of\nstructured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an\nattention-free language model that addresses both issues through architectural\nand data-centric innovations. Built on the state space dual (SSD) layers of\nMamba-2, our model eliminates the need for self-attention and key-value\ncaching, enabling fixed-memory, constant-time inference. To train it for\ncomplex reasoning, we propose a two-phase curriculum fine-tuning strategy based\non the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically\nstructured problems via abstract concept selection and rationale-guided\ngeneration. On benchmark evaluations, \\ourmodel-7B outperforms strong\nTransformer and hybrid models of comparable scale, and even surpasses the much\nlarger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on\nLivecodebench. These results highlight the potential of state space models as\nefficient and scalable alternatives to attention-based architectures for\nhigh-capacity reasoning."
                },
                "authors": [
                    {
                        "name": "Xueliang Zhao"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Lingpeng Kong"
                    }
                ],
                "author_detail": {
                    "name": "Lingpeng Kong"
                },
                "author": "Lingpeng Kong",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22418v1",
                "updated": "2025-05-28T14:43:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    43,
                    55,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T14:43:55Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    43,
                    55,
                    2,
                    148,
                    0
                ],
                "title": "AI Trust Reshaping Administrative Burdens: Understanding Trust-Burden\n  Dynamics in LLM-Assisted Benefits Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Trust Reshaping Administrative Burdens: Understanding Trust-Burden\n  Dynamics in LLM-Assisted Benefits Systems"
                },
                "summary": "Supplemental Nutrition Assistance Program (SNAP) is an essential benefit\nsupport system provided by the US administration to 41 million federally\ndetermined low-income applicants. Through interviews with such applicants\nacross a diverse set of experiences with the SNAP system, our findings reveal\nthat new AI technologies like LLMs can alleviate traditional burdens but also\nintroduce new burdens. We introduce new types of learning, compliance, and\npsychological costs that transform the administrative burden on applicants. We\nalso identify how trust in AI across three dimensions--competence, integrity,\nand benevolence--is perceived to reduce administrative burdens, which may stem\nfrom unintended and untoward overt trust in the system. We discuss calibrating\nappropriate levels of user trust in LLM-based administrative systems,\nmitigating newly introduced burdens. In particular, our findings suggest that\nevidence-based information disclosure is necessary in benefits administration\nand propose directions for future research on trust-burden dynamics in\nAI-assisted administration systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supplemental Nutrition Assistance Program (SNAP) is an essential benefit\nsupport system provided by the US administration to 41 million federally\ndetermined low-income applicants. Through interviews with such applicants\nacross a diverse set of experiences with the SNAP system, our findings reveal\nthat new AI technologies like LLMs can alleviate traditional burdens but also\nintroduce new burdens. We introduce new types of learning, compliance, and\npsychological costs that transform the administrative burden on applicants. We\nalso identify how trust in AI across three dimensions--competence, integrity,\nand benevolence--is perceived to reduce administrative burdens, which may stem\nfrom unintended and untoward overt trust in the system. We discuss calibrating\nappropriate levels of user trust in LLM-based administrative systems,\nmitigating newly introduced burdens. In particular, our findings suggest that\nevidence-based information disclosure is necessary in benefits administration\nand propose directions for future research on trust-burden dynamics in\nAI-assisted administration systems."
                },
                "authors": [
                    {
                        "name": "Jeongwon Jo"
                    },
                    {
                        "name": "He Zhang"
                    },
                    {
                        "name": "Jie Cai"
                    },
                    {
                        "name": "Nitesh Goyal"
                    }
                ],
                "author_detail": {
                    "name": "Nitesh Goyal"
                },
                "author": "Nitesh Goyal",
                "arxiv_doi": "10.1145/3715275.3732077",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3715275.3732077",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.22418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "FAccT 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19618v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19618v2",
                "updated": "2025-05-28T14:42:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    42,
                    9,
                    2,
                    148,
                    0
                ],
                "published": "2025-03-25T13:03:09Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    3,
                    9,
                    1,
                    84,
                    0
                ],
                "title": "Beyond Verifiable Rewards: Scaling Reinforcement Learning for Language\n  Models to Unverifiable Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Verifiable Rewards: Scaling Reinforcement Learning for Language\n  Models to Unverifiable Data"
                },
                "summary": "We propose to scale RL to unverifiable data with a novel algorithm JEPO\n(Jensen's Evidence lower bound Policy Optimization). While most prior efforts\non scaling RL for LLMs focus on verifiable data where ground truth answers are\ntypically short-form and can be matched easily; we investigate the case where\nsuch assumptions are less valid (e.g., when answers are long-form such as\nmathematical proofs). To scale RL training to unverifiable data with\ncontemporary training constraints, we propose JEPO. JEPO applies Jensen's\nevidence lower bound, a pragmatic simplification of the evidence lower bound\nwhich views chain-of-thought as a latent variable in the generative process. We\nshow that on verifiable data (math), JEPO is as effective as RL with verifiable\nrewards; on semi-verifiable data (numina), JEPO improves on soft-match based\nevaluations compared to RL with verifiable rewards which can only leverage a\nsubset of the data source; finally, on unverifiable data (numina-proof), JEPO\noutperforms SFT and a few ablation baselines on likelihood evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose to scale RL to unverifiable data with a novel algorithm JEPO\n(Jensen's Evidence lower bound Policy Optimization). While most prior efforts\non scaling RL for LLMs focus on verifiable data where ground truth answers are\ntypically short-form and can be matched easily; we investigate the case where\nsuch assumptions are less valid (e.g., when answers are long-form such as\nmathematical proofs). To scale RL training to unverifiable data with\ncontemporary training constraints, we propose JEPO. JEPO applies Jensen's\nevidence lower bound, a pragmatic simplification of the evidence lower bound\nwhich views chain-of-thought as a latent variable in the generative process. We\nshow that on verifiable data (math), JEPO is as effective as RL with verifiable\nrewards; on semi-verifiable data (numina), JEPO improves on soft-match based\nevaluations compared to RL with verifiable rewards which can only leverage a\nsubset of the data source; finally, on unverifiable data (numina-proof), JEPO\noutperforms SFT and a few ablation baselines on likelihood evaluations."
                },
                "authors": [
                    {
                        "name": "Yunhao Tang"
                    },
                    {
                        "name": "Sid Wang"
                    },
                    {
                        "name": "Lovish Madaan"
                    },
                    {
                        "name": "Rémi Munos"
                    }
                ],
                "author_detail": {
                    "name": "Rémi Munos"
                },
                "author": "Rémi Munos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19618v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19618v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22407v1",
                "updated": "2025-05-28T14:37:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    37,
                    21,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T14:37:21Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    37,
                    21,
                    2,
                    148,
                    0
                ],
                "title": "Self-Reflective Reinforcement Learning for Diffusion-based Image\n  Reasoning Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Reflective Reinforcement Learning for Diffusion-based Image\n  Reasoning Generation"
                },
                "summary": "Diffusion models have recently demonstrated exceptional performance in image\ngeneration task. However, existing image generation methods still significantly\nsuffer from the dilemma of image reasoning, especially in logic-centered image\ngeneration tasks. Inspired by the success of Chain of Thought (CoT) and\nReinforcement Learning (RL) in LLMs, we propose SRRL, a self-reflective RL\nalgorithm for diffusion models to achieve reasoning generation of logical\nimages by performing reflection and iteration across generation trajectories.\nThe intermediate samples in the denoising process carry noise, making accurate\nreward evaluation difficult. To address this challenge, SRRL treats the entire\ndenoising trajectory as a CoT step with multi-round reflective denoising\nprocess and introduces condition guided forward process, which allows for\nreflective iteration between CoT steps. Through SRRL-based iterative diffusion\ntraining, we introduce image reasoning through CoT into generation tasks\nadhering to physical laws and unconventional physical phenomena for the first\ntime. Notably, experimental results of case study exhibit that the superior\nperformance of our SRRL algorithm even compared with GPT-4o. The project page\nis https://jadenpan0.github.io/srrl.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have recently demonstrated exceptional performance in image\ngeneration task. However, existing image generation methods still significantly\nsuffer from the dilemma of image reasoning, especially in logic-centered image\ngeneration tasks. Inspired by the success of Chain of Thought (CoT) and\nReinforcement Learning (RL) in LLMs, we propose SRRL, a self-reflective RL\nalgorithm for diffusion models to achieve reasoning generation of logical\nimages by performing reflection and iteration across generation trajectories.\nThe intermediate samples in the denoising process carry noise, making accurate\nreward evaluation difficult. To address this challenge, SRRL treats the entire\ndenoising trajectory as a CoT step with multi-round reflective denoising\nprocess and introduces condition guided forward process, which allows for\nreflective iteration between CoT steps. Through SRRL-based iterative diffusion\ntraining, we introduce image reasoning through CoT into generation tasks\nadhering to physical laws and unconventional physical phenomena for the first\ntime. Notably, experimental results of case study exhibit that the superior\nperformance of our SRRL algorithm even compared with GPT-4o. The project page\nis https://jadenpan0.github.io/srrl.github.io/."
                },
                "authors": [
                    {
                        "name": "Jiadong Pan"
                    },
                    {
                        "name": "Zhiyuan Ma"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19623v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19623v2",
                "updated": "2025-05-28T14:32:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    32,
                    56,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-26T07:45:11Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    45,
                    11,
                    0,
                    146,
                    0
                ],
                "title": "AgentRecBench: Benchmarking LLM Agent-based Personalized Recommender\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentRecBench: Benchmarking LLM Agent-based Personalized Recommender\n  Systems"
                },
                "summary": "The emergence of agentic recommender systems powered by Large Language Models\n(LLMs) represents a paradigm shift in personalized recommendations, leveraging\nLLMs' advanced reasoning and role-playing capabilities to enable autonomous,\nadaptive decision-making. Unlike traditional recommendation approaches, agentic\nrecommender systems can dynamically gather and interpret user-item interactions\nfrom complex environments, generating robust recommendation strategies that\ngeneralize across diverse scenarios. However, the field currently lacks\nstandardized evaluation protocols to systematically assess these methods. To\naddress this critical gap, we propose: (1) an interactive textual\nrecommendation simulator incorporating rich user and item metadata and three\ntypical evaluation scenarios (classic, evolving-interest, and cold-start\nrecommendation tasks); (2) a unified modular framework for developing and\nstudying agentic recommender systems; and (3) the first comprehensive benchmark\ncomparing 10 classical and agentic recommendation methods. Our findings\ndemonstrate the superiority of agentic systems and establish actionable design\nguidelines for their core components. The benchmark environment has been\nrigorously validated through an open challenge and remains publicly available\nwith a continuously maintained\nleaderboard~\\footnote[2]{https://tsinghua-fib-lab.github.io/AgentSocietyChallenge/pages/overview.html},\nfostering ongoing community engagement and reproducible research. The benchmark\nis available at:\n\\hyperlink{https://huggingface.co/datasets/SGJQovo/AgentRecBench}{https://huggingface.co/datasets/SGJQovo/AgentRecBench}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of agentic recommender systems powered by Large Language Models\n(LLMs) represents a paradigm shift in personalized recommendations, leveraging\nLLMs' advanced reasoning and role-playing capabilities to enable autonomous,\nadaptive decision-making. Unlike traditional recommendation approaches, agentic\nrecommender systems can dynamically gather and interpret user-item interactions\nfrom complex environments, generating robust recommendation strategies that\ngeneralize across diverse scenarios. However, the field currently lacks\nstandardized evaluation protocols to systematically assess these methods. To\naddress this critical gap, we propose: (1) an interactive textual\nrecommendation simulator incorporating rich user and item metadata and three\ntypical evaluation scenarios (classic, evolving-interest, and cold-start\nrecommendation tasks); (2) a unified modular framework for developing and\nstudying agentic recommender systems; and (3) the first comprehensive benchmark\ncomparing 10 classical and agentic recommendation methods. Our findings\ndemonstrate the superiority of agentic systems and establish actionable design\nguidelines for their core components. The benchmark environment has been\nrigorously validated through an open challenge and remains publicly available\nwith a continuously maintained\nleaderboard~\\footnote[2]{https://tsinghua-fib-lab.github.io/AgentSocietyChallenge/pages/overview.html},\nfostering ongoing community engagement and reproducible research. The benchmark\nis available at:\n\\hyperlink{https://huggingface.co/datasets/SGJQovo/AgentRecBench}{https://huggingface.co/datasets/SGJQovo/AgentRecBench}."
                },
                "authors": [
                    {
                        "name": "Yu Shang"
                    },
                    {
                        "name": "Peijie Liu"
                    },
                    {
                        "name": "Yuwei Yan"
                    },
                    {
                        "name": "Zijing Wu"
                    },
                    {
                        "name": "Leheng Sheng"
                    },
                    {
                        "name": "Yuanqing Yu"
                    },
                    {
                        "name": "Chumeng Jiang"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19623v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19623v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22401v1",
                "updated": "2025-05-28T14:28:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    28,
                    31,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T14:28:31Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    28,
                    31,
                    2,
                    148,
                    0
                ],
                "title": "Facial Age Estimation: A Research Roadmap for Technological and Legal\n  Development and Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facial Age Estimation: A Research Roadmap for Technological and Legal\n  Development and Deployment"
                },
                "summary": "Automated facial age assessment systems operate in either estimation mode -\npredicting age based on facial traits, or verification mode - confirming a\nclaimed age. These systems support access control to age-restricted goods,\nservices, and content, and can be used in areas like e-commerce, social media,\nforensics, and refugee support. They may also personalise services in\nhealthcare, finance, and advertising. While improving technological accuracy is\nessential, deployment must consider legal, ethical, sociological, alongside\ntechnological factors. This white paper reviews the current challenges in\ndeploying such systems, outlines the relevant legal and regulatory landscape,\nand explores future research for fair, robust, and ethical age estimation\ntechnologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated facial age assessment systems operate in either estimation mode -\npredicting age based on facial traits, or verification mode - confirming a\nclaimed age. These systems support access control to age-restricted goods,\nservices, and content, and can be used in areas like e-commerce, social media,\nforensics, and refugee support. They may also personalise services in\nhealthcare, finance, and advertising. While improving technological accuracy is\nessential, deployment must consider legal, ethical, sociological, alongside\ntechnological factors. This white paper reviews the current challenges in\ndeploying such systems, outlines the relevant legal and regulatory landscape,\nand explores future research for fair, robust, and ethical age estimation\ntechnologies."
                },
                "authors": [
                    {
                        "name": "Richard Guest"
                    },
                    {
                        "name": "Eva Lievens"
                    },
                    {
                        "name": "Martin Sas"
                    },
                    {
                        "name": "Elena Botoeva"
                    },
                    {
                        "name": "Temitope Adeyemo"
                    },
                    {
                        "name": "Valerie Verdoodt"
                    },
                    {
                        "name": "Elora Fernandes"
                    },
                    {
                        "name": "Chris Allgrove"
                    }
                ],
                "author_detail": {
                    "name": "Chris Allgrove"
                },
                "author": "Chris Allgrove",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05242v2",
                "updated": "2025-05-28T14:27:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    27,
                    44,
                    2,
                    148,
                    0
                ],
                "published": "2025-02-07T13:25:33Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    25,
                    33,
                    4,
                    38,
                    0
                ],
                "title": "Beyond External Monitors: Enhancing Transparency of Large Language\n  Models for Easier Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond External Monitors: Enhancing Transparency of Large Language\n  Models for Easier Monitoring"
                },
                "summary": "Large language models (LLMs) are becoming increasingly capable, but the\nmechanisms of their thinking and decision-making process remain unclear.\nChain-of-thoughts (CoTs) have been commonly utilized to monitor LLMs, but this\nstrategy fails to accurately reflect LLMs' thinking process. Techniques based\non LLMs' hidden representations provide an inner perspective to monitor their\nlatent thinking. However, previous methods only try to develop external\nmonitors instead of making LLMs themselves easier to monitor. In this paper, we\npropose a novel method TELLME, improving the transparency of LLMs and helping\nmonitors identify unsuitable and sensitive behaviors. Furthermore, we showcase\nthe applications of TELLME on trustworthiness tasks (\\eg, safety risks\nmonitoring tasks and detoxification tasks), where LLMs achieve consistent\nimprovement in transparency and task performance. More crucially, we\ntheoretically analyze the improvement of TELLME on LLMs' generalization ability\nthrough optimal transport theory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are becoming increasingly capable, but the\nmechanisms of their thinking and decision-making process remain unclear.\nChain-of-thoughts (CoTs) have been commonly utilized to monitor LLMs, but this\nstrategy fails to accurately reflect LLMs' thinking process. Techniques based\non LLMs' hidden representations provide an inner perspective to monitor their\nlatent thinking. However, previous methods only try to develop external\nmonitors instead of making LLMs themselves easier to monitor. In this paper, we\npropose a novel method TELLME, improving the transparency of LLMs and helping\nmonitors identify unsuitable and sensitive behaviors. Furthermore, we showcase\nthe applications of TELLME on trustworthiness tasks (\\eg, safety risks\nmonitoring tasks and detoxification tasks), where LLMs achieve consistent\nimprovement in transparency and task performance. More crucially, we\ntheoretically analyze the improvement of TELLME on LLMs' generalization ability\nthrough optimal transport theory."
                },
                "authors": [
                    {
                        "name": "Guanxu Chen"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Tao Luo"
                    },
                    {
                        "name": "Lijie Hu"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "arxiv_comment": "25 pages,6 figures,13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12339v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12339v2",
                "updated": "2025-05-28T14:24:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    24,
                    12,
                    2,
                    148,
                    0
                ],
                "published": "2025-04-15T01:44:56Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    1,
                    44,
                    56,
                    1,
                    105,
                    0
                ],
                "title": "GOAT-TTS: Expressive and Realistic Speech Generation via A Dual-Branch\n  LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GOAT-TTS: Expressive and Realistic Speech Generation via A Dual-Branch\n  LLM"
                },
                "summary": "While large language models (LLMs) have revolutionized text-to-speech (TTS)\nsynthesis through discrete tokenization paradigms, current architectures\nexhibit fundamental tensions between three critical dimensions: 1) irreversible\nloss of acoustic characteristics caused by quantization of speech prompts; 2)\nstringent dependence on precisely aligned prompt speech-text pairs that limit\nreal-world deployment; and 3) catastrophic forgetting of the LLM's native text\ncomprehension during optimization for speech token generation. To address these\nchallenges, we propose an LLM-based text-to-speech Generation approach\nOptimized via a novel dual-branch ArchiTecture (GOAT-TTS). Our framework\nintroduces two key innovations: (1) The modality-alignment branch combines a\nspeech encoder and projector to capture continuous acoustic embeddings,\nenabling bidirectional correlation between paralinguistic features (language,\ntimbre, emotion) and semantic text representations without transcript\ndependency; (2) The speech-generation branch employs modular fine-tuning on\ntop-k layers of an LLM for speech token prediction while freezing the bottom-n\nlayers to preserve foundational linguistic knowledge. Moreover, multi-token\nprediction is introduced to support real-time streaming TTS synthesis.\nExperimental results demonstrate that our GOAT-TTS achieves performance\ncomparable to state-of-the-art TTS models while validating the efficacy of\nsynthesized dialect speech data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have revolutionized text-to-speech (TTS)\nsynthesis through discrete tokenization paradigms, current architectures\nexhibit fundamental tensions between three critical dimensions: 1) irreversible\nloss of acoustic characteristics caused by quantization of speech prompts; 2)\nstringent dependence on precisely aligned prompt speech-text pairs that limit\nreal-world deployment; and 3) catastrophic forgetting of the LLM's native text\ncomprehension during optimization for speech token generation. To address these\nchallenges, we propose an LLM-based text-to-speech Generation approach\nOptimized via a novel dual-branch ArchiTecture (GOAT-TTS). Our framework\nintroduces two key innovations: (1) The modality-alignment branch combines a\nspeech encoder and projector to capture continuous acoustic embeddings,\nenabling bidirectional correlation between paralinguistic features (language,\ntimbre, emotion) and semantic text representations without transcript\ndependency; (2) The speech-generation branch employs modular fine-tuning on\ntop-k layers of an LLM for speech token prediction while freezing the bottom-n\nlayers to preserve foundational linguistic knowledge. Moreover, multi-token\nprediction is introduced to support real-time streaming TTS synthesis.\nExperimental results demonstrate that our GOAT-TTS achieves performance\ncomparable to state-of-the-art TTS models while validating the efficacy of\nsynthesized dialect speech data."
                },
                "authors": [
                    {
                        "name": "Yaodong Song"
                    },
                    {
                        "name": "Hongjie Chen"
                    },
                    {
                        "name": "Jie Lian"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Guangmin Xia"
                    },
                    {
                        "name": "Zehan Li"
                    },
                    {
                        "name": "Genliang Zhao"
                    },
                    {
                        "name": "Jian Kang"
                    },
                    {
                        "name": "Jie Li"
                    },
                    {
                        "name": "Yongxiang Li"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12339v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12339v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19385v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19385v4",
                "updated": "2025-05-28T14:20:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    20,
                    39,
                    2,
                    148,
                    0
                ],
                "published": "2025-03-25T06:30:45Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    30,
                    45,
                    1,
                    84,
                    0
                ],
                "title": "Inference-Time Scaling for Flow Models via Stochastic Generation and\n  Rollover Budget Forcing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Scaling for Flow Models via Stochastic Generation and\n  Rollover Budget Forcing"
                },
                "summary": "We propose an inference-time scaling approach for pretrained flow models.\nRecently, inference-time scaling has gained significant attention in LLMs and\ndiffusion models, improving sample quality or better aligning outputs with user\npreferences by leveraging additional computation. For diffusion models,\nparticle sampling has allowed more efficient scaling due to the stochasticity\nat intermediate denoising steps. On the contrary, while flow models have gained\npopularity as an alternative to diffusion models--offering faster generation\nand high-quality outputs in state-of-the-art image and video generative\nmodels--efficient inference-time scaling methods used for diffusion models\ncannot be directly applied due to their deterministic generative process. To\nenable efficient inference-time scaling for flow models, we propose three key\nideas: 1) SDE-based generation, enabling particle sampling in flow models, 2)\nInterpolant conversion, broadening the search space and enhancing sample\ndiversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of\ncomputational resources across timesteps to maximize budget utilization. Our\nexperiments show that SDE-based generation, particularly variance-preserving\n(VP) interpolant-based generation, improves the performance of particle\nsampling methods for inference-time scaling in flow models. Additionally, we\ndemonstrate that RBF with VP-SDE achieves the best performance, outperforming\nall previous inference-time scaling approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an inference-time scaling approach for pretrained flow models.\nRecently, inference-time scaling has gained significant attention in LLMs and\ndiffusion models, improving sample quality or better aligning outputs with user\npreferences by leveraging additional computation. For diffusion models,\nparticle sampling has allowed more efficient scaling due to the stochasticity\nat intermediate denoising steps. On the contrary, while flow models have gained\npopularity as an alternative to diffusion models--offering faster generation\nand high-quality outputs in state-of-the-art image and video generative\nmodels--efficient inference-time scaling methods used for diffusion models\ncannot be directly applied due to their deterministic generative process. To\nenable efficient inference-time scaling for flow models, we propose three key\nideas: 1) SDE-based generation, enabling particle sampling in flow models, 2)\nInterpolant conversion, broadening the search space and enhancing sample\ndiversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of\ncomputational resources across timesteps to maximize budget utilization. Our\nexperiments show that SDE-based generation, particularly variance-preserving\n(VP) interpolant-based generation, improves the performance of particle\nsampling methods for inference-time scaling in flow models. Additionally, we\ndemonstrate that RBF with VP-SDE achieves the best performance, outperforming\nall previous inference-time scaling approaches."
                },
                "authors": [
                    {
                        "name": "Jaihoon Kim"
                    },
                    {
                        "name": "Taehoon Yoon"
                    },
                    {
                        "name": "Jisung Hwang"
                    },
                    {
                        "name": "Minhyuk Sung"
                    }
                ],
                "author_detail": {
                    "name": "Minhyuk Sung"
                },
                "author": "Minhyuk Sung",
                "arxiv_comment": "Project page: https://flow-inference-time-scaling.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19385v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19385v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21277v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21277v2",
                "updated": "2025-05-28T14:16:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    16,
                    10,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-27T14:48:44Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    48,
                    44,
                    1,
                    147,
                    0
                ],
                "title": "Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks\n  through Expanding Strategy Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks\n  through Expanding Strategy Space"
                },
                "summary": "Large Language Models (LLMs), despite advanced general capabilities, still\nsuffer from numerous safety risks, especially jailbreak attacks that bypass\nsafety protocols. Understanding these vulnerabilities through black-box\njailbreak attacks, which better reflect real-world scenarios, offers critical\ninsights into model robustness. While existing methods have shown improvements\nthrough various prompt engineering techniques, their success remains limited\nagainst safety-aligned models, overlooking a more fundamental problem: the\neffectiveness is inherently bounded by the predefined strategy spaces. However,\nexpanding this space presents significant challenges in both systematically\ncapturing essential attack patterns and efficiently navigating the increased\ncomplexity. To better explore the potential of expanding the strategy space, we\naddress these challenges through a novel framework that decomposes jailbreak\nstrategies into essential components based on the Elaboration Likelihood Model\n(ELM) theory and develops genetic-based optimization with intention evaluation\nmechanisms. To be striking, our experiments reveal unprecedented jailbreak\ncapabilities by expanding the strategy space: we achieve over 90% success rate\non Claude-3.5 where prior methods completely fail, while demonstrating strong\ncross-model transferability and surpassing specialized safeguard models in\nevaluation accuracy. The code is open-sourced at:\nhttps://github.com/Aries-iai/CL-GSO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), despite advanced general capabilities, still\nsuffer from numerous safety risks, especially jailbreak attacks that bypass\nsafety protocols. Understanding these vulnerabilities through black-box\njailbreak attacks, which better reflect real-world scenarios, offers critical\ninsights into model robustness. While existing methods have shown improvements\nthrough various prompt engineering techniques, their success remains limited\nagainst safety-aligned models, overlooking a more fundamental problem: the\neffectiveness is inherently bounded by the predefined strategy spaces. However,\nexpanding this space presents significant challenges in both systematically\ncapturing essential attack patterns and efficiently navigating the increased\ncomplexity. To better explore the potential of expanding the strategy space, we\naddress these challenges through a novel framework that decomposes jailbreak\nstrategies into essential components based on the Elaboration Likelihood Model\n(ELM) theory and develops genetic-based optimization with intention evaluation\nmechanisms. To be striking, our experiments reveal unprecedented jailbreak\ncapabilities by expanding the strategy space: we achieve over 90% success rate\non Claude-3.5 where prior methods completely fail, while demonstrating strong\ncross-model transferability and surpassing specialized safeguard models in\nevaluation accuracy. The code is open-sourced at:\nhttps://github.com/Aries-iai/CL-GSO."
                },
                "authors": [
                    {
                        "name": "Yao Huang"
                    },
                    {
                        "name": "Yitong Sun"
                    },
                    {
                        "name": "Shouwei Ruan"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Xingxing Wei"
                    }
                ],
                "author_detail": {
                    "name": "Xingxing Wei"
                },
                "author": "Xingxing Wei",
                "arxiv_comment": "19 pages, 20 figures, accepted by ACL 2025, Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21277v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21277v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22375v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22375v2",
                "updated": "2025-05-29T01:59:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    1,
                    59,
                    0,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-28T14:03:02Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    3,
                    2,
                    2,
                    148,
                    0
                ],
                "title": "Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition"
                },
                "summary": "This work presents Pangu Embedded, an efficient Large Language Model (LLM)\nreasoner developed on Ascend Neural Processing Units (NPUs), featuring flexible\nfast and slow thinking capabilities. Pangu Embedded addresses the significant\ncomputational costs and inference latency challenges prevalent in existing\nreasoning-optimized LLMs. We propose a two-stage training framework for its\nconstruction. In Stage 1, the model is finetuned via an iterative distillation\nprocess, incorporating inter-iteration model merging to effectively aggregate\ncomplementary knowledge. This is followed by reinforcement learning on Ascend\nclusters, optimized by a latency-tolerant scheduler that combines stale\nsynchronous parallelism with prioritized data queues. The RL process is guided\nby a Multi-source Adaptive Reward System (MARS), which generates dynamic,\ntask-specific reward signals using deterministic metrics and lightweight LLM\nevaluators for mathematics, coding, and general problem-solving tasks. Stage 2\nintroduces a dual-system framework, endowing Pangu Embedded with a \"fast\" mode\nfor routine queries and a deeper \"slow\" mode for complex inference. This\nframework offers both manual mode switching for user control and an automatic,\ncomplexity-aware mode selection mechanism that dynamically allocates\ncomputational resources to balance latency and reasoning depth. Experimental\nresults on benchmarks including AIME 2024, GPQA, and LiveCodeBench demonstrate\nthat Pangu Embedded with 7B parameters, outperforms similar-size models like\nQwen3-8B and GLM4-9B. It delivers rapid responses and state-of-the-art\nreasoning quality within a single, unified model architecture, highlighting a\npromising direction for developing powerful yet practically deployable LLM\nreasoners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents Pangu Embedded, an efficient Large Language Model (LLM)\nreasoner developed on Ascend Neural Processing Units (NPUs), featuring flexible\nfast and slow thinking capabilities. Pangu Embedded addresses the significant\ncomputational costs and inference latency challenges prevalent in existing\nreasoning-optimized LLMs. We propose a two-stage training framework for its\nconstruction. In Stage 1, the model is finetuned via an iterative distillation\nprocess, incorporating inter-iteration model merging to effectively aggregate\ncomplementary knowledge. This is followed by reinforcement learning on Ascend\nclusters, optimized by a latency-tolerant scheduler that combines stale\nsynchronous parallelism with prioritized data queues. The RL process is guided\nby a Multi-source Adaptive Reward System (MARS), which generates dynamic,\ntask-specific reward signals using deterministic metrics and lightweight LLM\nevaluators for mathematics, coding, and general problem-solving tasks. Stage 2\nintroduces a dual-system framework, endowing Pangu Embedded with a \"fast\" mode\nfor routine queries and a deeper \"slow\" mode for complex inference. This\nframework offers both manual mode switching for user control and an automatic,\ncomplexity-aware mode selection mechanism that dynamically allocates\ncomputational resources to balance latency and reasoning depth. Experimental\nresults on benchmarks including AIME 2024, GPQA, and LiveCodeBench demonstrate\nthat Pangu Embedded with 7B parameters, outperforms similar-size models like\nQwen3-8B and GLM4-9B. It delivers rapid responses and state-of-the-art\nreasoning quality within a single, unified model architecture, highlighting a\npromising direction for developing powerful yet practically deployable LLM\nreasoners."
                },
                "authors": [
                    {
                        "name": "Hanting Chen"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Zhenni Bi"
                    },
                    {
                        "name": "Jinpeng Li"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Fei Mi"
                    },
                    {
                        "name": "Mingjian Zhu"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Kaikai Song"
                    },
                    {
                        "name": "Yifei Fu"
                    },
                    {
                        "name": "Xu He"
                    },
                    {
                        "name": "Yu Luo"
                    },
                    {
                        "name": "Chong Zhu"
                    },
                    {
                        "name": "Quan He"
                    },
                    {
                        "name": "Xueyu Wu"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Hailin Hu"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Dacheng Tao"
                    },
                    {
                        "name": "Xinghao Chen"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "arxiv_affiliation": "and Other Contributors",
                "author": "Yunhe Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22375v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08820v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08820v3",
                "updated": "2025-05-28T13:59:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    59,
                    23,
                    2,
                    148,
                    0
                ],
                "published": "2024-10-11T14:02:42Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    2,
                    42,
                    4,
                    285,
                    0
                ],
                "title": "Which Demographics do LLMs Default to During Annotation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Demographics do LLMs Default to During Annotation?"
                },
                "summary": "Demographics and cultural background of annotators influence the labels they\nassign in text annotation -- for instance, an elderly woman might find it\noffensive to read a message addressed to a \"bro\", but a male teenager might\nfind it appropriate. It is therefore important to acknowledge label variations\nto not under-represent members of a society. Two research directions developed\nout of this observation in the context of using large language models (LLM) for\ndata annotations, namely (1) studying biases and inherent knowledge of LLMs and\n(2) injecting diversity in the output by manipulating the prompt with\ndemographic information. We combine these two strands of research and ask the\nquestion to which demographics an LLM resorts to when no demographics is given.\nTo answer this question, we evaluate which attributes of human annotators LLMs\ninherently mimic. Furthermore, we compare non-demographic conditioned prompts\nand placebo-conditioned prompts (e.g., \"you are an annotator who lives in house\nnumber 5\") to demographics-conditioned prompts (\"You are a 45 year old man and\nan expert on politeness annotation. How do you rate {instance}\"). We study\nthese questions for politeness and offensiveness annotations on the POPQUORN\ndata set, a corpus created in a controlled manner to investigate human label\nvariations based on demographics which has not been used for LLM-based analyses\nso far. We observe notable influences related to gender, race, and age in\ndemographic prompting, which contrasts with previous studies that found no such\neffects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demographics and cultural background of annotators influence the labels they\nassign in text annotation -- for instance, an elderly woman might find it\noffensive to read a message addressed to a \"bro\", but a male teenager might\nfind it appropriate. It is therefore important to acknowledge label variations\nto not under-represent members of a society. Two research directions developed\nout of this observation in the context of using large language models (LLM) for\ndata annotations, namely (1) studying biases and inherent knowledge of LLMs and\n(2) injecting diversity in the output by manipulating the prompt with\ndemographic information. We combine these two strands of research and ask the\nquestion to which demographics an LLM resorts to when no demographics is given.\nTo answer this question, we evaluate which attributes of human annotators LLMs\ninherently mimic. Furthermore, we compare non-demographic conditioned prompts\nand placebo-conditioned prompts (e.g., \"you are an annotator who lives in house\nnumber 5\") to demographics-conditioned prompts (\"You are a 45 year old man and\nan expert on politeness annotation. How do you rate {instance}\"). We study\nthese questions for politeness and offensiveness annotations on the POPQUORN\ndata set, a corpus created in a controlled manner to investigate human label\nvariations based on demographics which has not been used for LLM-based analyses\nso far. We observe notable influences related to gender, race, and age in\ndemographic prompting, which contrasts with previous studies that found no such\neffects."
                },
                "authors": [
                    {
                        "name": "Johannes Schäfer"
                    },
                    {
                        "name": "Aidan Combs"
                    },
                    {
                        "name": "Christopher Bagdon"
                    },
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Nadine Probol"
                    },
                    {
                        "name": "Lynn Greschner"
                    },
                    {
                        "name": "Sean Papay"
                    },
                    {
                        "name": "Yarik Menchaca Resendiz"
                    },
                    {
                        "name": "Aswathy Velutharambath"
                    },
                    {
                        "name": "Amelie Wührl"
                    },
                    {
                        "name": "Sabine Weber"
                    },
                    {
                        "name": "Roman Klinger"
                    }
                ],
                "author_detail": {
                    "name": "Roman Klinger"
                },
                "author": "Roman Klinger",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08820v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08820v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16169v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16169v2",
                "updated": "2025-05-28T13:56:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    56,
                    42,
                    2,
                    148,
                    0
                ],
                "published": "2025-02-22T10:03:19Z",
                "published_parsed": [
                    2025,
                    2,
                    22,
                    10,
                    3,
                    19,
                    5,
                    53,
                    0
                ],
                "title": "Patterns Over Principles: The Fragility of Inductive Reasoning in LLMs\n  under Noisy Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patterns Over Principles: The Fragility of Inductive Reasoning in LLMs\n  under Noisy Observations"
                },
                "summary": "Inductive reasoning, a cornerstone of human cognition, enables generalization\nfrom limited data but hasn't yet been fully achieved by large language models\n(LLMs). While modern LLMs excel at reasoning tasks, their ability to maintain\nstable and consistent rule abstraction under imperfect observations remains\nunderexplored. To fill this gap, in this work, we introduce Robust Rule\nInduction, a task that evaluates LLMs' capability in inferring rules from data\nthat are fused with noisy examples. To address this task, we further propose\nSample-steered Rule Refinement (SRR), a method enhancing reasoning stability\nvia observation diversification and execution-guided feedback. Experiments\nacross arithmetic, cryptography, and list functions reveal: (1) SRR outperforms\nother methods with minimal performance degradation under noise; (2) Despite\nslight accuracy variation, LLMs exhibit instability under noise (e.g., 0%\naccuracy change with only 70% consistent score); (3) Counterfactual task gaps\nhighlight LLMs' reliance on memorized patterns over genuine abstraction. Our\nfindings challenge LLMs' reasoning robustness, revealing susceptibility to\nhypothesis drift and pattern overfitting, while providing empirical evidence\ncritical for developing human-like inductive systems. Code and data are\navailable at https://github.com/HKUST-KnowComp/Robust-Rule-Induction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inductive reasoning, a cornerstone of human cognition, enables generalization\nfrom limited data but hasn't yet been fully achieved by large language models\n(LLMs). While modern LLMs excel at reasoning tasks, their ability to maintain\nstable and consistent rule abstraction under imperfect observations remains\nunderexplored. To fill this gap, in this work, we introduce Robust Rule\nInduction, a task that evaluates LLMs' capability in inferring rules from data\nthat are fused with noisy examples. To address this task, we further propose\nSample-steered Rule Refinement (SRR), a method enhancing reasoning stability\nvia observation diversification and execution-guided feedback. Experiments\nacross arithmetic, cryptography, and list functions reveal: (1) SRR outperforms\nother methods with minimal performance degradation under noise; (2) Despite\nslight accuracy variation, LLMs exhibit instability under noise (e.g., 0%\naccuracy change with only 70% consistent score); (3) Counterfactual task gaps\nhighlight LLMs' reliance on memorized patterns over genuine abstraction. Our\nfindings challenge LLMs' reasoning robustness, revealing susceptibility to\nhypothesis drift and pattern overfitting, while providing empirical evidence\ncritical for developing human-like inductive systems. Code and data are\navailable at https://github.com/HKUST-KnowComp/Robust-Rule-Induction."
                },
                "authors": [
                    {
                        "name": "Chunyang Li"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16169v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16169v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22368v1",
                "updated": "2025-05-28T13:56:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    56,
                    22,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T13:56:22Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    56,
                    22,
                    2,
                    148,
                    0
                ],
                "title": "AgentDNS: A Root Domain Naming System for LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentDNS: A Root Domain Naming System for LLM Agents"
                },
                "summary": "The rapid evolution of Large Language Model (LLM) agents has highlighted\ncritical challenges in cross-vendor service discovery, interoperability, and\ncommunication. Existing protocols like model context protocol and\nagent-to-agent protocol have made significant strides in standardizing\ninteroperability between agents and tools, as well as communication among\nmulti-agents. However, there remains a lack of standardized protocols and\nsolutions for service discovery across different agent and tool vendors. In\nthis paper, we propose AgentDNS, a root domain naming and service discovery\nsystem designed to enable LLM agents to autonomously discover, resolve, and\nsecurely invoke third-party agent and tool services across organizational and\ntechnological boundaries. Inspired by the principles of the traditional DNS,\nAgentDNS introduces a structured mechanism for service registration, semantic\nservice discovery, secure invocation, and unified billing. We detail the\narchitecture, core functionalities, and use cases of AgentDNS, demonstrating\nits potential to streamline multi-agent collaboration in real-world scenarios.\nThe source code will be published on https://github.com/agentdns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Model (LLM) agents has highlighted\ncritical challenges in cross-vendor service discovery, interoperability, and\ncommunication. Existing protocols like model context protocol and\nagent-to-agent protocol have made significant strides in standardizing\ninteroperability between agents and tools, as well as communication among\nmulti-agents. However, there remains a lack of standardized protocols and\nsolutions for service discovery across different agent and tool vendors. In\nthis paper, we propose AgentDNS, a root domain naming and service discovery\nsystem designed to enable LLM agents to autonomously discover, resolve, and\nsecurely invoke third-party agent and tool services across organizational and\ntechnological boundaries. Inspired by the principles of the traditional DNS,\nAgentDNS introduces a structured mechanism for service registration, semantic\nservice discovery, secure invocation, and unified billing. We detail the\narchitecture, core functionalities, and use cases of AgentDNS, demonstrating\nits potential to streamline multi-agent collaboration in real-world scenarios.\nThe source code will be published on https://github.com/agentdns."
                },
                "authors": [
                    {
                        "name": "Enfang Cui"
                    },
                    {
                        "name": "Yujun Cheng"
                    },
                    {
                        "name": "Rui She"
                    },
                    {
                        "name": "Dan Liu"
                    },
                    {
                        "name": "Zhiyuan Liang"
                    },
                    {
                        "name": "Minxin Guo"
                    },
                    {
                        "name": "Tianzheng Li"
                    },
                    {
                        "name": "Qian Wei"
                    },
                    {
                        "name": "Wenjuan Xing"
                    },
                    {
                        "name": "Zhijie Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Zhong"
                },
                "author": "Zhijie Zhong",
                "arxiv_comment": "7 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01562v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01562v2",
                "updated": "2025-05-28T13:55:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    55,
                    35,
                    2,
                    148,
                    0
                ],
                "published": "2025-02-03T17:45:46Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    17,
                    45,
                    46,
                    0,
                    34,
                    0
                ],
                "title": "Memento No More: Coaching AI Agents to Master Multiple Tasks via Hints\n  Internalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memento No More: Coaching AI Agents to Master Multiple Tasks via Hints\n  Internalization"
                },
                "summary": "As the general capabilities of artificial intelligence (AI) agents continue\nto evolve, their ability to learn to master multiple complex tasks through\nexperience remains a key challenge. Current LLM agents, particularly those\nbased on proprietary language models, typically rely on prompts to incorporate\nknowledge about the target tasks. This approach does not allow the agent to\ninternalize this information and instead relies on ever-expanding prompts to\nsustain its functionality in diverse scenarios. This resembles a system of\nnotes used by a person affected by anterograde amnesia, the inability to form\nnew memories. In this paper, we propose a novel method to train AI agents to\nincorporate knowledge and skills for multiple tasks without the need for either\ncumbersome note systems or prior high-quality demonstration data. Our approach\nemploys an iterative process where the agent collects new experiences, receives\ncorrective feedback from humans in the form of hints, and integrates this\nfeedback into its weights via a context distillation training procedure. We\ndemonstrate the efficacy of our approach by implementing it in a Llama-3-based\nagent that, after only a few rounds of feedback, outperforms advanced models\nGPT-4o and DeepSeek-V3 in tasksets requiring correct sequencing of information\nretrieval, tool use, and question answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the general capabilities of artificial intelligence (AI) agents continue\nto evolve, their ability to learn to master multiple complex tasks through\nexperience remains a key challenge. Current LLM agents, particularly those\nbased on proprietary language models, typically rely on prompts to incorporate\nknowledge about the target tasks. This approach does not allow the agent to\ninternalize this information and instead relies on ever-expanding prompts to\nsustain its functionality in diverse scenarios. This resembles a system of\nnotes used by a person affected by anterograde amnesia, the inability to form\nnew memories. In this paper, we propose a novel method to train AI agents to\nincorporate knowledge and skills for multiple tasks without the need for either\ncumbersome note systems or prior high-quality demonstration data. Our approach\nemploys an iterative process where the agent collects new experiences, receives\ncorrective feedback from humans in the form of hints, and integrates this\nfeedback into its weights via a context distillation training procedure. We\ndemonstrate the efficacy of our approach by implementing it in a Llama-3-based\nagent that, after only a few rounds of feedback, outperforms advanced models\nGPT-4o and DeepSeek-V3 in tasksets requiring correct sequencing of information\nretrieval, tool use, and question answering."
                },
                "authors": [
                    {
                        "name": "Minttu Alakuijala"
                    },
                    {
                        "name": "Ya Gao"
                    },
                    {
                        "name": "Georgy Ananov"
                    },
                    {
                        "name": "Samuel Kaski"
                    },
                    {
                        "name": "Pekka Marttinen"
                    },
                    {
                        "name": "Alexander Ilin"
                    },
                    {
                        "name": "Harri Valpola"
                    }
                ],
                "author_detail": {
                    "name": "Harri Valpola"
                },
                "author": "Harri Valpola",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01562v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01562v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22358v1",
                "updated": "2025-05-28T13:38:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    38,
                    21,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T13:38:21Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    38,
                    21,
                    2,
                    148,
                    0
                ],
                "title": "Budget-Adaptive Adapter Tuning in Orthogonal Subspaces for Continual\n  Learning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Budget-Adaptive Adapter Tuning in Orthogonal Subspaces for Continual\n  Learning in LLMs"
                },
                "summary": "Large language models (LLMs) often suffer from catastrophic forgetting in\ncontinual learning (CL) scenarios, where performance on previously learned\ntasks degrades severely while training on sequentially arriving tasks. Although\npioneering CL approaches using orthogonal subspaces can mitigate task\ninterference, they typically employ fixed budget allocation, neglecting the\nvarying complexity across tasks and layers. Besides, recent budget-adaptive\ntuning methods for LLMs often adopt multi-stage paradigms that decouple\noptimization and budget allocation. Such decoupling results in potential\nmisalignment, which hinders those approaches' practical application in CL\nscenarios. To address these limitations, we propose OA-Adapter, a novel\nparameter-efficient approach for continual learning in LLMs that unifies\ndynamic budget adaptation with orthogonal subspace learning in a single\nend-to-end training stage. Specifically, OA-Adapter introduces a dynamic\nbottleneck dimension adaptation mechanism that simultaneously allocates an\nefficient parameter budget and optimizes task objectives without misalignment.\nTo effectively preserve previously acquired knowledge while coordinating with\nthe dynamic budget allocation, orthogonal constraints are applied specifically\nbetween the parameter subspace of the current task and the dynamically\nallocated parameter subspaces of historical tasks. Experimental results on\ncontinual learning benchmarks demonstrate that OA-Adapter outperforms\nstate-of-the-art methods in both accuracy and parameter efficiency, achieving\nhigher average accuracy while using 58.5% fewer parameters on the standard CL\nbenchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often suffer from catastrophic forgetting in\ncontinual learning (CL) scenarios, where performance on previously learned\ntasks degrades severely while training on sequentially arriving tasks. Although\npioneering CL approaches using orthogonal subspaces can mitigate task\ninterference, they typically employ fixed budget allocation, neglecting the\nvarying complexity across tasks and layers. Besides, recent budget-adaptive\ntuning methods for LLMs often adopt multi-stage paradigms that decouple\noptimization and budget allocation. Such decoupling results in potential\nmisalignment, which hinders those approaches' practical application in CL\nscenarios. To address these limitations, we propose OA-Adapter, a novel\nparameter-efficient approach for continual learning in LLMs that unifies\ndynamic budget adaptation with orthogonal subspace learning in a single\nend-to-end training stage. Specifically, OA-Adapter introduces a dynamic\nbottleneck dimension adaptation mechanism that simultaneously allocates an\nefficient parameter budget and optimizes task objectives without misalignment.\nTo effectively preserve previously acquired knowledge while coordinating with\nthe dynamic budget allocation, orthogonal constraints are applied specifically\nbetween the parameter subspace of the current task and the dynamically\nallocated parameter subspaces of historical tasks. Experimental results on\ncontinual learning benchmarks demonstrate that OA-Adapter outperforms\nstate-of-the-art methods in both accuracy and parameter efficiency, achieving\nhigher average accuracy while using 58.5% fewer parameters on the standard CL\nbenchmark."
                },
                "authors": [
                    {
                        "name": "Zhiyi Wan"
                    },
                    {
                        "name": "Wanrou Du"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Miao Pan"
                    },
                    {
                        "name": "Xiaoqi Qin"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqi Qin"
                },
                "author": "Xiaoqi Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14775v2",
                "updated": "2025-05-28T13:38:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    38,
                    16,
                    2,
                    148,
                    0
                ],
                "published": "2024-12-19T11:59:17Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    11,
                    59,
                    17,
                    3,
                    354,
                    0
                ],
                "title": "Energy and polarization based on-line interference mitigation in radio\n  interferometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy and polarization based on-line interference mitigation in radio\n  interferometry"
                },
                "summary": "Radio frequency interference (RFI) is a persistent contaminant in terrestrial\nradio astronomy. While new radio interferometers are becoming operational,\nnovel sources of RFI are also emerging. In order to strengthen the mitigation\nof RFI in modern radio interferometers, we propose an on-line RFI mitigation\nscheme that can be run in the correlator of such interferometers. We combine\nstatistics based on the energy as well as the polarization alignment of the\ncorrelated signal to develop an on-line RFI mitigation scheme that can be\napplied to a data stream produced by the correlator in real-time, especially\ntargeted at low duty-cycle or transient RFI detection. In order to improve the\ncomputational efficiency, we explore the use of both single precision and half\nprecision floating point operations in implementing the RFI mitigation\nalgorithm. This ideally suits its deployment in accelerator computing devices\nsuch as graphics processing units (GPUs) as used by the LOFAR correlator. We\nprovide results based on real data to demonstrate the efficacy of the proposed\nmethod.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radio frequency interference (RFI) is a persistent contaminant in terrestrial\nradio astronomy. While new radio interferometers are becoming operational,\nnovel sources of RFI are also emerging. In order to strengthen the mitigation\nof RFI in modern radio interferometers, we propose an on-line RFI mitigation\nscheme that can be run in the correlator of such interferometers. We combine\nstatistics based on the energy as well as the polarization alignment of the\ncorrelated signal to develop an on-line RFI mitigation scheme that can be\napplied to a data stream produced by the correlator in real-time, especially\ntargeted at low duty-cycle or transient RFI detection. In order to improve the\ncomputational efficiency, we explore the use of both single precision and half\nprecision floating point operations in implementing the RFI mitigation\nalgorithm. This ideally suits its deployment in accelerator computing devices\nsuch as graphics processing units (GPUs) as used by the LOFAR correlator. We\nprovide results based on real data to demonstrate the efficacy of the proposed\nmethod."
                },
                "authors": [
                    {
                        "name": "Sarod Yatawatta"
                    },
                    {
                        "name": "Albert-Jan Boonstra"
                    },
                    {
                        "name": "Chris P. Broekema"
                    }
                ],
                "author_detail": {
                    "name": "Chris P. Broekema"
                },
                "author": "Chris P. Broekema",
                "arxiv_comment": "Accepted: Astronomy and Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22356v1",
                "updated": "2025-05-28T13:37:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    37,
                    4,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T13:37:04Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    37,
                    4,
                    2,
                    148,
                    0
                ],
                "title": "Suitability Filter: A Statistical Framework for Classifier Evaluation in\n  Real-World Deployment Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Suitability Filter: A Statistical Framework for Classifier Evaluation in\n  Real-World Deployment Settings"
                },
                "summary": "Deploying machine learning models in safety-critical domains poses a key\nchallenge: ensuring reliable model performance on downstream user data without\naccess to ground truth labels for direct validation. We propose the suitability\nfilter, a novel framework designed to detect performance deterioration by\nutilizing suitability signals -- model output features that are sensitive to\ncovariate shifts and indicative of potential prediction errors. The suitability\nfilter evaluates whether classifier accuracy on unlabeled user data shows\nsignificant degradation compared to the accuracy measured on the labeled test\ndataset. Specifically, it ensures that this degradation does not exceed a\npre-specified margin, which represents the maximum acceptable drop in accuracy.\nTo achieve reliable performance evaluation, we aggregate suitability signals\nfor both test and user data and compare these empirical distributions using\nstatistical hypothesis testing, thus providing insights into decision\nuncertainty. Our modular method adapts to various models and domains. Empirical\nevaluations across different classification tasks demonstrate that the\nsuitability filter reliably detects performance deviations due to covariate\nshift. This enables proactive mitigation of potential failures in high-stakes\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying machine learning models in safety-critical domains poses a key\nchallenge: ensuring reliable model performance on downstream user data without\naccess to ground truth labels for direct validation. We propose the suitability\nfilter, a novel framework designed to detect performance deterioration by\nutilizing suitability signals -- model output features that are sensitive to\ncovariate shifts and indicative of potential prediction errors. The suitability\nfilter evaluates whether classifier accuracy on unlabeled user data shows\nsignificant degradation compared to the accuracy measured on the labeled test\ndataset. Specifically, it ensures that this degradation does not exceed a\npre-specified margin, which represents the maximum acceptable drop in accuracy.\nTo achieve reliable performance evaluation, we aggregate suitability signals\nfor both test and user data and compare these empirical distributions using\nstatistical hypothesis testing, thus providing insights into decision\nuncertainty. Our modular method adapts to various models and domains. Empirical\nevaluations across different classification tasks demonstrate that the\nsuitability filter reliably detects performance deviations due to covariate\nshift. This enables proactive mitigation of potential failures in high-stakes\napplications."
                },
                "authors": [
                    {
                        "name": "Angéline Pouget"
                    },
                    {
                        "name": "Mohammad Yaghini"
                    },
                    {
                        "name": "Stephan Rabanser"
                    },
                    {
                        "name": "Nicolas Papernot"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Papernot"
                },
                "author": "Nicolas Papernot",
                "arxiv_comment": "Accepted to ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22354v1",
                "updated": "2025-05-28T13:35:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    35,
                    7,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T13:35:07Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    35,
                    7,
                    2,
                    148,
                    0
                ],
                "title": "LLMs Struggle to Reject False Presuppositions when Misinformation Stakes\n  are High",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Struggle to Reject False Presuppositions when Misinformation Stakes\n  are High"
                },
                "summary": "This paper examines how LLMs handle false presuppositions and whether certain\nlinguistic factors influence their responses to falsely presupposed content.\nPresuppositions subtly introduce information as given, making them highly\neffective at embedding disputable or false information. This raises concerns\nabout whether LLMs, like humans, may fail to detect and correct misleading\nassumptions introduced as false presuppositions, even when the stakes of\nmisinformation are high. Using a systematic approach based on linguistic\npresupposition analysis, we investigate the conditions under which LLMs are\nmore or less sensitive to adopt or reject false presuppositions. Focusing on\npolitical contexts, we examine how factors like linguistic construction,\npolitical party, and scenario probability impact the recognition of false\npresuppositions. We conduct experiments with a newly created dataset and\nexamine three LLMs: OpenAI's GPT-4-o, Meta's LLama-3-8B, and MistralAI's\nMistral-7B-v03. Our results show that the models struggle to recognize false\npresuppositions, with performance varying by condition. This study highlights\nthat linguistic presupposition analysis is a valuable tool for uncovering the\nreinforcement of political misinformation in LLM responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines how LLMs handle false presuppositions and whether certain\nlinguistic factors influence their responses to falsely presupposed content.\nPresuppositions subtly introduce information as given, making them highly\neffective at embedding disputable or false information. This raises concerns\nabout whether LLMs, like humans, may fail to detect and correct misleading\nassumptions introduced as false presuppositions, even when the stakes of\nmisinformation are high. Using a systematic approach based on linguistic\npresupposition analysis, we investigate the conditions under which LLMs are\nmore or less sensitive to adopt or reject false presuppositions. Focusing on\npolitical contexts, we examine how factors like linguistic construction,\npolitical party, and scenario probability impact the recognition of false\npresuppositions. We conduct experiments with a newly created dataset and\nexamine three LLMs: OpenAI's GPT-4-o, Meta's LLama-3-8B, and MistralAI's\nMistral-7B-v03. Our results show that the models struggle to recognize false\npresuppositions, with performance varying by condition. This study highlights\nthat linguistic presupposition analysis is a valuable tool for uncovering the\nreinforcement of political misinformation in LLM responses."
                },
                "authors": [
                    {
                        "name": "Judith Sieker"
                    },
                    {
                        "name": "Clara Lachenmaier"
                    },
                    {
                        "name": "Sina Zarrieß"
                    }
                ],
                "author_detail": {
                    "name": "Sina Zarrieß"
                },
                "author": "Sina Zarrieß",
                "arxiv_comment": "8 pages (including References). Accepted at CogSci 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09454v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09454v3",
                "updated": "2025-05-28T13:31:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    31,
                    44,
                    2,
                    148,
                    0
                ],
                "published": "2025-03-12T14:57:08Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    57,
                    8,
                    2,
                    71,
                    0
                ],
                "title": "Explicit Learning and the LLM in Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explicit Learning and the LLM in Machine Translation"
                },
                "summary": "This study explores an LLM's ability to learn new languages using\nexplanations found in a grammar book$\\unicode{x2014}$a process we term\n\"explicit learning.\" To rigorously assess this ability, we design controlled\ntranslation experiments between English and constructed languages\ngenerated$\\unicode{x2014}$by specific cryptographic means$\\unicode{x2014}$out\nof Latin or French. Contrary to previous studies, our results demonstrate that\nLLMs do possess a measurable capacity for explicit learning. This ability,\nhowever, diminishes as the complexity of the linguistic phenomena to be learned\nincreases. Supervised fine-tuning on ad hoc chains of thought significantly\nenhances LLM performance but struggles to generalize to typologically novel or\nmore complex linguistic features. These findings point to the need for more\ndiverse training sets and alternative fine-tuning strategies to further improve\nexplicit learning by LLMs, benefiting low-resource languages typically\ndescribed in grammar books but lacking extensive corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores an LLM's ability to learn new languages using\nexplanations found in a grammar book$\\unicode{x2014}$a process we term\n\"explicit learning.\" To rigorously assess this ability, we design controlled\ntranslation experiments between English and constructed languages\ngenerated$\\unicode{x2014}$by specific cryptographic means$\\unicode{x2014}$out\nof Latin or French. Contrary to previous studies, our results demonstrate that\nLLMs do possess a measurable capacity for explicit learning. This ability,\nhowever, diminishes as the complexity of the linguistic phenomena to be learned\nincreases. Supervised fine-tuning on ad hoc chains of thought significantly\nenhances LLM performance but struggles to generalize to typologically novel or\nmore complex linguistic features. These findings point to the need for more\ndiverse training sets and alternative fine-tuning strategies to further improve\nexplicit learning by LLMs, benefiting low-resource languages typically\ndescribed in grammar books but lacking extensive corpora."
                },
                "authors": [
                    {
                        "name": "Malik Marmonier"
                    },
                    {
                        "name": "Rachel Bawden"
                    },
                    {
                        "name": "Benoît Sagot"
                    }
                ],
                "author_detail": {
                    "name": "Benoît Sagot"
                },
                "author": "Benoît Sagot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09454v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09454v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22349v1",
                "updated": "2025-05-28T13:31:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    31,
                    8,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T13:31:08Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    31,
                    8,
                    2,
                    148,
                    0
                ],
                "title": "ChatPD: An LLM-driven Paper-Dataset Networking System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatPD: An LLM-driven Paper-Dataset Networking System"
                },
                "summary": "Scientific research heavily depends on suitable datasets for method\nvalidation, but existing academic platforms with dataset management like\nPapersWithCode suffer from inefficiencies in their manual workflow. To overcome\nthis bottleneck, we present a system, called ChatPD, that utilizes Large\nLanguage Models (LLMs) to automate dataset information extraction from academic\npapers and construct a structured paper-dataset network. Our system consists of\nthree key modules: \\textit{paper collection}, \\textit{dataset information\nextraction}, and \\textit{dataset entity resolution} to construct paper-dataset\nnetworks. Specifically, we propose a \\textit{Graph Completion and Inference}\nstrategy to map dataset descriptions to their corresponding entities. Through\nextensive experiments, we demonstrate that ChatPD not only outperforms the\nexisting platform PapersWithCode in dataset usage extraction but also achieves\nabout 90\\% precision and recall in entity resolution tasks. Moreover, we have\ndeployed ChatPD to continuously extract which datasets are used in papers, and\nprovide a dataset discovery service, such as task-specific dataset queries and\nsimilar dataset recommendations. We open source ChatPD and the current\npaper-dataset network on this [GitHub\nrepository]{https://github.com/ChatPD-web/ChatPD}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific research heavily depends on suitable datasets for method\nvalidation, but existing academic platforms with dataset management like\nPapersWithCode suffer from inefficiencies in their manual workflow. To overcome\nthis bottleneck, we present a system, called ChatPD, that utilizes Large\nLanguage Models (LLMs) to automate dataset information extraction from academic\npapers and construct a structured paper-dataset network. Our system consists of\nthree key modules: \\textit{paper collection}, \\textit{dataset information\nextraction}, and \\textit{dataset entity resolution} to construct paper-dataset\nnetworks. Specifically, we propose a \\textit{Graph Completion and Inference}\nstrategy to map dataset descriptions to their corresponding entities. Through\nextensive experiments, we demonstrate that ChatPD not only outperforms the\nexisting platform PapersWithCode in dataset usage extraction but also achieves\nabout 90\\% precision and recall in entity resolution tasks. Moreover, we have\ndeployed ChatPD to continuously extract which datasets are used in papers, and\nprovide a dataset discovery service, such as task-specific dataset queries and\nsimilar dataset recommendations. We open source ChatPD and the current\npaper-dataset network on this [GitHub\nrepository]{https://github.com/ChatPD-web/ChatPD}."
                },
                "authors": [
                    {
                        "name": "Anjie Xu"
                    },
                    {
                        "name": "Ruiqing Ding"
                    },
                    {
                        "name": "Leye Wang"
                    }
                ],
                "author_detail": {
                    "name": "Leye Wang"
                },
                "author": "Leye Wang",
                "arxiv_doi": "10.1145/3711896.3737202",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711896.3737202",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.22349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by KDD Applied Data Science Track 2025",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11441v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11441v3",
                "updated": "2025-05-28T13:29:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    29,
                    46,
                    2,
                    148,
                    0
                ],
                "published": "2025-02-17T04:55:02Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    4,
                    55,
                    2,
                    0,
                    48,
                    0
                ],
                "title": "Which Retain Set Matters for LLM Unlearning? A Case Study on Entity\n  Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Retain Set Matters for LLM Unlearning? A Case Study on Entity\n  Unlearning"
                },
                "summary": "Large language models (LLMs) risk retaining unauthorized or sensitive\ninformation from their training data, which raises privacy concerns. LLM\nunlearning seeks to mitigate these risks by selectively removing specified data\nwhile maintaining overall model performance. However, most existing work focus\non methods to achieve effective forgetting and does not provide a detailed\nanalysis of the retain set, the portion of training data that is not targeted\nfor removal. In this paper, we investigate the effects of unlearning on various\nsubsets of the retain set through a case study on entity unlearning. We\nintroduce the Syntactically Similar Neighbor Set, a group of queries that share\nsimilar syntactic structures with the data targeted for removal, and show that\nthis subset suffers the greatest performance drop during unlearning. Moreover,\nwhen used for regularization, this set not only preserves performance on\nsyntactically similar queries but also delivers comparable or improved results\nacross other data subsets. Our results highlight that syntactic similarity is a\ncritical factor, potentially more so than domain or entity relationships, in\nachieving effective and practical LLM unlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) risk retaining unauthorized or sensitive\ninformation from their training data, which raises privacy concerns. LLM\nunlearning seeks to mitigate these risks by selectively removing specified data\nwhile maintaining overall model performance. However, most existing work focus\non methods to achieve effective forgetting and does not provide a detailed\nanalysis of the retain set, the portion of training data that is not targeted\nfor removal. In this paper, we investigate the effects of unlearning on various\nsubsets of the retain set through a case study on entity unlearning. We\nintroduce the Syntactically Similar Neighbor Set, a group of queries that share\nsimilar syntactic structures with the data targeted for removal, and show that\nthis subset suffers the greatest performance drop during unlearning. Moreover,\nwhen used for regularization, this set not only preserves performance on\nsyntactically similar queries but also delivers comparable or improved results\nacross other data subsets. Our results highlight that syntactic similarity is a\ncritical factor, potentially more so than domain or entity relationships, in\nachieving effective and practical LLM unlearning."
                },
                "authors": [
                    {
                        "name": "Hwan Chang"
                    },
                    {
                        "name": "Hwanhee Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hwanhee Lee"
                },
                "author": "Hwanhee Lee",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11441v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11441v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22343v1",
                "updated": "2025-05-28T13:27:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    27,
                    7,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T13:27:07Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    27,
                    7,
                    2,
                    148,
                    0
                ],
                "title": "Empowering Intelligent Low-altitude Economy with Large AI Model\n  Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering Intelligent Low-altitude Economy with Large AI Model\n  Deployment"
                },
                "summary": "Low-altitude economy (LAE) represents an emerging economic paradigm that\nredefines commercial and social aerial activities. Large artificial\nintelligence models (LAIMs) offer transformative potential to further enhance\nthe intelligence of LAE services. However, deploying LAIMs in LAE poses several\nchallenges, including the significant gap between their computational/storage\ndemands and the limited onboard resources of LAE entities, the mismatch between\nlab-trained LAIMs and dynamic physical environments, and the inefficiencies of\ntraditional decoupled designs for sensing, communication, and computation. To\naddress these issues, we first propose a hierarchical system architecture\ntailored for LAIM deployment and present representative LAE application\nscenarios. Next, we explore key enabling techniques that facilitate the mutual\nco-evolution of LAIMs and low-altitude systems, and introduce a task-oriented\nexecution pipeline for scalable and adaptive service delivery. Then, the\nproposed framework is validated through real-world case studies. Finally, we\noutline open challenges to inspire future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-altitude economy (LAE) represents an emerging economic paradigm that\nredefines commercial and social aerial activities. Large artificial\nintelligence models (LAIMs) offer transformative potential to further enhance\nthe intelligence of LAE services. However, deploying LAIMs in LAE poses several\nchallenges, including the significant gap between their computational/storage\ndemands and the limited onboard resources of LAE entities, the mismatch between\nlab-trained LAIMs and dynamic physical environments, and the inefficiencies of\ntraditional decoupled designs for sensing, communication, and computation. To\naddress these issues, we first propose a hierarchical system architecture\ntailored for LAIM deployment and present representative LAE application\nscenarios. Next, we explore key enabling techniques that facilitate the mutual\nco-evolution of LAIMs and low-altitude systems, and introduce a task-oriented\nexecution pipeline for scalable and adaptive service delivery. Then, the\nproposed framework is validated through real-world case studies. Finally, we\noutline open challenges to inspire future research."
                },
                "authors": [
                    {
                        "name": "Zhonghao Lyu"
                    },
                    {
                        "name": "Yulan Gao"
                    },
                    {
                        "name": "Junting Chen"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Kaibin Huang"
                    },
                    {
                        "name": "Dong In Kim"
                    }
                ],
                "author_detail": {
                    "name": "Dong In Kim"
                },
                "author": "Dong In Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22334v1",
                "updated": "2025-05-28T13:21:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    21,
                    38,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T13:21:38Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    21,
                    38,
                    2,
                    148,
                    0
                ],
                "title": "Advancing Multimodal Reasoning via Reinforcement Learning with Cold\n  Start",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Multimodal Reasoning via Reinforcement Learning with Cold\n  Start"
                },
                "summary": "Recent advancements in large language models (LLMs) have demonstrated\nimpressive chain-of-thought reasoning capabilities, with reinforcement learning\n(RL) playing a crucial role in this progress. While \"aha moment\"\npatterns--where models exhibit self-correction through reflection--are often\nattributed to emergent properties from RL, we first demonstrate that these\npatterns exist in multimodal LLMs (MLLMs) prior to RL training but may not\nnecessarily correlate with improved reasoning performance. Building on these\ninsights, we present a comprehensive study on enhancing multimodal reasoning\nthrough a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start\nwith structured chain-of-thought reasoning patterns, followed by (2)\nreinforcement learning via GRPO to further refine these capabilities. Our\nextensive experiments show that this combined approach consistently outperforms\nboth SFT-only and RL-only methods across challenging multimodal reasoning\nbenchmarks. The resulting models achieve state-of-the-art performance among\nopen-source MLLMs at both 3B and 7B scales, with our 7B model showing\nsubstantial improvements over base models (e.g., 66.3 %$\\rightarrow$73.4 % on\nMathVista, 62.9 %$\\rightarrow$70.4 % on We-Math) and our 3B model achieving\nperformance competitive with several 7B models. Overall, this work provides\npractical guidance for building advanced multimodal reasoning models. Our code\nis available at https://github.com/waltonfuture/RL-with-Cold-Start.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have demonstrated\nimpressive chain-of-thought reasoning capabilities, with reinforcement learning\n(RL) playing a crucial role in this progress. While \"aha moment\"\npatterns--where models exhibit self-correction through reflection--are often\nattributed to emergent properties from RL, we first demonstrate that these\npatterns exist in multimodal LLMs (MLLMs) prior to RL training but may not\nnecessarily correlate with improved reasoning performance. Building on these\ninsights, we present a comprehensive study on enhancing multimodal reasoning\nthrough a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start\nwith structured chain-of-thought reasoning patterns, followed by (2)\nreinforcement learning via GRPO to further refine these capabilities. Our\nextensive experiments show that this combined approach consistently outperforms\nboth SFT-only and RL-only methods across challenging multimodal reasoning\nbenchmarks. The resulting models achieve state-of-the-art performance among\nopen-source MLLMs at both 3B and 7B scales, with our 7B model showing\nsubstantial improvements over base models (e.g., 66.3 %$\\rightarrow$73.4 % on\nMathVista, 62.9 %$\\rightarrow$70.4 % on We-Math) and our 3B model achieving\nperformance competitive with several 7B models. Overall, this work provides\npractical guidance for building advanced multimodal reasoning models. Our code\nis available at https://github.com/waltonfuture/RL-with-Cold-Start."
                },
                "authors": [
                    {
                        "name": "Lai Wei"
                    },
                    {
                        "name": "Yuting Li"
                    },
                    {
                        "name": "Kaipeng Zheng"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Weiran Huang"
                    }
                ],
                "author_detail": {
                    "name": "Weiran Huang"
                },
                "author": "Weiran Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21082v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21082v2",
                "updated": "2025-05-28T13:20:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    20,
                    32,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-27T12:06:16Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    12,
                    6,
                    16,
                    1,
                    147,
                    0
                ],
                "title": "LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for\n  Black-Box Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for\n  Black-Box Large Language Models"
                },
                "summary": "Large language models (LLMs) have recently achieved impressive performance\nacross a wide range of natural language tasks and are now widely used in\nreal-world applications. Among them, black-box LLMs--served via APIs without\naccess to model internals--are especially dominant due to their scalability and\nease of deployment. Despite their strong capabilities, these models typically\nproduce generalized responses that overlook personal preferences and reasoning\nstyles. This has led to growing interest in black-box LLM personalization,\nwhich aims to tailor model outputs to user-specific context without modifying\nmodel parameters. However, existing approaches primarily focus on\nresponse-level personalization, attempting to match final outputs without\nmodeling personal thought process. To address this limitation, we propose RPM,\na framework for reasoning-level personalization that aligns the model's\nreasoning process with a user's personalized logic. RPM first constructs\nstatistical user-specific factors by extracting and grouping\nresponse-influential features from user history. It then builds personalized\nreasoning paths that reflect how these factors are used in context. In the\ninference stage, RPM retrieves reasoning-aligned examples for new queries via\nfeature-level similarity and performs inference conditioned on the structured\nfactors and retrieved reasoning paths, enabling the model to follow\nuser-specific reasoning trajectories. This reasoning-level personalization\nenhances both predictive accuracy and interpretability by grounding model\noutputs in user-specific logic through structured information. Extensive\nexperiments across diverse tasks show that RPM consistently outperforms\nresponse-level personalization methods, demonstrating the effectiveness of\nreasoning-level personalization in black-box LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently achieved impressive performance\nacross a wide range of natural language tasks and are now widely used in\nreal-world applications. Among them, black-box LLMs--served via APIs without\naccess to model internals--are especially dominant due to their scalability and\nease of deployment. Despite their strong capabilities, these models typically\nproduce generalized responses that overlook personal preferences and reasoning\nstyles. This has led to growing interest in black-box LLM personalization,\nwhich aims to tailor model outputs to user-specific context without modifying\nmodel parameters. However, existing approaches primarily focus on\nresponse-level personalization, attempting to match final outputs without\nmodeling personal thought process. To address this limitation, we propose RPM,\na framework for reasoning-level personalization that aligns the model's\nreasoning process with a user's personalized logic. RPM first constructs\nstatistical user-specific factors by extracting and grouping\nresponse-influential features from user history. It then builds personalized\nreasoning paths that reflect how these factors are used in context. In the\ninference stage, RPM retrieves reasoning-aligned examples for new queries via\nfeature-level similarity and performs inference conditioned on the structured\nfactors and retrieved reasoning paths, enabling the model to follow\nuser-specific reasoning trajectories. This reasoning-level personalization\nenhances both predictive accuracy and interpretability by grounding model\noutputs in user-specific logic through structured information. Extensive\nexperiments across diverse tasks show that RPM consistently outperforms\nresponse-level personalization methods, demonstrating the effectiveness of\nreasoning-level personalization in black-box LLMs."
                },
                "authors": [
                    {
                        "name": "Jieyong Kim"
                    },
                    {
                        "name": "Tongyoung Kim"
                    },
                    {
                        "name": "Soojin Yoon"
                    },
                    {
                        "name": "Jaehyung Kim"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21082v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21082v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22327v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22327v1",
                "updated": "2025-05-28T13:14:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    14,
                    44,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T13:14:44Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    14,
                    44,
                    2,
                    148,
                    0
                ],
                "title": "NLP for Social Good: A Survey of Challenges, Opportunities, and\n  Responsible Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NLP for Social Good: A Survey of Challenges, Opportunities, and\n  Responsible Deployment"
                },
                "summary": "Recent advancements in large language models (LLMs) have unlocked\nunprecedented possibilities across a range of applications. However, as a\ncommunity, we believe that the field of Natural Language Processing (NLP) has a\ngrowing need to approach deployment with greater intentionality and\nresponsibility. In alignment with the broader vision of AI for Social Good\n(Toma\\v{s}ev et al., 2020), this paper examines the role of NLP in addressing\npressing societal challenges. Through a cross-disciplinary analysis of social\ngoals and emerging risks, we highlight promising research directions and\noutline challenges that must be addressed to ensure responsible and equitable\nprogress in NLP4SG research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have unlocked\nunprecedented possibilities across a range of applications. However, as a\ncommunity, we believe that the field of Natural Language Processing (NLP) has a\ngrowing need to approach deployment with greater intentionality and\nresponsibility. In alignment with the broader vision of AI for Social Good\n(Toma\\v{s}ev et al., 2020), this paper examines the role of NLP in addressing\npressing societal challenges. Through a cross-disciplinary analysis of social\ngoals and emerging risks, we highlight promising research directions and\noutline challenges that must be addressed to ensure responsible and equitable\nprogress in NLP4SG research."
                },
                "authors": [
                    {
                        "name": "Antonia Karamolegkou"
                    },
                    {
                        "name": "Angana Borah"
                    },
                    {
                        "name": "Eunjung Cho"
                    },
                    {
                        "name": "Sagnik Ray Choudhury"
                    },
                    {
                        "name": "Martina Galletti"
                    },
                    {
                        "name": "Rajarshi Ghosh"
                    },
                    {
                        "name": "Pranav Gupta"
                    },
                    {
                        "name": "Oana Ignat"
                    },
                    {
                        "name": "Priyanka Kargupta"
                    },
                    {
                        "name": "Neema Kotonya"
                    },
                    {
                        "name": "Hemank Lamba"
                    },
                    {
                        "name": "Sun-Joo Lee"
                    },
                    {
                        "name": "Arushi Mangla"
                    },
                    {
                        "name": "Ishani Mondal"
                    },
                    {
                        "name": "Deniz Nazarova"
                    },
                    {
                        "name": "Poli Nemkova"
                    },
                    {
                        "name": "Dina Pisarevskaya"
                    },
                    {
                        "name": "Naquee Rizwan"
                    },
                    {
                        "name": "Nazanin Sabri"
                    },
                    {
                        "name": "Dominik Stammbach"
                    },
                    {
                        "name": "Anna Steinberg"
                    },
                    {
                        "name": "David Tomás"
                    },
                    {
                        "name": "Steven R Wilson"
                    },
                    {
                        "name": "Bowen Yi"
                    },
                    {
                        "name": "Jessica H Zhu"
                    },
                    {
                        "name": "Arkaitz Zubiaga"
                    },
                    {
                        "name": "Anders Søgaard"
                    },
                    {
                        "name": "Alexander Fraser"
                    },
                    {
                        "name": "Zhijing Jin"
                    },
                    {
                        "name": "Rada Mihalcea"
                    },
                    {
                        "name": "Joel R. Tetreault"
                    },
                    {
                        "name": "Daryna Dementieva"
                    }
                ],
                "author_detail": {
                    "name": "Daryna Dementieva"
                },
                "author": "Daryna Dementieva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22327v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22327v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22323v1",
                "updated": "2025-05-28T13:09:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    9,
                    47,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T13:09:47Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    9,
                    47,
                    2,
                    148,
                    0
                ],
                "title": "Advancing Expert Specialization for Better MoE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Expert Specialization for Better MoE"
                },
                "summary": "Mixture-of-Experts (MoE) models enable efficient scaling of large language\nmodels (LLMs) by activating only a subset of experts per input. However, we\nobserve that the commonly used auxiliary load balancing loss often leads to\nexpert overlap and overly uniform routing, which hinders expert specialization\nand degrades overall performance during post-training. To address this, we\npropose a simple yet effective solution that introduces two complementary\nobjectives: (1) an orthogonality loss to encourage experts to process distinct\ntypes of tokens, and (2) a variance loss to encourage more discriminative\nrouting decisions. Gradient-level analysis demonstrates that these objectives\nare compatible with the existing auxiliary loss and contribute to optimizing\nthe training process. Experimental results over various model architectures and\nacross multiple benchmarks show that our method significantly enhances expert\nspecialization. Notably, our method improves classic MoE baselines with\nauxiliary loss by up to 23.79%, while also maintaining load balancing in\ndownstream tasks, without any architectural modifications or additional\ncomponents. We will release our code to contribute to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models enable efficient scaling of large language\nmodels (LLMs) by activating only a subset of experts per input. However, we\nobserve that the commonly used auxiliary load balancing loss often leads to\nexpert overlap and overly uniform routing, which hinders expert specialization\nand degrades overall performance during post-training. To address this, we\npropose a simple yet effective solution that introduces two complementary\nobjectives: (1) an orthogonality loss to encourage experts to process distinct\ntypes of tokens, and (2) a variance loss to encourage more discriminative\nrouting decisions. Gradient-level analysis demonstrates that these objectives\nare compatible with the existing auxiliary loss and contribute to optimizing\nthe training process. Experimental results over various model architectures and\nacross multiple benchmarks show that our method significantly enhances expert\nspecialization. Notably, our method improves classic MoE baselines with\nauxiliary loss by up to 23.79%, while also maintaining load balancing in\ndownstream tasks, without any architectural modifications or additional\ncomponents. We will release our code to contribute to the community."
                },
                "authors": [
                    {
                        "name": "Hongcan Guo"
                    },
                    {
                        "name": "Haolang Lu"
                    },
                    {
                        "name": "Guoshun Nan"
                    },
                    {
                        "name": "Bolun Chu"
                    },
                    {
                        "name": "Jialin Zhuang"
                    },
                    {
                        "name": "Yuan Yang"
                    },
                    {
                        "name": "Wenhao Che"
                    },
                    {
                        "name": "Sicong Leng"
                    },
                    {
                        "name": "Qimei Cui"
                    },
                    {
                        "name": "Xudong Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xudong Jiang"
                },
                "author": "Xudong Jiang",
                "arxiv_comment": "33pages, 6figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12465v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12465v2",
                "updated": "2025-05-28T13:05:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    5,
                    34,
                    2,
                    148,
                    0
                ],
                "published": "2024-12-17T01:54:08Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    54,
                    8,
                    1,
                    352,
                    0
                ],
                "title": "Core Context Aware Transformers for Long Context Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Core Context Aware Transformers for Long Context Language Modeling"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have exhibited remarkable\nsuccess in extensive tasks primarily attributed to self-attention mechanism,\nwhich requires a token to consider all preceding tokens as its context to\ncompute attention. However, when the context length L becomes very large (e.g.,\n128K), the amount of potentially redundant information in the context tends to\nincrease. The redundant context not only hampers the modeling representation\nperformance but also incurs unnecessary computational and storage overhead. In\nthis paper, we propose a plug-and-play Core Context Aware (CCA) Attention for\nefficient long-context modeling, comprising two complementary modules: 1)\nGlobality-aware pooling module groups input tokens and dynamically compresses\neach group into one core token based on their significance. In this way, our\nmethod automatically focuses and strengthens core context while diminishing\nredundancy during the learning process, leading to effective long-term\ndependency modeling. 2) Locality-preserving module incorporates neighboring\ntokens to preserve local context for detailed representation. Notably, our\nCCA-Attention is able to replace the self-attention module in existing LLMs\nwith minimal fine-tuning cost. Extensive experimental results show the\nsuperiority of our method in both long-context modeling and computational\nefficiency over state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have exhibited remarkable\nsuccess in extensive tasks primarily attributed to self-attention mechanism,\nwhich requires a token to consider all preceding tokens as its context to\ncompute attention. However, when the context length L becomes very large (e.g.,\n128K), the amount of potentially redundant information in the context tends to\nincrease. The redundant context not only hampers the modeling representation\nperformance but also incurs unnecessary computational and storage overhead. In\nthis paper, we propose a plug-and-play Core Context Aware (CCA) Attention for\nefficient long-context modeling, comprising two complementary modules: 1)\nGlobality-aware pooling module groups input tokens and dynamically compresses\neach group into one core token based on their significance. In this way, our\nmethod automatically focuses and strengthens core context while diminishing\nredundancy during the learning process, leading to effective long-term\ndependency modeling. 2) Locality-preserving module incorporates neighboring\ntokens to preserve local context for detailed representation. Notably, our\nCCA-Attention is able to replace the self-attention module in existing LLMs\nwith minimal fine-tuning cost. Extensive experimental results show the\nsuperiority of our method in both long-context modeling and computational\nefficiency over state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yaofo Chen"
                    },
                    {
                        "name": "Zeng You"
                    },
                    {
                        "name": "Shuhai Zhang"
                    },
                    {
                        "name": "Haokun Li"
                    },
                    {
                        "name": "Yirui Li"
                    },
                    {
                        "name": "Yaowei Wang"
                    },
                    {
                        "name": "Mingkui Tan"
                    }
                ],
                "author_detail": {
                    "name": "Mingkui Tan"
                },
                "author": "Mingkui Tan",
                "arxiv_comment": "Accepted for publication at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12465v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12465v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21072v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21072v2",
                "updated": "2025-05-28T13:05:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    5,
                    12,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-27T11:56:59Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    11,
                    56,
                    59,
                    1,
                    147,
                    0
                ],
                "title": "Faithfulness-Aware Uncertainty Quantification for Fact-Checking the\n  Output of Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faithfulness-Aware Uncertainty Quantification for Fact-Checking the\n  Output of Retrieval Augmented Generation"
                },
                "summary": "Large Language Models (LLMs) enhanced with external knowledge retrieval, an\napproach known as Retrieval-Augmented Generation (RAG), have shown strong\nperformance in open-domain question answering. However, RAG systems remain\nsusceptible to hallucinations: factually incorrect outputs that may arise\neither from inconsistencies in the model's internal knowledge or incorrect use\nof the retrieved context. Existing approaches often conflate factuality with\nfaithfulness to the retrieved context, misclassifying factually correct\nstatements as hallucinations if they are not directly supported by the\nretrieval. In this paper, we introduce FRANQ (Faithfulness-based Retrieval\nAugmented UNcertainty Quantification), a novel method for hallucination\ndetection in RAG outputs. FRANQ applies different Uncertainty Quantification\n(UQ) techniques to estimate factuality based on whether a statement is faithful\nto the retrieved context or not. To evaluate FRANQ and other UQ techniques for\nRAG, we present a new long-form Question Answering (QA) dataset annotated for\nboth factuality and faithfulness, combining automated labeling with manual\nvalidation of challenging examples. Extensive experiments on long- and\nshort-form QA across multiple datasets and LLMs show that FRANQ achieves more\naccurate detection of factual errors in RAG-generated responses compared to\nexisting methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) enhanced with external knowledge retrieval, an\napproach known as Retrieval-Augmented Generation (RAG), have shown strong\nperformance in open-domain question answering. However, RAG systems remain\nsusceptible to hallucinations: factually incorrect outputs that may arise\neither from inconsistencies in the model's internal knowledge or incorrect use\nof the retrieved context. Existing approaches often conflate factuality with\nfaithfulness to the retrieved context, misclassifying factually correct\nstatements as hallucinations if they are not directly supported by the\nretrieval. In this paper, we introduce FRANQ (Faithfulness-based Retrieval\nAugmented UNcertainty Quantification), a novel method for hallucination\ndetection in RAG outputs. FRANQ applies different Uncertainty Quantification\n(UQ) techniques to estimate factuality based on whether a statement is faithful\nto the retrieved context or not. To evaluate FRANQ and other UQ techniques for\nRAG, we present a new long-form Question Answering (QA) dataset annotated for\nboth factuality and faithfulness, combining automated labeling with manual\nvalidation of challenging examples. Extensive experiments on long- and\nshort-form QA across multiple datasets and LLMs show that FRANQ achieves more\naccurate detection of factual errors in RAG-generated responses compared to\nexisting methods."
                },
                "authors": [
                    {
                        "name": "Ekaterina Fadeeva"
                    },
                    {
                        "name": "Aleksandr Rubashevskii"
                    },
                    {
                        "name": "Roman Vashurin"
                    },
                    {
                        "name": "Shehzaad Dhuliawala"
                    },
                    {
                        "name": "Artem Shelmanov"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Maxim Panov"
                    }
                ],
                "author_detail": {
                    "name": "Maxim Panov"
                },
                "author": "Maxim Panov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21072v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21072v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22320v1",
                "updated": "2025-05-28T13:04:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    4,
                    48,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T13:04:48Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    4,
                    48,
                    2,
                    148,
                    0
                ],
                "title": "Chain-of-Thought for Large Language Model-empowered Wireless\n  Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought for Large Language Model-empowered Wireless\n  Communications"
                },
                "summary": "Recent advances in large language models (LLMs) have opened new possibilities\nfor automated reasoning and decision-making in wireless networks. However,\napplying LLMs to wireless communications presents challenges such as limited\ncapability in handling complex logic, generalization, and reasoning.\nChain-of-Thought (CoT) prompting, which guides LLMs to generate explicit\nintermediate reasoning steps, has been shown to significantly improve LLM\nperformance on complex tasks. Inspired by this, this paper explores the\napplication potential of CoT-enhanced LLMs in wireless communications.\nSpecifically, we first review the fundamental theory of CoT and summarize\nvarious types of CoT. We then survey key CoT and LLM techniques relevant to\nwireless communication and networking. Moreover, we introduce a multi-layer\nintent-driven CoT framework that bridges high-level user intent expressed in\nnatural language with concrete wireless control actions. Our proposed framework\nsequentially parses and clusters intent, selects appropriate CoT reasoning\nmodules via reinforcement learning, then generates interpretable control\npolicies for system configuration. Using the unmanned aerial vehicle (UAV)\nnetwork as a case study, we demonstrate that the proposed framework\nsignificantly outperforms a non-CoT baseline in both communication performance\nand quality of generated reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have opened new possibilities\nfor automated reasoning and decision-making in wireless networks. However,\napplying LLMs to wireless communications presents challenges such as limited\ncapability in handling complex logic, generalization, and reasoning.\nChain-of-Thought (CoT) prompting, which guides LLMs to generate explicit\nintermediate reasoning steps, has been shown to significantly improve LLM\nperformance on complex tasks. Inspired by this, this paper explores the\napplication potential of CoT-enhanced LLMs in wireless communications.\nSpecifically, we first review the fundamental theory of CoT and summarize\nvarious types of CoT. We then survey key CoT and LLM techniques relevant to\nwireless communication and networking. Moreover, we introduce a multi-layer\nintent-driven CoT framework that bridges high-level user intent expressed in\nnatural language with concrete wireless control actions. Our proposed framework\nsequentially parses and clusters intent, selects appropriate CoT reasoning\nmodules via reinforcement learning, then generates interpretable control\npolicies for system configuration. Using the unmanned aerial vehicle (UAV)\nnetwork as a case study, we demonstrate that the proposed framework\nsignificantly outperforms a non-CoT baseline in both communication performance\nand quality of generated reasoning."
                },
                "authors": [
                    {
                        "name": "Xudong Wang"
                    },
                    {
                        "name": "Jian Zhu"
                    },
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Lei Feng"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Shiwen Mao"
                    },
                    {
                        "name": "Zhu Han"
                    }
                ],
                "author_detail": {
                    "name": "Zhu Han"
                },
                "author": "Zhu Han",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22318v1",
                "updated": "2025-05-28T13:03:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    3,
                    18,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T13:03:18Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    3,
                    18,
                    2,
                    148,
                    0
                ],
                "title": "If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?"
                },
                "summary": "Large Language Models (LLMs) demonstrate impressive reasoning capabilities in\nfamiliar contexts, but struggle when the context conflicts with their\nparametric knowledge. To investigate this phenomenon, we introduce\nCounterLogic, a dataset containing 1,800 examples across 9 logical schemas,\nexplicitly designed to evaluate logical reasoning through counterfactual\n(hypothetical knowledge-conflicting) scenarios. Our systematic evaluation of 11\nLLMs across 6 different datasets reveals a consistent performance degradation,\nwith accuracies dropping by 27% on average when reasoning through\ncounterfactual information. We propose Self-Segregate, a prompting method\nenabling metacognitive awareness (explicitly identifying knowledge conflicts)\nbefore reasoning. Our method dramatically narrows the average performance gaps\nfrom 27% to just 11%, while significantly increasing the overall accuracy\n(+7.5%). We discuss the implications of these findings and draw parallels to\nhuman cognitive processes, particularly on how humans disambiguate conflicting\ninformation during reasoning tasks. Our findings offer practical insights for\nunderstanding and enhancing LLMs reasoning capabilities in real-world\napplications, especially where models must logically reason independently of\ntheir factual knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate impressive reasoning capabilities in\nfamiliar contexts, but struggle when the context conflicts with their\nparametric knowledge. To investigate this phenomenon, we introduce\nCounterLogic, a dataset containing 1,800 examples across 9 logical schemas,\nexplicitly designed to evaluate logical reasoning through counterfactual\n(hypothetical knowledge-conflicting) scenarios. Our systematic evaluation of 11\nLLMs across 6 different datasets reveals a consistent performance degradation,\nwith accuracies dropping by 27% on average when reasoning through\ncounterfactual information. We propose Self-Segregate, a prompting method\nenabling metacognitive awareness (explicitly identifying knowledge conflicts)\nbefore reasoning. Our method dramatically narrows the average performance gaps\nfrom 27% to just 11%, while significantly increasing the overall accuracy\n(+7.5%). We discuss the implications of these findings and draw parallels to\nhuman cognitive processes, particularly on how humans disambiguate conflicting\ninformation during reasoning tasks. Our findings offer practical insights for\nunderstanding and enhancing LLMs reasoning capabilities in real-world\napplications, especially where models must logically reason independently of\ntheir factual knowledge."
                },
                "authors": [
                    {
                        "name": "Ishwar B Balappanawar"
                    },
                    {
                        "name": "Vamshi Krishna Bonagiri"
                    },
                    {
                        "name": "Anish R Joishy"
                    },
                    {
                        "name": "Manas Gaur"
                    },
                    {
                        "name": "Krishnaprasad Thirunarayan"
                    },
                    {
                        "name": "Ponnurangam Kumaraguru"
                    }
                ],
                "author_detail": {
                    "name": "Ponnurangam Kumaraguru"
                },
                "author": "Ponnurangam Kumaraguru",
                "arxiv_comment": "16 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22312v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22312v2",
                "updated": "2025-05-29T09:07:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    7,
                    33,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-28T12:56:04Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    12,
                    56,
                    4,
                    2,
                    148,
                    0
                ],
                "title": "Skywork Open Reasoner 1 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skywork Open Reasoner 1 Technical Report"
                },
                "summary": "The success of DeepSeek-R1 underscores the significant role of reinforcement\nlearning (RL) in enhancing the reasoning capabilities of large language models\n(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL\nimplementation for long Chain-of-Thought (CoT) models. Building on the\nDeepSeek-R1-Distill model series, our RL approach achieves notable performance\ngains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench\nfrom 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)\nfor the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and\nQwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable\nresults on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models\ndemonstrate competitive reasoning capabilities among models of similar size. We\nperform comprehensive ablation studies on the core components of our training\npipeline to validate their effectiveness. Additionally, we thoroughly\ninvestigate the phenomenon of entropy collapse, identify key factors affecting\nentropy dynamics, and demonstrate that mitigating premature entropy collapse is\ncritical for improved test performance. To support community research, we fully\nopen-source our model weights, training code, and training datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of DeepSeek-R1 underscores the significant role of reinforcement\nlearning (RL) in enhancing the reasoning capabilities of large language models\n(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL\nimplementation for long Chain-of-Thought (CoT) models. Building on the\nDeepSeek-R1-Distill model series, our RL approach achieves notable performance\ngains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench\nfrom 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)\nfor the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and\nQwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable\nresults on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models\ndemonstrate competitive reasoning capabilities among models of similar size. We\nperform comprehensive ablation studies on the core components of our training\npipeline to validate their effectiveness. Additionally, we thoroughly\ninvestigate the phenomenon of entropy collapse, identify key factors affecting\nentropy dynamics, and demonstrate that mitigating premature entropy collapse is\ncritical for improved test performance. To support community research, we fully\nopen-source our model weights, training code, and training datasets."
                },
                "authors": [
                    {
                        "name": "Jujie He"
                    },
                    {
                        "name": "Jiacai Liu"
                    },
                    {
                        "name": "Chris Yuhao Liu"
                    },
                    {
                        "name": "Rui Yan"
                    },
                    {
                        "name": "Chaojie Wang"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Xiaoyu Zhang"
                    },
                    {
                        "name": "Fuxiang Zhang"
                    },
                    {
                        "name": "Jiacheng Xu"
                    },
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Liang Zeng"
                    },
                    {
                        "name": "Tianwen Wei"
                    },
                    {
                        "name": "Cheng Cheng"
                    },
                    {
                        "name": "Bo An"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yahui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yahui Zhou"
                },
                "author": "Yahui Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22312v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22312v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22311v1",
                "updated": "2025-05-28T12:54:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    12,
                    54,
                    7,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T12:54:07Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    12,
                    54,
                    7,
                    2,
                    148,
                    0
                ],
                "title": "From Large AI Models to Agentic AI: A Tutorial on Future Intelligent\n  Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Large AI Models to Agentic AI: A Tutorial on Future Intelligent\n  Communications"
                },
                "summary": "With the advent of 6G communications, intelligent communication systems face\nmultiple challenges, including constrained perception and response\ncapabilities, limited scalability, and low adaptability in dynamic\nenvironments. This tutorial provides a systematic introduction to the\nprinciples, design, and applications of Large Artificial Intelligence Models\n(LAMs) and Agentic AI technologies in intelligent communication systems, aiming\nto offer researchers a comprehensive overview of cutting-edge technologies and\npractical guidance. First, we outline the background of 6G communications,\nreview the technological evolution from LAMs to Agentic AI, and clarify the\ntutorial's motivation and main contributions. Subsequently, we present a\ncomprehensive review of the key components required for constructing LAMs. We\nfurther categorize LAMs and analyze their applicability, covering Large\nLanguage Models (LLMs), Large Vision Models (LVMs), Large Multimodal Models\n(LMMs), Large Reasoning Models (LRMs), and lightweight LAMs. Next, we propose a\nLAM-centric design paradigm tailored for communications, encompassing dataset\nconstruction and both internal and external learning approaches. Building upon\nthis, we develop an LAM-based Agentic AI system for intelligent communications,\nclarifying its core components such as planners, knowledge bases, tools, and\nmemory modules, as well as its interaction mechanisms. We also introduce a\nmulti-agent framework with data retrieval, collaborative planning, and\nreflective evaluation for 6G. Subsequently, we provide a detailed overview of\nthe applications of LAMs and Agentic AI in communication scenarios. Finally, we\nsummarize the research challenges and future directions in current studies,\naiming to support the development of efficient, secure, and sustainable\nnext-generation intelligent communication systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of 6G communications, intelligent communication systems face\nmultiple challenges, including constrained perception and response\ncapabilities, limited scalability, and low adaptability in dynamic\nenvironments. This tutorial provides a systematic introduction to the\nprinciples, design, and applications of Large Artificial Intelligence Models\n(LAMs) and Agentic AI technologies in intelligent communication systems, aiming\nto offer researchers a comprehensive overview of cutting-edge technologies and\npractical guidance. First, we outline the background of 6G communications,\nreview the technological evolution from LAMs to Agentic AI, and clarify the\ntutorial's motivation and main contributions. Subsequently, we present a\ncomprehensive review of the key components required for constructing LAMs. We\nfurther categorize LAMs and analyze their applicability, covering Large\nLanguage Models (LLMs), Large Vision Models (LVMs), Large Multimodal Models\n(LMMs), Large Reasoning Models (LRMs), and lightweight LAMs. Next, we propose a\nLAM-centric design paradigm tailored for communications, encompassing dataset\nconstruction and both internal and external learning approaches. Building upon\nthis, we develop an LAM-based Agentic AI system for intelligent communications,\nclarifying its core components such as planners, knowledge bases, tools, and\nmemory modules, as well as its interaction mechanisms. We also introduce a\nmulti-agent framework with data retrieval, collaborative planning, and\nreflective evaluation for 6G. Subsequently, we provide a detailed overview of\nthe applications of LAMs and Agentic AI in communication scenarios. Finally, we\nsummarize the research challenges and future directions in current studies,\naiming to support the development of efficient, secure, and sustainable\nnext-generation intelligent communication systems."
                },
                "authors": [
                    {
                        "name": "Feibo Jiang"
                    },
                    {
                        "name": "Cunhua Pan"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Kezhi Wang"
                    },
                    {
                        "name": "Octavia A. Dobre"
                    },
                    {
                        "name": "Merouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Merouane Debbah"
                },
                "author": "Merouane Debbah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22310v1",
                "updated": "2025-05-28T12:53:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    12,
                    53,
                    8,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T12:53:08Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    12,
                    53,
                    8,
                    2,
                    148,
                    0
                ],
                "title": "From Dormant to Deleted: Tamper-Resistant Unlearning Through\n  Weight-Space Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Dormant to Deleted: Tamper-Resistant Unlearning Through\n  Weight-Space Regularization"
                },
                "summary": "Recent unlearning methods for LLMs are vulnerable to relearning attacks:\nknowledge believed-to-be-unlearned re-emerges by fine-tuning on a small set of\n(even seemingly-unrelated) examples. We study this phenomenon in a controlled\nsetting for example-level unlearning in vision classifiers. We make the\nsurprising discovery that forget-set accuracy can recover from around 50%\npost-unlearning to nearly 100% with fine-tuning on just the retain set -- i.e.,\nzero examples of the forget set. We observe this effect across a wide variety\nof unlearning methods, whereas for a model retrained from scratch excluding the\nforget set (gold standard), the accuracy remains at 50%. We observe that\nresistance to relearning attacks can be predicted by weight-space properties,\nspecifically, $L_2$-distance and linear mode connectivity between the original\nand the unlearned model. Leveraging this insight, we propose a new class of\nmethods that achieve state-of-the-art resistance to relearning attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent unlearning methods for LLMs are vulnerable to relearning attacks:\nknowledge believed-to-be-unlearned re-emerges by fine-tuning on a small set of\n(even seemingly-unrelated) examples. We study this phenomenon in a controlled\nsetting for example-level unlearning in vision classifiers. We make the\nsurprising discovery that forget-set accuracy can recover from around 50%\npost-unlearning to nearly 100% with fine-tuning on just the retain set -- i.e.,\nzero examples of the forget set. We observe this effect across a wide variety\nof unlearning methods, whereas for a model retrained from scratch excluding the\nforget set (gold standard), the accuracy remains at 50%. We observe that\nresistance to relearning attacks can be predicted by weight-space properties,\nspecifically, $L_2$-distance and linear mode connectivity between the original\nand the unlearned model. Leveraging this insight, we propose a new class of\nmethods that achieve state-of-the-art resistance to relearning attacks."
                },
                "authors": [
                    {
                        "name": "Shoaib Ahmed Siddiqui"
                    },
                    {
                        "name": "Adrian Weller"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Gintare Karolina Dziugaite"
                    },
                    {
                        "name": "Michael Curtis Mozer"
                    },
                    {
                        "name": "Eleni Triantafillou"
                    }
                ],
                "author_detail": {
                    "name": "Eleni Triantafillou"
                },
                "author": "Eleni Triantafillou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20839v2",
                "updated": "2025-05-28T12:51:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    12,
                    51,
                    23,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-27T07:58:35Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    7,
                    58,
                    35,
                    1,
                    147,
                    0
                ],
                "title": "FireQ: Fast INT4-FP8 Kernel and RoPE-aware Quantization for LLM\n  Inference Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FireQ: Fast INT4-FP8 Kernel and RoPE-aware Quantization for LLM\n  Inference Acceleration"
                },
                "summary": "As large language models become increasingly prevalent, memory bandwidth\nconstraints significantly limit inference throughput, motivating post-training\nquantization (PTQ). In this paper, we propose FireQ, a co-designed PTQ\nframework and an INT4-FP8 matrix multiplication kernel that accelerates LLM\ninference across all linear layers. Specifically, FireQ quantizes linear layer\nweights and key-values to INT4, and activations and queries to FP8,\nsignificantly enhancing throughput. Additionally, we introduce a three-stage\npipelining for the prefill phase, which modifies the FlashAttention-3 kernel,\neffectively reducing time-to-first-token in the prefill phase. To minimize\naccuracy loss from quantization, we develop novel outlier smoothing techniques\ntailored separately for linear and attention layers. In linear layers, we\nexplicitly use per-tensor scaling to prevent underflow caused by the FP8\nquantization scaling factor of INT4 quantization, and channel-wise scaling to\ncompensate for coarse granularity of INT4. In attention layers, we address\nquantization challenges posed by rotary positional embeddings (RoPE) by\ncombining pre-RoPE and post-RoPE scaling strategies. FireQ significantly\noutperforms state-of-the-art methods, achieving 1.68x faster inference in\nfeed-forward network layers on Llama2-7B and 1.26x faster prefill phase\nperformance on Llama3-8B compared to QServe, with negligible accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models become increasingly prevalent, memory bandwidth\nconstraints significantly limit inference throughput, motivating post-training\nquantization (PTQ). In this paper, we propose FireQ, a co-designed PTQ\nframework and an INT4-FP8 matrix multiplication kernel that accelerates LLM\ninference across all linear layers. Specifically, FireQ quantizes linear layer\nweights and key-values to INT4, and activations and queries to FP8,\nsignificantly enhancing throughput. Additionally, we introduce a three-stage\npipelining for the prefill phase, which modifies the FlashAttention-3 kernel,\neffectively reducing time-to-first-token in the prefill phase. To minimize\naccuracy loss from quantization, we develop novel outlier smoothing techniques\ntailored separately for linear and attention layers. In linear layers, we\nexplicitly use per-tensor scaling to prevent underflow caused by the FP8\nquantization scaling factor of INT4 quantization, and channel-wise scaling to\ncompensate for coarse granularity of INT4. In attention layers, we address\nquantization challenges posed by rotary positional embeddings (RoPE) by\ncombining pre-RoPE and post-RoPE scaling strategies. FireQ significantly\noutperforms state-of-the-art methods, achieving 1.68x faster inference in\nfeed-forward network layers on Llama2-7B and 1.26x faster prefill phase\nperformance on Llama3-8B compared to QServe, with negligible accuracy loss."
                },
                "authors": [
                    {
                        "name": "Daehyeon Baek"
                    },
                    {
                        "name": "Jieun Choi"
                    },
                    {
                        "name": "Jimyoung Son"
                    },
                    {
                        "name": "Kyungmin Bin"
                    },
                    {
                        "name": "Seungbeom Choi"
                    },
                    {
                        "name": "Kihyo Moon"
                    },
                    {
                        "name": "Minsung Jang"
                    },
                    {
                        "name": "Hyojung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hyojung Lee"
                },
                "author": "Hyojung Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18436v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18436v2",
                "updated": "2025-05-28T12:49:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    12,
                    49,
                    15,
                    2,
                    148,
                    0
                ],
                "published": "2024-10-24T05:14:03Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    14,
                    3,
                    3,
                    298,
                    0
                ],
                "title": "Can Code-Switched Texts Activate a Knowledge Switch in LLMs? A Case\n  Study on English-Korean Code-Switching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Code-Switched Texts Activate a Knowledge Switch in LLMs? A Case\n  Study on English-Korean Code-Switching"
                },
                "summary": "Recent large language models (LLMs) demonstrate multilingual abilities, yet\nthey are English-centric due to dominance of English in training corpora. The\nlimited resource for low-resource languages remains a crucial challenge.\nCode-switching (CS), a phenomenon where multilingual speakers alternate between\nlanguages in a discourse, can convey subtle cultural and linguistic nuances\nthat can be otherwise lost in translation and elicits language-specific\nknowledge in human communications. In light of this, we investigate whether\ncode-switching can 'activate', or identify and leverage knowledge for reasoning\nwhen LLMs solve low-resource language tasks. To facilitate the research, we\nfirst present EnKoQA, a synthetic English-Korean CS question-answering dataset.\nWe provide comprehensive analysis on a variety of multilingual LLMs by\nsubdividing activation process into knowledge identification and knowledge\nleveraging. Our results demonstrate that compared to English text, CS can\nfaithfully activate knowledge inside LLMs especially on language-specific\ndomains, suggesting the potential of code-switching on low-resource language\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) demonstrate multilingual abilities, yet\nthey are English-centric due to dominance of English in training corpora. The\nlimited resource for low-resource languages remains a crucial challenge.\nCode-switching (CS), a phenomenon where multilingual speakers alternate between\nlanguages in a discourse, can convey subtle cultural and linguistic nuances\nthat can be otherwise lost in translation and elicits language-specific\nknowledge in human communications. In light of this, we investigate whether\ncode-switching can 'activate', or identify and leverage knowledge for reasoning\nwhen LLMs solve low-resource language tasks. To facilitate the research, we\nfirst present EnKoQA, a synthetic English-Korean CS question-answering dataset.\nWe provide comprehensive analysis on a variety of multilingual LLMs by\nsubdividing activation process into knowledge identification and knowledge\nleveraging. Our results demonstrate that compared to English text, CS can\nfaithfully activate knowledge inside LLMs especially on language-specific\ndomains, suggesting the potential of code-switching on low-resource language\ntasks."
                },
                "authors": [
                    {
                        "name": "Seoyeon Kim"
                    },
                    {
                        "name": "Huiseo Kim"
                    },
                    {
                        "name": "Chanjun Park"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "arxiv_comment": "25 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18436v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18436v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22303v1",
                "updated": "2025-05-28T12:40:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    12,
                    40,
                    37,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T12:40:37Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    12,
                    40,
                    37,
                    2,
                    148,
                    0
                ],
                "title": "Voice CMS: updating the knowledge base of a digital assistant through\n  conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Voice CMS: updating the knowledge base of a digital assistant through\n  conversation"
                },
                "summary": "In this study, we propose a solution based on a multi-agent LLM architecture\nand a voice user interface (VUI) designed to update the knowledge base of a\ndigital assistant. Its usability is evaluated in comparison to a more\ntraditional graphical content management system (CMS), with a focus on\nunderstanding the relationship between user preferences and the complexity of\nthe information being provided. The findings demonstrate that, while the\noverall usability of the VUI is rated lower than the graphical interface, it is\nalready preferred by users for less complex tasks. Furthermore, the quality of\ncontent entered through the VUI is comparable to that achieved with the\ngraphical interface, even for highly complex tasks. Obtained qualitative\nresults suggest that a hybrid interface combining the strengths of both\napproaches could address the key challenges identified during the experiment,\nsuch as reducing cognitive load through graphical feedback while maintaining\nthe intuitive nature of voice-based interactions. This work highlights the\npotential of conversational interfaces as a viable and effective method for\nknowledge management in specific business contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we propose a solution based on a multi-agent LLM architecture\nand a voice user interface (VUI) designed to update the knowledge base of a\ndigital assistant. Its usability is evaluated in comparison to a more\ntraditional graphical content management system (CMS), with a focus on\nunderstanding the relationship between user preferences and the complexity of\nthe information being provided. The findings demonstrate that, while the\noverall usability of the VUI is rated lower than the graphical interface, it is\nalready preferred by users for less complex tasks. Furthermore, the quality of\ncontent entered through the VUI is comparable to that achieved with the\ngraphical interface, even for highly complex tasks. Obtained qualitative\nresults suggest that a hybrid interface combining the strengths of both\napproaches could address the key challenges identified during the experiment,\nsuch as reducing cognitive load through graphical feedback while maintaining\nthe intuitive nature of voice-based interactions. This work highlights the\npotential of conversational interfaces as a viable and effective method for\nknowledge management in specific business contexts."
                },
                "authors": [
                    {
                        "name": "Grzegorz Wolny"
                    },
                    {
                        "name": "Michał Szczerbak"
                    }
                ],
                "author_detail": {
                    "name": "Michał Szczerbak"
                },
                "author": "Michał Szczerbak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03810v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03810v3",
                "updated": "2025-05-29T03:19:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    3,
                    19,
                    51,
                    3,
                    149,
                    0
                ],
                "published": "2024-10-04T13:31:24Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    13,
                    31,
                    24,
                    4,
                    278,
                    0
                ],
                "title": "Exploring the Limitations of Mamba in COPY and CoT Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Limitations of Mamba in COPY and CoT Reasoning"
                },
                "summary": "Transformers have become the backbone of modern Large Language Models (LLMs);\nhowever, their inference overhead grows linearly with the sequence length,\nposing challenges for modeling long sequences. In light of this, Mamba has\nattracted attention for maintaining a constant inference size, with empirical\nevidence demonstrating that it can match Transformer performance in sequence\nmodeling while significantly reducing computational costs. However, an open\nquestion remains: can Mamba always bring savings while achieving performance\ncomparable to Transformers? In this paper, we focus on analyzing the expressive\nability of Mamba to perform our defined COPY operation and Chain of Thought\n(CoT) reasoning. First, inspired by the connection between Mamba and linear\nattention, we show that constant-sized Mamba may struggle to perform COPY\noperations while Transformers can handle them more easily. However, when the\nsize of Mamba grows linearly with the input sequence length, it can accurately\nperform COPY, but in this case, Mamba no longer provides overhead savings.\nBased on this observation, we further analyze Mamba's ability to tackle CoT\ntasks, which can be described by the Dynamic Programming (DP) problems. Our\nfindings suggest that to solve arbitrary DP problems, the total cost of Mamba\nis still comparable to standard Transformers. However, similar to efficient\nTransformers, when facing DP problems with favorable properties such as\nlocality, Mamba can provide savings in overhead. Our experiments on the copy\nand CoT tasks further demonstrate Mamba's limitations compared to Transformers\nin learning these tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the backbone of modern Large Language Models (LLMs);\nhowever, their inference overhead grows linearly with the sequence length,\nposing challenges for modeling long sequences. In light of this, Mamba has\nattracted attention for maintaining a constant inference size, with empirical\nevidence demonstrating that it can match Transformer performance in sequence\nmodeling while significantly reducing computational costs. However, an open\nquestion remains: can Mamba always bring savings while achieving performance\ncomparable to Transformers? In this paper, we focus on analyzing the expressive\nability of Mamba to perform our defined COPY operation and Chain of Thought\n(CoT) reasoning. First, inspired by the connection between Mamba and linear\nattention, we show that constant-sized Mamba may struggle to perform COPY\noperations while Transformers can handle them more easily. However, when the\nsize of Mamba grows linearly with the input sequence length, it can accurately\nperform COPY, but in this case, Mamba no longer provides overhead savings.\nBased on this observation, we further analyze Mamba's ability to tackle CoT\ntasks, which can be described by the Dynamic Programming (DP) problems. Our\nfindings suggest that to solve arbitrary DP problems, the total cost of Mamba\nis still comparable to standard Transformers. However, similar to efficient\nTransformers, when facing DP problems with favorable properties such as\nlocality, Mamba can provide savings in overhead. Our experiments on the copy\nand CoT tasks further demonstrate Mamba's limitations compared to Transformers\nin learning these tasks."
                },
                "authors": [
                    {
                        "name": "Ruifeng Ren"
                    },
                    {
                        "name": "Zhicong Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "Mamba, Chain of Thought",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03810v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03810v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01394v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01394v5",
                "updated": "2025-05-28T12:38:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    12,
                    38,
                    37,
                    2,
                    148,
                    0
                ],
                "published": "2024-06-03T14:57:39Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    14,
                    57,
                    39,
                    0,
                    155,
                    0
                ],
                "title": "PrivacyRestore: Privacy-Preserving Inference in Large Language Models\n  via Privacy Removal and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrivacyRestore: Privacy-Preserving Inference in Large Language Models\n  via Privacy Removal and Restoration"
                },
                "summary": "The widespread usage of online Large Language Models (LLMs) inference\nservices has raised significant privacy concerns about the potential exposure\nof private information in user inputs to malicious eavesdroppers. Existing\nprivacy protection methods for LLMs suffer from either insufficient privacy\nprotection, performance degradation, or large inference time overhead. To\naddress these limitations, we propose PrivacyRestore, a plug-and-play method to\nprotect the privacy of user inputs during LLM inference. The server first\ntrains restoration vectors for each privacy span and then release to clients.\nPrivacy span is defined as a contiguous sequence of tokens within a text that\ncontain private information. The client then aggregate restoration vectors of\nall privacy spans in the input into a single meta restoration vector which is\nlater sent to the server side along with the input without privacy spans.The\nprivate information is restored via activation steering during inference.\nFurthermore, we prove that PrivacyRestore inherently prevents the linear growth\nof the privacy budget.We create three datasets, covering medical and legal\ndomains, to evaluate the effectiveness of privacy preserving methods. The\nexperimental results show that PrivacyRestore effectively protects private\ninformation and maintain acceptable levels of performance and inference\noverhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread usage of online Large Language Models (LLMs) inference\nservices has raised significant privacy concerns about the potential exposure\nof private information in user inputs to malicious eavesdroppers. Existing\nprivacy protection methods for LLMs suffer from either insufficient privacy\nprotection, performance degradation, or large inference time overhead. To\naddress these limitations, we propose PrivacyRestore, a plug-and-play method to\nprotect the privacy of user inputs during LLM inference. The server first\ntrains restoration vectors for each privacy span and then release to clients.\nPrivacy span is defined as a contiguous sequence of tokens within a text that\ncontain private information. The client then aggregate restoration vectors of\nall privacy spans in the input into a single meta restoration vector which is\nlater sent to the server side along with the input without privacy spans.The\nprivate information is restored via activation steering during inference.\nFurthermore, we prove that PrivacyRestore inherently prevents the linear growth\nof the privacy budget.We create three datasets, covering medical and legal\ndomains, to evaluate the effectiveness of privacy preserving methods. The\nexperimental results show that PrivacyRestore effectively protects private\ninformation and maintain acceptable levels of performance and inference\noverhead."
                },
                "authors": [
                    {
                        "name": "Ziqian Zeng"
                    },
                    {
                        "name": "Jianwei Wang"
                    },
                    {
                        "name": "Junyao Yang"
                    },
                    {
                        "name": "Zhengdong Lu"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Huiping Zhuang"
                    },
                    {
                        "name": "Cen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Cen Chen"
                },
                "author": "Cen Chen",
                "arxiv_comment": "Accepted by ACL2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01394v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01394v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22298v1",
                "updated": "2025-05-28T12:37:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    12,
                    37,
                    6,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T12:37:06Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    12,
                    37,
                    6,
                    2,
                    148,
                    0
                ],
                "title": "Adaptive Detoxification: Safeguarding General Capabilities of LLMs\n  through Toxicity-Aware Knowledge Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Detoxification: Safeguarding General Capabilities of LLMs\n  through Toxicity-Aware Knowledge Editing"
                },
                "summary": "Large language models (LLMs) exhibit impressive language capabilities but\nremain vulnerable to malicious prompts and jailbreaking attacks. Existing\nknowledge editing methods for LLM detoxification face two major challenges.\nFirst, they often rely on entity-specific localization, making them ineffective\nagainst adversarial inputs without explicit entities. Second, these methods\nsuffer from over-editing, where detoxified models reject legitimate queries,\ncompromising overall performance. In this paper, we propose ToxEdit, a\ntoxicity-aware knowledge editing approach that dynamically detects toxic\nactivation patterns during forward propagation. It then routes computations\nthrough adaptive inter-layer pathways to mitigate toxicity effectively. This\ndesign ensures precise toxicity mitigation while preserving LLMs' general\ncapabilities. To more accurately assess over-editing, we also enhance the\nSafeEdit benchmark by incorporating instruction-following evaluation tasks.\nExperimental results on multiple LLMs demonstrate that our ToxEdit outperforms\nprevious state-of-the-art methods in both detoxification performance and\nsafeguarding general capabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit impressive language capabilities but\nremain vulnerable to malicious prompts and jailbreaking attacks. Existing\nknowledge editing methods for LLM detoxification face two major challenges.\nFirst, they often rely on entity-specific localization, making them ineffective\nagainst adversarial inputs without explicit entities. Second, these methods\nsuffer from over-editing, where detoxified models reject legitimate queries,\ncompromising overall performance. In this paper, we propose ToxEdit, a\ntoxicity-aware knowledge editing approach that dynamically detects toxic\nactivation patterns during forward propagation. It then routes computations\nthrough adaptive inter-layer pathways to mitigate toxicity effectively. This\ndesign ensures precise toxicity mitigation while preserving LLMs' general\ncapabilities. To more accurately assess over-editing, we also enhance the\nSafeEdit benchmark by incorporating instruction-following evaluation tasks.\nExperimental results on multiple LLMs demonstrate that our ToxEdit outperforms\nprevious state-of-the-art methods in both detoxification performance and\nsafeguarding general capabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Yigeng Zhou"
                    },
                    {
                        "name": "Yihui Zhang"
                    },
                    {
                        "name": "Wenya Wang"
                    },
                    {
                        "name": "Xiucheng Li"
                    },
                    {
                        "name": "Meishan Zhang"
                    },
                    {
                        "name": "Fangming Liu"
                    },
                    {
                        "name": "Jun Yu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]